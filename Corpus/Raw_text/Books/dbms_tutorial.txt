DBMS
 
9
 
 
 
Database 
S
chema
 
A 
d
atabase schema 
is the 
skeleton structure 
that 
represents the logical view of 
the 
entire database. It 
 
defines how 
the data is organized and how 
the 
relation
s
 
among them 
are 
associated. It formulates all 
the 
constraints that 
are to be 
applied on the data. 
 
A database schema def
ines its entities and the relationship among them. 
It 
contains 
a descriptive detail of the database, which can be depicted by means of 
schema diagrams. 

database designer
s
 
who design the schema 
to
 
help 
programmers 
understand the database and make it useful. 
 
[
Image: Database Schemas
]
 
 
 
A 
d
ata
base schema can be divided broadly in
to
 
two categories:
 
4. 
DATA SCHEMAS
 

DBMS
 
10
 
 
 

 
Physical Database Schema
: This schema pertains to the actual storage 
of data and its form of storage like files, indices
,
 
etc. It defines 
how 
the 
data will be stored in 
a 
secondary storage
.
 

 
L
ogical Database Schema
: This 
schema 
defines all 
the 
logical 
constraints that need to be applied on 
the 
data stored. It defines tables, 
views
,
 
and integrity constraints
.
 
Database Instance
 
It is important that we distinguish these two terms individually
. Database 
schema is the skeleton of database. It is designed when 
the 
database doesn't 
exist at all
.
 
O
nce the database is operational, it is 
very 
difficult 
to 
make 
any 
changes 
to it.
 
A 
d
atabase schema does no
t contain any data or information.
 
A 
d
atabase instance
 
is a state of operational database with data at any given 
time. 
It contains 
a snapshot of 
the 
database. Database instances tend to change 
with time. 
A 
DBMS ensures that its every instance (s
tate) 
is in 
a valid state
,
 
by 
diligently following 
all 
the 
validation
s
, constraints
,
 
and condition
s
 
that 
the 
database designers 
have 
imposed
.
 
 

DBMS
 
11
 
 
 
If 
a 
database system is not
 
multi
-
layered
,
 
then it 
becomes difficult
 
to make any 
changes in the database system. Database systems are designed in multi
-
layers 
as we 
learnt 
earlier.
 
Data Independence
 
A 
database system normally 
contains a 
lot of data 
in addition to 
user
s

 
data. 
For 
example, it stores data about data, known as metadata, 
to locate and retrieve 
data easily. 
It is rather difficult to modify 
or update 
a set of 
m
eta
data 
once it 
is 
stored 
in the
 
database
. 
But as 
a 
DBMS expands, it needs to 
change
 
over 
time 
to 
satisfy the requirements of 
the 
u
sers. 
I
f the 
entire data is
 
dependent
,
 
it would 
become 
a 
tedious and highly complex
 
job
.
 
 
[
Image: Data independence
]
 
Meta
data 
itself 
follows a 
 
layered architecture
,
 
so that when we change data at 
one layer
,
 
it does not affect the data 
at another level. 
This data is independent 
but mapped 
to 
each other.
 
Logical Data Independence
 
Logica
l data is data about database, that is, it stores information about how data 
is managed inside. For example, a table (relation) stored in the database and all 
its 
constraints
 
applied on that relation.
 
5. 
DATA INDEPENDENCE
 

DBMS
 
12
 
 
 
Logical data independence is a kind of mecha
nism, which liberalizes itself from 
actual data stored on the disk. If we do some changes on table format
,
 
it should 
not change the data residing on 
the 
disk.
 
Physical Data Independence
 
All 
the 
schemas are logical
,
 
and 
the 
actual data is stored in bit form
at on the 
disk. Physical data independence is the power to change the physical data 
without impacting the schema or logical data.
 
For example, in case we want to change or upgrade the storage system itself
 

 
suppose we want to replace hard
-
disks with 
SSD 

 
it
 
should not have any 
impact on 
the 
logical data or schemas.
 
 

DBMS
 
13
 
 
 
The ER 
model defines the conceptual view of 
a 
database. It works around real
-
world entit
ies
 
and 
the 
association
s
 
among them. At view level, 
the 
ER model is 
considered 
a good option 
for designing databases.
 
Entity
 
A
n entity can be a
 
real
-
world 
object, 
either animate or inanimate
,
 
that can be 
easily identifiable
.
 
For exa
mple, in a school database, student
s
, teachers, 
class
es,
 
and course
s
 
offered can be considered as entities. All 
these 
entities 
have some attributes or properties that give them their identity.
 
An entity set is a collection of similar types of entities. 
An 
e
ntity set may contain 
entities with attribute sharing similar values. For example, 
a 
Students set may 
contain all the student
s
 
of a school; likewise 
a 
Teachers set may contain all the 
teachers of 
a 
school from all faculties. Entit
y
 
sets need not 
be
 
disjoint.
 
Attributes
 
Entities are represented by means of their properties
 
called 
attributes
. All 
attributes have values. For example, a student entity may have name, class, 
and 
age as attributes.
 
There exist
s
 
a domain or range of values that can be assi
gned to attributes. For 
example, a student's name cannot be a numeric value. It has to be alphabetic. A 
student's age cannot be negative, etc.
 
Types of 
A
ttributes
 

 
Simple attribute
: 
Simple attributes are atomic values, which cannot be 
divided further. F
or example, 
a 
student's phone
 
number is an atomic 
value of 10 digits.
 

 
Composite attribute
: 
Composite attributes are made of more than one 
simple attribute. For example, a student's complete name may have 
first_name and last_name.
 

 
Derived attribute
: 
De
rived attributes are 
the 
attributes
 
that
 
do not exist 
in the 
physical 
database, but the
i
r
 
values are derived from other 
attributes present
 
in the database. For example, average_salary in a 
department should 
not 
be saved 
directly 
in 
the 
dat
abase
,
 
instead it can be 
derived. For another example, age can be derived from data_of_birth.
 
6. 
ER MODEL 

 
BASIC CON
CEPTS
 

DBMS
 
14
 
 
 

 
Single
-
value
 
attribute
: 
Single
-
value
 
attributes contain 
single value. For 
example: Social_Security_Number.
 

 
Multi
-
value attribute
: 
Multi
-
value attribute
s
 
may contain more than one 
values. For example, a person can have more than one phone number
, 
email_address
,
 
etc.
 
These attribute types can come together in a way like:
 

 
simple single
-
valued attributes
 

 
simple multi
-
valued attributes
 

 
composite single
-
valu
ed attributes
 

 
composite multi
-
valued attributes
 
Entity
-
S
et and Keys
 
Key is an attribute or collection of attributes that uniquely identifies an entity 
among entity set.
 
For example, 
the 
roll_number of a student makes 
him
/her
 
identifiable among 
student
s.
 

 
Super Key
: 
A 
s
et of attributes (one or more) that collectively identifies 
an entity in an entity set.
 

 
Candidate Key
: 
A 
m
inimal super key is called 
a 
candidate key
.
 
An entity 
set may hav
e more than one candidate key.
 

 
Primary Key
: 
A primary key 
is one of the candidate key
s
 
chosen by the 
database designer to uniquely identify the entity set.
 
Relationship
 
The association among entities is called 
a 
relationship. For example, 
an 
employee
 
works_at
 
a 
department
, a student 
enrolls
 
in a course
. 
Here, 
Works_at and Enrolls are called relationship
s
.
 
Relationship Set
 
A set of 
r
elationship
s
 
of similar type is cal
led 
a 
relationship set. Like entities, a 
relationship too can have attributes. These attributes are called 
descriptive 
attributes
.
 
Degree of 
R
elationship
 
The number of participating entities in a
 
relationship defines the degree of the 
relationship.
 

DBMS
 
15
 
 
 

 
Binar
y = degree 2
 

 
Ternary = degree 3
 

 
n
-
ary = degree
 
Mapping Cardinalities
 
Cardinality
 
defines the number of entities in one entity set
,
 
which can be 
associated 
with 
the number of entities of other set via relationship set.
 

 
One
-
to
-
one
: 
O
ne entity from enti
ty set A can be associated with at most 
one entity of entity set B and vice versa.
 
 
[Image: One
-
to
-
one relation]
 

 
One
-
to
-
many
: One entity from entity set A can be
 
associated with more 
than one entities of entity set B
, however an entity
 
from entity set B 
can 
be associated with at most one entity.
 
 
[Image: O
ne
-
to
-
many relation]
 

 
Many
-
to
-
one
: More than one entities from entity set A can be associated 
with at most one entity of entity set B
,
 
however an 
entity from entity set 
B can be associated with more than one entity from entity set A.
 

DBMS
 
16
 
 
 
 
[Image: Many
-
to
-
one relation]
 

 
Many
-
to
-
many
: 
O
ne entity from A can be associated with more than one 
entity from B and vice versa.
 
 
[Image: Many
-
to
-
many relation]
 

DBMS
 
17
 
 
 
Let us now 
learn how 
the 
ER Model is represented by means of 
an 
ER diagram. 
Any 
object
, for example,
 
entities,
 
attributes of an entity, relationship set
s
, and 
attributes of relationship set
s,
 
can be represented 
with the help of an 
ER 
diagram.
 
Entity
 
Entities are represented by means of rectangles. Rectangles are named with the 
entity set they represent
.
 
 
[
Image: Entities in a school database
]
 
Attributes
 
Attributes are 
the 
properties of entities. Attributes are represented by means of 
ellipses. Every ellipse represents one
 
attribute and is directly connected to its 
entity (rectangle).
 
 
[
Image: Simple Attributes
]
 
If the attributes are
 
composite
, they are further divided in a tree like str
ucture. 
Every node is then connected to its attribute. That is
,
 
composite attributes are 
represented by ellipses that are connected with an ellipse.
 
7. 
ER DIAGRAM REPRESENT
ATION
 

DBMS
 
18
 
 
 
 
[
Image: 
Composite Attributes
]
 
Multivalued
 
attributes are depicted by double ellipse.
 
 
[
Image: Multivalued Attributes
]
 
Derived
 
attributes are depicted by dashed elli
pse.
 

DBMS
 
19
 
 
 
 
[
Image: Derived Attributes
]
 
Relationship
 
Relationships are represented by diamond
-
shaped box. Name of the relationship 
is written in
side
 
the diamond
-
box.
 
All 
the 
entities (rectangles)
 
participating in 
a 
relationship
 
are connected to it by a line.
 
Binary 
R
elationship and 
C
ardinality
 
A relationship where two entities are participating
 
is called a
 
binary
 
relationship
. Cardinality is the number of instan
ce of an entity from a relation 
that can be associated with the relation.
 

 
One
-
to
-
one
: 
When only one instance of 
an 
entity is associated with the 
relationship, it is marked as '1
:1
'. 
The following 
image 
reflects that only 
one 
instance of each 
entity should be associated with the relationship. It 
depicts one
-
to
-
one relationship
.
 
[Image: One
-
to
-
one]
 

DBMS
 
20
 
 
 

 
One
-
to
-
many
: 
When more than one instance of 
an 
enti
ty is associated 
with 
a
 
relationship, it is marked as '
1:
N'. 
The following
 
image 
reflects that 
only 
one
 
instance of entity on the left and more than one instance of 
an 
entity on the right can be associated with the relationship. It depicts on
e
-
to
-
many relationship
.
 
[Image: One
-
to
-
many]
 

 
Many
-
to
-
one
: 
When more than one instance of entity is associated with 
the relationship, it is marked as 'N
:1
'. 
The following 
image 
reflects that 
more than one instance of 
an 
entity on the left and only one instance of 
an 
entity on the right can be associated with the relationship. It depicts 
many
-
to
-
one relationship
.
 
[Image: Many
-
to
-
one]
 

 
Many
-
to
-
many
: 
The following 
image 
reflects that more than one instance 
of 
an 
entity on the left and more than one instance of 
an 
entity on the 
right can be associa
ted with the relationship. It depicts many
-
to
-
many 
relationship
.
 
[
Image: Many
-
to
-
many
]
 

DBMS
 
21
 
 
 
Participation Constraints
 

 
Total Participation
: Each entity 
is involved in the relationship. Total 
participation is represented by double lines.
 

 
Partial participation
: Not all entities are involved in the relation
ship. 
Partial participation is represented by single line
s
.
 
 
[Image: Participation Constraints]
 

DBMS
 
22
 
 
 
The 
ER Model has the power of expressing database entities in 
a 
conceptual 
hierarchical manner
.
 
As 
the hierarch
y
 
goes up
,
 
it generalize
s
 
the view of entities
,
 
and as we go deep in the hierarchy
,
 
it gives us 
the 
detail of every entity 
included.
 
Going up in this structure is called 
generalization
, where entities are clubbed 
together to represent a 
more generalized view. For example, a particular student 
named
 
Mira can be generalized along with all the students
.
 
T
he entity shall be 
a 
student, and further
,
 
the
 
student is 
a 
person. The reverse is called 
specialization
 
where a person is 
a 
student, a
nd that student is Mira.
 
Generalization
 
As mentioned above, the process of generalizing entities, where the generalized 
entities contain the properties of all the generalized entities
,
 
is called 
g
eneralization. In generalization, a number of entities are 
brought together into 
one generalized entity based on their similar characteristics. For 
example, 
pigeon, house sparrow, crow
,
 
and dove 
can 
all 
be generalized as Birds.
 
 
[
Image: Generalization
]
 
Specialization
 
Specialization 
is 
the 
opposite 
of 
generalization
.
 
In specialization, a group of 
entities is divided into sub
-
groups based on their characteristics. Take a group 

P
erson

 
for example. A person has name, date of birth, gender
,
 
etc. These 
properties are common in all persons, human beings. But in a company, 
person
s
 
can be identified as employee, employer, customer
,
 
or vendor
,
 
based on what 
role 
they play in 
the 
co
mpany.
 
 
8. 
GENERALIZATION 
& 
SPECIALIZATION
 

DBMS
 
23
 
 
 
 
[
Image: Specialization
]
 
Similarly, in a school database, 
person
s
 
can be specialized as teacher, student
,
 
or 
a 
staff
,
 
based on what role 
they play in scho
ol as entities.
 
Inheritance
 
We use all 
the 
above features of ER
-
Model
 
in order to create classes of objects in 
object
-
oriented programming. 
The 
d
etails of entitie
s are generally hidden from 
the user
;
 
this process known as 
abstraction
.
 
Inheritance is an 
important feature
 
of Generalization and Specialization
. It allows 
lower
-
level entities to inherit 
the attributes of higher
-
le
vel entities
.
 
 

DBMS
 
24
 
 
 
 
[
Image: Inheritance
]
 
For example, 
the 
attributes of a 
P
erson 
class such as 
name, age, and gender can 
be in
herited by lower
-
level entities 
such as 
S
tudent 
or 
T
eacher
.
 

DBMS
 
25
 
 
 
Dr Edgar F. Codd
, after his
 
extensive research 
on the 
Relational Model of 
database systems
,
 
came up with twelve rules of his own
,
 
which according
 
to 
him, a database must obey in order to be 
regarded as 
a true relational database.
 
These rules can be applied on a
ny
 
database system that 
manages 
stored data 
using only its relational capabilities. This is a foundation rule, whi
ch 
acts as a 
base for all the other rules. 
 
Rule 1: Information 
R
ule
 
The data stored in a database, may it be user data or 
metadata
,
 
must be a value 
of some table cell. 
Everything in a database must be stored in 
a 
table format
. 
 
Rule 2: Guaranteed Access 
R
ule
 
E
very single data ele
ment (value) is guaranteed to be accessible logically with 
a 
combination of table
-
name, primary
-
key (row value)
,
 
and attribute
-
name 
(column value). No other means, such as pointers, can be used to access data.
 
Rule 3: Systematic Treatment of NULL 
V
alues
 
T
he NULL values in 
a
 
database must be given a systematic 
and uniform 
treatment. 
This is a very important rule because a 
NULL can be interpreted as 
one the following: data is missing, data i
s not known, 
or 
data is not applicable
.
 
Rule 4: Active 
O
nline 
C
atalog
 
T
he structure description of 
the entire 
database must be stored in an online 
catalog, 
known as
 
data dictionary
, which can be accessed by 
author
ized users. 
Users can use the same query language to access the catalog which they use to 
access the database itself.
 
Rule 5: Comprehensive 
D
ata 
S
ub
-
L
anguage 
R
ule
 
A 
database 
can only be accessed 
using
 
a language having linear sy
ntax 
that 
supports 
data definition, data manipulation
,
 
and transaction management 
operations. 
T
his language 
can be used
 
d
irectly or by means of some application. 
If the database 
allows access to data 
without any help of this language, 
then 
it is 
considered as 
a violation.
 
9. 

 

DBMS
 
26
 
 
 
Rule 6: View 
U
pdating 
R
ule
 
A
ll 
the 
views of 
a 
database, which can theoretically be updated, must also be 
updatable by the system.
 
Rule 7: High
-
L
evel 
I
nsert, 
U
pdate
,
 
and 
D
elete 
R
ule
 
A 
database must 
support high
-
level insertion, updation
,
 
and deletion
. This must 
not be limited to a single row
,
 
that is, it must also support union, intersection 
and minus operations to yield sets of data records.
 
Rule 8: Physical 
D
ata 
I
ndependence
 
The data stored in a database must be independent of the applications tha
t 
access the database. 
A
ny change in 
the 
physical structure 
of a database 
must 
not have any impact on 
how the data is being accessed by ex
ternal 
application
s
.
 
Rule 9: Logical 
D
ata 
I
ndependence
 
The 
logical data 
in a database 

(application). Any change in logical data must not 
affect 
the application
s
 
using it
. 
For example, if two tables are merged or one is split into two different tables, 
there should be no impact 
or
 
change on 
the 
user application. This is one of the 
most difficult rule to apply.
 
Rule 10: Integrity 
I
ndependence
 
A
 
database must be independent of the application 
that uses 
it. All its integrity 
constraints can be independently modified without the need of any change in the 
application. This rule makes 
a 
database independent of the front
-
end application 
and its 
interface.
 
Rule 11: Distribution 
I
ndependence
 
The 
end
-
user must not be able to see that the data is distributed over various 
locations. User
s should always get the impression that the
 
data is located at one 
si
te only. This rule has been 
regarded
 
as 
the 
foundation of distributed database 
systems.
 
Rule 12: Non
-
S
ubversion 
R
ule
 
I
f a system has an interface that provides access to low
-
level records, 
then the 
interface 
must
 
not be able to subvert the system and bypass security and 
integrity constraints.
 

DBMS
 
27
 
 
 
Relational data model is the primary data model, which is used widely around 
the world for data storage and processing. This model is simple and 
it 
has 
all the 
properties and capabilities required to process data with storage efficiency.
 
Concepts
 
Tables
: In relation
al
 
data model, relations are saved in the format of Tables. 
This format stores the relation among entities. A table has rows and col
umns, 
where rows represent records and columns represent
 
the attributes.
 
Tuple
: A single row of a table, which contains a single record for that relation is 
called a tuple.
 
Relation instance
: A finite set of tuples in the relational database system 
repr
esents relation instance. Relation instances do not have duplicate tuples.
 
Relation schema
: 
A relation schema 
describes the relation name (table name), 
attributes
,
 
and their names.
 
Relation key
: Each row has one or more attributes
, known as relation
 
key,
 
which can identify the row in the relation (table) uniquely
.
 
Attribute domain
: Every attribute has some pre
defined value scope, known as 
attribute domain.
 
Constraints
 
Every relation has some conditions that must hold for
 
it to be a valid relation. 
These conditions are called 
Relational Integrity Constraints
. There are three 
main integrity constraints
:
 

 
Key 
c
onstraints
 

 
Domain constraints
 

 
Referential integrity constraints
 
Key Constraints
 
There must be at least one minimal
 
subset of attributes in the relation, which 
can identify a tuple uniquely. This minimal subset of attributes is called
 
key
 
for 
that relation. If there are more than one such minimal subsets, these are 
called
 
candidate keys
.
 
Key constraints force
 
that:
 
10. 
RELATION
AL
 
DATA MODEL
 

DBMS
 
28
 
 
 

 
in
 
a relation with a key attribute, no two tuples can have identical value
s
 
for key attributes.
 

 
a 
key attribute can
not have NULL values.
 
Key constrain
t
s are also referred to as 
Entity Constraints
.
 
Domain 
C
onstraints
 
Attributes have specific values in real
-
world scenario. For example, age can only 
be 
a 
positive integer. The same constraints ha
ve
 
been tried to employ on the 
attributes of a relation. Every attribute is bound to have a specific range of 
values. For example, age 
cannot
 
be less than zero 
and telephone number
s
 
cannot
 
contain a digit 
outside 0
-
9.
 
Referential 
I
ntegrity 
C
onstraints
 
Referential 
integrity constraints work
 
on the concept of Foreign Key
s
. A 
foreign 
key 
is a key 
attribute of a relation 
that 
can be referred in ot
her relation
. 
 
Referential integrity constraint states that if a relation refers to a
 
key attribute of 
a different or same relation, 
then 
that key element must exist
.
 
 
 
 
 
 
 
 
 
 
 

DBMS
 
29
 
 
 
Relational database s
ystems are expected to be equipped 
with 
a query language 
that can assist its user
s
 
to query the database instances. 
There are two kinds of 
query languages
:
 
relational algebra a
nd relational calculus.
 
Relational 
A
lgebra
 
Relational algebra is a procedural query language, which takes instances of 
relations as input and yields instances of relations as output. It uses operators to 
perform queries. An operator can be either 
unary
 
or
 
binary
. They accept 
relations as their input and yield
 
relations as their output. Relational algebra is 
performed recursively on a relation and intermediate results are also considered 
relations.
 
The 
f
undamental operations of 
r
elational algebra
 
are
 
as 
follows
:
 

 
Select
 

 
Project
 

 
Union
 

 
Set different
 

 
Cartesian product
 

 
Rename
 
We will discuss all these operations in the following sections. 
 

 
It 
s
elects tuples that satisfy the given predicate from a relat
ion.
 
Notation
:
 

p
(r)
 
Where
 

 
stands for selection predicate and 
r
 
stands for relation.
 
p
 
is 
prepositional logic formula
 
which may use connectors like 
and
, 
or
,
 
and 
not
. 
These terms may use relational operators like: =,
 

 

 
<
,
 
>, 

 
For example
:
 

sub
ject
=
"database"
(
Books
)
 
 
Output
: Selects tuples from books where subject is 'database'.
 

subject
=
"database"
 
and
 
price
=
"450"
(
Books
)
 
 
11. RELATIONAL ALGEB
RA
 

DBMS
 
30
 
 
 
Output
: Selects tuples from books where subject is 'database' and 'price' is 450.
 

subject
=
"database"
 
and
 
price 
<
 
"450"
 
or
 
year 
>
 
"2010"
(
Books
)
 
 
Output
: Selects tuples from books where subject is 'database' and 'price' is 450 
or 
those books 
published after 2010.
 

 
It 
p
rojects column(s) that satisfy 
a 
give
n predicate.
 
Notation

A
1
, A
2
, A
n
 
(r)
 
Where 
A
1
, 
A
2
, 
A
n
 
are attribute names of relation 
r
.
 
Duplicate rows are automatically eliminated, as relation is a set.
 
F
or example:
 
 

subject
,
 
author 
(
Books
)
 
 
Selects and projects columns named as subject and aut
hor from 
the 
relation 
Books.
 
Union Operation (

)
 
It 
performs binary union between two given relations and is defined as:
 
 
r 

 
s 
=
 
{
 
t 
|
 
t 

 
r 
or
 
t 

 
s
}
 
 
Notion
: r U s
 
Where 
r
 
and 
s
 
are either database relations or relation result set (tempo
rary 
relation).
 
For a union operation to be valid, the following conditions must hold:
 

 
r
 
and 
s
 
must have 
the 
same number of attributes.
 

 
Attribute domains must be compatible.
 

 
Duplicate tuples are automatically eliminated.
 
 

 
author
 
(
Books
)
 

 

 
author
 
(
Arti
cles
)
 
 
Output
: Projects the name
s
 
of 
the 
author
s
 
who 
have 
either written a book or an 
article or both.
 

DBMS
 
31
 
 
 
Set Difference (

)
 
The result of set difference operation is tuples
,
 
which 
are 
present in one relation 
but are not in the second relation.
 
Notatio
n
: 
r
 

s
 
Finds all 
the 
tuples that are present in 
r
 
but not
 
in
 
s
.
 
 

author
(
Books
)
 

 

author
(
Articles
)
 
 
Output
: 
Provides 
the name of authors who 
have 
written books but not articles.
 

 
Combines information of two different
 
relations into one.
 
Notation

 
Where 
r
 
and 
s
 
are relations and 
their 
output will be defined as:
 


 
r and t 

 
s}
 
 

author 
=
 
'tutorialspoint'
(
Books
 

 
Articles
)
 
 
Output
: 
Y
ields a relation
,
 
which shows all 
the 
books a
nd articles written by 
tutorialspoint.
 
Rename 
O
peration (

)
 
The 
r
esults of relational algebra are also relations but without any name. The 
rename operation allows us to rename the output relation. 

rename

 
operation is 
denoted with small 
G
reek letter 
rho
 

.
 
Notation
:
 

 
x
 
(E)
 
Where the result of expression 
E
 
is saved with name of 
x
.
 
Additional operations are:
 

 
Set intersection
 

 
Assignment
 

 
Natural join
 
Relational Calculus
 
In contrast 
to 
Relational Algebra, Relational Calculus is 
a 
non
-
procedural query
 
language, that is, it tells what to do but never explains 
how to do it.
 
Relational calculus exists in two forms:
 

DBMS
 
32
 
 
 
Tuple 
R
elational 
C
alculus (TRC)
 
Filtering variable ranges over tuples
 
Notation
: {
T | Condition
}
 
Returns all tuples T that satisfi
es 
a 
condition.
 
For 
e
xample:
 
 
{
 
T
.
name 
|
  
Author
(
T
)
 
AND T
.
article 
=
 
'database'
 
}
 
Output
: 
R
eturns tuples with 'name' from Author who has written article on 
'database'.
 
TRC can be quantified
. We can use Existential (

)
 
and Universal Quantifiers (

).
 
For example:
 
{
 
R
|
 

T 
 
 

 
Authors
(
T
.
article
=
'database'
 
AND R
.
name
=
T
.
name
)}
 
Output
: 
T
he 
above 
query will yield the same result as the previous one.
 
Domain 
R
elational 
C
alculus (DRC)
 
In DRC
,
 
the filtering variable uses 
the 
domain of attributes instead of entire 
tuple values (as done in TRC, mentioned above).
 
Notation
:
 
{ a
1
, a
2
, a
3
, ..., a
n
 
| P (a
1
, a
2
, a
3
, ... ,a
n
)}
 
W
here a
1
, a
2
 
are attributes and 
P
 
stands for formulae built by inner attributes.
 
For example:
 
{<
 
article
,
 
page
,
 
subject 
>
 
|
  

 
TutorialsPoint
 

 
subject 
=
 
'database'
}
 
Output
: Yields Article, Page
,
 
and Subject from 
the 
relation TutorialsPoint
,
 
where 
s
ubject is database.
 
Just like TRC, DRC 
can 
also 
be written using existential and universal quantifiers. 
DRC also involves relat
ional operators.
 
The 
e
xpression power of Tuple 
R
elation 
C
alculus and Domain 
R
elation 
C
alculus is 
equivalent to Relational Algebra.
 
 
 

DBMS
 
33
 
 
 
ER Model
,
 
when conceptualized into diagrams
,
 
gives a good overview of entity
-
relation
ship, which is easier to understand. ER diagrams can be mapped to 
r
elational schema
,
 
that is, it is possible to create relational schema using ER 
diagram. 
W
e cannot import all the ER constraints into 
r
elational model
,
 
but an 
approximate schema ca
n be generated.
 
There are 
several 
processes and algorithms available to convert ER Diagrams 
into Relational Schema. Some of them are automated and some of them are 
manual
. We may focus here on the mapping diagram contents to relationa
l 
basics.
 
ER 
d
iagrams mainly comprise
 
of:
 

 
Entity and its attributes
 

 
Relationship, which is association among entities
 
Mapping Entity
 
An entity is a real
-
world object with some attributes.
 
 
 
[
Image: Mapping Entity
]
 
Mapping Process (Algorithm)
 

 
Create table for each entity
.
 

 
Entity's attributes should become fields of tables with their respective data 
types.
 

 
Declare primary key
.
 
12. ER MODEL TO RELA
TIONAL 
MODEL
 

DBMS
 
34
 
 
 
Mapping 
R
elationship
 
A relationship is 
an 
association among entities.
 
 
[
Image: Mapping relationship
]
 
Mapping Process:
 

 
Create table for a relat
ionship
.
 

 
Add the primary keys of all participating Entities as fields of table with 
their respective data types.
 

 
If relationship has any attribute, add each attribute as field of table.
 

 
Declare a primary key composing all the primary keys of participating 
entities.
 

 
Declare all foreign key constraints.
 
Mapping Weak Entity Sets
 
A weak entity set
 
is one which does not have any primary key associated with it.
 
 
[
Image: Mapping Weak Entity Sets
]
 

DBMS
 
35
 
 
 
Mapping Process:
 

 
Create table for weak entity set
.
 

 
Add all its attributes to table as field
.
 

 
Add the primary key of identifying entity set
.
 

 
Declare all foreign key constraints
.
 
Mapping 
H
ierarchic
al 
E
ntities
 
ER specialization or generalization comes in the form of hierarchical entity sets.
 
 
[
Image: Mapping hierarchical entities
]
 
Mappi
ng Process
 

 
Create tables for all higher
-
level entities
.
 

 
Create tables for lower
-
level entities
.
 

 
Add primary keys of higher
-
level entities in the table of lower
-
level 
entities
.
 

 
In lower
-
level tables, add all other attributes of lower
-
level
 
entities.
 

 
De
clare primary key of higher
-
level table
 
and
 
the primary key for lower
-
level table
.
 

DBMS
 
36
 
 
 

 
Declare foreign key constraints.
 
 
 
 

DBMS
 
37
 
 
 
SQL is a programming language for Relational Databases. It is designed over 
relational algebra and tuple relational ca
lculus. SQL comes as a package with all 
major distributions of RDBMS.
 
SQL comprises both data definition and data manipulation languages. Using the 
data definition properties of SQL, one can design and modify database schema
,
 
whereas data manipulation prop
erties allows SQL to store and retrieve data from 
database.
 
Data 
D
efinition Language
 
SQL uses the following set of commands to define database schema:
 
CREATE
 
Creates new databases, tables
,
 
and views from RDBMS
.
 
For example:
 
Create
 
database tutorialspoint
;
 
Create
 
table article
;
 
Create
 
view for_students
;
 
DROP
 
Drop
s
 
commands
,
 
views, tables
,
 
and databases from RDBMS
.
 
For example:
 
Drop
 
object_type object_name
;
 
Drop
 
database tutorialspoint
;
 
Drop
 
table article
;
 
Drop
 
view for_students
;
 
ALTER
 
Modifies datab
ase schema.
 
Alter
 
object_type object_name parameters
;
 
13. SQL OVERVIEW
 

DBMS
 
38
 
 
 
F
or example:
 
Alter
 
table article add subject varchar
;
 
This command adds an attribute in 
the 
relation 
article
 
with 
the 
name 
subject
 
of 
string type.
 
Data Manipulation Language
 
SQL is equipped with data m
anipulation language
 
(DML)
. DML modifies the 
database instance by inserting, updating
,
 
and deleting its data. DML is 
responsible for all 
forms 
data modification in 
a 
database
. SQL contains the 
following set of command
s
 
in 
its 
DML section:
 

 
SELECT/FROM/WHER
E
 

 
INSERT INTO/VALUES
 

 
UPDATE/SET/WHERE
 

 
DELETE FROM/WHERE
 
These basic constructs allow
 
database programmers and users to enter data and 
information into the database and retrieve efficiently using a number of filter 
options.
 
SELECT/FROM/WHERE
 

 
SELECT
 
This is
 
one of the fundamental query command of SQL. It is similar to 
the 
projection operation of relational algebra. It selects the attributes based on 
the condition described by WHERE clause.
 

 
FROM
 
This clause takes a relation name as an argument from which attr
ibutes are 
to be selected/projected. In case more than one relation names are given
,
 
this clause corresponds to 
C
artesian product.
 

 
WHERE
 
This clause defines predicate or conditions
,
 
which must match in order to 
qualify the attributes to be projected.
 
For 
example:
 
Select
 
author_name
 
From
 
book_author
 
Where
 
age 
>
 
50
;
 

DBMS
 
39
 
 
 
This command will 
yield the 
names of author
s from 
the relation 
book_author
 
whose age is greater than 50.
 
INSERT INTO/VALUES
 
This command is used for inserting values into 
the 
ro
ws of 
a 
table (relation).
 
Syntax
:
 
INSERT INTO table 
(
column1 
[,
 
column2
,
 
column3 
...
 
])
 
VALUES 
(
value1 
[,
 
value2
,
 
value3 
...
 
])
 
Or
 
INSERT INTO table VALUES 
(
value1
,
 
[
value2
,
 
...
 
])
 
For 
e
xample:
 
INSERT INTO tutorialspoint 
(
Author
,
 
Subject
)
 
VALUES 
(
"anon
ymous"
,
 
"computers"
);
 
UPDATE/SET/WHERE
 
This command is used for updating or modifying 
the 
values of columns 
in a 
table 
(relation).
 
Syntax
:
 
UPDATE table_name SET column_name 
=
 
value 
[,
 
column_name 
=
 
value 
...]
 
[
WHERE condition
]
 
For example:
 
UPDATE tut
orialspoint SET 
Author
=
"webmaster"
 
WHERE 
Author
=
"anonymous"
;
 
DELETE/FROM/WHERE
 
This command is used for removing one or more rows from 
a 
table (relation).
 
Syntax
:
 
DELETE FROM table_name 
[
WHERE condition
];
 
 

DBMS
 
40
 
 
 
For example:
 
DELETE FROM tutorialspoint
 
  
WHER
E 
Author
=
"unknown"
;
 

DBMS
 
41
 
 
 
Functional Dependency
 
Functional dependency (FD) is 
a 
set of constraints between two attributes in a 
relation. Functional dependency says that if two tuples have 
same values for 
attributes A
1
, A
2
,..., A
n
,
 
then those two tuples must have to have same values 
for attributes B
1
, B
2
, ..., B
n
.
 
Functional dependency is represented by 
an 
arrow sign (

)
 
that is
,
 
X

Y, where 
X functionally determines Y. The left
-
hand side attributes determine
 
the values 
of attributes 
on the 
right
-
hand side.
 
Armstrong's Axioms
 
If F is 
a 
set of functional dependencies then the closure of F, denoted as F
+
, is 
the set
 
of all functional dependencies logically implied by F. Armstrong's Axioms 
are 
a 
set of rules
 
that
, when applied repeatedly
,
 
generates 
a 
closure of 
functional dependencies.
 

 
Reflexive rule
: If alpha is a set of attributes and beta is_subset_of 
alpha, then 
alpha holds beta.
 

 
Augmentation rule
: 
I
f a 

 
b holds and y is attribute set, then ay 

 
by 
also holds. That is adding attributes in dependencies, does not change the 
basic dependencies.
 

 
Transitivity rule
: Same as transitive rule in algebra, if a 

 
b holds and b 

 
c holds
,
 
then a 

 
c also hold
s
. a 

 
b is called as a functionally 
that 
determines b.
 
Trivial Functional Dependency
 

 
Trivial
: If 
a functional dependency (FD) 
X 

 
Y holds
,
 
where Y 
is a 
subset 
of X, then it is called a trivial FD. Trivial FDs 
always hold.
 

 
Non
-
trivial
: If an FD X 

 
Y
 
holds
,
 
where Y is not 
a 
subset of X, then it is 
called 
a 
non
-
trivial FD.
 

 
Completely non
-
trivial
: If an FD X 

 
Y holds
,
 
where x intersect Y = 

, 
it 
is said to be 
a 
completely non
-
trivial FD.
 
14. NORMALIZATION
 

DBMS
 
42
 
 
 
Normalization
 
If a database design is not perfect
,
 
it may contai
n anomalies, which are like a 
bad dream for 
any 
database 
administrator
. Managing a database with anomalies 
is next to impossible.
 

 
Update anomalies
: 
I
f data items are scattered and are not linked to 
each other properly, then 
it could lead to strange
 
situations
. 
For example, 
when we try to update one data item 
having its copies 
scattered 
over 
several places, 
a 
few instances 
get updated properly while 
a 
few 
others 
are left with 
old value
s. 
Such instances 
leave
 
the 
database in an 
inconsistent state.
 

 
Deletion anomalies
: 
W
e tried to delete a record, but parts of it 
was 
left 
undeleted because of unawareness, the data is also saved somewhere 
else.
 

 
Insert anomalies
: 
W
e tried to insert
 
data in a record that does not exist 
at all.
 
Normalization is a method to remove all these anomalies and bring 
the 
database 
to 
a 
consistent state
.
 
First Normal Form
 
First Normal Form 
is defined in the definition 
of relations (tables) itself. This rule 
defines that all the attributes in a relation must have atomic domains. 
The 
v
alues in 
an 
atomic domain are indivisible units.
 
 
[
Image: Unorganized relation
]
 
We re
-
arrange the relation (table) as below, to convert it to First Normal Form
.
 
 

DBMS
 
43
 
 
 
[
Image: Relation in 1NF
]
 
Each attribute must contain only 
a
 
single value from its pre
defined domain.
 
Second Normal Form
 
Before we learn about 
the 
second normal form, we need to understand the 
following:
 

 
Prime attribute
: 
A
n attribute, which is 
a 
part of 
the 
prime
-
key, is 
known 
as a 
prime attribute.
 

 
Non
-
prime at
tribute
: 
A
n attribute, which is not a part of 
the 
prime
-
key, 
is said to be a non
-
prime attribute.
 
If we follow 
s
econd normal form
,
 
then 
every non
-
prime attribute should be fully 
functionally dependent on prime key attribute. 
That is, if X 

 
A holds, then there 
should not be any proper subset Y of X
 
for 
which 
Y 

 
A also holds
 
true
.
 
 
[
Image: Relation not in 2NF
]
 
We see here in Student_Project relation that
 
the prime key attributes are Stu_ID 
and Proj_ID. According to the rule, non
-
key attributes, i.e.
,
 
Stu_Name and 
Proj_Name must be dependent upon both and not on any of the prime key 
attribute individually. But we find that Stu_Name can be identified by Stu
_ID 
and Proj_Name can be identified by Proj_ID independently. This is called 
partial
 
dependency
, which is not allowed in Second Normal Form.
 
 
[
Image: Relation in 2NF
]
 

DBMS
 
44
 
 
 
We broke th
e relation in two as depicted in the above picture. So there exists no 
partial dependency.
 
Third Normal Form
 
For a relation to be in Third Normal Form, it must be in Second Normal form and 
the following must satisfy:
 

 
No non
-
prime attribute is transitively
 
dependent on prime key attribute
.
 

 
For any non
-
trivial functional dependency, X 

 
A, then either
:
 
o
 
X is a superkey or,
 
o
 
A is prime attribute.
 
 
[
Image: Relation not in 3NF
]
 
 
We find that in 
the 
above 
Student_detail relation, Stu_ID is 
the 
key and o
nly 
prime key attribute. We find that City can be identified by Stu_ID as well as Zip 
itself. Neither Zip is a superkey nor 
is 
City
 
a prime attribute. Additionally, Stu_ID 

 
Zip 

 
City, so there exists
 
transitive dependency
.
 
To bring this relation into 
third normal form, we break the relation into two 
relations as follows:
 
 
[
Image: Relation in 3NF
]
 
 

DBMS
 
45
 
 
 
Boyc
e
-
Codd Normal Form
 
Boyce
-
Codd Normal Form (
BCNF
)
 
is an extension of Third Normal Form 
on 
strict 
terms
. BCNF states that
 
-
 

 
For any non
-
trivial functional dependency, X 

 
A, 
X must be a super
-
key.
 
In the above 
image, 
Stu_ID is 
the 
super
-
key in 
the relation 
Student_Detail 
and 
Zip is 
the 
super
-
key in 
the relation 
ZipCodes
.
 
So,
 
Stu_ID 

 
Stu_Name, Zip
 
a
nd
 
Zip 

 
City
 
Which 
c
onfirms
 
that both 
the 
relations are in BCNF.
 

DBMS
 
46
 
 
 
We understand the benefits of 
taking a 
Cartesian product of two relation
s
, which 
gives us all the possible tuples that are paired together. But 
it might not be 
feasible for us in certain cases to take a 
Cartes
ian product 
where we encounter 
huge relations with thousands of tuples 
having a considerable large number of 
attributes. 
 
Join
 
is 
a 
combination of 
a 
Cartesian product followed by 
a 
selection process. 
A 
Join operation pairs two tuples from different relations
,
 
if and only if 
a 
given join 
condition is satisfied.
 
We will 
briefly 
describe 
various 
join types
 
in the following sections
.
 

J
oin
 
Theta join
 
combines tuples from different relations provided they satisfy the 
theta condition.
 
The join condition is denoted by the symbol 

.
 
Notation:
 
R1 


 
R2
 
R1 and R2 are relations 
havi
ng 
attributes (A
1
, A
2
, .., A
n
) and (B
1
, B
2
,.. ,B
n
) such 
that 
the 
attribute
s
 

t have anything in common, 
that is
,
 
R1 

 
R2 = 

.
 
 
Theta join can use all kinds of comparison operators.
 
Student
 
SID
 
Name
 
Std
 
101
 
Alex
 
10
 
102
 
Maria
 
11
 
[Table: Student Relation]
 
 
 
 
15. JOINS
 

DBMS
 
47
 
 
 
Subjects
 
Class
 
Subject
 
10
 
Math
 
10
 
English
 
11
 
Music
 
11
 
Sports
 
[Table: Subjects Relation]
 
Student_Detail =
 
STUDENT 

Student
.
Std
 
=
 
Subject
.
Class
 
SUBJECT
 
Student_detail
 
SID
 
Name
 
Std
 
Class
 
Subject
 
101
 
Alex
 
10
 
10
 
Math
 
101
 
Alex
 
10
 
10
 
English
 
102
 
Maria
 
11
 
11
 
Music
 
102
 
Maria
 
11
 
11
 
Sports
 
[Table: Output of theta join]
 
Equi
j
oin
 
When Theta join uses only
 
equality
 
comparison operator
,
 
it is said to be 
e
qui
j
oin. The above example 
corresponds
 
to equi
join
.
 
Natural Join 
(

)
 
Natural join does not use any comparison operator. It does not concatenate the 
way 
a 
Cartesian product does. 
We can perform a 
Natural Join 
only 
if 
there is at 
least one common attribute 
that 
exists between 
two 
relatio
n
s
. 
In addition, 
t
h
e
 
attributes must have 
the 
same name and domain.
 

DBMS
 
48
 
 
 
Natural join acts on those matching attributes where the values of attributes in 
both 
the 
relation
s
 
are
 
same.
 
Courses
 
CID
 
Course
 
Dept
 
CS01
 
Database
 
CS
 
ME01
 
Mechanics
 
ME
 
EE01
 
Elec
tronics
 
EE
 
[Table: Relation Courses]
 
HoD
 
Dept
 
Head
 
CS
 
Alex
 
ME
 
Maya
 
EE
 
Mira
 
[Table: Relation HoD]
 
Courses 

 
HoD
 
Dept
 
CID
 
Course
 
Head
 
CS
 
CS01
 
Database
 
Alex
 
ME
 
ME01
 
Mechanics
 
Maya
 
EE
 
EE01
 
Electronics
 
Mira
 
[Table: Relation Courses 

 
HoD]
 

DBMS
 
49
 
 
 
Outer Joins
 
Theta Join, Equi
j
oin
,
 
and Natural Join are called inner
 
joins. An inner
 
join 
includes only 
those 
tuples with matching attributes
 
and the
 
rest are discarded in 
the 
resulting relation. There
fore, we need to use outer joins to include all the 
tuples 
from 
the 
participating
 
relations
 
in the resulting relation.
 
There are three 
kinds of outer joins:
 
left outer join, right outer join, and full outer join.
 
Left 
O
uter 
J
oin
 
(R
 
 
S
)
 
All 
the 
tuples 
from the
 
Left relation, R, are included in the resulting relation
.
 
If 
there 
are
 
tuples in R without any matching tuple in 
the Right relation 
S
,
 
then the 
S
-
attributes of 
the 
resulting relation are made NUL
L.
 
Left
 
A
 
B
 
100
 
Database
 
101
 
Mechanics
 
102
 
Electronics
 
[Table: Left Relation]
 
Right
 
A
 
B
 
100
 
Alex
 
102
 
Maya
 
104
 
Mira
 
[Table: Right Relation]
 
 
 
 

DBMS
 
50
 
 
 
Courses
 
 
HoD
 
A
 
B
 
C
 
D
 
100
 
Database
 
100
 
Alex
 
101
 
Mechanics
 
---
 
---
 
102
 
Electronics
 
102
 
Maya
 
[Table: Left outer join output]
 
Right 
O
uter 
J
oin: 
(R
 
 
S)
 
All 
the 
tupl
es 
from
 
the Right relation, S, are included in the resulting relation
.
 
If 
there 
are
 
tuples in S without any matching tuple in R
,
 
then the R
-
attributes of 
resulting relation are made NULL.
 
Courses
 
 
HoD
 
A
 
B
 
C
 
D
 
100
 
Database
 
100
 
Alex
 
102
 
Electronics
 
102
 
Maya
 
---
 
---
 
104
 
Mira
 
[Table: Right outer join output]
 
Full 
O
uter 
J
oin: (
R
 
 
S
)
 
All 
the 
tuples 
from 
both participating relations are included in the resulting 
relation
.
 
If
 
there 
are 
no matching tuples for both relations, their respective 
unmatched attributes are made NULL.
 
 
 
 
 

DBMS
 
51
 
 
 
Courses
 
 
HoD
 
A
 
B
 
C
 
D
 
100
 
Database
 
100
 
Alex
 
101
 
Mechanics
 
---
 
---
 
102
 
Electronics
 
102
 
Maya
 
---
 
---
 
104
 
Mira
 
[Table: Full outer join output]
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

DBMS
 
52
 
 
 
D
atabases are stored in file formats, which contain
 
records. At physical level, 
the 
actual data is stored in electromagnetic format on some device
. These storage 
devices can be broadly categorized in
to
 
thre
e types:
 
 
[
Image: Memory Types
]
 

 
Primary Storage
: The memory storage
 
that 
is directly accessible 
to 
the 
CPU
 
comes under this category. CPU's internal memory 
(registers), fast 
memory (cache)
,
 
and main memory (RAM) are directly accessible to 
the 
CPU
,
 
as they 
are 
all 
placed on the motherboard or CPU chipset. This 
storage is typically very small, ultra
-
fast
,
 
and volatile. 
Primary 
storage 
requires 
c
ontinuous power supply in order to maintain its state
. 
In 
case of 
a 
power failure
,
 
all 
its 
data 
is 
lost.
 

 
Secondary Storage
: 
Secondary storage devices are used 
to store data 
for 
future use or as backup. 
Secondary storage includes 
memory devices
 
that are 
not 
a 
part of 
the 
CPU chipset or motherboard
, for example,
 
magnetic disks, 
optical disks (DVD, CD
,
 
etc.), 
hard disks, 
flash drives
,
 
and 
magnetic tapes
.
 
 

 
Tertiary Storage
: 
Tertiary storage 
is used to store huge 
volumes 
of 
data. 
Since such 
storage 
devices are 
ext
ernal to the computer system, 
they are 
the slowest in speed. These storage devices are mostly used to 
take the 
back
 
up 
of 
an 
entire system. Optical disk
s
 
and magnetic tapes 
are widely used 
as tertiary storage.
 
Memory Hierarchy
 
A c
omputer system has 
a 
well
-
defined hierarchy of memory. 
A 
CPU has 
direct 
access to it main memory as well as its 
inbuilt registers
. 
The 
access time of 
the 
16. STORAGE SYSTEM
 

DBMS
 
53
 
 
 
main memory
 
is obviously less than 
the 
CPU speed
.
 
To 
minimize 
this speed 
mismatch, 
cache memory is introduced. Cache memory 
provides the fastest 
access time and it 
contains 
data that is 
most 
frequently accessed 
by 
the 
CPU
.
 
 
The memory with 
the 
fastest access is the costliest one
.
 
Larger storage 
devices 
offer
 
slow speed
 
and they
 
are less expensive
,
 
however
 
they 
can store huge 
volumes
 
of data 
as 
compared to CPU registers or 
c
ache memory
.
 
 
Magnetic Disks
 
Hard disk drives are the most common secondary storage devices in present 
computer sys
tems. These are called magnetic disks because 
they 
use
 
the 
concept of magnetization to store information. Hard disks consist of metal disks 
coated with magnetizable material. These disks are placed vertically 
on 
a 
spindle. A read/write head moves in be
tween the disks and is used to magnetize 
or de
-
magnetize the spot under it. 
A 
m
agnetized spot can be recognized as 0 
(zero) or 1 (one).
 
Hard disks are formatted in a well
-
defined order to store
 
data efficiently. A hard 
disk plate has many concentric circ
les on it, called 
tracks
. Every track is further 
divided into 
sectors
. A sector on a hard disk typically stores 512 bytes of data.
 
RAID
 
RAID stands for 
R
edundant 
A
rray of 
I
ndependent 
D
isks, which is a technology 
to connect multiple secondary storage devices and 
use 
them as a single storage 
media.
 
RAID consists 
of 
an array of disk
s
 
in which multiple disks are conne
cted together 
to achieve different goals. RAID levels define the use of disk arrays.
 

 
RAID 0
: In this level
,
 
a striped array of disks is implemented. The data is 
broken down into blocks and 
the 
blocks are distributed among 
disks. Each 
disk receives
 
a block of data to write/read in parallel. 
It 
enhances the 
speed and performance of 
the 
storage device. There is no parity and 
backup in Level 0.
 
 
[Image: RAID 0]
 

 
RAID 1
: 
RAID 1 
uses mirroring techniques. When data is sent to 
a 
RAID 
controller
,
 
it sends a copy of data to all 
the 
disks in 
the 
array. RAID level 

DBMS
 
54
 
 
 
1 is also called 
mirroring
 
and provides 100% redundancy in case of 
a 
failure.
 
 
[Image: RAID 1]
 

 
RAID 2
: 
RAID 2 
records 
Error Correction Code using Hamming distance 
for its data
,
 
striped on different disks. Like level 0, each data bit in a word 
is recorded on a s
eparate disk and ECC codes of the data words are stored 
on 
a 
different set disks. 
Due to 
its complex structure and high cost, RAID 
2 is not commercially available.
 
 
[Image: RAID 2]
 

 
RAID 3
: 
RAID 3 
stripes the data onto multiple disks
.
 
The parity bit 
generated for data word is stored on a different disk. This technique 
makes it to overcome single disk failure
s.
 
 
[
Image: RAID 3
]
 

 
RAID 4
: In this level
,
 
an entire block of data is written onto data disks 
and then the parity is generated and stored on a differen
t disk.
 
Note that 
level 3 uses byte
-
level striping
,
 
whereas level 4 uses block
-
level striping. 
Both level 3 and 
level 
4 require
 
at least 
three 
disks to implement RAID.
 

DBMS
 
55
 
 
 
[
Image: RAID 4
]
 

 
RAID 5
: 
RAID 5 
writes whole data blocks on
to different disks
,
 
but the 
parity 
bits 
generated for data block stripe 
are 
distributed among all the 
data disks 
rather than storing t
hem 
on a different dedicated disk
. 
 
 
[Image: RAID 5]
 

 
RAID 6
: 
RAID 6 
is an extension of level 5. In this
 
level
,
 
two independent 
parities are generated and stored in distributed fashion among 
mu
ltiple 
disks. Two parities provide additional fault tolerance. This level requires at 
least 
four 
disk drives to 
implement
 
RAID
.
 
 
[Image: RAID 6]
 

DBMS
 
56
 
 
 
Relative data and information is stored collectively in file formats. A file is 
a 
sequence of records stored in binary format. A disk drive is formatted into 
several block
s
 
that can store records.
 
File records are mapped onto those disk 
blocks.
 
File Organization
 
File Organization defines how file records are mapped onto disk blocks. 
We have 
four types of File Organization to organize
 
file records: 
 
 
[
Image: File Organization
]
 
Heap File Organization
 
When a file is created using Heap File Organization
, the Operating System
 
allocates memory area to that file without any further accounting details. File 
records ca
n be placed anywhere in that memory area. It is the responsibility of 
the 
software to manage the records. Heap File does not support any ordering, 
sequencing
,
 
or indexing on its own.
 
17. FILE STRUCTURE
 

DBMS
 
57
 
 
 
Sequential File Organization
 
Every file record contains a data field (
attribute) to uniquely identify that record. 
In sequential file organization
, records are placed in the file in 
some sequential 
order based on the unique key field or search key. Practically, it is not possible 
to store all the records sequen
tially in physical form.
 
Hash File Organization
 
 
Hash File Organization 
uses 
Hash function computation on some field
s
 
of the 
records. 
The output of 
the 
hash 
function 
determines the location of disk block 
where the records 
are to be placed
.
 
Clustered File Organization
 
 
Clustered file organization is not cons
idered good for large databases. In this 
mechanism, related records from one or more relations are kept in 
the
 
same 
disk block, that is, the ordering of records is not based on primary key or search 
key. 
 
File Operations
 
Operations on database files can be 
broadly 
classified into two categories
:
 
 

 
Update Operations
 

 
Retrie
val Operations
 
Update operations change the data values by insertion, deletion
,
 
or update. 
Retrieval operations
,
 
on the other hand
,
 
do not alter the data but retrieve them 
after optional conditional filtering. In both types of operations, selection plays 
a
 
significant role. Other than creation and deletion of a file, there could be several 
operations, which can be done on files.
 

 
Open
: A file can be opened in one of 
the 
two modes, 
read mode
 
or 
write mode
. In read mode, 
the 
operating system does not allow an
yone 
to alter data
.
 
In other words, data is read only. 
Files opened in read mode 
can be shared among several entities. 
W
rite mode
 
allows
 
data 
modification
. Files opened in write mod
e can be read
 
but cannot be 
shared.
 

 
Locate
: Every file has a file pointer, which tells the current position where 
the data is to be read or written. This pointer can be adjusted accordingly. 
Using find (seek) operation
,
 
it can be moved forward or bac
kward.
 

 
Read
: By default, when files are opened in read mode
,
 
the file pointer 
points to the beginning of 
the 
file. There are options where the user can 
tell the operating system
 
where 
to locate 
the file pointer 
at the time of 
openin
g
 
a file
. The very next data to the file pointer is read.
 

DBMS
 
58
 
 
 

 
Write
: User can select to open 
a 
file
 
in write mode, which enables them 
to edit 
its 
content
s
. It can be deletion, insertion
,
 
or modification. The file 
pointer can be located at the
 
time of opening or can be dynamically 
changed if the operating system allow
s to do so.
 
 

 
Close
: This 
is 
the 
most important operation from 
the 
operating system

 
point of view. When a request to close a file is generated,
 
the operating 
syst
em
 
 
o
 
removes all the locks (if in shared mode)
,
 
 
o
 
saves the 
data (if altered) to the secondary storage media
,
 
and 
 
o
 
release
s
 
all the buffers and file handlers associated with the file.
 
The organization of data 
insid
e 
a
 
file plays a major role here. 
The process to 
l
ocat
e
 
the file pointer to 
a
 
desired record inside 
a 
file
 
various based on whether 
the 
records 
are 
arranged sequentially or clustered
.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

DBMS
 
59
 
 
 
We know that 
data is 
stored in 
the 
form of records. Every record 
has a 
key field, 
which helps it to be recognized uniquely.
 
Indexing is a data structure technique to effi
ciently retrieve records from 
the 
database files based on some attributes on which the indexing has been done. 
Indexing in database systems is similar to 
what 
we see in books.
 
Indexing is defined based on its indexing attributes. Indexing can be 
of the 
following types:
 

 
Primary Index
: 
Primary index 
is defined on an ordered data file. The 
data file is ordered on a 
key field
.
 
The key field 
is 
generally 
the primary 
key of the relation.
 

 
Secondary Index
: 
S
econdary index may be 
generated from 
a field which 
is a candidate key and has a unique value in every record, or a non
-
key 
wit
h duplicate values.
 

 
Clustering Index
: 
Clustering index is defined on an ordered data file. 
The data file is ordered on a non
-
key field
. 
 
Ordered Indexing is of two types:
 

 
Dense Index
 

 
Sparse Index
 
Dense Index
 
In dense index, there is an index record for every search key value in the 
database. Th
is makes searching faster but requires more space to store index 
records itself. Index record
s
 
contain
 
search key value and a pointer to the actual 
record on the disk.
 
 
[
Image: Dense Index
]
 
18. INDEXING
 

DBMS
 
60
 
 
 
Sparse Index
 
In sparse index, index records are not created for every search key. An index 
record here contains 
a 
search key and 
an 
actual pointer to the data on the disk. 
To search a record
,
 
we first proceed by index record and reach 
at the actual 
location of the data. If the data we are looking for is not where we directly reach 
by following 
the 
index, the
n the
 
system starts sequential search until the desired 
data is found.
 
 
[
Image: Sparse Index
]
 
Multilevel Index
 
Index records 
comprise
 
search
-
key value
s
 
and data pointers. 
Multilevel 
index 
is 
stored on the disk along with the actual database files. As the size of 
the 
database grow
s
,
 
so does the size of 
the 
indices. There is an immense need to 
keep the index records in the main memory so 
as to 
speed up
 
the search 
operations
. If single
-
level index is used
,
 
then a large size index cannot be kept 
in memory 
which 
leads to multiple disk accesses.
 
 
[
Image: Multi
-
level Index
]
 

DBMS
 
61
 
 
 
Multi
-
level Index helps 
in 
breaking down the index into several smaller indices in 
orde
r to make the outer
most level so small that it can be saved in 
a 
single disk 
block
,
 
which can easily be accommodated anywhere in the main memory.
 
B
+
 
Tree
 
A B
+
 
tree is 
a 
balanced binary search tree that follo
ws a 
multi
-
level index format
.
 
The 
leaf nodes of 
a
 
B
+
 
tree denote actual
 
data pointers. B
+
 
tree ensures that all 
leaf nodes remain at the same height, thus balanced. Additionally, 
the 
leaf 
nodes are linked using 
a 
link list
; 
therefore, a 
B
+
 
tree 
can
 
support random 
access as well as sequential access.
 
Stru
cture of B
+
 
T
ree
 
Every leaf node is at equal distance from the root node. A B
+
 
tree is of 
the 
order 
n
 
where 
n
 
is fixed for every B
+
 
tree.
 
 
[
Image: B
+
 
tree
]
 
Internal nod
es:
 

 
Internal (non
-
leaf) nodes contain
 
at least 

n/2

 
pointers, except the root 
node.
 

 
At most, 
an 
internal node
 
can
 
contain 
n
 
pointers.
 
Leaf nodes:
 

 
Leaf nodes contain at least 

n/2

 
record pointers and 

n/2

 
key values
.
 

 
At most, 
a 
leaf node
 
can
 
contain 
n
 
record pointers and 
n
 
key values
.
 

 
Every leaf no
de contains one block pointer 
P
 
to point to next leaf node 
and forms a linked list.
 
B
+
 
T
ree 
I
nsertion
 

 
B
+
 
tree
s
 
are filled from bottom
 
a
nd each 
entry 
is 
done 
at 
the 
leaf node.
 

 
If 
a 
leaf node overflows:
 
o
 
Split node into two parts
.
 

DBMS
 
62
 
 
 
o
 
Partition
 
at 
i = 

(m+1)
/2

.
 
o
 
First 
i
 
entries are stored in one node
.
 
o
 
Rest of the entries (i+1 onwards) are moved to a new node
.
 
o
 
i
th
 
key is duplicated 
at 
the parent of the leaf
.
 

 
If 
a 
non
-
leaf node overflows:
 
o
 
Split node into two parts
.
 
o
 
Partition the node at 
i = 

(m+1)
/2

.
 
o
 
Entries up
 
to 
i
 
are kept in one node
.
 
o
 
Rest of the entries are moved to a new node
.
 
B
+
 
T
ree 
D
eletion
 

 
B
+
 
tree entries are deleted 
at the 
leaf nodes.
 

 
The target entry is searched and deleted.
 
o
 
If it is 
an 
internal node, delete and replace with the entry 
from the left 
position.
 

 
After deletion
,
 
underflow is tested
,
 
o
 
If underflow occurs
,
 
d
istribute 
the 
entries from 
the 
nodes left to it.
 

 
If distribution 
is not possible
 
from left
,
 
then
 
o
 
Distribute 
the entries 
from 
the 
nodes right to it
.
 

 
If distribut
ion 
is not possible
 
from left or from right
,
 
then
 
o
 
Merge the node with left and right to it.
 
 
 
 
 
 
 
 

DBMS
 
63
 
 
 
For a huge database structure
,
 
it 
can 
be almost next to impossible to 
search 
all 
the 
index 
values
 
through all its level and then reach the destination data block to 
retrieve the desired data. Hashing is an effective technique to calculate 
the 
direct location of 
a 
data record on the disk without using index structure.
 
Hashing
 
uses 
hash function
s
 
with search keys as parameters 
to 
generate
 
the 
address 
of a data record. 
 
Hash Organization
 

 
Bucket
: 
A 
h
ash file store
s data in bucket format. Bucket is considered a unit 
of storage. 
A 
b
ucket typically stores one complete disk block, which in turn 
can store one or more records.
 

 
Hash Function
: A hash function
,
 
h
, is a mapping function that maps all 
the 
set of search
-
keys
 
K
 
to the address where actual records are placed. It is a 
function from search keys to bucket addresses.
 
Static Hashing
 
In static hashing, when a search
-
key value is provided
,
 
the hash function always 
computes the same address. For example, if mod
-
4 hash 
function is used
,
 
then 
it shall generate only 5 values. The output address shall always be same for that 
function. The number
 
of buckets provided remain
s
 
unchanged 
at all times.
 
 
[
Image: Static Hashing
]
 
19. HASHING
 

DBMS
 
64
 
 
 
Operation:
 

 
Insertion
: When a record is required to be entered using static hash, the 
hash function 
h
 
computes the bucket address for search key 
K
, where the 
record will be stored.
 
Bucket address = h(K)
 

 
Search
: When
 
a record needs to be retrieved
,
 
the same hash function 
can be used to retrieve the address of 
the 
bucket where the data is 
stored.
 

 
Delete
: This is simply 
a 
search followed by 
a 
deletion operation.
 
Bucket Overflow
 
The condition of bucket
-
overflow is know
n as 
collision
. This is a fatal state for 
any static hash function. In this case
,
 
overflow chaining can be used.
 

 
Overflow Chaining
: When buckets are full, a new bucket is allocated for 
the same hash result and is linked after the previous one. This mechan
ism 
is called 
Closed Hashing
.
 
 
[
Image: Overflow chaining
]
 

 
Linear Probing
: When 
a 
hash function generates an address at which 
data is already stored, the next free buck
et is allocated to it. This 
mechanism is called 
Open Hashing
.
 

DBMS
 
65
 
 
 
 
[
Image: Linear Probing
]
 
Dynamic Hashing
 
The 
p
roblem with static hashing is that it does not expand or shrink dynamically 
as the size of 
the 
database grows or shrinks. Dynamic
 
hashing provides a 
mechanism in which data buckets are added and removed dynamically and on
-
demand. Dynamic hashing is also known as 
extended hashing
.
 
Hash function, in dynamic hashing, is made to produce 
a 
large number of values 
and only a few are used i
nitially.
 
[
Image: Dynamic Hashing
]
 

DBMS
 
66
 
 
 
Organization
 
The prefix of 
an 
entire hash value is taken as 
a 
hash index. Only a portion of 
the 
hash value is used for computing bu
cket addresses. Every hash index has a 
depth value
 
to signify 
how many bits are used for computing 
a 
hash function. 
These bits 
can 
address 2n buckets. When all these bits are consumed
 

 
that is, 
when 
all 
the 
buckets are full
 

 
then the depth value is increased linearly and 
twice the buckets are allocated.
 
Operation
 

 
Querying
: Look at the depth value of
 
the
 
hash index and use those bits 
to compute the bucket address.
 

 
Update
: Perform a query as above and update 
the 
data.
 

 
Dele
tion
: Perform a query to locate 
the 
desired data and delete
 
the 
same.
 
 

 
Insertion
: 
C
ompute the address of 
the 
bucket
.
 
o
 
If the bucket is already full
,
 

 
Add more buckets
.
 

 
Add additional bit
s
 
to 
the 
hash value
.
 

 
Re
-
compute the hash function
.
 
o
 
Else
,
 

 
Add dat
a to the bucket
,
 
o
 
If all 
the 
buckets are full, perform the remedies of static hashing.
 
Hashing is not favorable when the data is organized in some ordering and 
the 
queries require 
a 
range of data. When data is discrete and random, hash 
performs the best.
 
Ha
shing algorithm
s
 
have high complexity than indexing. All hash operations are 
done in constant time.
 
 
 
 
 
 
 
 

DBMS
 
67
 
 
 
 
 
 
 

DBMS
 
68
 
 
 
A transaction can be defined as a group of tasks. A single task is the minimum 
processing unit 
which 
cannot be divided further.
 

Suppose 
a bank employee 
transfers 
Rs
 
500 from A's account to B's account
.
 
This very simple and small 
transaction 
involves
 
several 
low
-
level tasks
.
 
 

 
Open_Account
(
A
)
 
Old_Balance
 
=
 
A
.
balance
 
New_Balance
 
=
 
Old_Balance
 
-
 
500
 
A
.
balance
 
=
 
New_Balance
 
Close_Account
(
A
)
 

 
Open_Account
(
B
)
 
Old_Balance
 
=
 
B
.
balance
 
New_Balance
 
=
 
Old_Balance
 
+
 
500
 
B
.
balance 
=
 
New_Balance
 
Close_Account
(
B
)
 
ACID Properties
 
A transaction 
is a very small unit of a program and it 
may contain several low
-
level tasks
. 
A transaction in a database system must maintain 
A
tomicity, 
C
onsistency, 
I
solat
ion, and 
D
urability
 

 
commonly known as ACID properties 

in order to ensure 
accuracy
,
 
completeness
,
 
and data integrity. 
 

 
Atomicity
: 
T
his property states that a transaction must be treated as an 
atomic unit, that is, either all of its operations are executed or none. 
There must be no state in 
a 
database
 
where 
a 
transaction is left partia
lly 
completed. States should be defined either before the execution of the 
transaction or after the execution/abortion/failure of the transaction.
 

 
Consistency
: 
The 
database must remain in a 
consistent state
 
after any 
transaction
. 
No transaction should have any adverse effect on the data 
residing in the database. 
If the database was in a c
onsistent state before 
20. TRANSACTION
 

DBMS
 
69
 
 
 
the execution of 
a
 
transaction, it must remain 
consistent 
after the 
execution of the transaction
 
as well
.
 

 
Durability
: 
The database should be durable enough to hold all its latest 
updates 
even if the system fails 
or 
restarts. If a transaction 
updates 
a 
chunk of 
data in 
a 
database and commits
,
 
then the database will hold the 
modified data. 
If 
a
 
transaction commits but 
the system fails before the 
data 
could be 
written on 
to 
the disk
,
 
then 
that data will be updated once 
the system 
springs back into action. 
 

 
Isolation
: In a database syst
em
 
where more than one transaction are 
being executed simultaneously and in parallel, the property of isolation 
states that all the transactions will be carried out and executed as if it is 
the only transaction in the system. No transaction will affect th
e existence 
of any other transaction.
 
Serializability
 
When 
multiple 
transaction
s
 
are being 
executed by the operating system in a 
multiprogramming environment, there are possibilities that instructions of one 
transaction
 
are interleaved wi
th some other transaction.
 

 
Schedule
: A chronological execution sequence of 
a 
transaction is called 
a 
schedule. A schedule can have many transactions in it, each comprising of 
a 
number of instructions/tasks.
 

 
Serial Schedule
: 
It is 
a
 
schedule in which tra
nsactions are aligned in 
such a way that one transaction is executed first. When the first 
transaction completes its cycle
,
 
then 
the 
next transaction is executed. 
Transactions are ordered one after 
the 
other. This type of schedule is 
called 
a 
serial schedu
le
,
 
as transactions are executed in a serial manner.
 
In a multi
-
transaction environment, serial schedules are considered as 
a 
benchmark. The execution sequence of 
an 
instruction in a transaction cannot be 
changed
,
 
but two transactions can have their instru
ction
s
 
executed in 
a 
random 
fashion. This execution does no harm if two transactions are mutually 
independent and working on different segment
s
 
of data
;
 
but in case these two 
transactions are working on 
the 
same data, 
then the 
results may vary. This 
ever
-
v
arying result may 
bring 
the database 
to 
an inconsistent state.
 
To resolve 
this 
problem, we allow parallel execution of 
a 
transaction schedule
,
 
if 
its 
transactions 
are either serializable or have some equivalence relation 
among
 
them. 
 
Equivalence 
S
chedules
 
 
An equivalence schedule can be of the following types: 
 

DBMS
 
70
 
 
 
Result Equivalence
 
If two schedules produce 
the 
same result
 
after execution, 
they 
are said to be 
result
 
equivalent. They may yield 
the 
same result for some value and 
different 
results for another 
set of 
values. That's why this equivalence is not generally 
considered significant.
 
View Equivalence
 
Two schedules 
would be 
view equivalence if 
the 
transactions in both 
the 
schedules perform similar actions in 
a 
similar manner.
 
For example:
 
o
 
If T reads 
the 
initial data in S1
,
 
then 
it 
also reads 
the 
initial data in S2
.
 
o
 
If T reads 
the 
value written by J in S1
,
 
then 
it 
also reads 
the 
value 
written by 
J in S2
.
 
o
 
If T performs 
the 
final write on 
the 
data value in S1
,
 
then 
it 
also 
performs 
the 
final write on 
the 
data value in S2
.
 
Conflict Equivalence
 
Two 
schedules would be 
conflicting if they have the following properties:
 
o
 
Both 
belong to separate transactions
.
 
o
 
Both accesses the same data item
.
 
o
 
At least one of them is "write" operation
.
 
Two schedules 
having multiple 
transactions with conflicting operations are said 
to be conflict equivalent if and only if:
 
o
 
Both 
the 
schedules contain 
the 
same set of Transactions
.
 
o
 
The order of conflicting pairs of operation is maintained in both 
the 
schedules
.
 
Note
: 
View equivalent schedules are view serializable and conflict equivalent 
schedules are conflict serializable. All conf
lict serializable schedules are view 
serializable too.
 
 
 
 
 

DBMS
 
71
 
 
 
States of Transactions
 
A transaction in a database can be in one of the following state
s
:
 
[
Image: Trans
action States
]
 

 
Active
: In this state
,
 
the transaction is being executed. This is the initial 
state of every transaction.
 

 
Partially Committed
: When a transaction executes its final operation, it 
is said to be in 
a partially committed
 
state. 
 

 
Failed
: 
A transaction is said to be in a failed state 
if 
any 
of the 
checks 
made by 
the 
dat
abase recovery system fails
. 
A failed transaction 
can no 
longer proceed further.
 

 
Aborted
: If any of 
the 
checks fails
 
and 
the 
transaction 
has
 
reached 
a 
f
ailed state, 
then 
the recovery ma
nager rolls back all its write operation
s
 
on the database to 
bring
 
the 
database 
back to its original 
state where it 
was prior to 
the 
execution of 
the 
transaction. Transactions in this state 
are called aborted. 
The 
d
atabase recovery
 
module can select one of the 
two operations after a transaction aborts:
 
o
 
Re
-
start the transaction
 
o
 
Kill the transaction
 

 
Committed
: If 
a 
transaction executes all its operations successfully
,
 
it is 
said to be committed. All its effects are now permanently 
established 
on 
the 
database system.
 
 
 
 

DBMS
 
72
 
 
 
In a multiprogramming environment where multiple transactions can be 
executed
 
simultaneously, it is highly important to control 
the 
concurrency
 
of 
transactions
. 
We have concurrency control protocols to ensure atomicity, 
isolation, and serializability of concurrent transactions. 
Concurrency control 
protocols can be broadly divided into two categories:
 

 
Lock
-
based protocols
 

 
Time
stamp
-
based protocols
 
Lock
-
b
ased 
P
rotocols
 
Database systems
 
equipped with lock
-
based protocols
 
use 
a 
mechanism by 
which a
ny transaction cannot read or write data until it acquires 
an 
appropriate 
lock on it
. Locks are of two kinds:
 

 
Binary Locks
 
A
 
lock on 
a 
data item can be in two states; it is either 
locked or unlocked.
 

 
Shared/exclusive
 
Locks
 
 
T
his type of lockin
g mechanism differentiates 
the 
lock
s
 
based on their uses. If a lock is acquired on a data item to 
perform a write operation, it is 
an 
exclusive lock. 
A
llowing more than one 
transaction
 
to write on 
the 
same data item would lead the database into 
an inconsistent state. Read locks are shared because no data value is 
being changed.
 
 
There are four type
s
 
of 
lock protocols available:
 
Simplistic
 
Lock Protocol
 
 
Simplistic lock
-
based protocols allow transaction
s
 
to obtain 
a 
lock on every 
object before 
a 
'write' operation is performed. 
T
ransactions may unlock the data 
item
 

.
 
Pre
-
claiming
 
Lock Protocol
 
Pre
-
claiming 
protocol
s
 
evaluate
 
their 
oper
ations and create
 
a list of data items 
on which 
they
 
need
 
locks. Before 
initiating an
 
execution, 
the 
transaction 
requests the system for all 
the 
locks it needs beforehand. If all the locks are 
granted, the transaction executes and releases 
all the locks when all its 
operations are over. 
If
 
all the locks are not granted, the transaction rolls back 
and waits until all 
the 
locks are granted.
 
21. CONCURRENCY CONT
ROL
 

DBMS
 
73
 
 
 
 
[Image: 
Pre
-
claiming]
 
Two
-
Phase Locking 

 
2PL
 
This locking protocol 
divides 
the 
execution phase 
of a transaction 
into three 
parts. In the first part, when 
the 
transaction starts executing, 
it
 
seeks 
permission 
for 
the 
locks it
 
requires
. 
The 
s
econd part is where the transaction 
acquires all 
the 
locks
. 
As soon as the transaction releases its first lock, the third 
phase starts. In this phase
,
 
the 
transaction cannot demand 
any 
new 
lock
s;
 
it 
only releases the acquired locks.
 
 
[Image: Two Phase Locking]
 
 
Two
-
phase locking has two phases, one is 
growing
,
 
w
here all 
the 
locks are 
being acquired by 
the 
transaction
;
 
and 
the 
second 
phase 
is 
shrinking
, where 
the 
locks held by the transaction are being released.
 
To claim an exclusive (write) lock, a transaction must first acquire a shared 
(read) lock and th
en upgrade it to 
an 
exclusive lock.
 
Strict Two
-
Phase Locking
 
The first phase of 
S
trict
-
2PL is same as 2PL. After acquiring all 
the 
locks in the 
first phase, 
the 
transaction continues to execute normally. But in contrast to 
2PL, Strict
-
2PL does not releas
e 
a 
lock 
after using it. 
Strict
-
2PL 
holds all 
the 
locks until 
the 
commit 
point 
and releases all the locks at a time. 
 

DBMS
 
74
 
 
 
 
[Image: Strict Two Phase Locking]
 
Strict
-
2PL does not have cascading abort as 2PL does.
 
Time
s
tamp
-
b
ased 
P
rotocols
 
The most commonly used concurrency protocol is 
the 
time
stamp based protocol
. 
This protocol uses either system time or logical counter 
as a time
stamp.
 
Lock
-
based protocols manage the order between 
the 
conflicting pairs among 
transaction
s
 
at the time of execution
,
 
whereas time
stamp
-
based protocols start 
working as so
on as 
a 
transaction is created.
 
Every transaction has a time
stamp associated with it
,
 
and the ordering is 
determined by the age of the transaction. A transaction created at 0002 clock 
time would be older than all other transaction
s
 
that 
come after 
it. For example, 
any transaction 'y' entering the system at 0004 is two seconds younger and 
the 
priority 
would 
be given to the older one.
 
In addition, every data item is given the latest read and write
-
timestamp. This 
lets the system know
 
when 
the 
last 

read and write

 
operation 
was 
performed 
on 
the data item.
 
Time
stamp 
O
rdering 
P
rotocol
 
The timestamp
-
ordering protocol ensures serializability among transaction
s
 
in 
their conflicting read and write operations. This is the responsibility of th
e 
protocol system that the conflicting pair of tasks should be executed according 
to the timestamp values of the transactions.
 

 
The 
t
ime
stamp of 
t
ransaction T
i
 
is denoted as TS(T
i
).
 

 
Read time
stamp of data
-
item X is denoted by R
-
timestamp(X).
 

 
Write time
stamp of data
-
item X is denoted by W
-
timestamp(X).
 
Timestamp ordering protocol works as follows:
 

 
If a transaction T
i
 
issues 
a 
read(X) operation:
 
o
 
If TS(T
i
) < W
-
timestamp(X)
 

DBMS
 
75
 
 
 

 
Operation rejected.
 
o
 
If TS(T
i
) >= W
-
timestamp(X)
 

 
Operation executed.
 
o
 
All data
-
item 
t
imestamps updated.
 

 
If a transaction T
i
 
issues 
a 
write(X) operation:
 
o
 
If TS(T
i
) < R
-
timestamp(X)
 

 
Operation rejected.
 
o
 
If TS(T
i
) < W
-
timestamp(X)
 

 
Operation rejected and T
i
 
rolled back.
 
o
 
Otherwise, operation executed.
 
Thomas' Write 
R
ule
 
This rule states 
i
f TS(Ti) < W
-
timestamp(X)
, then the operation is 
rejected and 
T
i
 
is 
rolled back.
 
Timestamp ordering rules can be modified to make the schedule view 
serializable.
 
Instead of making Ti rolled back, the 'write' operation itself is ign
ored.
 
 
 
 
 
 
 
 
 
 

DBMS
 
76
 
 
 
In a multi
-
process system, deadlock is a
n unwanted
 
situation
 
that 
arises in 
a 
shared resource environment
,
 
where a process indefinitely waits for a resource
 
that 
is held by 
an
other process
. 
 
For example, assume a set of transactions {T
0
, T
1
, T
2
, ...,T
n
}. T
0
 
needs a 
resource X to complete its task. Resource X is held by T
1
,
 
and T
1
 
is waiting for a 
resource Y, which is held by T
2
. T
2
 
is waiting f
or resource Z, which is held by T
0
. 
Thus, all 
the 
processes wait for each other to release resources. In this situation, 
none of 
the 
processes can finish their task. This situation is known as 
a 
deadlock
.
 
Deadlock
s
 
are 
not 
healthy for a system. 
In case 
a
 
system is stuck 
in a 
deadlock, 
the transactions involved in 
the 
deadlock are 
either 
rolled back 
or
 
restarted.
 
Deadlock Prevention
 
To prevent any deadlock situation in the system, the DBMS aggressively inspects 
all the operations
,
 
where 
transactions are about to execute. 
The 
DBMS inspects 
the 
operations and analyze
s
 
if they can create a deadlock situation. If it finds 
that a dea
dlock situation might occur
,
 
then that transaction is never allowed to 
be executed.
 
There are deadlock prevention schemes
 
that
 
use
 
time
stamp ordering mechanism 
of transactions in order to 
predetermine
 
a deadlock situation.
 
Wait
-
Die Schem
e
 
In this scheme, if a transaction request
s
 
to lock a resource (data item), which is 
already held with 
a 
conflicting lock by 
an
other transaction, 
then 
one of the two 
possibilities may occur:
 

 
If TS(T
i
) < TS(T
j
)
 

 
that is T
i
, which is requesting a con
flicting lock, is 
older than T
j
 

 
then 
T
i
 
is allowed to wait until the data
-
item is available.
 

 
If TS(T
i
) > TS(t
j
)
 

 
that is T
i
 
is younger than T
j
 

 
then 
T
i
 
dies. T
i
 
is 
restarted later with 
a 
random delay but with 
the 
same timestamp.
 
This scheme allows t
he older transaction to wait but kills the younger one.
 
Wound
-
Wait Scheme
 
In this scheme, if a transaction request
s
 
to lock a resource (data item), which is 
already held with conflicting lock by 
an
other transaction, one of the two 
possibilities may o
ccur:
 
22. DEADLOCK
 

DBMS
 
77
 
 
 

 
If TS(T
i
) < TS(T
j
), 
then 
T
i
 
forces T
j
 
to be rolled back
 

 
that is T
i
 
wounds T
j
. 
T
j
 
is restarted later with 
a 
random delay but with 
the 
same timestamp.
 

 
If TS(T
i
) > TS(T
j
), 
then 
T
i
 
is forced to wait until the resource is available.
 
This scheme
 
allows the younger transaction to wait
;
 
but when an older 
transaction request
s
 
an item held by 
a 
younger one, the older transaction forces 
the younger one to ab
ort and release the item.
 
In both 
the 
cases, 
the 
transaction
 
that 
enters 
the system
 
at a 
later 
stage 
is 
aborted.
 
Deadlock Avoidance
 
Aborting a transaction is not always a practical approach. Instead
,
 
deadlock 
avoidance mechanisms can be u
sed to detect any deadlock situation in advance. 
Methods like "wait
-
for graph" are available but 
they are suitable 
for 
only those 
system
s
 
where transactions are light
weight 
having
 
fewer instances of resource. 
In a bulky system
,
 
dead
lock prevention techniques may work well.
 
Wait
-
for Graph
 
This is a simple method available to track if any deadlock situation may arise. 
For each transaction entering in
to
 
the system, a node is created. When 
a 
transaction T
i
 
requests for a lock on 
an 
item,
 
say X, which is held by some other 
transaction T
j
, a directed edge is created from T
i
 
to T
j
. If T
j
 
releases item X, the 
edge between them is dropped and T
i
 
locks the data item.
 
The system maintains this wait
-
for graph for every transaction waiting for som
e 
data items held by others. 
The 
s
ystem keeps checking if there's any cycle in the 
graph.
 
 
[
Image: Wait
-
for Graph
]
 
 

DBMS
 
78
 
 
 
Here, we can use any of the 
t
wo 
following 
approac
hes
:
 
 

 
F
irst
, do
 
not 
allow any request for an item, which is already locked by 
an
other transaction. This is not always feasible and may cause starvation, 
where a transaction indefinitely waits for 
a 
data item and can never 
acquire it. 
 

 
The 
s
econd option is to roll back one of the transactions.
 
It is not 
always 
feasible to 
roll back the younger transaction, as it may be important than 
the older one. With 
the 
help of some relative algorithm
,
 
a transaction is 
chosen, which is to b
e aborted
.
 
T
his transaction is 
known as the 
victim
 
and the process is known as
 
victim selection
.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

DBMS
 
79
 
 
 
L
oss 
of 
Volatile 
S
torage
 
A volatile storage like RAM stores all the active logs, disk buffers, an
d related 
data. 
In addition, it stores all the transactions that are being currently executed
. 
What happens if such a 
volatile storage 
crashes
 
abruptly?
 
It would obviously 
take
 
away all the logs and active cop
ies
 
of 
the 
database. It makes recovery 
almost impossible
,
 
a
s everything 
that is required to recover the data is lost. 
 
Following techniques may be adopted in case of loss of 
volatile storage
:
 

 
We can have 
checkpoints
 
at multiple stages so as to save the contents 
of the database pe
riodically. 
 

 
A 
s
tate of active database in 
the 
volatile memory can be 
periodically 
dumped
 
onto 
a 
stable storage
, which may also c
ontain logs and active 
transactions and buffer blocks.
 

 
<dump> can be marked on 
a 
log file
,
 
whenever the database contents are 
dumped from 
a 
non
-
volatile memory to a stable one.
 
Recovery:
 

 
When the system recovers from
 
a
 
failure, it can restore the latest du
mp.
 

 
It can maintain 
a 
redo
-
list and 
an 
undo
-
list as 
checkpoints.
 

 
It can recover the system by consulting undo
-
redo lists to restore the 
state of all transaction
s
 
up to 
the 
last checkpoint.
 
Database 
B
ackup & 
R
ecovery from 
C
atastrophic 
F
ailure
 
A catas
trophic failure is one where a stable, secondary storage device 
gets 
corrupt. 
With the storage device, all the valuable data 
that is 
stored inside is 
lost. 
We have two 
different 
strategies to recover data from such a catastrophic 
failure
: 
 

 
Remote backup
 

 
H
ere a backup copy of the database is stored at a 
remote location from where it can be restored in case of a 
catastrophe
. 
 
 

 
Alternatively, 
database backups can be taken on magnetic 
tapes and 
stored at a safer place. This backup can later be 
transferred 
on
to 
a freshly 
installed database 
to 
bring it to the 
point of backup.
 
Grown
-
up databases are too 
bulky
 
to be frequently backed
 
up. 
In such cases, we 
have 
techniques
 
where we can restore a database 
just 
by 
looking at 
its 
logs. 
So, 
23. DATA BACKUP
 

DBMS
 
80
 
 
 
all that we need to do here is to take a 
backup of 
all the 
logs at frequent 
intervals of time. 
The 
d
atabase can be backed
 
up once a week
,
 
and 
the 
logs
 
being very small can be backed
 
up every
 
day or as frequent
ly
 
as 
possible
.
 
Remote Backup
 
Remote backup provides a sense of security 
in case the primary location where 
the database is located gets destroyed. Remote backup can be offline or real
-
time 
or 
online. In case it is offline
,
 
it is maintained manually.
 
[
Image: Remote D
ata Backup
]
 
Online backup systems are more real
-
time and lifesavers for database 
administrators and investors. An online backup system is a mechanism
 
where 
every bit of 
the 
real
-
time data is backed
 
up simultaneously at two distant place
s
. 
One of them is 
directly connected to 
the 
system and 
the 
other one is kept at 
a 
remote place as backup.
 
As soon as the primary database storage fails, the backup system sense
s
 
the 
failure and switch
es
 
the user system to the remote storage. Sometimes this is so 
instant 
tha
t 
the users 

even 
realize a failure.
 

DBMS
 
81
 
 
 
Crash Recovery
 
DBMS is 
a 
highly complex system with hundreds of transactions being executed 
every second. 
The 
durability and robustness
 
of 
a 
DBMS depends on its complex 
architecture and 
i
ts
 
underlying hardware 
and
 
system software. If it fails or 
crashes amid transactions
,
 
it is expected that the system would follow some sort 
of algorithm or techniques to recover 
lost data. 
 
Failure Classification
 
To see where the problem has occurred
,
 
we generalize 
a
 
failure into various 
categories, as follows:
 
Transaction 
F
ailure
 
A transaction has to abort when it fails to execute or when 
it reaches a point 

This is called transaction failure
 
w
here only 
a 
few transaction
s
 
or process
es
 
are hurt.
 
Reason
s
 
for 
a 
transaction failure could be:
 

 
Logical err
ors
: 
W
here a transaction cannot complete because 
it has 
some code error or any internal error condition
.
 

 
System errors
: 
W
here the database system itself terminates an active 
transaction because 
the 
DBMS is not able to execute it
,
 
or it has to stop 
b
ecause of some system condition. For example, in case of deadlock or 
resource unavailability
, the
 
system
 
aborts an active transaction.
 
System 
C
rash
 
There are problems
 

 
external to the system
 

 
that
 
may cause the system to 
stop abruptly 
and cause the system to crash. For example
,
 
interruption
s
 
in 
power supply
 
may cause the
 
failure of underlying hardware or software failure.
 
Examples may include operating system errors.
 
Disk 
F
ailure
 
In early days of technology evolution, it was a common
 
problem where hard
-
disk 
drives or storage drives used to fail frequently.
 
24. DATA RECOVERY
 

DBMS
 
82
 
 
 
Disk failures include formation of bad sectors, unreachability to the disk, disk 
head crash or any other failure, which destroys all or 
a 
part of disk storage
.
 
Storage Structure
 
We
 
have already described 
the 
storage system
.
 
In brief, the storage structure 
can be divided in
to
 
two
 
categories:
 

 
Volatile storage
: As 
the 
name suggests, 
a volatile
 
storage 
can
not 
survive system crashes
. Volatile storage devices are 
placed very close
 
to 
the 
CPU
; normally they are embedded 
onto the chipset itself
.
 
F
or 
example
,
 
main memory
 
and
 
cache memory
 
are examples of volatile 
storage
. They are fast but can store 
only 
a small amount of information.
 

 
Non
-
volatile storage
: These memories are made to survive system 
crashes. They are huge in data storage capacity
,
 
but slower in 
accessibility. Examples may include
 
hard
-
disks, magnetic tapes, flash 
memory, 
and 
non
-
volatile (battery backed up) RAM.
 
Recove
ry and Atomicity
 
When a system crashes, it 
may 
have several transactions being executed and 
various files opened for them to modify
 
the 
data items. 
T
ransactions are made 
of various operations, which are atomic in nature. But accord
ing to ACID 
properties of DBMS, atomicity of transactions as a whole must be maintained
,
 
that is, either all 
the 
operations are executed or none.
 
When 
a 
DBMS recovers from a crash
,
 
it should maintain the following:
 

 
It should check the states of all 
the 
tra
nsactions, which were being 
executed.
 

 
A transaction may be in the middle of some operation; 
the 
DBMS must 
ensure the atomicity of 
the 
transaction in this case.
 

 
It should check whether the transaction can be completed now or 
it 
needs 
to be rolled back.
 

 
No t
ransactions would be allowed to 
leave
 
the 
DBMS in 
an 
inconsistent 
state.
 
There are two types of techniques, which can help 
a 
DBMS in recovering as well 
as maintaining the atomicity of 
a 
transaction:
 

 
Maintaining the logs of each transaction, and writing
 
them onto some 
stable storage before actually modifying the database.
 

 
Maintaining shadow paging, where 
the changes are done on a volatile 
memory
,
 
and later
,
 
the actual database is updated.
 

DBMS
 
83
 
 
 
Log
-
b
ased Recovery
 
Log is a sequence of records, which mainta
ins the records of actions performed 
by a transaction. It is important that the logs are written prior to 
the 
actual 
modification and stored on a stable storage media, which is failsafe.
 
Log
-
based recovery works as follows:
 

 
The log file is kept on 
a 
stabl
e storage media
.
 

 
When a transaction enters the system and starts execution, it writes a log 
about it
.
 
<T
n
, 
Start
>
 

 
When the transaction modifies an item X, it write logs as follows:
 
<T
n
, 
X
, 
V
1
, 
V
2
>
 
It reads Tn has changed the value of X, from V1 to V2.
 

 
When
 
the 
transaction finishes, it logs:
 
<T
n
, 
commit
>
 
The 
d
atabase can be modified using two approaches:
 

 
Deferred database modification
: All logs are written on to the stable 
storage and 
the 
database is updated when 
a 
transaction commits.
 

 
Immediate database m
odification
: Each log follows an actual database 
modification. That is, 
the 
database is modified immediately after every 
operation.
 
Recovery with 
C
oncurrent 
T
ransactions
 
When more than one transaction
 
are being executed in parallel, the logs are 
interl
eaved. At the time of recovery
,
 
it would become hard for 
the 
recovery 
system to backtrack all logs, and then start recovering. To ease this situation
,
 
most modern DBMS use
 
the concept of 'checkpoints'.
 
Checkpoint
 
Keeping and maintaining logs in real time 
and in real environment may fill out all 
the memory space available in the system. A
s
 
time passes
,
 
the 
log file may 
grow 
too big to be handled at all. Checkpoint is a mechanism where all the previous 
logs are removed from the system and stored permanen
tly in 
a 
storage disk. 
Checkpoint declares a point before which the DBMS was in consistent state
,
 
and 
all the transactions were committed.
 

DBMS
 
84
 
 
 
Recovery
 
When 
a 
system with concurrent transaction
s
 
crashes and recovers, it 
behave
s
 
in 
the following manner:
 
 
[
Image: Recovery with concurrent transactions
]
 

 
The recovery system reads the logs backwards from the end to the last 
c
heckpoint.
 

 
It maintains two lists, 
an 
undo
-
list and 
a 
r
edo
-
list.
 

 
If the recovery system sees a log with <T
n
, Start> and <T
n
, Commit> or 
just <T
n
, Commit>, it puts the transaction in 
the 
redo
-
list.
 

 
If the recovery system sees a log with <T
n
, Start> but no commit or abort 
log found, it puts the transaction in 
th
e 
undo
-
list.
 
All 
the 
transactions in 
the 
undo
-
list are then undone and their logs are removed. 
All 
the 
transaction
s
 
in 
the 
redo
-
list
 
and
 
their previous logs are removed and then 
redone 
before saving their logs
.
 
 
 
 
 

