***Stable hashing***
In computer science,  consistent hashing  is a special kind of hashing such that when a hash table is resized, only  
   
     
       
         K 
         
           / 
         
         n 
       
     
     {\displaystyle K/n} 
    keys need to be remapped on average, where  
   
     
       
         K 
       
     
     {\displaystyle K} 
    is the number of keys, and  
   
     
       
         n 
       
     
     {\displaystyle n} 
    is the number of slots.
 In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation. Consistent hashing is a particular case of rendezvous hashing, which has a conceptually simpler algorithm, and was first described in 1996.. Consistent hashing first appeared in 1997, uses a different algorithm. 

 **Motivations**

 While running collections of caching machines some limitations are experienced. A common way of load balancing  
   
     
       
         n 
       
     
     {\displaystyle n} 
    cache machines is to put object  
   
     
       
         o 
       
     
     {\displaystyle o} 
    in cache machine number  
   
     
       
         
           hash 
         
         ( 
         o 
         ) 
         
         
           ( 
           
             
               modÂ  
             
             n 
           
           ) 
         
       
     
     {\displaystyle {\text{hash}}(o)\;\left({\text{mod }}n\right)} 
   . But this will not work if a cache machine is added or removed because  
   
     
       
         n 
       
     
     {\displaystyle n} 
    changes and every object is hashed to a new location. This can be disastrous since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping of servers.
 Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared among the remaining machines.
 The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed, its interval is taken over by a cache with an adjacent interval while all the remaining caches are unchanged.
 

 **Complexity**

 The  
   
     
       
         O 
         ( 
         l 
         o 
         g 
         ( 
         N 
         ) 
         ) 
       
     
     {\displaystyle O(log(N))} 
    complexity for consistent hashing comes from the fact that a binary search among nodes angles is required to find the next node on the ring. 

 **Examples**

 Known examples of consistent hashing use include:
 
 Couchbase automated data partitioningTemplate:Fcat 
 OpenStack's Object Storage Service Swift 
 Partitioning component of Amazon's storage system Dynamo 
 Data partitioning in Apache Cassandra 
 Data partitioning in Voldemort 
 Akka's consistent hashing router 
 Riak, a distributed key-value database 
 GlusterFS, a network-attached storage file system 
 Skylable, an open-source distributed object-storage system 
 Akamai content delivery network 
 Discord chat application 
 Maglev network load balancer 

 