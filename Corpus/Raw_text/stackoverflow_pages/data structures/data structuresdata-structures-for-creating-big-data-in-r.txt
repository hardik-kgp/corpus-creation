*** data-structures-for-creating-big-data-in-r ***

 I'm writing a gene level analysis script in R and I'll have to handle large amounts of data.  
 My initial idea was to create a super list structure, a set of lists within lists. Essentially the structure is 
 
 This is huge and takes in excess of 12 mins purely to set up the data structure. Stream lining this process, I can get it down to about 1.6 mins when setting up one value of the 1:8 list, so essentially... 
 
 Normally, I'd create the structure as and when it's needed, on the fly, however, I'm distributing the 1:1000 steps which means, I don't know which order they'll come back in.  
 Are there any other packages for handling the creation of this level of data?
Could I use any more efficient data structures in my approach? 
 I apologise if this seems like the wrong approach entirely, but this is my first time handling big data in R. 
 
 Note that lists are vectors, and like any other vector, they can have a   attribute. 
 
 This is effectively instantaneous. You access individual elements with  , eg  . 
 
 A different strategy is to create a vector and a partitioning, e.g., to represent 
 
 as 
 
 then one can do vectorized calculations, e.g.,  
 
 and other clever things. This avoids creating complicated lists and the iterations that implies. This approach is formalized in the  Bioconductor   IRanges  package   classes. 
 
 One idiom for working with this data is to  , transform, then  ; both   and   are inexpensive, so the long-hand version of the above is  
 Depending on your data structure, the   class may be appropriate, e.g., the following can be manipulated like a   (subset, etc) but contains *List elements. 
 
 For genomic data where the coordinates of genes (or other features) on chromosomes is of fundamental importance, the  GenomicRanges  package and GRanges / GRangesList classes are appropriate.  
 