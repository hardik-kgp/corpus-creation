*** slow-data-structures-in-multiprocessing-python-library ***

 I am trying to make simple computations for large data structures with multiprocessing library. It is something that I need for my thesis, so please don't be stern for me. 
 When I decided to divide my computations for multiple "workers", "threads", "processes" or call it as you want, I started looking into python docs to find what I need and I found two modules, 'threading' and 'multiprocessing'. After reading, I decided to use 'multiprocessing' cause it looks like something I need. 
 The problem is that with multiple workers (processes) my computations are much slower. First thought was related to size of my input data. I understand that for small data, 'cost' of running threads is much bigger than simple computation, but for larger structures efficiency should growth. 
 I was really supriced that my computation (for example 2D Rosenbrock) is few times faster for iterative algorithm than with computation made with few processes. And computation is done for 100k tuples. 
 I also noticed that multiprocessing.Queue access is few times slower than access to collections.deque, but I really need to have this computation is some kind of "shared memory" or something similar. 
 Can someone explain me where is the problem? Is it Python so much efficient that it's not worth to compute it with multi processes? Do I use proper data structures? Can I change something in my perception of multiprocessing? Or maybe even I measure it in bad way? I really appreciate for any clue how I can speed it up. 
 Full code below 
 
 
 You are passing arguments across process boundaries to do a simple calculation.  I expect it will be extremely slow. 
 If you need speed, I recommend you fall back to a single threaded implementation, and find a way to vectorize it using numpy. Profile it with cProfile. Attack the hot spots. 
 A huge benefit of numpy is reduced python overhead (name resolution, looping, etc). 
 Once you've got the single-threaded approach fast, only then move on to parallel processing. 
 An added benefit of vectorizing your problem is that numpy unlocks the GIL for lengthy calls, allowing true threading, vs multiprocessing. 
 