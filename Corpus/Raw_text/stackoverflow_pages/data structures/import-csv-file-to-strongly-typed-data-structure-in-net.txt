*** import-csv-file-to-strongly-typed-data-structure-in-net ***

 
 
 
 
 
 
 
 
                            As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened,  visit the help center  for guidance.
                            
                         
 
 
 Closed  7 years ago . 
 
 
 What's the best way to import a CSV file into a strongly-typed data structure? 
 
 Check out  FileHelpers Open Source Library . 
 
 Microsoft's  TextFieldParser  is stable and follows  RFC 4180  for CSV files.  Don't be put off by the   namespace; it's a standard component in the .NET Framework, just add a reference to the global   assembly. 
 If you're compiling for Windows (as opposed to Mono) and don't anticipate having to parse "broken" (non-RFC-compliant) CSV files, then this would be the obvious choice, as it's free, unrestricted, stable, and actively supported, most of which cannot be said for FileHelpers. 
 See also:  How to: Read From Comma-Delimited Text Files in Visual Basic  for a VB code example. 
 
 Use an OleDB connection. 
 
 
 If you're expecting fairly complex scenarios for CSV parsing,  don't even think up of rolling our own parser . There are a lot of excellent tools out there, like  FileHelpers , or even ones from  CodeProject . 
 The point is this is a fairly common problem and you could bet that  a lot  of software developers have already thought about and solved this problem. 
 
 Brian gives a nice solution for converting it to a strongly typed collection. 
 Most of the CSV parsing methods given don't take into account escaping fields or some of the other subtleties of CSV files (like trimming fields). Here is the code I personally use. It's a bit rough around the edges and has pretty much no error reporting. 
 
 Note that this doesn't handle the edge case of fields not being deliminated by double quotes, but meerley having a quoted string inside of it. See  this post  for a bit of a better expanation as well as some links to some proper libraries. 
 
 I agree with @ NotMyself .  FileHelpers  is well tested and handles all kinds of edge cases that you'll eventually have to deal with if you do it yourself. Take a look at what FileHelpers does and only write your own if you're absolutely sure that either (1) you will never need to handle the edge cases FileHelpers does, or (2) you love writing this kind of stuff and are going to be overjoyed when you have to parse stuff like this: 
 1,"Bill","Smith","Supervisor", "No Comment" 
 2 , 'Drake,' , 'O'Malley',"Janitor, 
 Oops, I'm not quoted and I'm on a new line! 
 
 I was bored so i modified some stuff i wrote.  It try's to encapsulate the parsing in an OO manner whle cutting down on the amount of iterations through the file, it only iterates once at the top foreach. 
 
 
 There are two articles on CodeProject that provide code for a solution, one that uses  StreamReader  and one that  imports CSV data  using the  Microsoft Text Driver . 
 
 A good simple way to do it is to open the file, and read each line into an array, linked list, data-structure-of-your-choice. Be careful about handling the first line though. 
 This may be over your head, but there seems to be a direct way to access them as well using a  connection string . 
 Why not try using Python instead of C# or VB? It has a nice CSV module to import that does all the heavy lifting for you. 
 
 I had to use a CSV parser in .NET for a project this summer and settled on the Microsoft Jet Text Driver.  You specify a folder using a connection string, then query a file using a SQL Select statement.  You can specify strong types using a schema.ini file.  I didn't do this at first, but then I was getting bad results where the type of the data wasn't immediately apparent, such as IP numbers or an entry like "XYQ 3.9 SP1". 
 One limitation I ran into is that it cannot handle column names above 64 characters; it truncates.  This shouldn't be a problem, except I was dealing with very poorly designed input data.  It returns an ADO.NET DataSet. 
 This was the best solution I found.  I would be wary of rolling my own CSV parser, since I would probably miss some of the end cases, and I didn't find any other free CSV parsing packages for .NET out there. 
 EDIT:  Also, there can only be one schema.ini file per directory, so I dynamically appended to it to strongly type the needed columns.  It will only strongly-type the columns specified, and infer for any unspecified field.  I really appreciated this, as I was dealing with importing a fluid 70+ column CSV and didn't want to specify each column, only the misbehaving ones. 
 
 I typed in some code. The result in the datagridviewer looked good. It parses a single line of text to an arraylist of objects. 
 
 
 If you can guarantee that there are no commas in the data, then the simplest way would probably be to use  String.split . 
 For example: 
 
 There may be libraries you could use to help, but that's probably as simple as you can get.  Just make sure you can't have commas in the data, otherwise you will need to parse it better. 
 