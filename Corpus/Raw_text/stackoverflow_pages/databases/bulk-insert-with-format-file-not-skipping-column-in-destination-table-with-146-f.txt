*** bulk-insert-with-format-file-not-skipping-column-in-destination-table-with-146-f ***

 Here is the full error: 
 Msg 4864, Level 16, State 1, Line 3
Bulk load data conversion error (type mismatch or invalid character for the specified codepage) for row 1, column 5 (FK_User_CreatedBy). 
 And here is the existential snapshot of my pain :) 
 
 **Many questions touch on these issues, **
**but none of them do the trick...**

 I suspect my problem is something like described  here , but I am not sure. The destination table column that is not being skipped properly is NOT sparse. 
 
 Here is the two row data file for import (.csv) open in notepad and notepad++:
(Yes I am aware that the row terminator is \r\n and the field/column terminator is \t or ',') 
 
 Here it is in plain text: 
 
 **CONTEXT/BACKGROUND:**

 **Test on Small Table and Input File with Few Records**
 (remember it is column skipping on a table with many columns that ends up hurting)... 
 The import WORKS perfectly for a small database table that looks like this: 
 
 And is created thus: 
 
 Here is the code for the table create: 
 
 The .xml bulk insert format file looks like this: 
 
 Note that it also works if I skip the ID (PK + index) column since the database table is empty and the import file does not have an index. This is working fine for the small destination table, as the database is generating the primary key index. 
 Here the format file as text (): 
 
 And it was created using bcp at the command line like this: 
 
 Here is the bcp command line in text: 
 bcp YFP..tbl_Person_Importtest format nul -f PersonImportMapFile.xml -c -x -T 
 Now when I execute the import with all of these files against the empty small table, all is good: 
 
 If I insert more rows again, no problem...
 
 
 **THE LARGE TABLE**
 I cannot include a full description due to intellectual property issues, but the large destination table has 146 fields with no sparse fields and plenty of DATETIME and DATE fields, as well as stacks of foreign keys (mostly INT) some of which are nullable. Here is the map file as generated by bcp (With Field names truncated and some removed): 
 
 **Outcome**

 I should be able to import into this table using the same data file as I specified for the example with the smaller table above, but I am getting the error specified at the beginning of this question. 
 The field it is picking on is indeed the fifth field/column in the table, but it is supposed to be skipping to the fields named in the map only, according to  this MS tutorial . 
 IT just looks like I will need to use a staging table or other programmatic approach wit middleware or SQLBulkCopy (c# .NET), and I would prefer not to do this at this stage. I would just like the map file to work. 
 **Did I miss something, or is it a case of shoot the BULK INSERT with-map-file for-large-table horse and get a different ride?**

 
 What perhaps you have missed is that the example in the tutorial which uses an XML format file to skip columns inserts data into a view that includes only the target columns; it doesn't appear to be possible to use an XML format file to skip columns in the target table. 
 You could create a view of the relevant columns on   and insert to that.  
 Alternatively you could use an old-style non XML format file, or (perhaps easier, if your environment security setting allow it) use   - both of these options are covered in the tutorial. 
 There are a few other things you might consider changing: 
 1 - The sample wide table definition doesn't match your input file in a couple of ways: 
 
 There is no source in your file for the   column   - you might need a   constraint to set the value - perhaps this is present but was omitted from the example table definition? 
 The table contains no  , , ,  ,  or   columns, even though they are referenced format file - this might be a side-effect of your redaction of the column names. 
 
 2 - You can generate a format file which more closely matches your data file using a command like the one below, which correctly sets the field terminator to a comma: 
 
 3 - The screenshot of your   command includes the command: 
 
 before the bulk insert. There are two issues with this:  
 
 This doesn't do anything in the context of a   command, where you'd use the   option.  
 Setting   disables the insertion of identity values (i.e. the normal behaviour). If you use the   method, you'll need to set   before the command runs to enable identity insertion, then   after the command completes. 
 
 
 Number of Columns is not bcp problem for your case. 
 Most likely reason is Datatype mismatch or FK issue . 
 For debugging .   
 Drop constraints on table  
 OR 
 Create copy of the table ( select * into temptable from table where 1=2)  
 Do BCP to temptable with  **-e**
 option, if there are records in error file then it is datatype/ format issue. 
 If data is copied in temptable then check for all constrain like fk ,ak....  
 