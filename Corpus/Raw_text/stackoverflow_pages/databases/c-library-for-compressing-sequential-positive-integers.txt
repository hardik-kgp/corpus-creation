*** c-library-for-compressing-sequential-positive-integers ***

 I have the very common problem of creating an index for an in-disk array of strings. In short, I need to store the position of each string in the in-disk representation. For example, a very naive solution would be an index array as follows: 
 uint64 idx[] = { 0, 20, 500, 1024, ..., 103434 }; 
 Which says that the first string is at position 0, the second at position 20, the third at position 500 and the nth at position 103434. 
 The positions are always non-negative 64 bits integers in sequential order. Although the numbers could vary by any difference, in practice I expect the typical difference to be inside the range from 2^8 to 2^20. I expect this index to be mmap'ed in memory, and the positions will be accessed randomly (assume uniform distribution). 
 I was thinking about writing my own code for doing some sort of block delta encoding or other more sophisticated encoding, but there are so many different trade-offs between encoding/decoding speed and space that I would rather get a working library as a starting point and maybe even settle for something without any customizations. 
 Any hints? A c library would be ideal, but a c++ one would also allow me to run some initial benchmarks. 
 A few more details if you are still following. This will be used to build a library similar to cdb ( http://cr.yp.to/cdb/cdbmake.html ) on top the library cmph ( http://cmph.sf.net ). In short, it is for a large disk based read only associative map with a small index in memory. 
 Since it is a library, I don't have control over input, but the typical use case that I want to optimize have millions of hundreds of values, average value size in the few kilobytes ranges and maximum value at 2^31. 
 For the record, if I don't find a library ready to use I intend to implement delta encoding in blocks of 64 integers with the initial bytes specifying the block offset so far. The blocks themselves would be indexed with a tree, giving me O(log (n/64)) access time. There are way too many other options and I would prefer to not discuss them. I am really looking forward ready to use code rather than ideas on how to implement the encoding. I will be glad to share with everyone what I did once I have it working. 
 I appreciate your help and let me know if you have any doubts. 
 
 I use  fastbit  (Kesheng Wu LBL.GOV), it seems you need something good, fast and NOW, so fastbit is a highly competient improvement on Oracle's BBC (byte aligned bitmap code, berkeleydb).  It's easy to setup and very good gernally. 
 However, given more time, you may want to look at a  gray code  solution, it seems optimal for your purposes. 
 Daniel Lemire has a number of libraries for C/++/Java released on  code.google , I've read over some of his papers and they are quite nice, several advancements on fastbit and alternative approaches for column re-ordering with permutated grey codes's. 
 Almost forgot, I also came across  Tokyo Cabinet , though I do not think it will be well suited for my current project, I may of considered it more if I had known about it before  ;), it has a large degree of interoperability,  
 
 Tokyo Cabinet is written in the C
  language, and provided as API of C,
  Perl, Ruby, Java, and Lua. Tokyo
  Cabinet is available on platforms
  which have API conforming to C99 and
  POSIX. 
 
 As you referred to CDB, the TC benchmark has a TC mode (TC support's several operational constraint's for varying perf) where it surpassed CDB by 10 times for read performance and 2 times for write.  
 With respect to your delta encoding requirement, I am quite confident in  bsdiff  and it's ability to out-perform any file.exe content patching system, it may also have some fundimental interfaces for your general needs. 
 Google's new binary compression application,  courgette  may be worth checking out, in case you missed the press release, 10x smaller diff's than bsdiff in the one test case I have seen published. 
 
 You have two conflicting requirements:  
 
 You want to compress very small items (8 bytes each).   
 You need efficient random access for each item. 
 
 The second requirement is very likely to impose a fixed length for each item. 
 
 What exactly are you trying to compress? If you are thinking about the total space of index, is it really worth the effort to save the space? 
 If so one thing you could try is to chop the space into half and store it into two tables. First stores (upper uint, start index, length, pointer to second table) and the second would store (index, lower uint). 
 For fast searching, indices would be implemented using something like  B+ Tree . 
 
 I did something similar years ago for a full-text search engine. In my case, each indexed word generated a record which consisted of a record number (document id) and a word number (it could just as easily have stored word offsets) which needed to be compressed as much as possible. I used a delta-compression technique which took advantage of the fact that there would be a number of occurrences of the same word within a document, so the record number often did not need to be repeated at all. And the word offset delta would often fit within one or two bytes. Here is the code I used.  
 Since it's in C++, the code may is not going to be useful to you as is, but can be a good starting point for writing compressions routines. 
 Please excuse the hungarian notation and the magic numbers strewn within the code. Like I said, I wrote this many years ago :-) 
 **IndexCompressor.h**

 
 **IndexCompressor.cpp**

 
 
 You've omitted critical information about the number of strings you intend to index. 
 But given that you say you expect the  minimum  length of an indexed string to be 256, storing the indices as 64% incurs  at most  3% overhead.  If the total length of the string file is less than 4GB, you could use 32-bit indices and incur 1.5% overhead.  These numbers suggest to me that if compression matters,  **you're better off compressing the strings, not the indices**
.  For that problem a variation on LZ77 seems in order. 
 If you want to try a wild idea, put each string in a separate file, pull them all into a zip file, and see how you can do with  .  This probably won't be great, but it's nearly zero work on your part. 
 More data on the problem would be welcome: 
 
 Number of strings 
 Average length of a string 
 Maximum length of a string 
 Median length of strings 
 Degree to which the strings file compresses with  
 Whether you are allowed to change the order of strings to improve compression 
 
 **EDIT**

 The comment and revised question makes the problem much clearer.  I like your idea of grouping, and I would try a simple delta encoding, group the deltas, and use a variable-length code within each group. I wouldn't wire in 64 as the group size–I think you will probably want to determine that empirically.   
 You asked for existing libraries.  For the grouping and delta encoding I doubt you will find much.  For variable-length integer codes, I'm not seeing much in the way of C libraries, but you can find variable-length codings in  Perl  and  Python .  There are a ton of papers and some patents on this topic, and I suspect you're going to wind up having to roll your own.  But there are some simple codes out there, and you could give UTF-8 a try—it can code unsigned integers up to 32 bits, and you can grab C code from  Plan 9  and I'm sure many other sources. 
 
 Are you running on Windows?  If so, I recommend creating the mmap file using naive solution your originally proposed, and then compressing the file using  **NTLM compression**
.  Your application code never knows the file is compressed, and the OS does the file compression for you.  You might not think this would be very performant or get good compression, but I think you'll be surprised if you try it. 
 