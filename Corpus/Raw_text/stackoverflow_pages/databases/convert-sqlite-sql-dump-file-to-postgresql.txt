*** convert-sqlite-sql-dump-file-to-postgresql ***

 I've been doing development using SQLITE database with production in POSTGRESQL.  I just updated my local database with a huge amount of data and need to transfer a specific table to the production database. 
 Based on running  , SQLITE outputs a table dump in the following format: 
 
 How do I convert the above into a POSTGRESQL compatible dump file that I can import into my production server? 
 
 You should be able to feed that dump file straight into  : 
 
 If you want the   column to "auto increment" then change its type from "int" to "serial" in the table creation line. PostgreSQL will then attach a sequence to that column so that INSERTs with NULL ids will be automatically assigned the next available value. PostgreSQL will also not recognize   commands, so these need to be removed. 
 You'll also want to check for   columns in the SQLite schema and change them to   for PostgreSQL (thanks to  Clay  for pointing this out). 
 If you have booleans in your SQLite then you could convert   and   and   and   (respectively) or you could change the boolean column to an integer in the schema section of the dump and then fix them up by hand inside PostgreSQL after the import. 
 If you have BLOBs in your SQLite then you'll want to adjust the schema to use  . You'll probably need to mix in some   calls as well . Writing a quick'n'dirty copier in your favorite language might be easier than mangling the SQL if you a lot of BLOBs to deal with though. 
 As usual, if you have foreign keys then you'll probably want to look into   to avoid insert ordering problems, placing the command inside the BEGIN/COMMIT pair. 
 Thanks to  Nicolas Riley  for the boolean, blob, and constraints notes. 
 If you have   on your code, as generated by some SQLite3 clients, you need to remove them. 
 PostGRESQL also doesn't recognize   columns, you might want to drop that, or add a custom-made constraint such as this: 
 
 While SQLite defaults null values to  , PostgreSQL requires them to be set as  .  
 The syntax in the SQLite dump file appears to be mostly compatible with PostgreSQL so you can patch a few things and feed it to  . Importing a big pile of data through SQL INSERTs might take awhile but it'll work. 
 
 
 I came across this post when searching for a way to convert an SQLite dump to PostgreSQL. Even though this post has an accepted answer (and a good one at that +1), I think adding this is important.  
 I started looking into the solutions here and realized that I was looking for a more automated method. I looked up the wiki docs: 
 https://wiki.postgresql.org/wiki/Converting_from_other_Databases_to_PostgreSQL 
 and discovered  . Pretty cool application and it's relatively easy to use. You can convert the flat SQLite file into a usable PostgreSQL database. I installed from the   and created a   file like this in a test directory: 
 
 like the  docs  state. I then created a   with  : 
 
 I ran the   command like this: 
 
 and then connected to the new database: 
 
 After some queries to check the data, it appears it worked quite well. I know if I had tried to run one of these scripts or do the stepwise conversion mentioned herein, I would have spent much more time. 
 To prove the concept I dumped this   and imported into a development environment on a production server and the data transferred over nicely. 
 
 I wrote a script to do the   to   migration.  It doesn't handle all the schema/data translations mentioned in  https://stackoverflow.com/a/4581921/1303625 , but it does what I needed it to do.  Hopefully it will be a good starting point for others. 
 https://gist.github.com/2253099 
 
 The  sequel gem  (a Ruby library) offers data copying across different databases:
 http://sequel.jeremyevans.net/rdoc/files/doc/bin_sequel_rdoc.html#label-Copy+Databases 
 First install Ruby, then install the gem by running  . 
 In case of sqlite, it would be like this:
 
 
 You can use a one liner, here is an example with the help of sed command: 
 
 
 I have tried editing/regexping the sqlite dump so PostgreSQL accepts it, it is tedious and prone to error. 
 What I got to work really fast: 
 First recreate the schema on PostgreSQL without any data, either editing the dump or if you were using an ORM you may be lucky and it talks to both back-ends (sqlalchemy, peewee, ...). 
 Then migrate the data using pandas. Suppose you have a table with a bool field (which is 0/1 in sqlite, but must be t/f in PostgreSQL) 
 
 This works like a charm, is easy to write, read and debug each function, unlike (for me) the regular expressions. 
 Now you can try to load the resulting csv with PostgreSQL (even graphically with the admin tool), with the only caveat that you must load the tables with foreign keys after you have loaded the tables with the corresponding source keys. I did not have the case of a circular dependency, I guess you can suspend temporarily the key checking if that is the case. 
 
 pgloader work wonders on converting database in sqlite to postgresql. 
 Here's an example on converting a local sqlitedb to a remote PostgreSQL db: 
 pgloader  **sqlite.db**
 postgresql:// **username**
: **password**
@ **hostname**
/ **dbname**

 