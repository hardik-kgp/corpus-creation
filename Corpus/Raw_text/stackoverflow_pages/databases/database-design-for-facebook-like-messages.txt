*** database-design-for-facebook-like-messages ***

 
 
 
 
 
 
 
 
                            As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened,  visit the help center  for guidance.
                            
                         
 
 
 Closed  7 years ago . 
 
 
 I am currently planning a new system in PHP/MySQL and want to make sure my database can handle the amount of data that I am planning to store. One of the features of my new project is a "messages" feature like Facebook. I want to make sure I create the best possible experience for the end user. The website will eventually handle 1000's of users with potentially millions of messages collectively. What would be the best approach for the database design? Is MySQL even the right database to use? 
 
 MySQL has no problem with millions or hundreds of millions of records as long as you design you database correctly. 
 That being said, a "messages feature like Facebook" is a pretty broad definition.  Generally, you would define a   table that links each message to the user that created it (ie, have a   column in the messages table).  If you want messages to go to multiple users, you have a   table defining the 1-to-many relationship by storing multiple records consisting of the   and a  .  Add the proper indexes to these tables and you're 80% of the way there.   
 That being said, that remaining 20% can be a killer.  Unfortunately, how you use your database is going to determine what else you need to do, and you'd have to provide a lot more detail about your application before those judgments can be made.  For example, you might wish to consider having auto-archiving solution which keeps the main table relatively small, and moves old data to backup tables that can be accessed if necessary.  You probably won't need this right away, but it could help in the future. 
 
 Facebook started with MySQL and they only moved to  Cassandra  when they had 7TB of inbox data for over 100 million users.  
 Source:  Lakshman, Malik: Cassandra - A Decentralized Structured Storage System . 
 
 If you are planning to handle large amounts of data (of course millions doesn't even come close to qualifying as large), then hire a datbase professional. Efficient and effective database design for large data sets is a complex issue and requires a specialist. 
 In answer to your question yes mysql can handle millions of records easily if the design is good and will be a nightmare if the design is bad, pretty much like any other modern datbase. 
 
 **If**
 you design your database correctly, the performance should deteriorate  logarithmically  with the amount of data. In other words, the time for executing  your queries will grow much slower than the amount of data. 
 To achieve this goal, you'll have to be disciplined about a number of things: 
 
 Your database design must be sound. Understanding  ER modeling  and normalization is essential. So is understanding the  anatomy of indexes  and other physical data structures. 
 After you have a nice normalized database, consider if some "edges" of it should be judiciously denormalized purely for performance reasons. 
 Throughout this whole process, keep in mind what kind of queries will your client application 1  do:
 
 Design indexes accordingly - index specifically to queries you know you'll need, don't over-index! 
 Some design decisions, such as usage of natural vs. surrogate keys and identifying vs. non-identifying relationships might influence the amount of JOINs you'll need. 
 Try to keep your database design friendly to clustered range scans,  index-only scans  etc. 
 
 Use DBMS-specific mechanisms, such as  clustering , partitioning, key compression, materialized views (etc..) to your advantage. If the DBMS doesn't support some mechanism you deem essential, don't be afraid to switch the DBMS! For example,  InnoDB tables are always clustered , which is an advantage when querying on PK, but can be a disadvantage if you need secondary indexes. If you need both clustered and heap-based tables, use some DBMS that supports them both (such as Oracle or MS SQL Server). 2 
 Code the client application carefully. Religiously use bound parameters and query  preparation  - not only will you minimize the SQL parsing and query planning overhead, but will be SQL-injection-resistant as well! ORMs and libraries will often shield you from doing it manually, but you should still understand what's going on "under the covers". 
 And last but not least, don't relay on assumptions -  **measure**
 instead! Database performance can be a fine (and rather complex) balancing act, and the impact of certain decisions might not be immediately obvious 
 
 If you do all this correctly, you'll have to approach the actual Facebook's amounts of data before a "classic" DBMS stops being adequate. 1000s of users and millions or messages don't even qualify as "large" in this context. 
 
 1  A "client" from the DBMS perspective - this could be a middle tier as well. 
 2  The MyISAM is also not clustered, but has serious limitations (such as absence of transactions support) that should disqualify it from normal use anyway. 
 
 If you are on a budget, start with MySQL and use a system like Zend::DB or on a higher level Doctrine. 
 It's more important to make it easy to switch DMBS then to choose your DBMS at the beginning. 
 
 As long as you setup your tables to be relational and set the relationships between tables, MySQL should be fine. 
 Might I also suggest Postgres? 
 
 You are not very precise on what you want to learn. Okay. I'll try to give you some advice. 
 
 Normalization 
 Indexes 
 MyISAM for tables under high load 
 Denormalization (sic!), but you should understand what are you doing 
 Sharding 
 Minimalistic DB layer for flexibility 
 
 
 If you mean "what should my mysql table look like for a message system", I use the following columns in my message system: 
 
 Message_id is auto_increment, obviously.  Fromuser and touser are obvious.  Fromstatus and tostatus is active, deleted, purge, draft, and likewise.  Fromview and toview are set to 'yes' and 'no'.  Title, text, and 'poston' date are obvious.  Thread might take a little effort on your part depending on your html forms and message display scripts. 
 For your form, create a foreach loop based on the "to:" field and save a copy for each recipient. 
 I expect this message system to hold millions, but that millions is probably a couple of years away.  I'm keeping it small and simple. 
 
 Sharding is certainly not necessary for your "broadly" based requirements...I have dealt with a fair amount of data and didn't even consider partitioned tables and shard implementation until there were numerous tables housing over a billion records (then joining those could get a little slow). Index your tables with smart keys, and you may even consider using an eav type structure to keep the tables narrow and relieve yourself of null returns on queries. 
 Above was written while half asleep so ignore typos ;) 
 
 I'd say read about object oriented databases as well as nosql systems, it is a very interesting concept, actively used by famous frameworks like Ruby on rails, which allows you to worry less about your data, since you can simply dump your object straight into the database, I know it is a little off topic but less complex databases mean easier transition 
into scalable systems, and I'm just spreading awareness 
 However the tradeoff is, not having an as strong userbase as relational databases, which makes it harder to find answers to problems as you go along, and an equally longer amount of time it takes to adapt into using it, but consisting data without thinking about database design at every stage writing your business logic is an amazing thing to have and quickens your development time, however later on, when you face bottle necks and performance issues it will be harder to solve since there is less help around. 
 