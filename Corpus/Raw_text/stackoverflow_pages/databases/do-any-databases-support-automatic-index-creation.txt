*** do-any-databases-support-automatic-index-creation ***

 Why don't databases automatically index tables based on query frequency?  Do any tools exist to analyze a database and the queries it is receiving, and automatically create, or at least suggest which indexes to create? 
 I'm specifically interested in MySQL, but I'd be curious for other databases as well. 
 
 There are database optimizers that can be enabled or attached to databases to suggest (and in some cases perform) indexes that might help things out. 
 However, it's not actually a trivial problem, and when these aids first came out users sometimes found it actually slowed their databases down due to inferior optimizations. 
 Lastly, there's a LOT of money in the industry for database architects, and they prefer the status quo. 
 Still, databases are becoming more intelligent.  If you use SQL server profiler with Microsoft SQL server you'll find ways to speed your server up.  Other databases have similar profilers, and there are third party utilities to do this work. 
 But if you're the one writing the queries, hopefully you know enough about what you're doing to index the right fields.  If not then having the right indexes is likely the least of your problems... 
 -Adam 
 
 That is a best question I have seen on stackoverflow.  Unfortunately I don't have an answer.  Google's bigtable does automatially index the right columns, but BigTable doesn't allow arbitrary joins so the problem space is much smaller. 
 The only answer I can give is this: 
 One day someone asked, "Why can't the computer just analyze my code and and compile & statically type the pieces of code that run most often?"   
 People are solving this problem today (e.g. Tamarin in FF3.1), and I think "auto-indexing" relational databases is the same class of problem, but it isn't as much a priority.  A decade from now, manually adding indexes to a database will be considered a waste of time.  For now, we are stuck with monitoring slow queries and running optimizers.   
 
 MS SQL 2005 also maintains an internal reference of suggested indexes to create based on usage data. It's not as complete or accurate as the Tuning Advisor, but it is automatic. Research dm_db_missing_index_groups for more information. 
 
 There is a script on I think an MS SQL blog with a script for suggesting indexes in SQL 2005 but I can't find the exact script right now! Its just the thing from the description as I recall. Here's a link to some more info  http://blogs.msdn.com/bartd/archive/2007/07/19/are-you-using-sql-s-missing-index-dmvs.aspx 
 PS just for SQL Server 2005 + 
 
 There are tools out there for this. 
 For MS SQL, use the SQL Profiler (to record activity against the database), and the Database Engine Tuning Advisor (SQL 2005) or the Index Tuning Wizard (SQL 2000) to analyze the activities and recommend indexes or other improvements. 
 
 Yes, some engines DO support automatic indexing. One such example for mysql is Infobright, their engine does not support "conventional" indexes and instead implicitly indexes everything - this is a column-based storage engine. 
 The behaviour of such engines tends to be very different from what developers (And yes, you need ot be a DEVELOPER to even be thinking about using Infobright; it is not a plug-in replacement for a standard engine) expect. 
 
 I agree with what Adam Davis says in his comment.  I'll add that if such a mechanism existed to create indexes automatically, the most common reaction to this feature would be, "That's nice...  How do I turn it off?" 
 
 Part of the reason may be that indexes don't just give a small speedup. If you don't have a suitable index on a large table queries can run so slowly that the application is entirely unusable, and possibly if it is interacting with other software it simply won't work. So you really need the indexes to be right before you start trying to use the application. 
 Also, rather than building an index in the background, and slowing things down further while it's being built, it is better to have the index defined before you start adding significant amounts of data. 
 I'm sure we'll get more tools that take sample queries and work out what indexes are necessary; also probably we will eventually get databases that do as you suggest and monitor performance and add indexes they think are necessary, but I don't think they will be a replacement for starting off with the right indexes. 
 
 Seems that MySQL doesn't have a user-friendly profiler. Maybe you want to try something like  this , a php class based in MySQL profiler. 
 
 Amazon's SimpleDB has automatic indexing on all columns based on your usage: 
 http://aws.amazon.com/simpledb/ 
 It has other limitations though: 
 
 It's a key-value store, not an RDB. Obviously that means slow joins (and no built-in join support). 
 It has a 10gb limit on table size. There are libraries that will handle partitioning big data for you although this locks you into that library's way of doing things, which can have its own problems. 
 It stores all values as strings, even numbers, which makes sorting a column with a 1,9, and 10 come out like 1,10,9 unless you use a library which hacks this by 0 padding. This also impacts negative numbers. 
 
 The 10gb limit is bigger than many might assume, so you could proceed with this for a simple site that you plan on rewriting if it ever hits big. 
 It's unfortunate this kind of automatic indexing didn't make it into DynamoDb, which appears to have replaced it - they don't even mention SimpleDb in their Product list anymore, you have to find it through old links to it. 
 
 Google App Engine  does that (see the index.yaml file). 
 