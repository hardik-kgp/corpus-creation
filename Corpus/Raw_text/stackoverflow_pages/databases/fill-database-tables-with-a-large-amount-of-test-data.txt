*** fill-database-tables-with-a-large-amount-of-test-data ***

 I need to load a table with a large amount of test data. This is to be used for testing performance and scaling. 
 How can I easily create 100,000 rows of random/junk data for my database table? 
 
 You could also use a  stored procedure . Consider the following table as an example: 
 
 Then you could add a stored procedure like this: 
 
 When you call it, you'll have 100k records: 
 
 
 For multiple row cloning (data duplication) you could use 
 
 
 If you want more control over the data, try something like this (in PHP): 
 
 where function generate_test_values would return a string formatted like "('val1', 'val2', ...)".  If this takes a long time, you can batch them so you're not making so many db calls, e.g.: 
 
 would only run 10000 queries, each adding 10 rows. 
 
 Here it's solution with pure math and sql: 
 
 
 I've created a ruby script that can insert in practically "any" database that doesn't have foreign key validations between tables, and it insert random data, so you can benchmark the database with some data in it.
I'll be creating a gem later (when I have some free time) from this GIST ->
 https://gist.github.com/carlosveucv/137ea32892ef96ab496def5fcd21858b 
 
 try  filldb 
 you can either post your schema or use existing schema and generate dummy data and export from this site and import in your data base. 
 