*** how-are-varchar-values-stored-in-a-sql-server-database ***

 My fellow programmer has a strange requirement from his team leader; he insisted on creating   columns with a length of 16*2 n . 
 What is the point of such restriction? 
 I can suppose that short strings (less than 128 chars for example) a stored directly in the record of the table and from this point of view the restriction will help to align fields in the record, larger strings are stored in the database "heap" and only the reference to this string is saved in the table record. 
 Is it so?  
 Is this requirement has a reasonable background? 
 BTW, the DBMS is SQL Server 2008. 
 
 Completely pointless restriction as far as I can see. Assuming standard   format (as opposed to the formats used with row/page compression or sparse columns) and assuming you are talking about   columns 
 All   data is stored at the end of the row in a variable length section (or in offrow pages if it can't fit in row). The amount of space it consumes in that section (and whether or not it ends up off row) is entirely dependant upon the length of the actual data not the column declaration. 
 SQL Server will use the length declared in the column declaration when allocating memory (e.g. for   operations). The assumption it makes in that instance is that   columns will be  filled to 50% of their declared size on average  so this might be a better thing to look at when choosing a size. 
 
 I have heard of this practice before, but after researching this question a bit I don't think there is a practical reason for having varchar values in multiples of 16. I think this requirement probably comes from trying to optimize the space used on each page. In SQL Server, pages are set at 8 KB per page. Rows are stored in pages, so perhaps the thinking is that you could conserve space on the pages if the size of each row divided evenly into 8 KB (a more detailed description of how SQL Server stores data can be found  here ). However, since the amount of space used by a varchar field is determined by its actual content, I don't see how using lengths in multiples of 16 or any other scheme could help you optimize the amount of space used by each row on the page. The length of the varchar fields should just be set to whatever the business requirements dictate. 
 Additionally, this question covers similar ground and the conclusion also seems to be the same: 
 Database column sizes for character based data 
 
 You should always store the data in the data size that matches the data being stored. It is part of how the database can maintain integrity. For instance suppose you are storing email addresses. If your data size is the size of the maximum allowable emailaddress, then you will not be able to store bad data that is larger than that. That is a good thing. Some people want to make everything nvarchar(max) or varchar(max). However, this causes only indexing problems.  
 Personally I would have gone back to the person who make this requirement and asked for a reason. Then I would have presented my reasons as to why it might not be a good idea. I woul never just blindly implement something like this. In pushing back on a requirement like this, I would first do some research into how SQL Server organizes data on the disk, so I could show the impact of the requirement is likely to have on performance. I might even be surprised to find out the requirement made sense, but I doubt it at this point.  
 