*** how-do-i-interpret-precision-and-scale-of-a-number-in-a-database ***

 I have the following column specified in a database: decimal(5,2) 
 How does one interpret this? 
 According to the properties on the column as viewed in SQL Server Management studio I can see that it means: decimal(Numeric precision, Numeric scale). 
 What do precision and scale mean in real terms? 
 It would be easy to interpret this as a decimal with 5 digits and two decimals places...ie 12345.12 
 P.S. I've been able to determine the correct answer from a colleague but had great difficulty finding an answer online. As such, I'd like to have the question and answer documented here on stackoverflow for future reference. 
 
 Numeric precision refers to the maximum number of digits that are present in the number.  
 ie 1234567.89 has a precision of 9 
 Numeric scale refers to the maximum number of decimal places 
 ie 123456.789 has a scale of 3 
 Thus the maximum allowed value for decimal(5,2) is 999.99 
 
 Precision of a number is the number of digits. 
 Scale of a number is the number of digits after the decimal point. 
 What is generally implied when setting precision and scale on field definition is that they represent  **maximum**
 values. 
 Example, a decimal field defined with   and   would allow the following values: 
 
  (p=5,s=2) 
  (p=4,s=2) 
  (p=5,s=0) 
  (p=4,s=1) 
  (p=0,s=0) 
 
 The following values are not allowed or would cause a data loss: 
 
  (p=5,s=3) => could be truncated into   (p=4,s=2) 
  (p=6,s=2) => could be truncated into   (p=5,s=1) 
  (p=6,s=3) => could be truncated into   (p=5,s=2) 
  (p=6,s=0) => out of range 
 
 Note that the range is generally defined by the precision:   ... 
 
 Precision, Scale, and Length  in the SQL Server 2000 documentation reads: 
 
 Precision is the number of digits in a number. Scale is the number of digits to the right of the decimal point in a number. For example, the number 123.45 has a precision of 5 and a scale of 2. 
 
 