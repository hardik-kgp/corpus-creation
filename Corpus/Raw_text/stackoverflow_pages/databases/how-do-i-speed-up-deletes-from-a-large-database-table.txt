*** how-do-i-speed-up-deletes-from-a-large-database-table ***

 Here's the problem I am trying to solve: I have recently completed a data layer re-design that allows me to load-balance my database across multiple shards.  In order to keep shards balanced, I need to be able to migrate data from one shard to another, which involves copying from shard A to shard B, and then deleting the records from shard A.  But I have several tables that are very big, and have many foreign keys pointed to them, so deleting a single record from the table can take more than one second. 
 In some cases I need to delete millions of records from the tables, and it just takes too long to be practical. 
 Disabling foreign keys is not an option.  Deleting large batches of rows is also not an option because this is a production application and large deletes lock too many resources, causing failures.  I'm using Sql Server, and I know about partitioned tables, but the restrictions on partitioning (and the license fees for enterprise edition) are so unrealistic that they are not possible. 
 When I began working on this problem I thought the hard part would be writing the algorithm that figures out how to delete rows from the leaf level up to the top of the data model, so that no foreign key constraints get violated along the way.  But solving that problem did me no good since it takes weeks to delete records that need to disappear overnight. 
 I already built in a way to mark data as virtually deleted, so as far as the application is concerned, the data is gone, but I'm still dealing with large data files, large backups, and slower queries because of the sheer size of the tables. 
 Any ideas?  I have already read older related posts here and found nothing that would help. 
 
 Please see:  Optimizing Delete on SQL Server 
 This MS support article might be of interest:  How to resolve blocking problems that are caused by lock escalation in SQL Server : 
 
 **Break up large batch operations into several smaller operations**
. For
  example, suppose you ran the following
  query to remove several hundred
  thousand old records from an audit
  table, and then you found that it
  caused a lock escalation that blocked
  other users:  
 
 By removing these records a few
  hundred at a time, you can
  dramatically reduce the number of
  locks that accumulate per transaction
  and prevent lock escalation. For
  example: 
 
 **Reduce the query's lock footprint by making the query as efficient as
  possible.**
 Large scans or large
  numbers of Bookmark Lookups may
  increase the chance of lock
  escalation; additionally, it increases
  the chance of deadlocks, and generally
  adversely affects concurrency and
  performance. 
 
 
 
 You could achieve the same result using   as suggested by Mitch but  according to MSDN  it won't be supported for   and some other operations in future versions of SQL Server: 
 
 Using SET ROWCOUNT will not affect DELETE, INSERT, and UPDATE
  statements in a future release of SQL Server. Avoid using SET ROWCOUNT
  with DELETE, INSERT, and UPDATE statements in new development work,
  and plan to modify applications that currently use it. For a similar
  behavior, use the TOP syntax. For more information, see TOP
  (Transact-SQL). 
 
 
 You could create new files, copy all but the "deleted" rows, then swap the names on the tables. Finally, drop the old tables. If you're deleting a large percentage of the records, then this may actually be faster.  
 
 Another suggestion is to rename the table and add a status column. When status = 1 (deleted), then you won't want it to show. So you then create a view with the same name as the orginal table which selects from the table when status is null or = 0 (depending on how you implement it). The deletion appears immediate to the user and a background job can run every fifteen minutes deleting records that runs without anyone other than the dbas being aaware of it. 
 
 If you're using SQL 2005 or 2008, perhaps using "snapshot isolation" would help you. It allows the data to remain visible to users while there's an underlying data update operation processing, and then reveals the data as soon as it's committed. Even if you delete takes 30 minutes to run, your applications would stay online during this time. 
 Here's a quick primer of snapshot locking: 
 http://www.mssqltips.com/tip.asp?tip=1081 
 Though you should still try to speed up your delete so it's as quick as possible, this may alleviate some of the burden. 
 
 You can delete small batches using a while loop, something like this: 
 
 
 here is the solution to your problem. 
 
 