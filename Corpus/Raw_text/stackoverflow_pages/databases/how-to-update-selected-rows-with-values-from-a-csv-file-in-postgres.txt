*** how-to-update-selected-rows-with-values-from-a-csv-file-in-postgres ***

 I'm using Postgres and would like to make a big update query that would pick up from a CSV file, lets say I got a table that's got  . 
 I'd like to run an update that changes the Bananas and not the Apples, each new Banana and their ID would be in a CSV file. 
 I tried looking at the Postgres site but the examples are killing me. 
 
  the file to a temporary staging table and update the actual table from there. Like: 
 
 If the imported table matches the table to be updated exactly, this may be convenient: 
 
 Creates an empty temporary table matching the structure of the existing table, without constraints. 
 **Privileges**

 SQL   requires superuser privileges for this. ( The manual ): 
 
  naming a file or command is only allowed to database
  superusers, since it allows reading or writing any file that the
  server has privileges to access. 
 
 The  **psql**
 meta-command   works for any db role.  The manual: 
 
 Performs a frontend (client) copy. This is an operation that runs an
  SQL   command, but instead of the server reading or writing the
  specified file, psql reads or writes the file and routes the data
  between the server and the local file system. This means that file
  accessibility and privileges are those of the local user, not the
  server, and no SQL superuser privileges are required. 
 
 The scope of temporary tables is limited to a single  session  of a single role, so the above has to be executed in the same psql session: 
 
 If you are scripting this in a bash command, be sure to wrap it all in a  single  psql call. Like: 
 
 Normally, you need the meta-command   to switch between psql meta commands and SQL comands in psql, but   is an exception to this rule. The manual again: 
 
 special parsing rules apply to the   meta-command. Unlike most other meta-commands, the entire remainder of the line is always taken to be the arguments of  , and neither variable interpolation nor backquote expansion are performed in the arguments. 
 
 **Big tables**

 If the import-table is big it may pay to increase   temporarily for the session (first thing in the session): 
 
 Add an index to the temporary table: 
 
 And run   manually, since temporary tables are not covered by autovacuum / auto-analyze. 
 
 Related answers:   
 
 Best way to delete millions of rows by ID   
 How can I insert common data into a temp table from disparate schemas?   
 How to delete duplicate entries? 
 
 
 You can try the below code written in python, the input file is the csv file whose contents you want to update into the table. Each row is split based on comma so for each row, row[0]is the value under first column, row[1] is value under second column etc. 
 
 