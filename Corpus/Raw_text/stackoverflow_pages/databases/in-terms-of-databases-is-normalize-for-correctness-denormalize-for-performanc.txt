*** in-terms-of-databases-is-normalize-for-correctness-denormalize-for-performanc ***

 Normalization leads to many essential and desirable characteristics, including aesthetic pleasure. Besides it is also theoretically "correct". In this context, denormalization is applied as a compromise, a correction to achieve performance.
Is there any reason other than performance that a database could be denormalized? 
 
 The two most common reasons to denormalize are: 
 
 Performance 
 Ignorance 
 
 The former should be verified with profiling, while the latter should be corrected with a rolled-up newspaper ;-) 
 I would say a better mantra would be "normalize for correctness, denormalize for speed - and only when necessary" 
 
 To fully understand the import of the original question, you have to understand something about team dynamics in systems development, and the kind of behavior (or misbehavior) different roles / kinds of people are predisposed to.  Normalization is important because it isn't just a dispassionate debate of design patterns -- it also has a lot to do with how systems are designed and managed over time. 
 Database people are trained that data integrity is a paramount issue.  We like to think in terms of 100% correctness of data, so that once data is in the DB, you don't have to think about or deal with it ever being logically wrong.  This school of thought places a high value on normalization, because it causes (forces) a team to come to grips with the underlying logic of the data & system.  To consider a trivial example -- does a customer have just one name & address, or could he have several? Someone needs to decide, and the system will come to depend on that rule being applied consistently.  That sounds like a simple issue, but multiply that issue by 500x as you design a reasonably complex system and you will see the problem -- rules can't just exist on paper, they have to be enforced.  A well-normalized database design (with the additional help of uniqueness constraints, foreign keys, check values, logic-enforcing triggers etc.) can help you have a well-defined core data model and data-correctness rules, which is really important if you want the system to work as expected when many people work on different parts of the system (different apps, reports, whatever) and different people work on the system over time.  Or to put it another way -- if you don't have some way to define and operationally enforce a solid core data model, your system will suck. 
 Other people (often, less experienced developers) don't see it this way.  They see the database as at best a tool that's enslaved to the application they're developing, or at worst a bureaucracy to be avoided.  (Note that I'm saying "less experienced" developers.  A good developer will have the same awareness of the need for a solid data model and data correctness as a database person.  They might differ on what's the best way to achieve that, but in my experience are reasonably open to doing those things in a DB layer as long as the DB team knows what they're doing and can be responsive to the developers).  These less experienced folks are often the ones who push for denormalization, as more or less an excuse for doing a quick & dirty job of designing and managing the data model.  This is how you end up getting database tables that are 1:1 with application screens and reports, each reflecting a different developer's design assumptions, and a complete lack of sanity / coherence between the tables.  I've experienced this several times in my career.  It is a disheartening and deeply unproductive way to develop a system. 
 So one reason people have a strong feeling about normalization is that the issue is a stand-in for other issues they feel strongly about.  If you are sucked into a debate about normalization, think about the underlying (non-technical) motivation that the parties may be bringing to the debate. 
 Having said that, here's a more direct answer to the original question :) 
 It is useful to think of your database as consisting of a core design that is as close as possible to a logical design -- highly normalized and constrained -- and an extended design that addresses other issues like stable application interfaces and performance. 
 You should want to constrain and normalize your core data model, because to not do that compromises the fundamental integrity of the data and all the rules / assumptions your system is being built upon.  If you let those issues get away from you, your system can get crappy pretty fast.  Test your core data model against requirements and real-world data, and iterate like mad until it works.  This step will feel a lot more like clarifying requirements than building a solution, and it should.  Use the core data model as a forcing function to get clear answers on these design issues for everyone involved. 
 Complete your core data model before moving on to the extended data model.  Use it and see how far you can get with it.  Depending on the amount of data, number of users and patterns of use, you may never need an extended data model.  See how far you can get with indexing plus the 1,001 performance-related knobs you can turn in your DBMS. 
 If you truly tap out the performance-management capabilities of your DBMS, then you need to look at extending your data model in a way that adds denormalization.  Note this is not about denormalizing your core data model, but rather adding new resources that handle the denorm data.  For example, if there are a few huge queries that crush your performance, you might want to add a few tables that precompute the data those queries would produce -- essentially pre-executing the query.  It is important to do this in a way that maintains the coherence of the denormalized data with the core (normalized) data.  For example, in DBMS's that support them, you can use a MATERIALIZED VIEW to make the maintenance of the denorm data automatic.  If your DBMS doesn't have this option, then maybe you can do it by creating triggers on the tables where the underlying data exists. 
 There is a world of difference between  selectively  denormalizing a database in a  coherent  manner to deal with a realistic performance challenge vs. just having a weak data design and using performance as a justification for it. 
 When I work with low-to-medium experienced database people and developers, I insist they produce an absolutely normalized design ... then later may involve a small number of more experienced people in a discussion of selective denormalization.  Denormalization is more or less always bad in your core data model.  Outside the core, there is nothing at all wrong with denormalization if you do it in a considered and coherent way. 
 In other words, denormalizing from a normal design to one that preserves the normal while adding some denormal -- that deals with the physical reality of your data while preserving its essential logic -- is fine.  Designs that don't have a core of normal design -- that shouldn't even be called de-normalized, because they were never normalized in the first place, because they were never consciously designed in a disciplined way -- are not fine. 
 Don't accept the terminology that a weak, undisciplined design is a "denormalized" design.  I believe the confusion between intentionally / carefully denormalized data vs. plain old crappy database design that results in denormal data because the designer was a careless idiot is the root cause of many of the debates about denormalization. 
 
 Denormalization normally means some improvement in retrieval efficiency (otherwise, why do it at all), but at a huge cost in complexity of validating the data during modify (insert, update, sometimes even delete) operations.  Most often, the extra complexity is ignored (because it is too damned hard to describe), leading to bogus data in the database, which is often not detected until later - such as when someone is trying to work out why the company went bankrupt and it turns out that the data was self-inconsistent because it was denormalized. 
 I think the mantra should go "normalize for correctness, denormalize only when senior management offers to give your job to someone else", at which point you should accept the opportunity to go to pastures new since the current job may not survive as long as you'd like. 
 Or "denormalize only when management sends you an email that exonerates you for the mess that will be created". 
 Of course, this assumes that you are confident of your abilities and value to the company. 
 
 Mantras almost always oversimplify their subject matter.  This is a case in point. 
 The advantages of normalizing are more that merely theoretic or aesthetic.  For every departure from a normal form for 2NF and beyond, there is an update anomaly that occurs when you don't follow the normal form and that goes away when you do follow the normal form.  Departure from 1NF is a whole different can of worms, and I'm not going to deal with it here. 
 These update anomalies generally fall into inserting new data, updating existing data, and deleting rows.  You can generally work your way around these anomalies by clever, tricky programming.  The question then is was the benefit of using clever, tricky programming worth the cost.  Sometimes the cost is bugs.  Sometimes the cost is loss of adaptability.  Sometimes the cost is actually, believe it or not, bad performance.   
 If you learn the various normal forms, you should consider your learning incomplete until you understand the accompanying update anomaly.   
 The problem with "denormalize" as a guideline is that it doesn't tell you what to do.  There are myriad ways to denormalize a database.  Most of them are unfortunate, and that's putting it charitably.  One of the dumbest ways is to simply denormalize one step at a time,  every time you want to speed up some particular query.  You end up with a crazy mish mosh that cannot be understood without knowing the history of the application. 
 A lot of denormalizing steps that "seemed like a good idea at the time"  turn out later to be very bad moves.   
 Here's a better alternative, when you decide not to fully normalize:  adopt some design discipline that yields certain benefits, even when that design discipline departs from full normalization.  As an example,  there is star schema design,  widely used in data warehousing and data marts.  This is a far more coherent and disciplined approach than merely denormalizing by whimsy.  There are specific benefits you'll get out of a star schema design, and you can contrast them with the update anomalies you will suffer because star schema design contradicts normalized design. 
 In general, many people who design star schemas are building a secondary database, one that does not interact with the OLTP application programs.  One of the hardest problems in keeping such a database current is the so called ETL  (Extract, Transform, and Load) processing.  The good news is that all this processing can be collected in a handful of programs,  and the application programmers who deal with the normalized OLTP database don't have to learn this stuff.  There are tools out there to help with ETL,  and copying data from a normalized OLTP database to a star schema data mart or warehouse is a well understood case.   
 Once you have built a star schema, and if you have chosen your dimensions well, named your columns wisely, and especially chosen your granularity well,  using this star schema with OLAP tools like Cognos or Business Objects turns out to be almost as easy as playing a video game.  This permits your data analysts to focus on analysing the data instead of learning how the container of the data works. 
 There are other designs besides star schema that depart from normalization,  but star schema is worth a special mention.   
 
 Data warehouses in a dimensional model are often modelled in a (denormalized) star schema.  These kinds of schemas are not (normally) used for online production or transactional systems. 
 The underlying reason is performance, but the fact/dimensional model also allows for a number of temporal features like slowly changing dimensions which are doable in traditional ER-style models, but can be incredibly complex and slow (effective dates, archive tables, active records, etc). 
 
 Don't forget that each time you denormalize part of your database, your capacity to further adapt it decreases, as risks of bugs in code increases, making the whole system less and less sustainable. 
 Good luck! 
 
 Normalization has nothing to do with performance. I can't really put it better than Erwin Smout did in this thread:
 What is the resource impact from normalizing a database? 
 Most SQL DBMSs have limited support for changing the physical representation of data without also compromising the logical model, so unfortunately that's one reason why you may find it necessary to demormalize. Another is that many DBMSs don't have good support for multi-table integrity constraints, so as a workaround to implement those constraints you may be forced to put extraneous attributes into some tables. 
 
 Database normalization isn't just for theoretical correctness, it can help to prevent data corruption. I certainly would NOT denormalize for "simplicity" as @aSkywalker suggests. Fixing and cleaning corrupted data is anything but simple. 
 
 You don't normalize for 'correctness' per se. Here is the thing: 
 Denormalized table has the benefit of increasing performance but requires redundancy and more developer brain power. 
 Normalized tables has the benefit of reducing redundancy and increasing ease of development but requires performance. 
 It's almost like a classic balanced equation. So depending on your needs (such as how many that are hammering your database server) you should stick with normalized tables unless it is really needed. It is however easier and less costly for development to go from normalized to denormalized than vice versa. 
 
 No way. Keep in mind that what you're supposed to be normalizing is your relations (logical level), not your tables (physical level). 
 
 Denormalized data is much more often found at places where not enough normalization was done.  
 My mantra is 'normalize for correctness, eliminate for performance'. RDBMs are very flexible tools, but optimized for the OLTP situation. Replacing the RDBMS by something simpler (e.g. objects in memory with a transaction log) can help a lot. 
 
 I take issue with the assertion by folks here that Normalized databases are always associated with simpler, cleaner, more robust code.  It is certainly true that there are many cases where fully normalized are associated with simpler code than partially denormalized code, but at best this is a guideline, not a law of physics. 
 Someone once defined a word as the skin of a living idea.  In CS, you could say an object or table is defined in terms of the needs of the problem and the existing infrastructure, instead of being a platonic reflection of an ideal object.  In theory, there will be no difference between theory and practice, but in practice, you do find variations from theory.  This saying is particularly interesting for CS because one of the focuses of the field is to find these differences and to handle them in the best way possible. 
 Taking a break from the DB side of things and looking at the coding side of things, object oriented programming has saved us from a lot of the evils of spaghetti-coding, by grouping a lot of closely related code together under an object-class name that has an english meaning that is easy to remember and that somehow fits with all of the code it is associated with.  If too much information is clustered together, then you end up with large amounts of complexity within each object and it is reminiscent of spaghetti code.  If you make the clusters to small, then you can't follow threads of logic without searching through large numbers of objects with very little information in each object, which has been referred to as "Macaroni code". 
 If you look at the trade-off between the ideal object size on the programming side of things and the object size that results from normalizing your database, I will give a nod to those that would say it is often better to chose based on the database and then work around that choice in code.  Especially because you have the ability in some cases to create objects from joins with hibernate and technologies like that.  However I would stop far short of saying this is an absolute rule.  Any OR-Mapping layer is written with the idea of making the most complex cases simpler, possibly at the expense of adding complexity to the most simple cases.  And remember that complexity is not measured in units of size, but rather in units of complexity.  There are all sorts of different systems out there.  Some are expected to grow to a few thousand lines of code and stay there forever.  Others are meant to be the central portal to a company's data and could theoretically grow in any direction without constraint.  Some applications manage data that is read millions of times for every update.  Others manage data that is only read for audit and ad-hoc purposes.  In general the rules are: 
 
 Normalization is almost always a good idea in medium-sized apps or larger when data on both sides of the split can be modified and the potential modifications are independent of each other. 
 Updating or selecting from a single table is generally simpler than working with multiple tables, however with a well-written OR, this difference can be minimized for a large part of the data model space.  Working with straight SQL, this is almost trivial to work around for an individual use case, albeit it in a non-object-oriented way. 
 Code needs to be kept relatively small to be manage-able and one effective way to do this is to divide the data model and build a service-oriented architecture around the various pieces of the data model.  The goal of an optimal state of data (de)normalization should be thought of within the paradigm of your overall complexity management strategy. 
 
 In complex object hierarchies there are complexities that you don't see on the database side, like the cascading of updates.  If you model relational foreign keys and crosslinks with an object ownership relationship, then when updating the object, you have to decide whether to cascade the update or not.  This can be more complex than it would be in sql because of the difference between doing something once and doing something correctly always, sort of like the difference between loading a data file and writing a parser for that type of file.  The code that cascades an update or delete in C++, java, or whatever will need to make the decision correctly for a variety of different scenarios, and the consequences of mistakes in this logic can be pretty serious.  It remains to be proven that this can never be simplified with a bit of flexibility on the SQL side enough to make any sql complexities worthwhile. 
 There is also a point deserving delineation with one of the normalization precepts.  A central argument for normalization in databases is the idea that data duplication is always bad.  This is frequently true, but cannot be followed slavishly, especially when there are different owners for the different pieces of a solution.  I saw a situation once in which one group of developers managed a certain type of transactions, and another group of developers supported auditability of these transactions, so the second group of developers wrote a service which scraped several tables whenever a transaction occurred and created a denormalized snapshot record stating, in effect, what was the state of the system at the time the transaction.  This scenario stands as an interesting use case (for the data duplication part of the question at least), but it is actually part of a larger category of issues.  Data constistency desires will often put certain constraints on the structure of data in the database that can make error handling and troubleshooting simpler by making some of the incorrect cases impossible.  However this can also have the impact of "freezing" portions of data because changing that subset of the data would cause past transactions to become invalid under the consistancy rules.  Obviously some sort of versioning system is required to sort this out, so the obvious question is whether to use a normalized versioning system (effective and expiration times) or a snapshot-based approach (value as of transaction time).  There are several internal structure questions for the normalized version that you don't have to worry about with the snapshot approach, like: 
 
 Can date range queries be done efficiently even for large tables?     
 Is it possible to guarantee non-overlap of date ranges?     
 Is it possible to trace status events back to operator, transaction, or reason for change? (probably yes, but this is additional overhead)     
 By creating a more complicated versioning system, are you putting the right owners in charge of the right data? 
 
 I think the optimal goal here is to learn not only what is correct in theory, but why it is correct, and what are the consequences of violations, then when you are in the real world, you can decide which consequences are worth taking to gain which other benefits.  That is the real challenge of design. 
 
 Reporting system and transaction system have different requirements. 
 I would recommend for transaction system, always use normalization for data correctness. 
 For reporting system, use normalization unless denormaliztion is required for whatever reason, such as ease of adhoc query, performance, etc. 
 
 Simplicity?  Not sure if Steven is gonna swat me with his newspaper, but where I hang, sometimes the denormalized tables help the reporting/readonly guys get their jobs done without bugging the database/developers all the time... 
 