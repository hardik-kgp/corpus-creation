*** mysql-batch-insert-on-multiple-tables-with-last-insert-id ***

 I am trying to insert a lot of users into a MySQL database with two tables: 
 The first table contains the user data. An example   looks like this (  is the primary key,   is a unique key): 
 
 The second table contains the group the user belongs to. It only stores two foreign keys   and  . An example query looks like this: 
 
 This setup works perfectly fine for small data sets. When I import large amounts of data (>1M rows) the  s get slow. Obviously, it would be much better to do a batch insert: 
 
 and: 
 
 The problem of course is, that   only returns one (the first) id of a batch  . 
So, what I would need is a "nested" batch  , which IMO does not exist in MySQL. 
 **What can I do to make my **
**s faster?**

 
 Bulk inserts by default provide sequential auto increments, with this knowledge you can do your inserts like; 
 
 For more information on  bulk inserts and auto increment have a look at  http://dev.mysql.com/doc/refman/5.1/en/innodb-auto-increment-handling.html 
 Importantly, make sure that innodb_autoinc_lock_mode=1 
 
 Otherwise consider wrapping your inserts in  
 
 
 If you're putting millions of known rows into a table all at once, consider using   since it's intended for speed in just that type of scenario, as evidenced by this quote from  the docs : 
 
 The   statement reads rows from a text file into a
  table at a very high speed. 
 
 And at  Speed of   Statements : 
 
 When loading a table from a text file, use  . This is
  usually 20 times faster than using   statements. 
 
 This is assuming that your source data is coming from, or could be provided as, a text file. If you have the group ID in the file as well, you might do something like this: 
 
 Whether this approach ends up being faster than your current approach depends on whether you save more time using   than you spend performing two additional   statements to move the data into your desired tables. You may want to tweak keys on the temporary table; I can't benchmark it for you based solely on the contents of your question. I'd be interested to know how it works out, though. 
 The documentation also has a decent number of tips for  Bulk Data Loading for InnoDB Tables  and  Bulk Data Loading for MyISAM Tables . I won't go through them in detail, not least because you haven't given us any DDL or server info, but you may find it helpful to read through one or the other on your own time. 
 
 Had to deal with a similar issue. 
 MySQL doesnt really offer much of a way to reliably reserve large batches of table IDs for this purpose. I spent a good half a day researching to no avail. There are some hacks floating around, but nothing id bank my data on. 
 I just did the users table with one-by-one inserts (better slow than screwy) and returned the id of the new row to my ORM. I got a row ID to work with, so i was then able to throw it and the data that should be imported into a JSON that keeps them both together. That made it much easier for me to bulk insert and keep the data matched up. 
 Best. 
 
 Refer this links    How can I Insert many rows into a MySQL table and return the new IDs? 
 http://dev.mysql.com/doc/refman/5.6/en/information-functions.html#function_last-insert-id 
 