*** python-is-slow-when-iterating-over-a-large-list ***

 I am currently selecting a large list of rows from a database using pyodbc.  The result is then copied to a large list, and then i am trying to iterate over the list.  Before I abandon python, and try to create this in C#, I wanted to know if there was something I was doing wrong. 
 
 Some more information: 
 
 The for loop is currently running at about 5 loops per second, and that seems insanely slow to me.   
 The total rows selected is ~489,000. 
 The machine its running on has lots of RAM and CPU.  It seems to only run one or two cores, and ram is 1.72GB of 4gb. 
 
 Can anyone tell me whats wrong?  Do scripts just run this slow? 
 Thanks 
 
 This should not be slow with Python native lists - but maybe ODBC's driver is returning a "lazy" object that tries to be smart but just gets slow. Try just doing  
   
 in your code and post further benchmarks. 
 (Python lists can get slow if you start inserting things in its middle, but just iterating over a large list should be fast) 
 
 It's probably slow because you load all result in memory first and performing the iteration over a list. Try iterating the cursor instead. 
 And no, scripts shouldn't be  **that**
 slow. 
 
 
 More investigation is needed here... consider the following script: 
 
 This is pretty much the same as your script, minus the database stuff, and takes a few seconds to run on my not-terribly-fast machine. 
 
 When you connect to your database directly (I mean you get an SQL prompt), how many secods runs this query? 
 When query ends, you get a message like this: 
 
 So, if that time is so big, and your query is slow as "native", may be you have to create an index on that table. 
 
 This is slow because you are  
 
 Getting all the results 
 Allocating memory and assigning the values to that memory to create the list allIDRows 
 Iterating over that list and counting. 
 
 If execute gives you back a cursor then use the cursor to it's advantage and start counting as you get stuff back and save time on the mem allocation. 
 
 Other hints: 
 
 create an index on year 
 use 'select count(*) from ... to get the count for the year' this will probably be optimised on the db. 
 Remove the aID line if not needed this is converting the first item of the row to a string even though its not used. 
 
 