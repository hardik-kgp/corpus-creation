*** spark-optimise-writing-a-dataframe-to-sql-server ***

 I am using the code below to write a DataFrame of 43 columns and about 2,000,000 rows into a table in SQL Server: 
 
 Sadly, while it does work for small DataFrames it's either extremely slow or gets timed out for large ones. Any hints on how to optimize it? 
 I've tried setting  
 Thanks. 
 
 We resorted to using the  azure-sqldb-spark  library instead of the default built-in exporting functionality of Spark. This library gives you a   method which is a  real  batch insert and goes  a lot  faster. It's a bit less practical to use than the built-in functionality, but in my experience it's still worth it. 
 We use it more or less like this: 
 
 As you can see we generate the   query ourselves. You  can  let the library create the table, but it will just do   which can still be pretty inefficient, probably requires you to cache your  , and it doesn't allow you to choose the  . 
 Also potentially interesting: we had to use an   when adding this library to our sbt build, or the   task would fail. 
 
 
 Try adding   option to your statement with atleast  (change this value accordingly to get better performance)  and execute the write again. 
 From spark docs: 
 
 The JDBC batch size, which determines  **how many rows to insert per
  round trip**
. This can help performance on JDBC drivers. This option
  applies only to writing. It  **defaults to 1000**
. 
 
 **Also its worth to check out:**
  
 
    to increase the parallelism (This also determines the maximum number of concurrent JDBC connections) 
    to increase the timeouts for the write option. 
 
 
 is converting data to CSV files and copying those CSV's is an option for you?
we have automated this process for bigger tables and transferring those in GCP in CSV format. rather than reading this through JDBC. 
 