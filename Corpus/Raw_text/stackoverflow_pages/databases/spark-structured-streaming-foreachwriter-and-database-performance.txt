*** spark-structured-streaming-foreachwriter-and-database-performance ***

 I've had a go implementing a structured stream like so... 
 
 This all seems to work, but looking at the throughput of MyWrapper.myWriter is horrible. It's effectively trying to be a JDBC sink, it looks like: 
 
 So my question is - Is the new ForeachWriter instantiated for each row ? thus the open() and close() is called for every row in the dataset ? 
 Is there a better design to improve throughput ? 
 How to parse SQL statement once and execute many times, also keep the database connection open? 
 
 Opening and closing of the underlying sink depends on  your implementation  of  .  
 The relevant class which invokes   is the  , and this is the code which calls your writer: 
 
 Opening and closing of the writer is attempted foreach batch that is generated from your source. If you want   and   to be literally open and close the sink driver each time, you'll need to do so via your implementation. 
 If you want more control over how the data is handled, you can implement the   trait which gives a batch id and the underlying  : 
 
 