*** what-does-bulk-load-mean ***

 Jumping from article to article, I can see everywhere the expression "bulk loading".  
 What does it really (technically) mean? 
 What does it imply? 
 Explanation based on use-cases is welcome. 
 
 Indexes are usually optimized for inserting rows one at a time.  When you are adding a great deal of data at once, inserting rows one at a time may be inefficient.  For instance, with a B-Tree, the optimal way to insert a single key is very poor way of adding a bunch of data to an empty index.  
 Instead you pursue a different strategy with B-Trees.  You presort all of the data, and group it in blocks.  You can then build a new B-Tree by transforming the blocks into tree nodes.  Although both techniques have the same asymptotic performance, O(n log(n)), the bulk-load operation has much smaller factor.   
 
 Bulk loading is a way to load data (typically into a database) in 'large chunks'.  Where you might enter a customer or a purchase order or information about items in inventory one at a time into your system, bulk loading takes a file of this same sort of information and loads hundreds/thousands/millions of records in a short period of time. 
 If you convert from one kind of DBMS to another, you would hope not to enter all the information into the new DB from the old DB.  Instead, you would dump the information from the old DB to a file in a format that can be easily read by the new DB and then import that data into the new DB. 
 That's what bulk loading entails (at the 35K foot level, anyway) 
 
 Bulk loading is used to import/export large amounts of data. Usually bulk operations are not logged and transactional integrity might not work as expected. Often bulk operations bypass triggers and integrity checks like constraints. This improves performance, for large amounts of data, quite significantly. 
 
 One thing to remember is that bulk loading implies that the data content from the source to target is the same, but this is only true if the source system is acquiesced.  For any data source, and especially true of large data, the source data can change after it has been read and the data transfer is happening.  Traditionally online systems either have to go off line or suspend updates if an exact point it time capture that matches the source is required. 
 