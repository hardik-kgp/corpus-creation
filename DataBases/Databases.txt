<doc id="2271127" url="https://en.wikipedia.org/wiki?curid=2271127" title="Dynaset">
Dynaset

1. A dynaset (short for dynamic set) is a set of data that is dynamically linked back to the database. Instead of having the query result stored in a temporary table, where the data cannot be updated directly by the user, the dynaset allows the user to view and update the data contained in the dynaset. Thus, if a university lecturer queried all students who received a distinction in their assignment and found an error in that student's record, they would only need to update the data in the dynaset, which would automatically update the student's database record without the need for them to send a specific update query after storing the query results in a temporary table.

The concept was initially put forth by Dr. E.F. Codd, the inventor of the relational database management system (RDBMS) concept.

2. A dynaset is a temporary set of data taken from one or more tables in the underlying file. A dynaset may be a query that was defined in an Access database, a single table, a subset of a table, or the result of joining multiple tables. A dynaset can be updated if the file is not locked or opened for ReadOnly. The data in the dynaset are "live" i.e., any changes made to the data as a project (in Visual Basic) is executing will appear in the corresponding recordset. Note: Dynasets created from joining tables are typically non-updateable.

The word "dynaset" was coined by David Risher during a design meeting for Microsoft Access 1.0.


</doc>
<doc id="2666135" url="https://en.wikipedia.org/wiki?curid=2666135" title="Recordset">
Recordset

A recordset is a data structure that consists of a group of database records, and can either come from a base table or as the result of a query to the table. 

The concept is common to a number of platforms, notably Microsoft's Data Access Objects (DAO) and ActiveX Data Objects (ADO). The Recordset object contains a Fields collection, and a Properties collection. At any time, the Recordset object refers to only a single record within the set as the current record. 



</doc>
<doc id="4367801" url="https://en.wikipedia.org/wiki?curid=4367801" title="Serializability">
Serializability

In concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control. As such it is supported in all general purpose database systems. "Strong strict two-phase locking" (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

Serializability theory provides the formal framework to reason about and analyze serializability and its techniques. Though it is mathematical in nature, its fundamentals are informally (without mathematics notation) introduced below.

Serializability is used to keep the data in the data item in a consistent state. Serializability is a property of a transaction schedule (history). It relates to the "isolation" property of a database transaction.

Schedules that are not serializable are likely to generate erroneous outcomes. Well known examples are with transactions that debit and credit accounts with money: If the related schedules are not serializable, then the total sum of money may not be preserved. Money could disappear, or be generated from nowhere. This and violations of possibly needed other invariant preservations are caused by one transaction writing, and "stepping on" and erasing what has been written by another transaction before it has become permanent in the database. It does not happen if serializability is maintained.

If any specific order between some transactions is requested by an application, then it is enforced independently of the underlying serializability mechanisms. These mechanisms are typically indifferent to any specific order, and generate some unpredictable partial order that is typically compatible with multiple serial orders of these transactions. This partial order results from the scheduling orders of concurrent transactions' data access operations, which depend on many factors.

A major characteristic of a database transaction is "atomicity", which means that it either "commits", i.e., all its operations' results take effect in the database, or "aborts" (rolled-back), all its operations' results do not have any effect on the database ("all or nothing" semantics of a transaction). In all real systems transactions can abort for many reasons, and serializability by itself is not sufficient for correctness. Schedules also need to possess the "recoverability" (from abort) property. Recoverability means that committed transactions have not read data written by aborted transactions (whose effects do not exist in the resulting database states). While serializability is currently compromised on purpose in many applications for better performance (only in cases when application's correctness is not harmed), compromising recoverability would quickly violate the database's integrity, as well as that of transactions' results external to the database. A schedule with the recoverability property (a "recoverable" schedule) "recovers" from aborts by itself, i.e., aborts do not harm the integrity of its committed transactions and resulting database. This is false without recoverability, where the likely integrity violations (resulting incorrect database data) need special, typically manual, corrective actions in the database.

Implementing recoverability in its general form may result in "cascading aborts": Aborting one transaction may result in a need to abort a second transaction, and then a third, and so on. This results in a waste of already partially executed transactions, and may result also in a performance penalty. Avoiding cascading aborts (ACA, or Cascadelessness) is a special case of recoverability that exactly prevents such phenomena. Often in practice a special case of ACA is utilized: Strictness. Strictness allows efficient database recovery from failure.

Note that the "recoverability" property is needed even if no database failure occurs and no database "recovery" from failure is needed. It is, rather, needed to correctly automatically handle aborts, which may be unrelated to database failure and recovery from failure.

In many applications, unlike with finances, absolute correctness is not needed. For example, when retrieving a list of products according to specification, in most cases it does not matter much if a product, whose data was updated a short time ago, does not appear in the list, even if it meets the specification. It will typically appear in such a list when tried again a short time later. Commercial databases provide concurrency control with a whole range of isolation levels which are in fact (controlled) serializability violations in order to achieve higher performance. Higher performance means better transaction execution rate and shorter average transaction response time (transaction duration). "Snapshot isolation" is an example of a popular, widely utilized efficient relaxed serializability method with many characteristics of full serializability, but still short of some, and unfit in many situations.

Another common reason nowadays for distributed serializability relaxation (see below) is the requirement of availability of internet products and services. This requirement is typically answered by large-scale data replication. The straightforward solution for synchronizing replicas' updates of the same database object is including all these updates in a single atomic distributed transaction. However, with many replicas such a transaction is very large, and may span enough of a number of several computers and networks that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.
Consequently, Optimistic replication (Lazy replication) is often utilized (e.g., in many products and services by Google, Amazon, Yahoo, and the like), while serializability is relaxed and compromised for eventual consistency. Again, in this case, relaxation is done only for applications that are not expected to be harmed by this technique.

Classes of schedules defined by "relaxed serializability" properties either contain the serializability class, or are incomparable with it.

Mechanisms that enforce serializability need to execute in real time, or almost in real time, while transactions are running at high rates. In order to meet this requirement, special cases of serializability, sufficient conditions for serializability which can be enforced effectively, are utilized.

Two major types of serializability exist: "view-serializability", and "conflict-serializability". View-serializability matches the general definition of serializability given above. Conflict-serializability is a broad special case, i.e., any schedule that is conflict-serializable is also view-serializable, but not necessarily the opposite. Conflict-serializability is widely utilized because it is easier to determine and covers a substantial portion of the view-serializable schedules. Determining view-serializability of a schedule is an NP-complete problem (a class of problems with only difficult-to-compute, excessively time-consuming known solutions).

Operations upon data are "read" or "write" (a write: either "insert" or "modify" or "delete"). Two operations are "conflicting" if they are of different transactions, upon the same datum (data item), and at least one of them is "write". Each such pair of conflicting operations has a "conflict type": It is either a "read-write", or "write-read", or a "write-write" conflict. The transaction of the second operation in the pair is said to be "in conflict" with the transaction of the first operation. A more general definition of conflicting operations (also for complex operations, which may each consist of several "simple" read/write operations) requires that they are noncommutative (changing their order also changes their combined result). Each such operation needs to be atomic by itself (using proper system support) in order to be considered an operation for a commutativity check. For example, read-read operations are commutative (unlike read-write and the other possibilities) and thus read-read is not a conflict. Another more complex example: the operations "increment" and "decrement" of a "counter" are both "write" operations (both modify the counter), but do not need to be considered conflicting (write-write conflict type) since they are commutative (thus increment-decrement is not a conflict; e.g., already has been supported in the old IBM's IMS "fast path"). Only precedence (time order) in pairs of conflicting (non-commutative) operations is important when checking equivalence to a serial schedule, since different schedules consisting of the same transactions can be transformed from one to another by changing orders between different transactions' operations (different transactions' interleaving), and since changing orders of commutative operations (non-conflicting) does not change an overall operation sequence result, i.e., a schedule outcome (the outcome is preserved through order change between non-conflicting operations, but typically not when conflicting operations change order). This means that if a schedule can be transformed to any serial schedule without changing orders of conflicting operations (but changing orders of non-conflicting, while preserving operation order inside each transaction), then the outcome of both schedules is the same, and the schedule is conflict-serializable by definition.

Conflicts are the reason for blocking transactions and delays (non-materialized conflicts), or for aborting transactions due to serializability violation prevention. Both possibilities reduce performance. Thus reducing the number of conflicts, e.g., by commutativity (when possible), is a way to increase performance.

A transaction can issue/request a conflicting operation and be "in conflict" with another transaction while its conflicting operation is delayed and not executed (e.g., blocked by a lock). Only executed ("materialized") conflicting operations are relevant to "conflict serializability" (see more below).

Schedule compliance with conflict serializability can be tested with the precedence graph ("serializability graph", "serialization graph", "conflict graph") for committed transactions of the schedule. It is the directed graph representing precedence of transactions in the schedule, as reflected by precedence of conflicting operations in the transactions.

The following observation is a key characterization of conflict serializability:

Cycles of committed transactions can be prevented by aborting an "undecided" (neither committed, nor aborted) transaction on each cycle in the precedence graph of all the transactions, which can otherwise turn into a cycle of committed transactions (and a committed transaction cannot be aborted). One transaction aborted per cycle is both required and sufficient in number to break and eliminate the cycle (more aborts are possible, and can happen under some mechanisms, but are unnecessary for serializability). The probability of cycle generation is typically low, but, nevertheless, such a situation is carefully handled, typically with a considerable amount of overhead, since correctness is involved. Transactions aborted due to serializability violation prevention are "restarted" and executed again immediately.

Serializability-enforcing mechanisms typically do not maintain a precedence graph as a data structure, but rather prevent or break cycles implicitly (e.g., SS2PL below).

"Strong strict two-phase locking" (SS2PL) is a common mechanism utilized in database systems since their early days in the 1970s (the "SS" in the name SS2PL is newer, though) to enforce both conflict serializability and "strictness" (a special case of recoverability which allows effective database recovery from failure) of a schedule. Under this mechanism, each datum is locked by a transaction before its accessing it (in any read or write operation): the item is marked by and associated with a "lock" of a certain type depending on the operation being performed (and the specific transaction implementation; various models with different lock types exist; in some models, locks may change type during the transaction's life). As a result, access by another transaction may be blocked, typically upon a conflict (the lock delays or completely prevents the conflict from being materialized and be reflected in the precedence graph by blocking the conflicting operation), depending on lock type and the other transaction's access operation type. Employing an SS2PL mechanism means that all locks on data on behalf of a transaction are released only after the transaction has ended (either committed or aborted).

SS2PL is the name of the resulting schedule property as well, which is also called "rigorousness". SS2PL is a special case (proper subset) of Two-phase locking (2PL)

Mutual blocking between transactions results in a "deadlock", where execution of these transactions is stalled and no completion can be reached. Thus deadlocks need to be resolved to complete these transactions' execution and release related computing resources. A deadlock is a reflection of a potential cycle in the precedence graph that would occur without the blocking when conflicts are materialized. A deadlock is resolved by aborting a transaction involved with such a potential cycle and breaking the cycle. It is often detected using a "wait-for graph" (a graph of conflicts blocked by locks from being materialized; it can be also defined as the graph of non-materialized conflicts; conflicts not materialized are not reflected in the precedence graph and do not affect serializability), which indicates which transaction is "waiting for" the release of one of more locks by which other transaction or transactions, and a cycle in this graph means a deadlock. Aborting one transaction per cycle is sufficient to break the cycle. Transactions aborted due to deadlock resolution are "restarted" and executed again immediately.

Other known mechanisms include:

The above (conflict) serializability techniques in their general form do not provide recoverability. Special enhancements are needed for adding recoverability.

Concurrency control techniques are of three major types:

The main differences between the technique types is the conflict types that are generated by them. A pessimistic method blocks a transaction operation upon conflict and generates a non-materialized conflict, while an optimistic method does not block and generates a materialized conflict. A semi-optimistic method generates both conflict types. Both conflict types are generated by the chronological orders in which transaction operations are invoked, independently of the type of conflict. A cycle of committed transactions (with materialized conflicts) in the "precedence graph" (conflict graph) represents a serializability violation, and should be avoided for maintaining serializability. A cycle of (non-materialized) conflicts in the "wait-for graph" represents a deadlock situation, which should be resolved by breaking the cycle. Both cycle types result from conflicts and should be broken. Under any technique type, conflicts should be detected and considered, with similar overhead for both materialized and non-materialized conflicts (typically by using mechanisms like locking, while either blocking for locks or not blocking but recording conflict for materialized conflicts). In a blocking method, typically a context switching occurs upon conflict, with (additional) incurred overhead. Otherwise, blocked transactions' related computing resources remain idle, unutilized, which may be a worse alternative. When conflicts do not occur frequently, optimistic methods typically have an advantage. With different transaction loads (mixes of transaction types) one technique type (i.e., either optimistic or pessimistic) may provide better performance than the other.

Unless schedule classes are "inherently blocking" (i.e., they cannot be implemented without data-access operations blocking; e.g., 2PL, SS2PL and SCO above; see chart), they can also be implemented using optimistic techniques (e.g., Serializability, Recoverability).

Multi-version concurrency control (MVCC) is a common way today to increase concurrency and performance by generating a new version of a database object each time the object is written and allowing transactions' read operations of several last relevant versions (of each object), depending on scheduling method. MVCC can be combined with all the serializability techniques listed above (except SerializableSI, which is originally MVCC-based). It is utilized in most general-purpose DBMS products.

MVCC is especially popular nowadays through the "relaxed serializability" (see above) method "Snapshot isolation" (SI) which provides better performance than most known serializability mechanisms (at the cost of possible serializability violation in certain cases). SerializableSI, which is an efficient enhancement of SI to make it serializable, is intended to provide an efficient serializable solution. SerializableSI has been analyzed via a general theory of MVCC

Distributed serializability is the serializability of a schedule of a transactional distributed system (e.g., a distributed database system). Such a system is characterized by "distributed transactions" (also called "global transactions"), i.e., transactions that span computer processes (a process abstraction in a general sense, depending on computing environment; e.g., operating system's thread) and possibly network nodes. A distributed transaction comprises more than one of several "local sub-transactions" that each has states as described above for a database transaction. A local sub-transaction comprises a single process, or more processes that typically fail together (e.g., in a single processor core). Distributed transactions imply a need for an atomic commit protocol to reach consensus among its local sub-transactions on whether to commit or abort. Such protocols can vary from a simple (one-phase) handshake among processes that fail together to more sophisticated protocols, like two-phase commit, to handle more complicated cases of failure (e.g., process, node, communication, etc. failure). Distributed serializability is a major goal of distributed concurrency control for correctness. With the proliferation of the Internet, cloud computing, grid computing, and small, portable, powerful computing devices (e.g., smartphones,) the need for effective distributed serializability techniques to ensure correctness in and among distributed applications seems to increase.

Distributed serializability is achieved by implementing distributed versions of the known centralized techniques. Typically, all such distributed versions require utilizing conflict information (of either materialized or non-materialized conflicts, or, equivalently, transaction precedence or blocking information; conflict serializability is usually utilized) that is not generated locally, but rather in different processes, and remote locations. Thus information distribution is needed (e.g., precedence relations, lock information, timestamps, or tickets). When the distributed system is of a relatively small scale and message delays across the system are small, the centralized concurrency control methods can be used unchanged while certain processes or nodes in the system manage the related algorithms. However, in a large-scale system (e.g., "grid" and "cloud"), due to the distribution of such information, a substantial performance penalty is typically incurred, even when distributed versions of the methods (vs. the centralized ones) are used, primarily due to computer and communication latency. Also, when such information is distributed, related techniques typically do not scale well. A well-known example with respect to scalability problems is a distributed lock manager, which distributes lock (non-materialized conflict) information across the distributed system to implement locking techniques.




</doc>
<doc id="5004378" url="https://en.wikipedia.org/wiki?curid=5004378" title="Integrated test facility">
Integrated test facility

An integrated test facility (ITF) creates a fictitious entity in a database to process test transactions simultaneously with live input. 

It can be used to incorporate test transactions into a normal production run of a system. Its advantage is that periodic testing does not require separate test processes. However, careful planning is necessary, and test data must be isolated from production data. Moreover, ITF validates the correct operation of a transaction in an application, but it does not ensure that a system is being operated correctly. Integrated test facility is considered a useful audit tool during an IT audit because it uses the same programs to compare processing using independently calculated data. This involves setting up dummy entities on an application system and processing test or production data against the entity as a means of verifying processing accuracy.


</doc>
<doc id="7577329" url="https://en.wikipedia.org/wiki?curid=7577329" title="Online complex processing">
Online complex processing

Online complex processing (OLCP) is a class of realtime data processing involving complex queries, lengthy queries and/or simultaneous reads and writes to the same records.




</doc>
<doc id="1712665" url="https://en.wikipedia.org/wiki?curid=1712665" title="User-defined function">
User-defined function

A user-defined function (UDF) is a function provided by the user of a program or environment, in a context where the usual assumption is that functions are built into the program or environment.

In some old implementations of the BASIC programming language, user-defined functions are defined using the "DEF FN" syntax. More modern dialects of BASIC are influenced by the structured programming paradigm, where most or all of the code is written as user-defined functions or procedures, and the concept becomes practically redundant.

In relational database management systems, a user-defined function provides a mechanism for extending the functionality of the database server by adding a function, that can be evaluated in standard query language (usually SQL) statements. The standard distinguishes between scalar and table functions. A scalar function returns only a single value (or NULL), whereas a table function returns a (relational) table comprising zero or more rows, each row with one or more columns.

User-defined functions in SQL are declared using the codice_1 statement. For example, a function that converts Celsius to Fahrenheit might be declared like this:
Once created, a user-defined function may be used in expressions in SQL statements. For example, it can be invoked where most other intrinsic functions are allowed. This also includes SELECT statements, where the function can be used against data stored in tables in the database. Conceptually, the function is evaluated once per row in such usage. For example, assume a table named ELEMENTS, with a row for each known chemical element. The table has a column named BoilingPoint for the boiling point of that element, in Celsius. The query

would retrieve the name and the boiling point from each row. It invokes the CtoF user-defined function as declared above in order to convert the value in the column to a value in Fahrenheit.

Each user-defined function carries certain properties or characteristics. The SQL standard defines the following properties:

User-defined functions should not be confused with stored procedures. Stored procedures allow the user to group a set of SQL commands. A procedure can accept parameters and execute its SQL statements depending on those parameters. A procedure is not an expression and, thus, cannot be used like user-defined functions.

Some database management systems allow the creation of user defined functions in languages other than SQL. Microsoft SQL Server, for example, allows the user to use .NET languages including C# for this purpose. DB2 and Oracle support user-defined functions written in C or Java programming languages.

There are three types of UDF in Microsoft SQL Server 2000: scalar functions, inline table-valued functions, and multistatement table-valued functions.

Scalar functions return a single data value (not a table) with RETURNS clause. Scalar functions can use all scalar data types, with exception of timestamp and user-defined data types. 
Inline table-valued functions return the result set of a single SELECT statement. 
Multistatement table-valued functions return a table, which was built with many TRANSACT-SQL statements.

User-defined functions can be invoked from a query like built‑in functions such as OBJECT_ID, LEN, DATEDIFF, or can be executed through an EXECUTE statement like stored procedures.

Performance Notes:
1. On Microsoft SQL Server 2000 a table-valued function which "wraps" a View may be much faster than the View itself. The following MyFunction is an example of a "function-wrapper" which runs faster than the underlying view MyView:

2. On Microsoft SQL Server 2005 the result of the same code execution is the opposite: view is executed faster than the "function-wrapper".

User-defined functions are subroutines made of one or more Transact-SQL statements that can be used to encapsulate code for reuse. 
It takes zero or more arguments and evaluates a return value. Has both control-flow and DML statements in its body similar to stored procedures.
Does not allow changes to any Global Session State, like modifications to database or external resource, such as a file or network.
Does not support output parameter.
DEFAULT keyword must be specified to pass the default value of parameter.
Errors in UDF cause UDF to abort which, in turn, aborts the statement that invoked the UDF.

Data type supported in Microsoft SQL Server 2000
Like a temporary table used to store results 
Mostly used to define temporary variable of type (table) and the return value of a UDF
The scope is limited to function, stored procedure, or batch in which it is defined
Assignment operation is not allowed between (Table) variables
May be used in SELECT, INSERT, UPDATE, and DELETE 
CREATE FUNCTION to create UDF
ALTER FUNCTION to change the characteristics of UDF
DROP FUNCTION to remove UDF

Apache Hive defines, in addition to the regular user defined functions (UDF), also user defined aggregate functions (UDAF) and table-generating functions (UDTF). Hive enables developers to create their own custom functions with Java.



</doc>
<doc id="8444288" url="https://en.wikipedia.org/wiki?curid=8444288" title="Identity column">
Identity column

An identity column is a column (also known as a field) in a database table that is made up of values generated by the database. This is much like an AutoNumber field in Microsoft Access or a sequence in Oracle. Because the concept is so important in database science, many RDBMS systems implement some type of generated key, although each has its own terminology.

An identity column differs from a primary key in that its values are managed by the server and usually cannot be modified. In many cases an identity column is used as a primary key; however, this is not always the case.

It is a common misconception that an identity column will enforce uniqueness; however, this is not the case. If you want to enforce uniqueness on the column you must include the appropriate constraint too.

In Microsoft SQL Server you have options for both the seed (starting value) and the increment. By default the seed and increment are both 1.

or

It is often useful or necessary to know what identity value was generated by an INSERT command. Microsoft SQL Server provides several functions to do this: @@IDENTITY provides the last value generated on the current connection in the current scope, while IDENT_CURRENT("tablename") provides the last value generated, regardless of the connection or scope it was created on.

Example:




</doc>
<doc id="1665120" url="https://en.wikipedia.org/wiki?curid=1665120" title="Relvar">
Relvar

In relational databases, relvar is a term introduced by C. J. Date and Hugh Darwen as an abbreviation for relation variable in their 1995 paper "The Third Manifesto", to avoid the confusion sometimes arising from the use of the term relation, by the inventor of the relational model, E. F. Codd, for a variable to which a relation is assigned as well as for the relation itself. The term is used in Date's well-known database textbook "An Introduction to Database Systems" and in various other books authored or coauthored by him.

Relvar is not universally accepted as a term, and it is not used in the context of existing database management system products that support SQL, whose counterpart concept (but not exact equivalent) is the base table, this being something that, like computer language variables in general, has a name and is subject to update (i.e., being assigned different values from time to time). Other database textbooks continue to use the term relation for both the variable and the data it contains. Similarly, texts on SQL tend to use the term "table" for both purposes, though the qualified term "base table" is used in the standard for the variable.

A closely related term often used in academic texts is relation schema, this being a set of attributes paired with a set of constraints, together defining a set of relations for the purpose of some discussion (typically, database normalization). Constraints that mention just one relvar are termed "relvar constraints", so relation schema can be regarded as a single term encompassing a relvar and its relvar constraints.



</doc>
<doc id="7480157" url="https://en.wikipedia.org/wiki?curid=7480157" title="Trigger list">
Trigger list

Trigger list in its most general meaning refers to a list whose items are used to initiate ("trigger") certain actions.

In the United States, when a person applies for a mortgage loan, the lender makes a credit inquiry about the potential borrower from the national credit bureaus, Equifax, Experian and TransUnion. Unless the borrower is opted out, the credit bureaus put the applicants onto a "trigger list" of "leads" about persons who are interested in new loans. These lists are sold to numerous lenders all over the United States, and soon after the application the applicant starts receiving offers from all parts of the country. The trigger lists contain a significant amount of personal financial information. Among the buyers of trigger lists are "lead generators" which resell filtered information to borrowers, e.g., of people who live in a certain area and have a certain credit score.

While the Federal Trade Commission considers the market of "trigger lists" to be a legal business, many people and organizations (such as the National Association of Mortgage Brokers) consider this a serious breach of privacy and lobby for putting this practice under regulatory controls.

As of now, American consumers may opt-out from "trigger lists" by calling 1-888-5-OPTOUT (1-888-567-8688). 

The Zangger Committee and the Nuclear Suppliers Group maintain lists of items that may contribute to nuclear proliferation; The nuclear non-proliferation treaty forbids its members to export such items to non-treaty members. these items are said to trigger the countries' responsibilities under the NPT, hence the name.



</doc>
<doc id="9519016" url="https://en.wikipedia.org/wiki?curid=9519016" title="Event condition action">
Event condition action

Event condition action (ECA) is a short-cut for referring to the structure of active rules in event driven architecture and active database systems.

Such a rule traditionally consisted of three parts:

This structure was used by the early research in active databases which started to use the term ECA. Current state of the art ECA rule engines use many variations on rule structure. Also other features not considered by the early research is introduced, such as strategies for event selection into the event part.

In a memory-based rule engine, the condition could be some tests on local data and actions could be updates to object attributes. In a database system, the condition could simply be a query to the database, with the result set (if not null) being passed to the action part for changes to the database. In either case, actions could also be calls to external programs or remote procedures.

Note that for database usage, updates to the database are regarded as internal events. As a consequence, the execution of the action part of an active rule can match the event part of the same or another active rule, thus triggering it. The equivalent in a memory-based rule engine would be to invoke an external method that caused an external event to trigger another ECA rule. 

ECA rules can also be used in rule engines that use variants of the Rete algorithm for rule processing.




</doc>
<doc id="9780776" url="https://en.wikipedia.org/wiki?curid=9780776" title="Information schema">
Information schema

In relational databases, the information schema (information_schema) is an ANSI-standard set of read-only views which provide information about all of the tables, views, columns, and procedures in a database.
It can be used as a source of the information which some databases make available through non-standard commands, such as:


As a notable exception among major database systems, Oracle does not implement the information schema. An open-source project exists to address this.

RDBMSs which support information_schema include:


RDBMSs which do not support information_schema include:





</doc>
<doc id="8514669" url="https://en.wikipedia.org/wiki?curid=8514669" title="Intelligent database">
Intelligent database

Until the 1980s, databases were viewed as computer systems that stored record-oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping.

The term was introduced in 1989 by the book "Intelligent Databases" by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems: high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation.

In the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis.



</doc>
<doc id="10911355" url="https://en.wikipedia.org/wiki?curid=10911355" title="DUAL table">
DUAL table

The DUAL table is a special one-row, one-column table present by default in Oracle and other database installations. In Oracle, the table has a single VARCHAR2(1) column called DUMMY that has a value of 'X'. It is suitable for use in selecting a pseudo column such as SYSDATE or USER.

Oracle's SQL syntax requires the FROM clause but some queries don't require any tables - DUAL can be readily used in these cases.

Charles Weiss explains why he created DUAL:
I created the DUAL table as an underlying object in the Oracle Data Dictionary. It was never meant to be seen itself, but instead used inside a view that was expected to be queried. The idea was that you could do a JOIN to the DUAL table and create two rows in the result for every one row in your table. Then, by using GROUP BY, the resulting join could be summarized to show the amount of storage for the DATA extent and for the INDEX extent(s). The name, DUAL, seemed apt for the process of creating a pair of rows from just one. 

Beginning with 10g Release 1, Oracle no longer performs physical or logical I/O on the DUAL table, though the table still exists.

DUAL is readily available for all the users in database.

Several other databases (including Microsoft SQL Server, MySQL, PostgreSQL, SQLite, and Teradata) enable one to omit the FROM clause entirely if no table is needed. This avoids the need for any dummy table.

(What is the Oracle equivalent of “Dual” table in SqlServer?)


</doc>
<doc id="13329119" url="https://en.wikipedia.org/wiki?curid=13329119" title="Distributed concurrency control">
Distributed concurrency control

Distributed concurrency control is the concurrency control of a system distributed over a computer network (Bernstein et al. 1987, Weikum and Vossen 2001). 

In "database systems" and "transaction processing" ("transaction management") distributed concurrency control refers primarily to the concurrency control of a distributed database. It also refers to the concurrency control in a multidatabase (and other multi-transactional object) environment (e.g., federated database, grid computing, and cloud computing environments. A major goal for distributed concurrency control is distributed serializability (or global serializability for multidatabase systems). Distributed concurrency control poses special challenges beyond centralized one, primarily due to communication and computer latency. It often requires special techniques, like distributed lock manager over fast computer networks with low latency, like switched fabric (e.g., InfiniBand). Commitment ordering (or commit ordering) is a general serializability technique that achieves distributed serializability (and global serializability in particular) effectively on a large scale, without concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets), and thus without performance penalties that are typical to other serializability techniques (Raz 1992).

The most common distributed concurrency control technique is "strong strict two-phase locking" (SS2PL, also named "rigorousness"), which is also a common centralized concurrency control technique. SS2PL provides both the "serializability", "strictness", and "commitment ordering" properties. Strictness, a special case of recoverability, is utilized for effective recovery from failure, and commitment ordering allows participating in a general solution for global serializability. For large-scale distribution and complex transactions, distributed locking's typical heavy performance penalty (due to delays, latency) can be saved by using the atomic commitment protocol, which is needed in a distributed database for (distributed) transactions' atomicity (e.g., two-phase commit, or a simpler one in a reliable system), together with some local commitment ordering variant (e.g., local SS2PL) instead of distributed locking, to achieve global serializability in the entire system. All the commitment ordering theoretical results are applicable whenever atomic commitment is utilized over partitioned, distributed recoverable (transactional) data, including automatic "distributed deadlock" resolution. Such technique can be utilized also for a large-scale parallel database, where a single large database, residing on many nodes and using a distributed lock manager, is replaced with a (homogeneous) multidatabase, comprising many relatively small databases (loosely defined; any process that supports transactions over partitioned data and participates in atomic commitment complies), fitting each into a single node, and using commitment ordering (e.g., SS2PL, strict CO) together with some appropriate atomic commitment protocol (without using a distributed lock manager).




</doc>
<doc id="10633346" url="https://en.wikipedia.org/wiki?curid=10633346" title="MultiValue">
MultiValue

MultiValue is a type of NoSQL and multidimensional database, typically considered synonymous with PICK, a database originally developed as the Pick operating system.

MultiValue databases include commercial products from Rocket Software, Zumasys, Revelation, Ladybridge, InterSystems, Northgate Information Solutions, ONgroup, and other companies. These databases differ from a relational database in that they have features that support and encourage the use of attributes which can take a list of values, rather than all attributes being single-valued. They are often categorized with MUMPS within the category of post-relational databases, although the data model actually pre-dates the relational model. Unlike SQL-DBMS tools, most MultiValue databases can be accessed both with or without SQL.

Don Nelson designed the MultiValue data model in the early to mid-1960s. Dick Pick, a developer at TRW, worked on the first implementation of this model for the US Army in 1965. Pick considered the software to be in the public domain because it was written for the military. This was but the first dispute regarding MultiValue databases that was addressed by the courts.

Ken Simms wrote DataBASIC, sometimes known as S-BASIC, in the mid-70's. It was based on Dartmouth BASIC, but had enhanced features for data management. Simms played a lot of "Star Trek" while developing the language, in order to have the language function to his satisfaction.

Three of the implementations of MultiValue, PICK version R77, Microdata Reality 3.x, and Prime Information 1.0, were very similar. In spite of attempts to standardize, particularly by International Spectrum and the Spectrum Manufacturers Association, who designed a logo for all to use, there are no standards across MultiValue implementations. Subsequently, these flavors diverged, although with some cross-over. These streams of MultiValue database development could be classified as one stemming from PICK R83, one from Microdata Reality, and one from Prime Information. Because of the differences, some implementations have provisions for supporting several flavors of the languages. An attempt to document the similarities and differences can be found at the Post-Relational Database Reference (PRDB).

Marketing groups and others in the industry over the years have classified MultiValue databases as pre-relational, post-relational, relational, and embedded, with detractors often classifying it as legacy. It could now be classified as NoSQL. With a data model that aligns well with JSON and XML and that permits access with or without the use of SQL.

One reasonable hypothesis for this data model lasting 50 years, with new database implementations of the model even in the 21st century is that it provides inexpensive database solutions. Historically, with industry benchmarks tied to SQL transactions, this has been a difficult hypothesis to test, although there are considerable anecdotes of failed attempts to get the functionality of a MultiValue application into a relational database framework.

In spite of a history of more than 40 years of implementations, starting with TRW, many in the MultiValue industry have remained current so that various MultiValue implementations now employ object-oriented versions of Data/BASIC, support AJAX frameworks, and because no one needs to use SQL (but some can) with these databases, they fit under the NoSQL umbrella. In fact, MultiValue developers were the first to acquire nosql domain names, likely prior to other database products classifying their offerings as NoSQL as well. MultiValue is a seasoned data model with several vendors competing in the MultiValue space. It has been continuously enhanced over the years.

In a MultiValue database system:

Data is stored using two separate files: a "file" to store raw data and a "dictionary" to store the format for displaying the raw data.

For example, assume there's a file (table) called "PERSON". In this file, there is an attribute called "eMailAddress". The eMailAddress field can store a variable number of email address values in a single record. 
The list [joe@example.com, jdb@example.net, joe_bacde@example.org] can be stored and accessed via a single query when accessing the associated record.

Achieving the same (one-to-many) relationship within a traditional relational database system would include creating an additional table to store the variable number of email addresses associated with a single "PERSON" record. However, modern relational database systems support this multi-value data model too. For example, in PostgreSQL, a column can be an array of any base type.

Like the Java programming language, the typical Data/BASIC compiler compiles to P-code and runs in a P-machine, with jBASE being a notable exception. It has as many different implementations (compilers) as there are MultiValue databases.

Like PHP programming language, the Data/BASIC language does all the typecasting for the programmer.

Known as ENGLISH, ACCESS, AQL, UniQuery, Retrieve, CMQL, and by many other names over the years, corresponding to the different MultiValue implementations, the MultiValue query language differs from SQL in several respects. Each query is issued against a single dictionary within the schema, which could be understood as a virtual file or a portal to the database through which to view the data.

The above statement would list all e-mail addresses for each person whose last name starts with "Van". A single entry would be output for each person, with multiple lines showing the multiple e-mail addresses (without repeating other data about the person).





</doc>
<doc id="16702334" url="https://en.wikipedia.org/wiki?curid=16702334" title="Digital curation">
Digital curation

Digital curation is the selection, preservation, maintenance, collection and archiving of digital assets.
Digital curation establishes, maintains and adds value to repositories of digital data for present and future use. This is often accomplished by archivists, librarians, scientists, historians, and scholars. Enterprises are starting to use digital curation to improve the quality of information and data within their operational and strategic processes. Successful digital curation will mitigate digital obsolescence, keeping the information accessible to users indefinitely. Digital curation includes digital asset management, data curation, digital preservation, and electronic records management.

The term "curation" in the past commonly referred to museum and library professionals. It has since been applied to interaction with social media including compiling digital images, web links and movie files.

The term “digital curation” was first used in the e-science and biological science fields as a means of differentiating the additional suite of activities ordinarily employed by library and museum curators to add value to their collections and enable its reuse from the smaller subtask of simply preserving the data, a significantly more concise archival task. Additionally, the historical understanding of the term “curator” demands more than simple care of the collection. A curator is expected to command academic mastery of the subject matter as a requisite part of appraisal and selection of assets and any subsequent adding of value to the collection through application of metadata.

There are five commonly accepted principles that govern the occupation of digital curation:

The Digital Curation Center offers the following step-by-step life cycle procedures for putting the above principles into practice:

Sequential Actions:


Occasional Actions:


The term "digital curation" is sometimes used interchangeably with terms such as "digital preservation" and "digital archiving". While digital preservation does focus a significant degree of energy on optimizing reusability, preservation remains a subtask to the concept of digital archiving, which is in turn a subtask of digital curation. For example, archiving is a part of curation, but so are subsequent tasks such as themed collection-building, which is not considered an archival task. Similarly, preservation is a part of archiving, as are the tasks of selection and appraisal that are not necessarily part of preservation.

Data curation is another term that is often used interchangeably with digital curation, however common usage of the two terms differs. While “data” is a more all-encompassing term that can be used generally to indicate anything recorded in binary form, the term “data curation” is most common in scientific parlance and usually refers to accumulating and managing information relative to the process of research. Data-driven research of education request the role of information professional gradually develop tradition of digital service to data curation particularly at the management of digital research data. So, while documents and other discrete digital assets are technically a subset of the broader concept of data, in the context of scientific vernacular digital curation represents a broader purview of responsibilities than data curation due to its interest in preserving and adding value to digital assets of any kind.

The ever lowering cost, and increasing prevalence of entirely new categories of technology has led to a quickly growing flow of new data sets. These come from well established sources such as business and government, but the trend is also driven by new styles of sensors becoming embedded in more areas of modern life. This is particularly true of consumers, whose production of digital assets is no longer relegated strictly to work. Consumers now create wider ranges of digital assets, including videos, photos, location data, purchases, and fitness tracking data, just to name a few, and share them in wider ranges of social platforms.

Additionally, the advance of technology has introduced new ways of working with data. Some examples of this are international partnerships that leverage astronomical data to create “virtual observatories”, and similar partnerships have also leveraged data resulting from research at the Large Hadron Collider at CERN and the database of protein structures at the Protein Data Bank.

By comparison, archiving of analog assets is notably passive in nature, often limited to simply ensuring a suitable storage environment. Digital preservation requires a more proactive approach. Today’s artifacts of cultural significance are notably transient in nature and prone to obsolescence when social trends or dependent technologies change. This rapid progression of technology occasionally makes it necessary to migrate digital asset holdings from one file format to another in order to mitigate the dangers of hardware and software obsolescence which would render the asset unusable.

Modern tools for program planning often underestimate the amount of human labor costs required for adequate digital curation of large collections. As a result cost-benefit assessments often paint an inaccurate picture of both the amount of work involved, and the true cost to the institution for both successful outcomes and failures.

The concept of cost in business field would be more obvious. Varieties of business systems are running for daily operations. For example, human resources systems deal with recruitment and payroll, communication systems manage internal and external email, and administration systems handle finance, marketing and other aspects.  However, business systems in institutions are not designed for long-term information preservation initially. In some instances, business systems are revised to become Digital Curation systems for preserving transaction information due to cost consideration. The example of business systems are Enterprise Content Management (ECM) applications, which are used by designated group people such as business executives, customers for information management that support key processes organizationally. In the long run, to transfer digital content from ECM applications to Digital Curation (DC) applications would be a trend in large organizations domestically or internationally.  The improvement of maturity models of ECM and DC may add value to information that request cost deduction and extensive use for further modification.

An absence of coordination across different sectors of society and industry in areas such as the standardization of semantic and ontological definitions, and in forming partnerships for proper stewardship of assets has resulted in a lack of interoperability between institutions, and a partial breakdown in digital curation practice from the standpoint of the ordinary user. The example of coordination is Open Archival Information System (OAIS).

OAIS Reference Model allows professionals and many other organizations and individuals to contribute efforts to the OAIS open forums for developing international standards of archival information in long-term access.

The curation of digital objects is not limited to strictly born-digital assets. Many institutions have engaged in monumental efforts to digitize analog holdings in an effort to increase access to their collections. Examples of these materials are books, photographs, maps, audio recordings, and more. The process of converting printed resources into digital collections has been epitomized to some degree by librarians and related specialists. For example, The Digital Curation Centre is claimed to be a "world leading centre of expertise in digital information curation" that assists higher education research institutions in such conversions.

For some topics, knowledge is embodied in forms that have not been conducive to print, such as how choreography of dance or of the motion of skilled workers or artisans is difficult to encode. New digital approaches such as 3D holograms and other computer-programmed expressions are developing.

For mathematics, it seems possible for a new common language to be developed that would express mathematical ideas in ways that can be digitally stored, linked, and made accessible. The Global Digital Mathematics Library is a project to define and develop such a language.

The ability of the intended user community to access the repository’s holdings is of equal importance to all the preceding curatorial tasks. This must take into account not only the user community’s format and communication preferences, but also a consideration of communities that should not have access for various legal or privacy reasons.


There are three elements for essential needs of institutions dealing with issues of digital curation: Leadership, Resources, and Collaboration. Three elements related to the role of advance-guards for librarians and archivists working with open approaches to technology, standardized process and scholarly communication. The archivist with leadership, who needs to be a dynamic and active role to embrace technology, standardized process, and scholarly communication. In addition, Archivist leader might adopt the business concept and methods to deal with their workflow such as raise funds, invest technology system, and comply with industry standards, in order to obtain more resources. Collaboration in archives and digital curation community could provide and share training, technologies, standards and tools to help institutions on challengeable issues of digital curation. Digital Preservation Coalition (DPC), the Open Preservation Foundation or novel partnerships offer collaboration opportunity to institutions facing similar challenges in digital curation issues.


Information field especially in libraries, archives, and museums significantly need to bring knowledge of new technologies. Traditional graduate school education is not enough to meet that demand; training program for current staffs in cultural repository would be an efficient supplement for that request, such as professional workshops, and MOOCs (Massively Open Online Courses) in data curation and management.


International Digital Curation Conference (IDCC) is an established annual event since 2005, aiming to collaborate with individuals, organizations and institutions facing challenges, supporting development, and exchanging ideas in the field.


The International Journal of Digital Curation (IJDC) is administered by IJDC Editorial Board including the Editor-in-Chief, Digital Curation Center (DCC), and the following members.  IJDC dedicate to provide scholarly platform for sharing, discussing, and improving knowledge and information of digital curation within the worldwide community. IJDC has two types of submission under editorial guidelines, which are peer-reviewed papers and general articles base on original research, the field information and relevant events in digital curation. IJDC is published by the University of Edinburgh for the Digital Curation Centre in electronic form on a rolling basis two times a year. The open access to the public supports knowledge exchangeable in digital curation worldwide.

Many approaches to digital curation exist, and have evolved over time in response to the changing technological landscape. Two examples of this are sheer curation and channelization.

"Sheer curation" is an approach to digital curation where curation activities are quietly integrated into the normal work flow of those creating and managing data and other digital assets. The word sheer is used to emphasize the lightweight and virtually transparent nature of these curation activities. The term "sheer curation" was coined by Alistair Miles in the ImageStore project, and the UK Digital Curation Centre's SCARP project. The approach depends on curators having close contact or 'immersion' in data creators' working practices. An example is the case study of a neuroimaging research group by Whyte et al., which explored ways of building its digital curation capacity around the apprenticeship style of learning of neuroimaging researchers, through which they share access to datasets and re-use experimental procedures.

Sheer curation depends on the hypothesis that good data and digital asset management at the point of creation and primary use is also good practice in preparation for sharing, publication and/or long-term preservation of these assets. Therefore, sheer curation attempts to identify and promote tools and good practices in local data and digital asset management in specific domains, where those tools and practices add immediate value to the creators and primary users of those assets. Curation can best be supported by identifying existing practices of sharing, stewardship and re-use that add value, and augmenting them in ways that both have short-term benefits, and in the longer term reduce risks to digital assets or provide new opportunities to sustain their long-term accessibility and re-use value.

The aim of sheer curation is to establish a solid foundation for other curation activities which may not directly benefit the creators and primary users of digital assets, especially those required to ensure long-term preservation. By providing this foundation, further curation activities may be carried out by specialists at appropriate institutional and organisation levels, whilst causing the minimum of interference to others.

A similar idea is "curation at source" used in the context of Laboratory Information Management Systems LIMS. This refers more specifically to automatic recording of metadata or information about data at the point of capture, and has been developed to apply semantic web techniques to integrate laboratory instrumentation and documentation systems. Sheer curation and curation-at-source can be contrasted with post hoc digital preservation, where a project is initiated to preserve a collection of digital assets that have already been created and are beyond the period of their primary use.

"Channelization" is curation of digital assets on the web, often by brands and media companies, into continuous flows of content, turning the user experience from a lean-forward interactive medium, to a lean-back passive medium. The curation of content can be done by an independent third party, that selects media from any number of on-demand outlets from across the globe and adds them to a playlist to offer a digital "channel" dedicated to certain subjects, themes, or interests so that the end user would see and/or hear a continuous stream of content.




</doc>
<doc id="16934252" url="https://en.wikipedia.org/wiki?curid=16934252" title="Data pack">
Data pack

A data pack (or fact pack) is a pre-made database that can be fed to a software, such as software agents, Internet bots or chatterbots, to teach information and facts, which it can later look up. In other words, a data pack can be used to feed minor updates into a system.

Common data packs may include abbreviations, acronyms, dictionaries, lexicons and technical data, such as country codes, RFCs, filename extensions, TCP and UDP port numbers, country calling codes, and so on.

Data packs may come in formats of CSV and SQL that can easily be parsed or imported into a database management system.

The database may consist of a key-value pair, like an association list.

Data packs are commonly used within the gaming industry to provide minor updates within their games. When a user downloads an update for a game they will be downloading loads of data packs which will contain updates for the game such as minor bug fixes or additional content. An example of a data pack used to update a game can be found on the references.

A data pack DataPack Definition is similar to a data packet it contains loads of information (data) and stores it within a pack where the data can be compressed to reduce its file size. Only certain programs can read a data pack therefore when the data is packed it is vital to know whether the receiving program is able to unpack the data. An example of data packs which are able to effective deliver information can be found on the reference page.

When you refer to the word data pack it can come in many forms such as a mobile data pack. A mobile data pack refers to an add-on which can enable you to boost the amount of data which you can use on your mobile phone. The rate at which you use your data can also be monitored, so you know how much data you have left. Mobile data is a service which provides a similar service to Wi-Fi and allows you to connect to the Internet. So the purpose of a data pack is to increase the amount of data that your mobile has access to. An example of a mobile data pack can be found on the references.




</doc>
<doc id="17220957" url="https://en.wikipedia.org/wiki?curid=17220957" title="Range query (database)">
Range query (database)

A range query is a common database operation that retrieves all records where some value is between an upper and lower boundary. For example, list all employees with 3 to 5 years' experience. Range queries are unusual because it is not generally known in advance how many entries a range query will return, or if it will return any at all. Many other queries, such as the top ten most senior employees, or the newest employee, can be done more efficiently because there is an upper bound to the number of results they will return. A query that returns exactly one result is sometimes called a singleton.

Match at least one of the requested keys.



</doc>
<doc id="15219872" url="https://en.wikipedia.org/wiki?curid=15219872" title="Load file">
Load file

A load file in the litigation community is commonly referred to as the file used to import data (coded, captured or extracted data from ESI processing) into a database; or the file used to link images. These load files carry commands, commanding the software to carry out certain functions with the data found in them.

Load files are usually ASCII text files that have delimited fields of information. Such load files may have data about documents to be imported into a document management software such as Concordance or Summation. Or they may have the path or directory where images may reside so that the software can link such images to their corresponding records.

Some database programs take one load file for importing images and another for importing data while others take only one load file for both pieces of information.

OCR or Search-able Text which is considered "data" is also imported into most database programs via the same load files. Though some people prefer to load the OCR into their databases by running a separate command to search and find the desired text.

Commonly used databases and their corresponding file extensions are: Summation (DII
, CSV), Concordance (OPT, DAT), Sanction (SDT), IPRO (LFP), Ringtail (MDB) and DB/TextWorks (TXT).


</doc>
<doc id="18825039" url="https://en.wikipedia.org/wiki?curid=18825039" title="Halloween Problem">
Halloween Problem

In computing, the Halloween Problem refers to a phenomenon in databases in which an update operation causes a change in the physical location of a row, potentially allowing the row to be visited more than once during the operation. This could even cause an infinite loop in some cases where updates continually place the updated record ahead of the scan performing the update operation.

The potential for this database error was first discovered by Don Chamberlin, Pat Selinger, and Morton Astrahan in 1976, on Halloween day while working on a query that was supposed to give a ten percent raise to every employee who earned less than $25,000. This query would run successfully, with no errors, but when finished all the employees in the database earned at least $25,000, because it kept giving them a raise until they reached that level. The expectation was that the query would iterate over each of the employee records with a salary less than $25,000 precisely once. In fact, because even updated records were visible to the query execution engine and so continued to match the query's criteria, salary records were matching multiple times and each time being given a 10% raise until they were all greater than $25,000.

The name is not descriptive of the nature of the problem but rather was given due to the day it was discovered. As recounted by Don Chamberlin:

Pat and Morton discovered this problem on Halloween ... I remember they came into my office and said, ‘Chamberlin, look at this. We have to make sure that when the optimizer is making a plan for processing an update, it doesn’t use an index that is based on the field that is being updated. How are we going to do that?’ It happened to be on a Friday, and we said, ‘Listen, we are not going to be able to solve this problem this afternoon. Let’s just give it a name. We’ll call it the Halloween Problem and we’ll work on it next week.’ And it turns out it has been called that ever since.


</doc>
<doc id="15557750" url="https://en.wikipedia.org/wiki?curid=15557750" title="Spindling">
Spindling

In computers spindling is the allocation of different files (e.g., the data files and index files of a database) on different hard disks. This practice usually reduces contention for read or write resources, thus increasing the system's performance.

The word comes from spindle, the axis on which the hard disks spin.


</doc>
<doc id="1064009" url="https://en.wikipedia.org/wiki?curid=1064009" title="Object Data Management Group">
Object Data Management Group

The Object Data Management Group (ODMG) was conceived in the summer of 1991 at a breakfast with object database vendors that was organized by Rick Cattell of Sun Microsystems. In 1998, the ODMG changed its name from the Object Database Management Group to reflect the expansion of its efforts to include specifications for both object database and object-relational mapping products.

The primary goal of the ODMG was to put forward a set of specifications that allowed a developer to write portable applications for object database and object-relational mapping products. In order to do that, the data schema, programming language bindings, and data manipulation and query languages needed to be portable.

Between 1993 and 2001, the ODMG published five revisions to its specification. The last revision was ODMG version 3.0, after which the group disbanded.


ODMG 3.0 was published in book form in 2000. By 2001, most of the major object database and object-relational mapping vendors claimed conformance to the ODMG Java Language Binding. Compliance to the other components of the specification was mixed. In 2001, the ODMG Java Language Binding was submitted to the Java Community Process as a basis for the Java Data Objects specification. The ODMG member companies then decided to concentrate their efforts on the Java Data Objects specification. As a result, the ODMG disbanded in 2001.

In 2004, the Object Management Group (OMG) was granted the right to revise the ODMG 3.0 specification as an OMG specification by the copyright holder, Morgan Kaufmann Publishers. In February 2006, the OMG announced the formation of the Object Database Technology Working Group (ODBT WG) and plans to work on the 4th generation of an object database standard.





</doc>
<doc id="21064035" url="https://en.wikipedia.org/wiki?curid=21064035" title="Locks with ordered sharing">
Locks with ordered sharing

In databases and transaction processing the term Locks with ordered sharing comprises several variants of the "Two phase locking" (2PL) concurrency control protocol generated by changing the blocking semantics of locks upon conflicts. One variant is identical to Strict commitment ordering (SCO).



</doc>
<doc id="21463262" url="https://en.wikipedia.org/wiki?curid=21463262" title="Datasource">
Datasource

DataSource is a name given to the connection set up to a database from a server. The name is commonly used when creating a query to the database. The data source name (DSN) need not be the same as the filename for the database. For example, a database file named "friends.mdb" could be set up with a DSN of "school". Then DSN "school" would be used to refer to the database when performing a query.

A factory for connections to the physical data source that this DataSource object represents. An alternative to the DriverManager facility, a DataSource object is the preferred means of getting a connection. An object that implements the DataSource interface will typically be registered with a naming service based on the Java Naming and Directory Interface (JNDI) API.

The DataSource interface is implemented by a driver vendor. There are three types of implementations:
A DataSource object has properties that can be modified when necessary. For example, if the data source is moved to a different server, the property for the server can be changed. The benefit is that because the data source's properties can be changed, any code accessing that data source does not need to be changed.

A driver that is accessed via a DataSource object does not register itself with the DriverManager. Rather, a DataSource object is retrieved through a lookup operation and then used to create a Connection object. With a basic implementation, the connection obtained through a DataSource object is identical to a connection obtained through the DriverManager facility.

A DataSource object is the representation of a data source in the Java programming language. In basic terms, a data source is a facility for storing data. It can be as sophisticated as a complex database for a large corporation or as simple as a file with rows and columns. A data source can reside on a remote server, or it can be on a local desktop machine. Applications access a data source using a connection, and a DataSource object can be thought of as a factory for connections to the particular data source that the DataSource instance represents. The DataSource interface provides two methods for establishing a connection with a data source.

Using a DataSource object is the preferred alternative to using the DriverManager for establishing a connection to a data source. They are similar to the extent that the DriverManager class and DataSource interface both have methods for creating a connection, methods for getting and setting a timeout limit for making a connection, and methods for getting and setting a stream for logging.

Their differences are more significant than their similarities, however. Unlike the DriverManager, a DataSource object has properties that identify and describe the data source it represents. Also, a DataSource object works with a Java Naming and Directory Interface (JNDI) naming service and can be created, deployed, and managed separately from the applications that use it. A driver vendor will provide a class that is a basic implementation of the DataSource interface as part of its Java Database Connectivity (JDBC) 2.0 or 3.0 driver product. What a system administrator does to register a DataSource object with a JNDI naming service and what an application does to get a connection to a data source using a DataSource object registered with a JNDI naming service are described later in this chapter.

Being registered with a JNDI naming service gives a DataSource object two major advantages over the DriverManager. First, an application does not need to hardcode driver information, as it does with the DriverManager. A programmer can choose a logical name for the data source and register the logical name with a JNDI naming service. The application uses the logical name, and the JNDI naming service will supply the DataSource object associated with the logical name. The DataSource object can then be used to create a connection to the data source it represents.

The second major advantage is that the DataSource facility allows developers to implement a DataSource class to take advantage of features like connection pooling and distributed transactions. Connection pooling can increase performance dramatically by reusing connections rather than creating a new physical connection each time a connection is requested. The ability to use distributed transactions enables an application to do the heavy duty database work of large enterprises.

Although an application may use either the DriverManager or a DataSource object to get a connection, using a DataSource object offers significant advantages and is the recommended way to establish a connection.

Since 1.4

Since Java EE 6 a JNDI-bound DataSource can alternatively be configured in a declarative way directly from within the application. This alternative is particularly useful for self-sufficient applications or for transparently using an embedded database.

A DataSource is an abstract representation of a live set of data that presents a common predictable API for other objects to interact with. The nature of your data, its quantity, its complexity, and the logic for returning query results all play a role in determining your type of DataSource. For small amounts of simple textual data, a JavaScript array is a good choice. If your data has a small footprint but requires a simple computational or transformational filter before being displayed, a JavaScript function may be the right approach. For very large datasets—for example, a robust relational database—or to access a third-party webservice you'll certainly need to leverage the power of a Script Node or XHR DataSource.


</doc>
<doc id="22796020" url="https://en.wikipedia.org/wiki?curid=22796020" title="Autocommit">
Autocommit

In the context of data management, autocommit is a mode of operation of a database connection. Each individual database interaction (i.e., each SQL statement) submitted through the database connection in autocommit mode will be executed in its own transaction that is implicitly committed. A SQL statement executed in autocommit mode cannot be rolled back.

Autocommit mode, in theory, incurs per-statement transaction overhead, having often undesirable performance or resource utilization impact. Nonetheless, in systems such as Microsoft SQL Server, as well as connection technologies such as ODBC and Microsoft OLE DB, autocommit mode is the default for all statements that change data, in order to ensure that individual statements will conform to the ACID (atomicity-consistency-isolation-durability) properties of transactions.

The alternative to autocommit mode (non-autocommit) means that the SQL client application itself is responsible for issuing transaction initiation ("start transaction") and termination ("commit" or "rollback") commands. Non-autocommit mode enables grouping of multiple data manipulation SQL commands into a single atomic transaction.

Most DBMS (e.g. MariaDB) force autocommit for every DDL statement even in non-autocommit mode. Before DDL statement starts previous DML statements in transaction are (auto)committed. Each DDL is executed in own new autocommit transaction. For following DDL statements new transaction is automatically started if needed.



</doc>
<doc id="23718723" url="https://en.wikipedia.org/wiki?curid=23718723" title="Metadatabase">
Metadatabase

Metadatabase is a database model for (1) metadata management, (2) global query of independent databases, and (3) distributed data processing. The word "metadatabase" is an addition to the dictionary. Originally, metadata was only a common term referring simply to "data about data", such as tags, keywords, and markup headers. However, in this technology, the concept of metadata is extended to also include such data and knowledge representation as information models (e.g., relations, entities-relationships, and objects), application logic (e.g., production rules), and analytic models (e.g., simulation, optimization, and mathematical algorithms). In the case of analytic models, it is also referred to as a Modelbase.

These classes of metadata are integrated with some modeling ontology to give rise to a stable set of meta-relations (tables of metadata). Individual models are interpreted as metadata and entered into these tables. As such, models are inserted, retrieved, updated, and deleted in the same manner as ordinary data do in an ordinary (relational) database. Users will also formulate global queries and requests for processing of local databases through the Metadatabase, using the globally integrated metadata. The Metadatabase structure can be implemented in any open technology for relational databases.

The Metadatabase technology is developed at Rensselaer Polytechnic Institute at Troy, New York, by a group of faculty and students (see the references at the end of the article), starting in late 1980s. Its main contribution includes the extension of the concept of metadata and metadata management, and the original approach of designing a database for metadata applications. These conceptual results continue to motivate new research and new applications. At the level of particular design, its openness and scalability is tied to that of the particular ontology proposed: It requires reverse-representation of the application models in order to save them into the meta-relations. In theory, the ontology is neutral, and it has been proven in some industrial applications. However, it needs more development to establish it for the field as an open technology. The requirement of reverse-representation is common to any global information integration technology. A way to facilitate it is in the Metadatabase approach is to distribute a core portion of it at each local site, to allow for peer-to-peer translation on the fly.



</doc>
<doc id="4615965" url="https://en.wikipedia.org/wiki?curid=4615965" title="Snapshot isolation">
Snapshot isolation

In databases, and transaction processing (transaction management), snapshot isolation is a guarantee that all reads made in a transaction will see a consistent snapshot of the database (in practice it reads the last committed values that existed at the time it started), and the transaction itself will successfully commit only if no updates it has made conflict with any concurrent updates made since that snapshot.

Snapshot isolation has been adopted by several major database management systems, such as InterBase, Firebird, Oracle, MySQL, PostgreSQL, SQL Anywhere, MongoDB and Microsoft SQL Server (2005 and later). The main reason for its adoption is that it allows better performance than serializability, yet still avoids most of the concurrency anomalies that serializability avoids (but not always all). In practice snapshot isolation is implemented within multiversion concurrency control (MVCC), where generational values of each data item (versions) are maintained: MVCC is a common way to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object). Snapshot isolation has been used to criticize the ANSI SQL-92 standard's definition of isolation levels, as it exhibits none of the "anomalies" that the SQL standard prohibited, yet is not serializable (the anomaly-free isolation level defined by ANSI).

In spite of its distinction from serializability, snapshot isolation is sometimes referred to as "serializable" by Oracle.

A transaction executing under snapshot isolation appears to operate on a personal "snapshot" of the database, taken at the start of the transaction. When the transaction concludes, it will successfully commit only if the values updated by the transaction have not been changed externally since the snapshot was taken. Such a write–write conflict will cause the transaction to abort.

In a "write skew" anomaly, two transactions (T1 and T2) concurrently read an overlapping data set (e.g. values V1 and V2), concurrently make disjoint updates (e.g. T1 updates V1, T2 updates V2), and finally concurrently commit, neither having seen the update performed by the other. Were the system serializable, such an anomaly would be impossible, as either T1 or T2 would have to occur "first", and be visible to the other. In contrast, snapshot isolation permits write skew anomalies.

As a concrete example, imagine V1 and V2 are two balances held by a single person, Phil. The bank will allow either V1 or V2 to run a deficit, provided the total held in both is never negative (i.e. V1 + V2 ≥ 0). Both balances are currently $100. Phil initiates two transactions concurrently, T1 withdrawing $200 from V1, and T2 withdrawing $200 from V2.

If the database guaranteed serializable transactions, the simplest way of coding T1 is to deduct $200 from V1, and then verify that V1 + V2 ≥ 0 still holds, aborting if not. T2 similarly deducts $200 from V2 and then verifies V1 + V2 ≥ 0. Since the transactions must serialize, either T1 happens first, leaving V1 = −$100, V2 = $100, and preventing T2 from succeeding (since V1 + (V2 − $200) is now −$200), or T2 happens first and similarly prevents T1 from committing.

If the database is under snapshot isolation(MVCC), however, T1 and T2 operate on private snapshots of the database: each deducts $200 from an account, and then verifies that the new total is zero, using the other account value that held when the snapshot was taken. Since neither "update" conflicts, both commit successfully, leaving V1 = V2 = −$100, and V1 + V2 = −$200.

Some systems built using multiversion concurrency control (MVCC) may support (only) snapshot isolation to allow transactions to proceed without worrying about concurrent operations, and more importantly without needing to re-verify all read operations when the transaction finally commits. This is convenient because MVCC maintains a series of recent history consistent states. The only information that must be stored during the transaction is a list of updates made, which can be scanned for conflicts fairly easily before being committed. However MVCC systems (such as MarkLogic) will use locks to serialize writes together with MVCC to gain some of the performance gains and still support the stronger "serializability" level of isolation.

Potential inconsistency problems arising from write skew anomalies can be fixed by adding (otherwise unnecessary) updates to the transactions in order to enforce the serializability property.


In the example above, we can materialize the conflict by adding a new table which makes the hidden constraint explicit, mapping each person to their "total balance". Phil would start off with a total balance of $200, and each transaction would attempt to subtract $200 from this, creating a write–write conflict that would prevent the two from succeeding concurrently. However, this approach violates the normal form.

Alternatively, we can promote one of the transaction's reads to a write. For instance, T2 could set V1 = V1, creating an artificial write–write conflict with T1 and, again, preventing the two from succeeding concurrently. This solution may not always be possible.

In general, therefore, snapshot isolation puts some of the problem of maintaining non-trivial constraints onto the user, who may not appreciate either the potential pitfalls or the possible solutions. The upside to this transfer is better performance.

Snapshot isolation is called "serializable" mode in Oracle and PostgreSQL versions prior to 9.1, which may cause confusion with the "real serializability" mode. There are arguments both for and against this decision; what is clear is that users must be aware of the distinction to avoid possible undesired anomalous behavior in their database system logic.

Snapshot isolation arose from work on multiversion concurrency control databases, where multiple versions of the database are maintained concurrently to allow readers to execute without colliding with writers. Such a system allows a natural definition and implementation of such an isolation level. InterBase, later owned by Borland, was acknowledged to provide SI rather than full serializability in version 4, and likely permitted write-skew anomalies since its first release in 1985.

Unfortunately, the ANSI SQL-92 standard was written with a lock-based database in mind, and hence is rather vague when applied to MVCC systems. Berenson "et al." wrote a paper in 1995 critiquing the SQL standard, and cited snapshot isolation as an example of an isolation level that did not exhibit the standard anomalies described in the ANSI SQL-92 standard, yet still had anomalous behaviour when compared with serializable transactions.

In 2008, Cahill "et al." showed that write-skew anomalies could be prevented by detecting and aborting "dangerous" triplets of concurrent transactions. This implementation of serializability is well-suited to multiversion concurrency control databases, and has been adopted in PostgreSQL 9.1, 
where it is referred to as "Serializable Snapshot Isolation", abbreviated to SSI. When used consistently, this eliminates the need for the above workarounds. The downside over snapshot isolation is an increase in aborted transactions. This can perform better or worse than snapshot isolation with the above workarounds, depending on workload.

In 2011, Jimenez-Peris et al. filed a patent where it was shown how it was possible to scale to many millions of update transactions per second with a new method for attaining snapshot isolation in a distributed manner. The method is based on the observation that it becomes possible to commit transactions fully in parallel without any coordination and therefore removing the bottleneck of traditional transactional processing methods. The method uses a commit sequencer that generates commit timestamps and a snapshot server that advances the current snapshot as gaps are filled in the serialization order. This method is the base of the database LeanXcale. The first implementation of this method was made in 2010 as part of the CumuloNimbo European Project.



</doc>
<doc id="18218754" url="https://en.wikipedia.org/wiki?curid=18218754" title="Data masking">
Data masking

Data masking or data obfuscation is the process of hiding original data with modified content (characters or other data.)

The main reason for applying masking to a data field is to protect data that is classified as personally identifiable information, sensitive personal data, or commercially sensitive data. However, the data must remain usable for the purposes of undertaking valid test cycles. It must also look real and appear consistent. It is more common to have masking applied to data that is represented outside of a corporate production system. In other words, where data is needed for the purpose of application development, building program extensions and conducting various test cycles. It is common practice in enterprise computing to take data from the production systems to fill the data component, required for these non-production environments. However, this practice is not always restricted to non-production environments. In some organizations, data that appears on terminal screens to call centre operators may have masking dynamically applied based on user security permissions (e.g. preventing call centre operators from viewing Credit Card Numbers in billing systems).

The primary concern from a corporate governance perspective is that personnel conducting work in these non-production environments are not always security cleared to operate with the information contained in the production data. This practice represents a security hole where data can be copied by unauthorized personnel and security measures associated with standard production level controls can be easily bypassed. This represents an access point for a data security breach.

The overall practice of Data Masking at an organizational level should be tightly coupled with the Test Management Practice and underlying Methodology and should incorporate processes for the distribution of masked test data subsets.

Data involved in any data-masking or obfuscation must remain meaningful at several levels:


Substitution is one of the most effective methods of applying data masking and being able to preserve the authentic look and feel of the data records.

It allows the masking to be performed in such a manner that another authentic looking value can be substituted for the existing value. There are several data field types where this approach provides optimal benefit in disguising the overall data sub set as to whether or not it is a masked data set. For example, if dealing with source data which contains customer records, real life surname or first name can be randomly substituted from a supplied or customised look up file. If the first pass of the substitution allows for applying a male first name to all first names, then the second pass would need to allow for applying a female first name to all first names where gender equals "F". Using this approach we could easily maintain the gender mix within the data structure, apply anonymity to the data records but also maintain a realistic looking database which could not easily be identified as a database consisting of masked data.

This substitution method needs to be applied for many of the fields that are in DB structures across the world, such as telephone numbers, zip codes and postcodes, as well as credit card numbers and other card type numbers like Social Security numbers and Medicare numbers where these numbers actually need to conform to a checksum test of the Luhn algorithm.

In most cases, the substitution files will need to be fairly extensive so having large substitution datasets as well the ability to apply customized data substitution sets should be a key element of the evaluation criteria for any data masking solution.

The shuffling method is a very common form of data obfuscation. It is similar to the substitution method but it derives the substitution set from the same column of data that is being masked. In very simple terms, the data is randomly shuffled within the column. However, if used in isolation, anyone with any knowledge of the original data can then apply a "What If" scenario to the data set and then piece back together a real identity. The shuffling method is also open to being reversed if the shuffling algorithm can be deciphered.

Shuffling, however, has some real strengths in certain areas. If for instance, the end of year figures for financial information in a test data base, one can mask the names of the suppliers and then shuffle the value of the accounts throughout the masked database. It is highly unlikely that anyone, even someone with intimate knowledge of the original data could derive a true data record back to its original values.

The numeric variance method is very useful for applying to financial and date driven information fields. Effectively, a method utilising this manner of masking can still leave a meaningful range in a financial data set such as payroll. If the variance applied is around +/- 10% then it is still a very meaningful data set in terms of the ranges of salaries that are paid to the recipients.

The same also applies to the date information. If the overall data set needs to retain demographic and actuarial data integrity then applying a random numeric variance of +/- 120 days to date fields would preserve the date distribution but still prevent traceability back to a known entity based on their known actual date or birth or a known date value of whatever record is being masked...

Encryption is often the most complex approach to solving the data masking problem. The encryption algorithm often requires that a "key" be applied to view the data based on user rights. This often sounds like the best solution but in practice the key may then be given out to personnel without the proper rights to view the data and this then defeats the purpose of the masking exercise. Old databases may then be copied with the original credentials of the supplied key and the same uncontrolled problem lives on.

Recently, the problem of encrypting data while preserving the properties of the entities got a recognition and newly acquired interest among the vendors and academia. New challenge gave birth to algorithms called FPE (format preserving encryption). They are based on the accepted AES algorithmic mode that makes them being recognized by NIST.
Sometimes a very simplistic approach to masking is adopted through applying a null value to a particular field. The null value approach is really only useful to prevent visibility of the data element.

In almost all cases it lessens the degree of data integrity that is maintained in the masked data set. It is not a realistic value and will then fail any application logic validation that may have been applied in the front end software that is in the system under test. It also highlights to anyone that wishes to reverse engineer any of the identity data that data masking has been applied to some degree on the data set.

Character scrambling or masking out of certain fields is also another simplistic yet very effective method of preventing sensitive information to be viewed. It is really an extension of the previous method of nulling out but there is greater emphasis on keeping the data real and not fully masked all together.

This is commonly applied to credit card data in production systems. For instance, an operator in a Call Centre might bill an item to a customer's credit card. They then quote a billing reference to the card with the last 4 digits of XXXX XXXX xxxx 6789. As an operator they can only see the last 4 digits of the card number, but once the billing system passes the customer's details for charging, the full number is revealed to the payment gateway systems.

This system is not very effective for test systems but is very useful for the billing scenario detailed above. It is also commonly known as a dynamic data masking method.

Additional rules can also be factored into any masking solution regardless of how the masking methods are constructed. Product agnostic White Papers are a good source of information for exploring some of the more common complex requirements for enterprise masking solutions which include Row Internal Synchronisation Rules, Table Internal Synchronisation Rules and Table to Table Synchronisation Rules.

Data masking is tightly coupled with building test data. Two major types of data masking are static and on-the-fly data masking.
Static Data Masking is usually performed on the golden copy of the database, but can also be applied to values in other sources, including files. In DB environments, production DBAs will typically load table backups to a separate environment, reduce the dataset to a subset that holds the data necessary for a particular round of testing (a technique called "subsetting"), apply data masking rules while data is in stasis, apply necessary code changes from source control, and/or and push data to desired environment.

There are also alternatives to the static data masking that rely on stochastic perturbations of the data that preserve some of the statistical properties of the original data. Examples of statistical data obfuscation methods include differential privacy

and the "DataSifter" method

On-the-Fly Data Masking happens in the process of transferring data from environment to environment without data touching the disk on its way. The same technique is applied to "Dynamic Data Masking" but one record at a time. This type of data masking is most useful for environments that do continuous deployments as well as for heavily integrated applications. Organizations that employ continuous deployment or continuous delivery practices do not have the time necessary to create a backup and load it to the golden copy of the database. Thus, continuously sending smaller subsets (deltas) of masked testing data from production is important. In heavily integrated applications, developers get feeds from other production systems at the very onset of development and masking of these feeds is either overlooked and not budgeted until later, making organizations non-compliant. Having on-the-fly data masking in place becomes essential.

Dynamic Data Masking is similar to On-the-Fly Data Masking but it differs in the sense that On-the-Fly Data Masking is about copying data from one source to another source so that the latter can be shared. Dynamic data masking happens at runtime, dynamically, and on-demand so that there doesn't need to be a second data source where to store the masked data dynamically.

Dynamic data masking enables several scenarios, many of which revolve around strict privacy regulations e.g. the Singapore Monetary Authority or the Privacy regulations in Europe.

Dynamic data masking is attribute-based and policy-driven. Policies include:

Dynamic data masking can also be used to encrypt or decrypt values on the fly especially when using format-preserving encryption.

Several standards have emerged in recent years to implement dynamic data filtering and masking. For instance, XACML policies can be used to mask data inside databases.

There are five possible technologies to apply Dynamic data masking:

In latest years, organizations develop their new applications in the cloud more and more often, regardless of whether final applications will be hosted in the cloud or on- premises. The cloud solutions as of now allow organizations to use Infrastructure as a Service or IaaS, Platform as a Service or PaaS, and Software as a Service or SaaS. There are various modes of creating test data and moving it from on-premises databases to the cloud, or between different environments within the cloud. Data masking invariably becomes the part of these processes in SDLC as the development environments' SLAs are usually not as stringent as the production environments' SLAs regardless of whether application is hosted in the cloud or on-premises.



</doc>
<doc id="244602" url="https://en.wikipedia.org/wiki?curid=244602" title="Two-phase locking">
Two-phase locking

In databases and transaction processing, two-phase locking (2PL) is a concurrency control method that guarantees serializability.
It is also the name of the resulting set of database transaction schedules (histories). The protocol utilizes locks, applied by a transaction to data, which may block (interpreted as signals to stop) other transactions from accessing the same data during the transaction's life.

By the 2PL protocol, locks are applied and removed in two phases:
Two types of locks are utilized by the basic protocol: "Shared" and "Exclusive" locks. Refinements of the basic protocol may utilize more lock types. Using locks that block processes, 2PL may be subject to deadlocks that result from the mutual blocking of two or more transactions.

A lock is a system object associated with a shared resource such as a data item of an elementary type, a row in a database, or a page of memory. In a database, a lock on a database object (a data-access lock) may need to be acquired by a transaction before accessing the object. Correct use of locks prevents undesired, incorrect or inconsistent operations on shared resources by other concurrent transactions. When a database object with an existing lock acquired by one transaction needs to be accessed by another transaction, the existing lock for the object and the type of the intended access are checked by the system. If the existing lock type does not allow this specific attempted concurrent access type, the transaction attempting access is blocked (according to a predefined agreement/scheme). In practice, a lock on an object does not directly block a transaction's operation upon the object, but rather blocks that transaction from acquiring another lock on the same object, needed to be held/owned by the transaction before performing this operation. Thus, with a locking mechanism, needed operation blocking is controlled by a proper lock blocking scheme, which indicates which lock type blocks which lock type.

Two major types of locks are utilized:


The common interactions between these lock types are defined by blocking behavior as follows:


Several variations and refinements of these major lock types exist, with respective variations of blocking behavior. If a first lock blocks another lock, the two locks are called "incompatible"; otherwise the locks are "compatible". Often, lock types blocking interactions are presented in the technical literature by a "Lock compatibility table". The following is an example with the common, major lock types:

According to the two-phase locking protocol, a transaction handles its locks in two distinct, consecutive phases during the transaction's execution:


The two phase locking rule can be summarized as: never acquire a lock after a lock has been released. The serializability property is guaranteed for a schedule with transactions that obey this rule.

Typically, without explicit knowledge in a transaction on end of phase 1, it is safely determined only when a transaction has completed processing and requested commit. In this case, all the locks can be released at once (phase 2).

The difference between 2PL and C2PL is that C2PL's transactions obtain all the locks they need before the transactions begin. This is to ensure that a transaction that already holds some locks will not block waiting for other locks. Conservative 2PL "prevents" deadlocks.

To comply with the S2PL protocol, a transaction needs to comply with 2PL, and release its "write (exclusive)" locks only after it has ended, i.e., being either "committed" or "aborted". On the other hand, "read (shared)" locks are released regularly during phase 2. This protocol is not appropriate in B-trees because it causes Bottleneck (while B-trees always starts searching from the parent root). 

or Rigorousness, or Rigorous scheduling, or Rigorous two-phase locking

To comply with strong strict two-phase locking (SS2PL) the locking protocol releases both "write (exclusive)" and "read (shared)" locks applied by a transaction only after the transaction has ended, i.e., only after both completing executing (being "ready") and becoming either "committed" or "aborted". This protocol also complies with the S2PL rules. A transaction obeying SS2PL can be viewed as having phase 1 that lasts the transaction's entire execution duration, and no phase 2 (or a degenerate phase 2). Thus, only one phase is actually left, and "two-phase" in the name seems to be still utilized due to the historical development of the concept from 2PL, and 2PL being a super-class. The SS2PL property of a schedule is also called Rigorousness. It is also the name of the class of schedules having this property, and an SS2PL schedule is also called a "rigorous schedule". The term "Rigorousness" is free of the unnecessary legacy of "two-phase," as well as being independent of any (locking) mechanism (in principle other blocking mechanisms can be utilized). The property's respective locking mechanism is sometimes referred to as Rigorous 2PL.

SS2PL is a special case of S2PL, i.e., the SS2PL class of schedules is a proper subclass of S2PL (every SS2PL schedule is also an S2PL schedule, but S2PL schedules exist that are not SS2PL).

SS2PL has been the concurrency control protocol of choice for most database systems and utilized since their early days in the 1970s. It is proven to be an effective mechanism in many situations, and provides besides Serializability also Strictness (a special case of cascadeless Recoverability), which is instrumental for efficient database recovery, and also Commitment ordering (CO) for participating in distributed environments where a CO based distributed serializability and global serializability solutions are employed. Being a subset of CO, an efficient implementation of "distributed SS2PL" exists without a distributed lock manager (DLM), while distributed deadlocks (see below) are resolved automatically. The fact that SS2PL employed in multi database systems ensures global serializability has been known for years before the discovery of CO, but only with CO came the understanding of the role of an atomic commitment protocol in maintaining global serializability, as well as the observation of automatic distributed deadlock resolution (see a detailed example of Distributed SS2PL). As a matter of fact, SS2PL inheriting properties of Recoverability and CO is more significant than being a subset of 2PL, which by itself in its general form, besides comprising a simple serializability mechanism (however serializability is also implied by CO), in not known to provide SS2PL with any other significant qualities. 2PL in its general form, as well as when combined with Strictness, i.e., Strict 2PL (S2PL), are not known to be utilized in practice. The popular SS2PL does not require marking "end of phase 1" as 2PL and S2PL do, and thus is simpler to implement. Also, unlike the general 2PL, SS2PL provides, as mentioned above, the useful Strictness and Commitment ordering properties.

Many variants of SS2PL exist that utilize various lock types with various semantics in different situations, including cases of lock-type change during a transaction. Notable are variants that use Multiple granularity locking.

Comments:

Between any two schedule classes (define by their schedules' respective properties) that have common schedules, either one "contains" the other ("strictly contains" if they are not equal), or they are "incomparable". The containment relationships among the 2PL classes and other major schedule classes are summarized in the following diagram. 2PL and its subclasses are "inherently blocking", which means that no optimistic implementations for them exist (and whenever "Optimistic 2PL" is mentioned it refers to a different mechanism with a class that includes also schedules not in the 2PL class).

Locks block data-access operations. Mutual blocking between transactions results in a "deadlock", where execution of these transactions is stalled, and no completion can be reached. Thus deadlocks need to be resolved to complete these transactions' executions and release related computing resources. A deadlock is a reflection of a potential cycle in the "precedence graph", that would occur without the blocking. A deadlock is resolved by aborting a transaction involved with such potential cycle, and breaking the cycle. It is often detected using a "wait-for graph" (a graph of conflicts blocked by locks from being materialized; conflicts not materialized in the database due to blocked operations are not reflected in the precedence graph and do not affect "serializability"), which indicates which transaction is "waiting for" lock release by which transaction, and a cycle means a deadlock. Aborting one transaction per cycle is sufficient to break the cycle. If a transaction has been aborted due to deadlock resolution, it is up to the application to decide what to do next. Usually, an application will restart the transaction from the beginning but may delay this action to give other transactions sufficient time to finish in order to avoid causing another deadlock.

In a distributed environment an atomic commitment protocol, typically the Two-phase commit (2PC) protocol, is utilized for atomicity. When recoverable data (data under transaction control) partitioned among 2PC participants (i.e., each data object is controlled by a single 2PC participant), then distributed (global) deadlocks, deadlocks involving two or more participants in 2PC, are resolved automatically as follows:

When SS2PL is effectively utilized in a distributed environment, then global deadlocks due to locking generate voting-deadlocks in 2PC, and are resolved automatically by 2PC (see Commitment ordering (CO), in Exact characterization of voting-deadlocks by global cycles; No reference except the CO articles is known to notice this). For the general case of 2PL, global deadlocks are similarly resolved automatically by the synchronization point protocol of phase-1 end in a distributed transaction (synchronization point is achieved by "voting" (notifying local phase-1 end), and being propagated to the participants in a distributed transaction the same way as a decision point in atomic commitment; in analogy to decision point in CO, a conflicting operation in 2PL cannot happen before phase-1 end synchronization point, with the same resulting voting-deadlock in the case of a global data-access deadlock; the voting-deadlock (which is also a locking based global deadlock) is automatically resolved by the protocol aborting some transaction involved, with a missing vote, typically using a timeout).

Comment:



</doc>
<doc id="24770596" url="https://en.wikipedia.org/wiki?curid=24770596" title="Data system">
Data system

Data system is a term used to refer to an organized collection of symbols and processes that may be used to operate on such symbols. Any organised collection of symbols and symbol-manipulating operations can be considered a data system. Hence, human-speech analysed at the level of phonemes can be considered a data system as can the Incan artefact of the khipu and an image stored as pixels. A data system is defined in terms of some data model and bears a resemblance to the idea of a physical symbol system.

Symbols within some data systems may be persistent or not. Hence, the sounds of human speech are non-persistent symbols because they decay rapidly in air. In contrast, pixels stored on some peripheral storage device are persistent symbols.

In education, a data system is a computer system that aims to provide educators with student data to help solve educational problems. Examples of data systems include Student Information Systems (SISs), assessment systems, Instructional Management Systems (IMSs), and data-warehousing systems, but distinctions between different types of data systems are blurring as these separate systems begin to serve more of the same functions. Data systems that present data to educators in an over-the-counter data format embed labels, supplemental documentation, and help system and make key package/display and content decisions to improve the accuracy of data system users’ data analyses.



</doc>
<doc id="23718280" url="https://en.wikipedia.org/wiki?curid=23718280" title="Variable data publishing">
Variable data publishing

Variable-data publishing (VDP) (also known as database publishing) is a term referring to the output of a variable composition system. While these systems can produce both electronically viewable and hard-copy (print) output, the "variable-data publishing" term today often distinguishes output destined for electronic viewing, rather than that which is destined for hard-copy print (e.g. variable data printing).

Essentially the same techniques are employed to perform variable-data publishing, as those utilized with variable data printing. The difference is in the interpretation for output. While variable-data printing may be interpreted to produce various print streams or page-description files (e.g. AFP/IPDS, PostScript, PCL), variable-data publishing produces electronically viewable files, most commonly seen in the forms of PDF, HTML, or XML.

Variable-data composition involves the use of data to conditionally:

Variable-data may be as simple as an address block or salutation. However, it can be any or all of the document's textual content—including words, sentences, paragraphs, pages, or the entire document. In other words, it can make up as little or as much of the document as the composer desires. Variable data may also be used to exhibit various images, such as logos, products, or membership photos. Further, variable-data can be used to build rule-based design schemes, including fonts, colors, and page formats. The possibilities are vast.

The variable-data tools available today, make it possible to perform variable-data composition at nearly every stage of document production. However, the level of control that can be achieved varies, based upon how far into the document production process a variable-data tool is deployed. For example, if variable-data insertion occurs just prior to output...it's not likely that the text flow or layout can be altered with nearly as much control as would be available at the time of initial document composition.

Many organizations will produce multiple forms of output (aka: multi-channel output), for the same document. This ensures that the published content is available to recipients via any form of access method they might require. When multi-channel output is utilized, integrity between those output channels often becomes important.

Variable-data publishing may be performed on everything from a personal computer to a mainframe system. However, the speed and practical output volumes which can be achieved are directly affected by the computer power utilized.

The term variable-data publishing was likely an offshoot of the term "variable-data printing", first introduced to the printing industry by Frank Romano, Professor Emeritus, School of Print Media, at the College of Imaging Arts and Sciences at Rochester Institute of Technology. However, the concept of merging static document elements and variable document elements predates the term and has seen various implementations ranging from simple desktop 'mail merge', to complex mainframe applications in the financial and banking industry. In the past, the term VDP has been most closely associated with digital printing machines. However, in the past 3 years the application of this technology has spread to web pages, emails, and mobile messaging.



</doc>
<doc id="217356" url="https://en.wikipedia.org/wiki?curid=217356" title="Concurrency control">
Concurrency control

In information technology and computer science, especially in the fields of computer programming, operating systems, multiprocessors, and databases, concurrency control ensures that correct results for concurrent operations are generated, while getting those results as quickly as possible.

Computer systems, both software and hardware, consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in memory or storage), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and theories to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm.

For example, a failure in concurrency control can result in data corruption from torn read or write operations.

Comments:

Concurrency control in Database management systems (DBMS; e.g., Bernstein et al. 1987, Weikum and Vossen 2001), other transactional objects, and related distributed applications (e.g., Grid computing and Cloud computing) ensures that "database transactions" are performed concurrently without violating the data integrity of the respective databases. Thus concurrency control is an essential element for correctness in any system where two database transactions or more, executed with time overlap, can access the same data, e.g., virtually in any general-purpose database system. Consequently, a vast body of related research has been accumulated since database systems emerged in the early 1970s. A well established concurrency control theory for database systems is outlined in the references mentioned above: serializability theory, which allows to effectively design and analyze concurrency control methods and mechanisms. An alternative theory for concurrency control of atomic transactions over abstract data types is presented in (Lynch et al. 1993), and not utilized below. This theory is more refined, complex, with a wider scope, and has been less utilized in the Database literature than the classical theory above. Each theory has its pros and cons, emphasis and insight. To some extent they are complementary, and their merging may be useful.

To ensure correctness, a DBMS usually guarantees that only "serializable" transaction schedules are generated, unless "serializability" is intentionally relaxed to increase performance, but only in cases where application correctness is not harmed. For maintaining correctness in cases of failed (aborted) transactions (which can always happen for many reasons) schedules also need to have the "recoverability" (from abort) property. A DBMS also guarantees that no effect of "committed" transactions is lost, and no effect of "aborted" (rolled back) transactions remains in the related database. Overall transaction characterization is usually summarized by the ACID rules below. As databases have become distributed, or needed to cooperate in distributed environments (e.g., Federated databases in the early 1990, and Cloud computing currently), the effective distribution of concurrency control mechanisms has received special attention.

The concept of a "database transaction" (or "atomic transaction") has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time, and "recovery" from a crash to a well understood database state. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). Every database transaction obeys the following rules (by support in the database system; i.e., a database system is designed to guarantee them for the transactions it runs):

The concept of atomic transaction has been extended during the years to what has become Business transactions which actually implement types of Workflow and are not atomic. However also such enhanced transactions typically utilize atomic transactions as components.

If transactions are executed "serially", i.e., sequentially with no overlap in time, no transaction concurrency exists. However, if concurrent transactions with interleaving operations are allowed in an uncontrolled manner, some unexpected, undesirable results may occur, such as:

Most high-performance transactional systems need to run transactions concurrently to meet their performance requirements. Thus, without concurrency control such systems can neither provide correct results nor maintain their databases consistently.

The main categories of concurrency control mechanisms are:

Different categories provide different performance, i.e., different average transaction completion rates ("throughput"), depending on transaction types mix, computing level of parallelism, and other factors. If selection and knowledge about trade-offs are available, then category and method should be chosen to provide the highest performance.

The mutual blocking between two transactions (where each one blocks the other) or more results in a deadlock, where the transactions involved are stalled and cannot reach completion. Most non-optimistic mechanisms (with blocking) are prone to deadlocks which are resolved by an intentional abort of a stalled transaction (which releases the other transactions in that deadlock), and its immediate restart and re-execution. The likelihood of a deadlock is typically low.

Blocking, deadlocks, and aborts all result in performance reduction, and hence the trade-offs between the categories.

Many methods for concurrency control exist. Most of them can be implemented within either main category above. The major methods, which have each many variants, and in some cases may overlap or be combined, are:

Other major concurrency control types that are utilized in conjunction with the methods above include:


The most common mechanism type in database systems since their early days in the 1970s has been "Strong strict Two-phase locking" (SS2PL; also called "Rigorous scheduling" or "Rigorous 2PL") which is a special case (variant) of both Two-phase locking (2PL) and Commitment ordering (CO). It is pessimistic. In spite of its long name (for historical reasons) the idea of the SS2PL mechanism is simple: "Release all locks applied by a transaction only after the transaction has ended." SS2PL (or Rigorousness) is also the name of the set of all schedules that can be generated by this mechanism, i.e., these are SS2PL (or Rigorous) schedules, have the SS2PL (or Rigorousness) property.

Concurrency control mechanisms firstly need to operate correctly, i.e., to maintain each transaction's integrity rules (as related to concurrency; application-specific integrity rule are out of the scope here) while transactions are running concurrently, and thus the integrity of the entire transactional system. Correctness needs to be achieved with as good performance as possible. In addition, increasingly a need exists to operate effectively while transactions are distributed over processes, computers, and computer networks. Other subjects that may affect concurrency control are recovery and replication.

For correctness, a common major goal of most concurrency control mechanisms is generating schedules with the "Serializability" property. Without serializability undesirable phenomena may occur, e.g., money may disappear from accounts, or be generated from nowhere. Serializability of a schedule means equivalence (in the resulting database values) to some "serial" schedule with the same transactions (i.e., in which transactions are sequential with no overlap in time, and thus completely isolated from each other: No concurrent access by any two transactions to the same data is possible). Serializability is considered the highest level of isolation among database transactions, and the major correctness criterion for concurrent transactions. In some cases compromised, relaxed forms of serializability are allowed for better performance (e.g., the popular "Snapshot isolation" mechanism) or to meet availability requirements in highly distributed systems (see "Eventual consistency"), but only if application's correctness is not violated by the relaxation (e.g., no relaxation is allowed for money transactions, since by relaxation money can disappear, or appear from nowhere).

Almost all implemented concurrency control mechanisms achieve serializability by providing "Conflict serializablity", a broad special case of serializability (i.e., it covers, enables most serializable schedules, and does not impose significant additional delay-causing constraints) which can be implemented efficiently.

Comment: While in the general area of systems the term "recoverability" may refer to the ability of a system to recover from failure or from an incorrect/forbidden state, within concurrency control of database systems this term has received a specific meaning.

Concurrency control typically also ensures the "Recoverability" property of schedules for maintaining correctness in cases of aborted transactions (which can always happen for many reasons). Recoverability (from abort) means that no committed transaction in a schedule has read data written by an aborted transaction. Such data disappear from the database (upon the abort) and are parts of an incorrect database state. Reading such data violates the consistency rule of ACID. Unlike Serializability, Recoverability cannot be compromised, relaxed at any case, since any relaxation results in quick database integrity violation upon aborts. The major methods listed above provide serializability mechanisms. None of them in its general form automatically provides recoverability, and special considerations and mechanism enhancements are needed to support recoverability. A commonly utilized special case of recoverability is "Strictness", which allows efficient database recovery from failure (but excludes optimistic implementations; e.g., Strict CO (SCO) cannot have an optimistic implementation, but has semi-optimistic ones).

Comment: Note that the "Recoverability" property is needed even if no database failure occurs and no database "recovery" from failure is needed. It is rather needed to correctly automatically handle transaction aborts, which may be unrelated to database failure and recovery from it.

With the fast technological development of computing the difference between local and distributed computing over low latency networks or buses is blurring. Thus the quite effective utilization of local techniques in such distributed environments is common, e.g., in computer clusters and multi-core processors. However the local techniques have their limitations and use multi-processes (or threads) supported by multi-processors (or multi-cores) to scale. This often turns transactions into distributed ones, if they themselves need to span multi-processes. In these cases most local concurrency control techniques do not scale well.

As database systems have become distributed, or started to cooperate in distributed environments (e.g., Federated databases in the early 1990s, and nowadays Grid computing, Cloud computing, and networks with smartphones), some transactions have become distributed. A distributed transaction means that the transaction spans processes, and may span computers and geographical sites. This generates a need in effective distributed concurrency control mechanisms. Achieving the Serializability property of a distributed system's schedule (see "Distributed serializability" and "Global serializability" ("Modular serializability")) effectively poses special challenges typically not met by most of the regular serializability mechanisms, originally designed to operate locally. This is especially due to a need in costly distribution of concurrency control information amid communication and computer latency. The only known general effective technique for distribution is Commitment ordering, which was disclosed publicly in 1991 (after being patented). Commitment ordering (Commit ordering, CO; Raz 1992) means that transactions' chronological order of commit events is kept compatible with their respective precedence order. CO does not require the distribution of concurrency control information and provides a general effective solution (reliable, high-performance, and scalable) for both distributed and global serializability, also in a heterogeneous environment with database systems (or other transactional objects) with different (any) concurrency control mechanisms. CO is indifferent to which mechanism is utilized, since it does not interfere with any transaction operation scheduling (which most mechanisms control), and only determines the order of commit events. Thus, CO enables the efficient distribution of all other mechanisms, and also the distribution of a mix of different (any) local mechanisms, for achieving distributed and global serializability. The existence of such a solution has been considered "unlikely" until 1991, and by many experts also later, due to misunderstanding of the CO solution (see Quotations in "Global serializability"). An important side-benefit of CO is automatic distributed deadlock resolution. Contrary to CO, virtually all other techniques (when not combined with CO) are prone to distributed deadlocks (also called global deadlocks) which need special handling. CO is also the name of the resulting schedule property: A schedule has the CO property if the chronological order of its transactions' commit events is compatible with the respective transactions' precedence (partial) order.

SS2PL mentioned above is a variant (special case) of CO and thus also effective to achieve distributed and global serializability. It also provides automatic distributed deadlock resolution (a fact overlooked in the research literature even after CO's publication), as well as Strictness and thus Recoverability. Possessing these desired properties together with known efficient locking based implementations explains SS2PL's popularity. SS2PL has been utilized to efficiently achieve Distributed and Global serializability since the 1980, and has become the de facto standard for it. However, SS2PL is blocking and constraining (pessimistic), and with the proliferation of distribution and utilization of systems different from traditional database systems (e.g., as in Cloud computing), less constraining types of CO (e.g., Optimistic CO) may be needed for better performance.

Comments:

Unlike Serializability, "Distributed recoverability" and "Distributed strictness" can be achieved efficiently in a straightforward way, similarly to the way Distributed CO is achieved: In each database system they have to be applied locally, and employ a vote ordering strategy for the Two-phase commit protocol (2PC; Raz 1992, page 307).

As has been mentioned above, Distributed SS2PL, including Distributed strictness (recoverability) and Distributed commitment ordering (serializability), automatically employs the needed vote ordering strategy, and is achieved (globally) when employed locally in each (local) database system (as has been known and utilized for many years; as a matter of fact locality is defined by the boundary of a 2PC participant (Raz 1992) ).

The design of concurrency control mechanisms is often influenced by the following subjects:

All systems are prone to failures, and handling "recovery" from failure is a must. The properties of the generated schedules, which are dictated by the concurrency control mechanism, may affect the effectiveness and efficiency of recovery. For example, the Strictness property (mentioned in the section Recoverability above) is often desirable for an efficient recovery.

For high availability database objects are often "replicated". Updates of replicas of a same database object need to be kept synchronized. This may affect the way concurrency control is done (e.g., Gray et al. 1996).



Multitasking operating systems, especially real-time operating systems, need to maintain the illusion that all tasks running on top of them are all running at the same time, even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on. Such multitasking is fairly simple when all tasks are independent from each other. However, when several tasks try to use the same resource, or when tasks try to share information, it can lead to confusion and inconsistency. The task of concurrent computing is to solve that problem. Some solutions involve "locks" similar to the locks used in databases, but they risk causing problems of their own such as deadlock. Other solutions are Non-blocking algorithms and Read-copy-update.




</doc>
<doc id="718450" url="https://en.wikipedia.org/wiki?curid=718450" title="Result set">
Result set

An SQL result set is a set of rows from a database, as well as metadata about the query such as the column names, and the types and sizes of each column. Depending on the database system, the number of rows in the result set may or may not be known. Usually, this number is not known up front because the result set is built on-the-fly. 

A result set is effectively a table. The codice_1 clause can be used in a query to impose a certain sort condition on the rows. Without that clause, there is no guarantee whatsoever on the order in which the rows are returned.


</doc>
<doc id="1490598" url="https://en.wikipedia.org/wiki?curid=1490598" title="Index locking">
Index locking

In databases an "index" is a data structure, part of the database, used by a database system to efficiently navigate access to "user data". Index data are system data distinct from user data, and consist primarily of pointers. Changes in a database (by insert, delete, or modify operations), may require indexes to be updated to maintain accurate user data accesses. Index locking is a technique used to maintain index integrity. A portion of an index is locked during a database transaction when this portion is being accessed by the transaction as a result of attempt to access related user data. Additionally, special database system transactions (not user-invoked transactions) may be invoked to maintain and modify an index, as part of a system's self-maintenance activities. When a portion of an index is locked by a transaction, other transactions may be blocked from accessing this index portion (blocked from modifying, and even from reading it, depending on lock type and needed operation). Index Locking Protocol guarantees that phantom read phenomenon won't occur. 
Index locking protocol states:
Specialized concurrency control techniques exist for accessing indexes. These techniques depend on the index type, and take advantage of its structure. They are typically much more effective than applying to indexes common concurrency control methods applied to user data. Notable and widely researched are specialized techniques for B-trees (B-Tree concurrency control) which are regularly used as database indexes.

Index locks are used to coordinate threads accessing indexes concurrently, and typically shorter-lived than the common transaction locks on user data. In professional literature, they are often called "latches".



</doc>
<doc id="1711076" url="https://en.wikipedia.org/wiki?curid=1711076" title="Database index">
Database index

A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed. Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.

An index is a copy of selected columns of data from a table, called a "database key" or simply "key", that can be searched very efficiently that also includes a low-level disk block address or direct link to the complete row of data it was copied from. Some databases extend the power of indexing by letting developers create indexes on functions or expressions. For example, an index could be created on codice_1, which would only store the upper-case versions of the codice_2 field in the index. Another option sometimes supported is the use of partial indices, where index entries are created only for those records that satisfy some conditional expression. A further aspect of flexibility is to permit indexing on user-defined functions, as well as expressions formed from an assortment of built-in functions.

Most database software includes indexing technology that enables sub-linear time lookup to improve performance, as linear search is inefficient for large databases.

Suppose a database contains N data items and one must be retrieved based on the value of one of the fields. A simple implementation retrieves and examines each item according to the test. If there is only one matching item, this can stop when it finds that single item, but if there are multiple matches, it must test everything. This means that the number of operations in the worst case is O(N) or linear time. Since databases may contain many objects, and since lookup is a common operation, it is often desirable to improve performance.

An index is any data structure that improves the performance of lookup. There are many different used for this purpose. There are complex design trade-offs involving lookup performance, index size, and index-update performance. Many index designs exhibit logarithmic (O(log(N))) lookup performance and in some applications it is possible to achieve flat (O(1)) performance.

Indexes are used to police database constraints, such as UNIQUE, EXCLUSION, PRIMARY KEY and FOREIGN KEY. An index may be declared as UNIQUE, which creates an implicit constraint on the underlying table. Database systems usually implicitly create an index on a set of columns declared PRIMARY KEY, and some are capable of using an already-existing index to police this constraint. Many database systems require that both referencing and referenced sets of columns in a FOREIGN KEY constraint are indexed, thus improving performance of inserts, updates and deletes to the tables participating in the constraint.

Some database systems support an EXCLUSION constraint that ensures that, for a newly inserted or updated record, a certain predicate holds for no other record. This can be used to implement a UNIQUE constraint (with equality predicate) or more complex constraints, like ensuring that no overlapping time ranges or no intersecting geometry objects would be stored in the table. An index supporting fast searching for records satisfying the predicate is required to police such a constraint.

The data is present in arbitrary order, but the logical ordering is specified by the index. The data rows may be spread throughout the table regardless of the value of the indexed column or expression. The non-clustered index tree contains the index keys in sorted order, with the leaf level of the index containing the pointer to the record (page and the row number in the data page in page-organized engines; row offset in file-organized engines).

In a non-clustered index,


There can be more than one non-clustered index on a database table.

Clustering alters the data block into a certain distinct order to match the index, resulting in the row data being stored in order. Therefore, only one clustered index can be created on a given database table. Clustered indices can greatly increase overall speed of retrieval, but usually only where the data is accessed sequentially in the same or reverse order of the clustered index, or when a range of items is selected.

Since the physical records are in this sort order on disk, the next row item in the sequence is immediately before or after the last one, and so fewer data block reads are required. The primary feature of a clustered index is therefore the ordering of the physical data rows in accordance with the index blocks that point to them. Some databases separate the data and index blocks into separate files, others put two completely different data blocks within the same physical file(s).

When multiple databases and multiple tables are joined, it's referred to as a cluster (not to be confused with clustered index described above). The records for the tables sharing the value of a cluster key shall be stored together in the same or nearby data blocks. This may improve the joins of these tables on the cluster key, since the matching records are stored together and less I/O is required to locate them. The cluster configuration defines the data layout in the tables that are parts of the cluster. A cluster can be keyed with a B-Tree index or a hash table. The data block where the table record is stored is defined by the value of the cluster key.

The order that the index definition defines the columns in is important. It is possible to retrieve a set of row identifiers using only the first indexed column. However, it is not possible or efficient (on most databases) to retrieve the set of row identifiers using only the second or greater indexed column.

For example, imagine a phone book that is organized by city first, then by last name, and then by first name. If you are given the city, you can easily extract the list of all phone numbers for that city. However, in this phone book it would be very tedious to find all the phone numbers for a given last name. You would have to look within each city's section for the entries with that last name. Some databases can do this, others just won’t use the index.

In the phone book example with a composite index created on the columns (codice_3), if we search by giving exact values for all the three fields, search time is minimal—but if we provide the values for codice_4 and codice_5 only, the search uses only the codice_4 field to retrieve all matched records. Then a sequential lookup checks the matching with codice_5. So, to improve the performance, one must ensure that the index is created on the order of search columns.

Indexes are useful for many applications but come with some limitations. Consider the following SQL statement: codice_8. To process this statement without an index the database software must look at the last_name column on every row in the table (this is known as a full table scan). With an index the database simply follows the B-tree data structure until the Smith entry has been found; this is much less computationally expensive than a full table scan.
Consider this SQL statement: codice_9. This query would yield an email address for every customer whose email address ends with "@wikipedia.org", but even if the email_address column has been indexed the database must perform a full index scan. This is because the index is built with the assumption that words go from left to right. With a wildcard at the beginning of the search-term, the database software is unable to use the underlying B-tree data structure (in other words, the WHERE-clause is "not sargable"). This problem can be solved through the addition of another index created on codice_10 and a SQL query like this: codice_11. This puts the wild-card at the right-most part of the query (now gro.aidepikiw@%), which the index on reverse(email_address) can satisfy.

When the wildcard characters are used on both sides of the search word as "%wikipedia.org%", the index available on this field is not used. Rather only a sequential search is performed, which takes O(N) time.

A bitmap index is a special kind of indexing that stores the bulk of its data as bit arrays (bitmaps) and answers most queries by performing bitwise logical operations on these bitmaps. The most commonly used indexes, such as B+ trees, are most efficient if the values they index do not repeat or repeat a small number of times. In contrast, the bitmap index is designed for cases where the values of a variable repeat very frequently. For example, the sex field in a customer database usually contains at most three distinct values: male, female or unknown (not recorded). For such variables, the bitmap index can have a significant performance advantage over the commonly used trees.

A dense index in databases is a file with pairs of keys and pointers for every record in the data file. Every key in this file is associated with a particular pointer to "a record" in the sorted data file. 
In clustered indices with duplicate keys, the dense index points "to the first record" with that key.

A sparse index in databases is a file with pairs of keys and pointers for every block in the data file. Every key in this file is associated with a particular pointer "to the block" in the sorted data file. In clustered indices with duplicate keys, the sparse index points "to the lowest search key" in each block.

A reverse-key index reverses the key value before entering it in the index. E.g., the value 24538 becomes 83542 in the index. Reversing the key value is particularly useful for indexing data such as sequence numbers, where new key values monotonically increase.

The primary index contains the key fields of the table and a pointer to the non-key fields of the table. The primary index is created automatically when the table is created in the database.

It is used to index fields that are neither ordering fields nor key fields (there is no assurance that the file is organized on key field or primary key field). One index entry for every tuple in the data file (dense index) contains the value of the indexed attribute and pointer to the block/record.

Indices can be implemented using a variety of data structures. Popular indices include balanced trees, B+ trees and hashes.

In Microsoft SQL Server, the leaf node of the clustered index corresponds to the actual data, not simply a pointer to data that resides elsewhere, as is the case with a non-clustered index. Each relation can have a single clustered index and many unclustered indices.

An index is typically being accessed concurrently by several transactions and processes, and thus needs concurrency control. While in principle indexes can utilize the common database concurrency control methods, specialized concurrency control methods for indexes exist, which are applied in conjunction with the common methods for a substantial performance gain.

In most cases, an index is used to quickly locate the data record(s) from which the required data is read. In other words, the index is only used to locate data records in the table and not to return data.

A covering index is a special case where the index itself contains the required data field(s) and can answer the required data.

Consider the following table (other fields omitted):

To find the Name for ID 13, an index on (ID) is useful, but the record must still be read to get the Name. However, an index on (ID, Name) contains the required data field and eliminates the need to look up the record.

Covering indexes are each for a specific table. Queries which JOIN/ access across multiple tables, may potentially consider covering indexes on more than one of these tables.

A covering index can dramatically speed up data retrieval but may itself be large due to the additional keys, which slow down data insertion & update. To reduce such index size, some systems allow including non-key fields in the index. Non-key fields are not themselves part of the index ordering but only included at the leaf level, allowing for a covering index with less overall index size.

No standard defines how to create indexes, because the ISO SQL Standard does not cover physical aspects. Indexes are one of the physical parts of database conception among others like storage (tablespace or filegroups). RDBMS vendors all give a CREATE INDEX syntax with some specific options that depend on their software's capabilities.



</doc>
<doc id="11519324" url="https://en.wikipedia.org/wiki?curid=11519324" title="Connection string">
Connection string

In computing, a connection string is a string that specifies information about a data source and the means of connecting to it. It is passed in code to an underlying driver or provider in order to initiate the connection. Whilst commonly used for a database connection, the data source could also be a spreadsheet or text file.

The connection string may include attributes such as the name of the driver, server and database, as well as security information such as user name and password.

This example shows a Postgres connection string for connecting to wikipedia.com with SSL and a connection timeout of 180 seconds:

DRIVER={PostgreSQL Unicode};SERVER=www.wikipedia.com;SSL=true;SSLMode=require;DATABASE=wiki;UID=wikiuser;Connect Timeout=180;PWD=ashiknoor
Users of Oracle databases can specify connection strings:



</doc>
<doc id="11488814" url="https://en.wikipedia.org/wiki?curid=11488814" title="Schema matching">
Schema matching

The terms schema matching and "mapping" are often used interchangeably for a database process. For this article, we differentiate the two as follows: Schema matching is the process of identifying that two objects are semantically related (scope of this article) while mapping refers to the transformations between the objects. For example, in the two schemas DB1.Student (Name, SSN, Level, Major, Marks)
and DB2.Grad-Student (Name, ID, Major, Grades); possible matches would be: DB1.Student ≈ DB2.Grad-Student; DB1.SSN = DB2.ID etc. and possible transformations or mappings would be: DB1.Marks to DB2.Grades (100-90 A; 90-80 B: etc.).

Automating these two approaches has been one of the fundamental tasks of data integration. In general, it is not possible to determine fully automatically the different correspondences between two schemas — primarily because of the differing and often not explicated or documented semantics of the two schemas.

Among others, common challenges to automating matching and mapping have been previously classified in especially for relational DB schemas; and in – a fairly comprehensive list of heterogeneity not limited to the relational model recognizing schematic vs semantic differences/heterogeneity. Most of these heterogeneities exist because schemas use different representations or definitions to represent the same information (schema conflicts); OR different expressions, units, and precision result in conflicting representations of the same data (data conflicts).
Research in schema matching seeks to provide automated support to the process of finding semantic matches between two schemas. This process is made harder due to heterogeneities at the following levels

Discusses a generic methodology for the task of schema integration or the activities involved. According to the authors, one can view the integration.


Approaches to schema integration can be broadly classified as ones that exploit either just schema information or schema and instance level information.

Schema-level matchers only consider schema information, not instance data. The available information includes the usual properties of schema elements, such as name, description, data type, relationship types (part-of, is-a, etc.), constraints, and schema structure. Working at the element (atomic elements like attributes of objects) or structure level (matching combinations of elements that appear together in a structure), these properties are used to identify matching elements in two schemas. Language-based or linguistic matchers use names and text (i.e., words or sentences) to find semantically similar schema elements. Constraint based matchers exploit constraints often contained in schemas. Such constraints are used to define data types and value ranges, uniqueness, optionality, relationship types and cardinalities, etc. Constraints in two input schemas are matched to determine the similarity of the schema elements.

Instance-level matchers use instance-level data to gather important insight into the contents and meaning of the schema elements. These are typically used in addition to schema level matches in order to boost the confidence in match results, more so when the information available at the schema level is insufficient. Matchers at this level use linguistic and constraint based characterization of instances. For example, using linguistic techniques, it might be possible to look at the Dept, DeptName and EmpName instances to conclude that DeptName is a better match candidate for Dept than EmpName. Constraints like zipcodes must be 5 digits long or format of phone numbers may allow matching of such types of instance data.

Hybrid matchers directly combine several matching approaches to determine match candidates based on multiple criteria or information sources.
"Most of these techniques also employ additional information such as dictionaries, thesauri, and user-provided match or mismatch information"

Reusing matching information
Another initiative has been to re-use previous matching information as auxiliary information for future matching tasks. The motivation for this work is that structures or substructures often repeat, for example in schemas in the E-commerce domain. Such a reuse of previous matches however needs to be a careful choice. It is possible that such a reuse makes sense only for some part of a new schema or only in some domains. For example, Salary and Income may be considered identical in a payroll application but not in a tax reporting application. There are several open ended challenges in such reuse that deserves further work.

Sample Prototypes
Typically, the implementation of such matching techniques can be classified as being either rule based or learner based systems. The complementary nature of these different approaches has instigated a number of applications using a combination of techniques depending on the nature of the domain or application under consideration.

The relationship types between objects that are identified at the end of a matching process are typically those with set semantics such as overlap, disjointness, exclusion, equivalence, or subsumption. The logical encodings of these relationships are what they mean. Among others, an early attempt to use description logics for schema integration and identifying such relationships was presented. Several state of the art matching tools today and those benchmarked in the "Ontology Alignment Evaluation Initiative" are capable of identifying many such simple (1:1 / 1:n / n:1 element level matches) and complex matches (n:1 / n:m element or structure level matches) between objects.

The quality of schema matching is commonly measure by precision and recall. While precision measures the number of correctly matched pairs out of all pairs that 
were matched, recall measures how many of the actual pairs have been matched.




</doc>
<doc id="28559796" url="https://en.wikipedia.org/wiki?curid=28559796" title="Data item">
Data item

A data item describes an atomic state of a particular object concerning a specific property at a certain time point. A collection of data items for the same object at the same time forms an object instance (or table row). Any type of complex information can be broken down to elementary data items (atomic state). Data items are identified by object (o), property (p) and time (t), while the value (v) is a function of o, p and t: v = F(o,p,t).

Values typically are represented by symbols like numbers, texts, images, sounds, videos or others. Values are not necessarily atomic. The complexity of a value depends on the complexity of the property and time component.

When looking at databases or XML files, the object is usually identified by an object name or other type of object identifier, which is part of the "data". Properties are defined as columns (table row), properties (object instance) or tags (XML). Often, time is not explicitly expressed and is an attribute applying to the complete data set. Other data collections provide time on the instance level (time series), on column level or even on attribute/property level.


</doc>
<doc id="28547570" url="https://en.wikipedia.org/wiki?curid=28547570" title="Terminology model">
Terminology model

A terminology model is a refinement of a concept system. Within a terminology model the concepts (object types) of a specific problem or subject area are defined by subject matter experts in terms of concept (object type) definitions and definitions of subordinated concepts or characteristics (properties). Besides object types, the terminology model allows defining hierarchical classifications, definitions for object type and property behavior and definition of casual relations.

The terminology model is a means for subject matter experts to express their knowledge about the subject in subject specific terms. Since the terminology model is structured rather similar to an object-oriented database schema, is can be transformed without loss of information into an object-oriented database schema. Thus, the terminology model is a method for problem analysis on the one side and a mean of defining database schema on the other side.

Several terminology models have been developed and published in the field of statistics:




</doc>
<doc id="4379212" url="https://en.wikipedia.org/wiki?curid=4379212" title="Commitment ordering">
Commitment ordering

Commitment ordering (CO) is a class of interoperable "serializability" techniques in concurrency control of databases, transaction processing, and related applications. It allows optimistic (non-blocking) implementations. With the proliferation of multi-core processors, CO has been also increasingly utilized in concurrent programming, transactional memory, and especially in software transactional memory (STM) for achieving serializability optimistically. CO is also the name of the resulting transaction schedule (history) property, which was originally defined in 1988 with the name "dynamic atomicity". In a CO compliant schedule the chronological order of commitment events of transactions is compatible with the precedence order of the respective transactions. CO is a broad special case of "conflict serializability", and effective means (reliable, high-performance, distributed, and scalable) to achieve global serializability (modular serializability) across any collection of database systems that possibly use different concurrency control mechanisms (CO also makes each system serializability compliant, if not already).

Each not-CO-compliant database system is augmented with a CO component (the commitment order coordinator—COCO) which orders the commitment events for CO compliance, with neither data-access nor any other transaction operation interference. As such CO provides a low overhead, general solution for global serializability (and distributed serializability), instrumental for global concurrency control (and distributed concurrency control) of multi database systems and other transactional objects, possibly highly distributed (e.g., within cloud computing, grid computing, and networks of smartphones). An atomic commitment protocol (ACP; of any type) is a fundamental part of the solution, utilized to break global cycles in the conflict (precedence, serializability) graph. CO is the most general property (a necessary condition) that guarantees global serializability, if the database systems involved do not share concurrency control information beyond atomic commitment protocol (unmodified) messages, and have no knowledge whether transactions are global or local (the database systems are "autonomous"). Thus CO (with its variants) is the only general technique that does not require the typically costly distribution of local concurrency control information (e.g., local precedence relations, locks, timestamps, or tickets). It generalizes the popular "strong strict two-phase locking" (SS2PL) property, which in conjunction with the "two-phase commit protocol" (2PC) is the de facto standard to achieve global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join such SS2PL based solutions for global serializability.

In addition, locking based "global deadlocks" are resolved automatically in a CO based multi-database environment, an important side-benefit (including the special case of a completely SS2PL based environment; a previously unnoticed fact for SS2PL).

Furthermore, strict commitment ordering (SCO; Raz 1991c), the intersection of "Strictness" and CO, provides better performance (shorter average transaction completion time and resulting better transaction throughput) than SS2PL whenever read-write conflicts are present (identical blocking behavior for write-read and write-write conflicts; comparable locking overhead). The advantage of SCO is especially significant during lock contention. Strictness allows both SS2PL and SCO to use the same effective "database recovery" mechanisms.

Two major generalizing variants of CO exist, extended CO (ECO; Raz 1993a) and multi-version CO (MVCO; Raz 1993b). They as well provide global serializability without local concurrency control information distribution, can be combined with any relevant concurrency control, and allow optimistic (non-blocking) implementations. Both use additional information for relaxing CO constraints and achieving better concurrency and performance. Vote ordering (VO or Generalized CO (GCO); Raz 2009) is a container schedule set (property) and technique for CO and all its variants. Local VO is a necessary condition for guaranteeing global serializability, if the atomic commitment protocol (ACP) participants do not share concurrency control information (have the "generalized autonomy" property). CO and its variants inter-operate transparently, guaranteeing global serializability and automatic global deadlock resolution also together in a mixed, heterogeneous environment with different variants.

The "Commitment ordering" (CO; Raz 1990, 1992, 1994, 2009) schedule property has been referred to also as "Dynamic atomicity" (since 1988), "commit ordering", "commit order serializability", and "strong recoverability" (since 1991). The latter is a misleading name since CO is incomparable with "recoverability", and the term "strong" implies a special case. This means that a schedule with a strong recoverability property does not necessarily have the CO property, and vice versa.

In 2009 CO has been characterized as a major concurrency control method, together with the previously known (since the 1980s) three major methods: "Locking", "Time-stamp ordering", and "Serialization graph testing", and as an enabler for the interoperability of systems using different concurrency control mechanisms.

In a federated database system or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple and possibly Distributed databases. Enforcing global serializability in such system is problematic. Even if every local schedule of a single database is serializable, still, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach conflict serializability would lead to unacceptable performance, primarily due to computer and communication latency. The problem of achieving global serializability effectively had been characterized as open until the public disclosure of CO in 1991 by its inventor Yoav Raz (Raz 1991a; see also Global serializability).

Enforcing CO is an effective way to enforce conflict serializability globally in a distributed system, since enforcing CO locally in each database (or other transactional object) also enforces it globally. Each database may use any, possibly different, type of concurrency control mechanism. With a local mechanism that already provides conflict serializability, enforcing CO locally does not cause any additional aborts, since enforcing CO locally does not affect the data access scheduling strategy of the mechanism (this scheduling determines the serializability related aborts; such a mechanism typically does not consider the commitment events or their order). The CO solution requires no communication overhead, since it uses (unmodified) "atomic commitment" protocol messages only, already needed by each distributed transaction to reach atomicity. An atomic commitment protocol plays a central role in the distributed CO algorithm, which enforces CO globally, by breaking global cycles (cycles that span two or more databases) in the global conflict graph.
CO, its special cases, and its generalizations are interoperable, and achieve global serializability while transparently being utilized together in a single heterogeneous distributed environment comprising objects with possibly different concurrency control mechanisms. As such, "Commitment ordering", including its special cases, and together with its generalizations (see CO variants below), provides a general, high performance, fully distributed solution (no central processing component or central data structure are needed) for guaranteeing global serializability in heterogeneous environments of multidatabase systems and other multiple transactional objects (objects with states accessed and modified only by transactions; e.g., in the framework of transactional processes, and within Cloud computing and Grid computing). The CO solution scales up with network size and the number of databases without any negative impact on performance (assuming the statistics of a single distributed transaction, e.g., the average number of databases involved with a single transaction, are unchanged).

With the proliferation of Multi-core processors, Optimistic CO (OCO) has been also increasingly utilized to achieve serializability in software transactional memory, and numerous STM articles and patents utilizing "commit order" have already been published (e.g., Zhang et al. 2006).

"Commitment ordering" (CO) is a special case of conflict serializability. CO can be enforced with "non-blocking" mechanisms (each transaction can complete its task without having its data-access blocked, which allows optimistic concurrency control; however, commitment could be blocked). In a CO schedule the commitment events' (partial) precedence order of the transactions corresponds to the precedence (partial) order of the respective transactions in the (directed) conflict graph (precedence graph, serializability graph), as induced by their conflicting access operations (usually read and write (insert/modify/delete) operations; CO also applies to higher level operations, where they are conflicting if noncommutative, as well as to conflicts between operations upon multi-version data).


The commitment decision events are generated by either a local commitment mechanism, or an atomic commitment protocol, if different processes need to reach consensus on whether to commit or abort. The protocol may be distributed or centralized. Transactions may be committed concurrently, if the commit partial order allows (if they do not have conflicting operations). If different conflicting operations induce different partial orders of same transactions, then the conflict graph has cycles, and the schedule will violate serializability when all the transactions on a cycle are committed. In this case no partial order for commitment events can be found. Thus, cycles in the conflict graph need to be broken by aborting transactions. However, any conflict serializable schedule can be made CO without aborting any transaction, by properly delaying commit events to comply with the transactions' precedence partial order.

CO enforcement by itself is not sufficient as a concurrency control mechanism, since CO lacks the recoverability property, which should be supported as well.

A fully distributed "Global commitment ordering" enforcement algorithm exists, that uses local CO of each participating database, and needs only (unmodified) Atomic commitment protocol messages with no further communication. The distributed algorithm is the combination of local (to each database) CO algorithm processes, and an atomic commitment protocol (which can be fully distributed).
Atomic commitment protocol is essential to enforce atomicity of each distributed transaction (to decide whether to commit or abort it; this procedure is always carried out for distributed transactions, independently of concurrency control and CO). A common example of an atomic commitment protocol is the "two-phase commit protocol", which is resilient to many types of system failure. In a reliable environment, or when processes usually fail together (e.g., in the same integrated circuit), a simpler protocol for atomic commitment may be used (e.g., a simple handshake of distributed transaction's participating processes with some arbitrary but known special participant, the transaction's coordinator, i.e., a type of "one-phase commit" protocol). An atomic commitment protocol reaches consensus among participants on whether to "commit" or "abort" a distributed (global) transaction that spans these participants. An essential stage in each such protocol is the YES vote (either explicit, or implicit) by each participant, which means an obligation of the voting participant to obey the decision of the protocol, either commit or abort. Otherwise a participant can unilaterally abort the transaction by an explicit NO vote. The protocol commits the transaction only if YES votes have been received from "all" participants (thus a missing vote is typically considered a NO), otherwise the protocol aborts the transaction. The various atomic commit protocols only differ in their abilities to handle different computing environment failure situations, and the amounts of work and other computing resources needed in different situations.

The entire CO solution for global serializability is based on the fact that in case of a missing vote for a distributed transaction, the atomic commitment protocol eventually aborts this transaction.

In each database system a local CO algorithm determines the needed commitment order for that database. By the characterization of CO above, this order depends on the local precedence order of transactions, which results from the local data access scheduling mechanisms. Accordingly, YES votes in the atomic commitment protocol are scheduled for each (unaborted) distributed transaction (in what follows "a vote" means a YES vote). If a precedence relation (conflict) exists between two transactions, then the second will not be voted on before the first is completed (either committed or aborted), to prevent possible commit order violation by the atomic commitment protocol. Such can happen since the commit order by the protocol is not necessarily the same as the voting order. If no precedence relation exists, both can be voted on concurrently. This "vote ordering strategy" ensures that also the atomic commitment protocol maintains commitment order, and it is a "necessary condition" for guaranteeing Global CO (and the local CO of a database; without it both Global CO and Local CO (a property meaning that each database is CO compliant) may be violated).

However, since database systems schedule their transactions independently, it is possible that the transactions' precedence orders in two databases or more are not compatible (no global partial order exists that can embed the respective local partial orders together). With CO precedence orders are also the commitment orders. When participating databases in a same distributed transaction do not have compatible local precedence orders for that transaction (without "knowing" it; typically no coordination between database systems exists on conflicts, since the needed communication is massive and unacceptably degrades performance) it means that the transaction resides on a global cycle (involving two or more databases) in the global conflict graph. In this case the atomic commitment protocol will fail to collect all the votes needed to commit that transaction: By the "vote ordering strategy" above at least one database will delay its vote for that transaction indefinitely, to comply with its own commitment (precedence) order, since it will be waiting to the completion of another, preceding transaction on that global cycle, delayed indefinitely by another database with a different order. This means a voting-deadlock situation involving the databases on that cycle.
As a result, the protocol will eventually abort some deadlocked transaction on this global cycle, since each such transaction is missing at least one participant's vote. Selection of the specific transaction on the cycle to be aborted depends on the atomic commitment protocol's abort policies (a timeout mechanism is common, but it may result in more than one needed abort per cycle; both preventing unnecessary aborts and abort time shortening can be achieved by a dedicated abort mechanism for CO). Such abort will break the global cycle involving that distributed transaction. Both deadlocked transactions and possibly other in conflict with the deadlocked (and thus blocked) will be free to be voted on. It is worthwhile noting that each database involved with the voting-deadlock continues to vote regularly on transactions that are not in conflict with its deadlocked transaction, typically almost all the outstanding transactions. Thus, in case of incompatible local (partial) commitment orders, no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause of incompatibility. This means that the above "vote ordering strategy" is also a "sufficient condition" for guaranteeing Global CO.

The following is concluded:



Global CO implies Global serializability.

The Global CO algorithm comprises enforcing (local) CO in each participating database system by ordering commits of local transactions (see Enforcing CO locally below) and enforcing the "vote ordering strategy" in the theorem above (for global transactions).

The above global cycle elimination process by a voting deadlock can be explained in detail by the following observation:

First it is assumed, for simplicity, that every transaction reaches the ready-to-commit state and is voted on by at least one database (this implies that no blocking by locks occurs).
Define a ""wait for vote to commit" graph" as a directed graph with transactions as nodes, and a directed edge from any first transaction to a second transaction if the first transaction blocks the vote to commit of the second transaction (opposite to conventional edge direction in a wait-for graph). Such blocking happens only if the second transaction is in a conflict with the first transaction (see above). Thus this "wait for vote to commit" graph is identical to the global conflict graph. A cycle in the "wait for vote to commit" graph means a deadlock in voting. Hence there is a deadlock in voting if and only if there is a cycle in the conflict graph. Local cycles (confined to a single database) are eliminated by the local serializability mechanisms. Consequently, only global cycles are left, which are then eliminated by the atomic commitment protocol when it aborts deadlocked transactions with missing (blocked) respective votes.

Secondly, also local commits are dealt with: Note that when enforcing CO also waiting for a regular local commit of a local transaction can block local commits and votes of other transactions upon conflicts, and the situation for global transactions does not change also without the simplifying assumption above: The final result is the same also with local commitment for local transactions, without voting in atomic commitment for them.

Finally, blocking by a lock (which has been excluded so far) needs to be considered: A lock blocks a conflicting operation and prevents a conflict from being materialized. If the lock is released only after transaction end, it may block indirectly either a vote or a local commit of another transaction (which now cannot get to ready state), with the same effect as of a direct blocking of a vote or a local commit. In this case a cycle is generated in the conflict graph only if such a blocking by a lock is also represented by an edge. With such added edges representing events of blocking-by-a-lock, the conflict graph is becoming an "augmented conflict graph".




In the presence of CO the "augmented conflict graph" is in fact a (reversed edge) "local-commit and voting wait-for graph": An edge exists from a first transaction, either local or global, to a second, if the second is waiting for the first to end in order to be either voted on (if global), or locally committed (if local). All "global cycles" (across two or more databases) in this graph generate voting-deadlocks. The graph's global cycles provide complete characterization for voting deadlocks and may include any combination of materialized and non-materialized conflicts. Only cycles of (only) materialized conflicts are also cycles of the regular conflict graph and affect serializability. One or more (lock related) non-materialized conflicts on a cycle prevent it from being a cycle in the regular conflict graph, and make it a locking related deadlock. All the global cycles (voting-deadlocks) need to be broken (resolved) to both maintain global serializability and resolve global deadlocks involving data access locking, and indeed they are all broken by the atomic commitment protocol due to missing votes upon a voting deadlock.

Comment: This observation also explains the correctness of "Extended CO (ECO)" below: Global transactions' voting order must follow the conflict graph order with vote blocking when order relation (graph path) exists between two global transactions. Local transactions are not voted on, and their (local) commits are not blocked upon conflicts. This results in same voting-deadlock situations and resulting global cycle elimination process for ECO.

The "voting-deadlock" situation can be summarized as follows:


Also the following locking based special case is concluded:



Voting-deadlocks are the key for the operation of distributed CO.

Global cycle elimination (here voting-deadlock resolution by "atomic commitment") and resulting aborted transactions' re-executions are time consuming, regardless of concurrency control used. If databases schedule transactions independently, global cycles are unavoidable (in a complete analogy to cycles/deadlocks generated in local SS2PL; with distribution, any transaction or operation scheduling coordination results in autonomy violation, and typically also in substantial performance penalty). However, in many cases their likelihood can be made very low by implementing database and transaction design guidelines that reduce the number of conflicts involving a global transaction. This, primarily by properly handling hot spots (database objects with frequent access), and avoiding conflicts by using commutativity when possible (e.g., when extensively using counters, as in finances, and especially multi-transaction "accumulation counters", which are typically hot spots).

Atomic commitment protocols are intended and designed to achieve atomicity without considering database concurrency control. They abort upon detecting or heuristically finding (e.g., by timeout; sometimes mistakenly, unnecessarily) missing votes, and typically unaware of global cycles. These protocols can be specially enhanced for CO (including CO's variants below) both to prevent unnecessary aborts, and to accelerate aborts used for breaking global cycles in the global augmented conflict graph (for better performance by earlier release upon transaction-end of computing resources and typically locked data). For example, existing locking based global deadlock detection methods, other than timeout, can be generalized to consider also local commit and vote direct blocking, besides data access blocking. A possible compromise in such mechanisms is effectively detecting and breaking the most frequent and relatively simple to handle length-2 global cycles, and using timeout for undetected, much less frequent, longer cycles.

"Commitment ordering" can be enforced locally (in a single database) by a dedicated CO algorithm, or by any algorithm/protocol that provides any special case of CO. An important such protocol, being utilized extensively in database systems, which generates a CO schedule, is the "strong strict two phase locking" protocol (SS2PL: "release transaction's locks only after the transaction has been either committed or aborted"; see below). SS2PL is a proper subset of the intersection of 2PL and strictness.

A generic local CO algorithm (Raz 1992; Algorithm 4.1) is an algorithm independent of implementation details, that enforces exactly the CO property. It does not block data access (nonblocking), and consists of aborting a certain set of transactions (only if needed) upon committing a transaction. It aborts a (uniquely determined at any given time) minimal set of other undecided (neither committed, nor aborted) transactions that run locally and can cause serializability violation in the future (can later generate cycles of committed transactions in the conflict graph; this is the ABORT set of a committed transaction T; after committing T no transaction in ABORT at commit time can be committed, and all of them are doomed to be aborted). This set consists of all undecided transactions with directed edges in the conflict graph to the committed transaction. The size of this set cannot increase when that transaction is waiting to be committed (in ready state: processing has ended), and typically decreases in time as its transactions are being decided. Thus, unless real-time constraints exist to complete that transaction, it is preferred to wait with committing that transaction and let this set decrease in size. If another serializability mechanism exists locally (which eliminates cycles in the local conflict graph), or if no cycle involving that transaction exists, the set will be empty eventually, and no abort of set member is needed. Otherwise the set will stabilize with transactions on local cycles, and aborting set members will have to occur to break the cycles. Since in the case of CO conflicts generate blocking on commit, local cycles in the "augments conflict graph" (see above) indicate local commit-deadlocks, and deadlock resolution techniques as in SS2PL can be used (e.g., like "timeout" and "wait-for graph"). A local cycle in the "augmented conflict graph" with at least one non-materialized conflict reflects a locking-based deadlock. The local algorithm above, applied to the local augmented conflict graph rather than the regular local conflict graph, comprises the generic enhanced local CO algorithm, a single local cycle elimination mechanism, for both guaranteeing local serializability and handling locking based local deadlocks. Practically an additional concurrency control mechanism is always utilized, even solely to enforce recoverability. The generic CO algorithm does not affect local data access scheduling strategy, when it runs alongside of any other local concurrency control mechanism. It affects only the commit order, and for this reason it does not need to abort more transactions than those needed to be aborted for serializability violation prevention by any combined local concurrency control mechanism. The net effect of CO may be, at most, a delay of commit events (or voting in a distributed environment), to comply with the needed commit order (but not more delay than its special cases, for example, SS2PL, and on the average significantly less).

The following theorem is concluded:


With the proliferation of Multi-core processors, variants of the Generic local CO algorithm have been also increasingly utilized in Concurrent programming, Transactional memory, and especially in Software transactional memory for achieving serializability optimistically by "commit order" (e.g., Ramadan et al. 2009, Zhang et al. 2006, von Parun et al. 2007). Numerous related articles and patents utilizing CO have already been published.

A database system in a multidatabase environment is assumed. From a software architecture point of view a CO component that implements the generic CO algorithm locally, the "Commitment Order Coordinator" (COCO), can be designed in a straightforward way as a mediator between a (single) database system and an atomic commitment protocol component (Raz 1991b). However, the COCO is typically an integral part of the database system. The COCO's functions are to vote to commit on ready global transactions (processing has ended) according to the local commitment order, to vote to abort on transactions for which the database system has initiated an abort (the database system can initiate abort for any transaction, for many reasons), and to pass the atomic commitment decision to the database system. For local transactions (when can be identified) no voting is needed. For determining the commitment order the COCO maintains an updated representation of the local conflict graph (or local augmented conflict graph for capturing also locking deadlocks) of the undecided (neither committed nor aborted) transactions as a data structure (e.g., utilizing mechanisms similar to locking for capturing conflicts, but with no data-access blocking). The COCO component has an interface with its database system to receive "conflict," "ready" (processing has ended; readiness to vote on a global transaction or commit a local one), and "abort" notifications from the database system. It also interfaces with the atomic commitment protocol to vote and to receive the atomic commitment protocol's decision on each global transaction. The decisions are delivered from the COCO to the database system through their interface, as well as local transactions' commit notifications, at a proper commit order. The COCO, including its interfaces, can be enhanced, if it implements another variant of CO (see below), or plays a role in the database's concurrency control mechanism beyond voting in atomic commitment.

The COCO also guarantees CO locally in a single, isolated database system with no interface with an atomic commitment protocol.

If the databases that participate in distributed transactions (i.e., transactions that span more than a single database) do not use any shared concurrency control information and use unmodified atomic commitment protocol messages (for reaching atomicity), then maintaining (local) "commitment ordering" or one of its generalizing variants (see below) is a necessary condition for guaranteeing global serializability (a proof technique can be found in (Raz 1992), and a different proof method for this in (Raz 1993a)); it is also a sufficient condition. This is a mathematical fact derived from the definitions of "serializability" and a "transaction". It means that if not complying with CO, then global serializability cannot be guaranteed under this condition (the condition of no local concurrency control information sharing between databases beyond atomic commit protocol messages). Atomic commitment is a minimal requirement for a distributed transaction since it is always needed, which is implied by the definition of transaction.

(Raz 1992) defines "database autonomy" and "independence" as complying with this requirement without using any additional local knowledge:

Using this definition the following is concluded:



However, the definition of autonomy above implies, for example, that transactions are scheduled in a way that local transactions (confined to a single database) cannot be identified as such by an autonomous database system. This is realistic for some transactional objects, but too restrictive and less realistic for general purpose database systems. If autonomy is augmented with the ability to identify local transactions, then compliance with a more general property, "Extended commitment ordering" (ECO, see below), makes ECO the necessary condition.

Only in (Raz 2009) the notion of "Generalized autonomy" captures the intended notion of autonomy:

This definition is probably the broadest such definition possible in the context of database concurrency control, and it makes CO together with any of its (useful: No concurrency control information distribution) generalizing variants (Vote ordering (VO); see CO variants below) the necessary condition for Global serializability (i.e., the union of CO and its generalizing variants is the necessary set VO, which may include also new unknown useful generalizing variants).

The "Commitment ordering" (CO) solution (technique) for global serializability can be summarized as follows:

If each "database" (or any other "transactional object") in a multidatabase environment complies with CO, i.e., arranges its local transactions' commitments and its votes on (global, distributed) transactions to the "atomic commitment" protocol according to the local (to the database) partial order induced by the local conflict graph (serializability graph) for the respective transactions, then "Global CO" and "Global serializability" are guaranteed. A database's CO compliance can be achieved effectively with any local conflict serializability based concurrency control mechanism, with neither affecting any transaction's execution process or scheduling, nor aborting it. Also the database's autonomy is not violated. The only low overhead incurred is detecting conflicts (e.g., as with locking, but with no data-access blocking; if not already detected for other purposes), and ordering votes and local transactions' commits according to the conflicts.

In case of incompatible partial orders of two or more databases (no global partial order can embed the respective local partial orders together), a global cycle (spans two databases or more) in the global conflict graph is generated. This, together with CO, results in a cycle of blocked votes, and a "voting-deadlock" occurs for the databases on that cycle (however, allowed concurrent voting in each database, typically for almost all the outstanding votes, continue to execute). In this case the atomic commitment protocol fails to collect all the votes needed for the blocked transactions on that global cycle, and consequently the protocol aborts some transaction with a missing vote. This breaks the global cycle, the voting-deadlock is resolved, and the related blocked votes are free to be executed. Breaking the global cycle in the global conflict graph ensures that both global CO and global serializability are maintained. Thus, in case of incompatible local (partial) commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause for the incompatibility. Furthermore, also global deadlocks due to locking (global cycles in the "augmented conflict graph" with at least one data access blocking) result in voting deadlocks and are resolved automatically by the same mechanism.

"Local CO" is a necessary condition for guaranteeing "Global serializability," if the databases involved do not share any concurrency control information beyond (unmodified) atomic commitment protocol messages, i.e., if the databases are "autonomous" in the context of concurrency control. This means that every global serializability solution for autonomous databases must comply with CO. Otherwise global serializability may be violated (and thus, is likely to be violated very quickly in a high-performance environment).

The CO solution scales up with network size and the number of databases without performance penalty when it utilizes common distributed atomic commitment architecture.

A distinguishing characteristic of the CO solution to distributed serializability from other techniques is the fact that it requires no conflict information distributed (e.g., local precedence relations, locks, timestamps, tickets), which makes it uniquely effective. It utilizes (unmodified) atomic commitment protocol messages (which are already used) instead.

A common way to achieve distributed serializability in a (distributed) system is by a distributed lock manager (DLM). DLMs, which communicate lock (non-materialized conflict) information in a distributed environment, typically suffer from computer and communication latency, which reduces the performance of the system. CO allows to achieve distributed serializability under very general conditions, without a distributed lock manager, exhibiting the benefits already explored above for multidatabase environments; in particular: reliability, high performance, scalability, possibility of using "optimistic concurrency control" when desired, no conflict information related communications over the network (which have incurred overhead and delays), and automatic distributed deadlock resolution.

All "distributed transactional systems" rely on some atomic commitment protocol to coordinate atomicity (whether to commit or abort) among processes in a distributed transaction. Also, typically "recoverable data" (i.e., data under transactions' control, e.g., database data; not to be confused with the "recoverability" property of a schedule) are directly accessed by a single "transactional data manager" component (also referred to as a "resource manager") that handles local sub-transactions (the distributed transaction's portion in a single location, e.g., network node), even if these data are accessed indirectly by other entities in the distributed system during a transaction (i.e., indirect access requires a direct access through a local sub-transaction). Thus recoverable data in a distributed transactional system are typically partitioned among transactional data managers. In such system these transactional data managers typically comprise the participants in the system's atomic commitment protocol. If each participant complies with CO (e.g., by using SS2PL, or COCOs, or a combination; see above), then the entire distributed system provides CO (by the theorems above; each participant can be considered a separate transactional object), and thus (distributed) serializability. Furthermore: When CO is utilized together with an atomic commitment protocol also "distributed deadlocks" (i.e., deadlocks that span two or more data managers) caused by data-access locking are resolved automatically. Thus the following corollary is concluded:



This theorem also means that when SS2PL (or any other CO variant) is used locally in each transactional data manager, and each data manager has exclusive control of its data, no distributed lock manager (which is often utilized to enforce distributed SS2PL) is needed for distributed SS2PL and serializability. It is relevant to a wide range of distributed transactional applications, which can be easily designed to meet the theorem's conditions.

For implementing Distributed Optimistic CO (DOCO) the generic local CO algorithm is utilized in all the atomic commitment protocol participants in the system with no data access blocking and thus with no local deadlocks. The previous theorem has the following corollary:


A distributed database system that utilizes SS2PL resides on two remote nodes, A and B. The database system has two "transactional data managers" ("resource managers"), one on each node, and the database data are partitioned between the two data managers in a way that each has an exclusive control of its own (local to the node) portion of data: Each handles its own data and locks without any knowledge on the other manager's. For each distributed transaction such data managers need to execute the available atomic commitment protocol.

Two distributed transactions, formula_3 and formula_2, are running concurrently, and both access data x and y. x is under the exclusive control of the data manager on A (B's manager cannot access x), and y under that on B.

The respective "local sub-transactions" on A and B (the portions of formula_3 and formula_2 on each of the nodes) are the following:

The database system's schedule at a certain point in time is the following:

formula_3 holds a read-lock on x and formula_2 holds read-locks on y. Thus formula_27 and formula_30 are blocked by the lock compatibility rules of SS2PL and cannot be executed. This is a distributed deadlock situation, which is also a voting-deadlock (see below) with a distributed (global) cycle of length 2 (number of edges, conflicts; 2 is the most frequent length). The local sub-transactions are in the following states:

Since the atomic commitment protocol cannot receive votes for blocked sub-transactions (a voting-deadlock), it will eventually abort some transaction with a missing vote(s) by timeout, either formula_3, or formula_2, (or both, if the timeouts fall very close). This will resolve the global deadlock. The remaining transaction will complete running, be voted on, and committed. An aborted transaction is immediately "restarted" and re-executed.


In the scenario above both conflicts are "non-materialized", and the global voting-deadlock is reflected as a cycle in the global "wait-for graph" (but not in the global "conflict graph"; see Exact characterization of voting-deadlocks by global cycles above). However the database system can utilize any CO variant with exactly the same conflicts and voting-deadlock situation, and same resolution. Conflicts can be either "materialized" or "non-materialized", depending on CO variant used. For example, if SCO (below) is used by the distributed database system instead of SS2PL, then the two conflicts in the example are "materialized", all local sub-transactions are in "ready" states, and vote blocking occurs in the two transactions, one on each node, because of the CO voting rule applied independently on both A and B: due to conflicts formula_52 is not voted on before formula_53 ends, and formula_54 is not voted on before formula_55 ends, which is a voting-deadlock. Now the "conflict graph" has the global cycle (all conflicts are materialized), and again it is resolved by the atomic commitment protocol, and distributed serializability is maintained. Unlikely for a distributed database system, but possible in principle (and occurs in a multi-database), A can employ SS2PL while B employs SCO. In this case the global cycle is neither in the wait-for graph nor in the serializability graph, but still in the "augmented conflict graph" (the union of the two). The various combinations are summarized in the following table:


Comment: While the examples above describe real, recommended utilization of CO, this example is hypothetical, for demonstration only.

Certain experimental distributed memory-resident databases advocate multi single-threaded core (MuSiC) transactional environments. "Single-threaded" refers to transaction threads only, and to "serial" execution of transactions. The purpose is possible orders of magnitude gain in performance (e.g., H-Store and VoltDB) relatively to conventional transaction execution in multiple threads on a same core. In what described below MuSiC is independent of the way the cores are distributed. They may reside in one integrated circuit (chip), or in many chips, possibly distributed geographically in many computers. In such an environment, if recoverable (transactional) data are partitioned among threads (cores), and it is implemented in the conventional way for distributed CO, as described in previous sections, then DOCO and Strictness exist automatically. However, downsides exist with this straightforward implementation of such environment, and its practicality as a general-purpose solution is questionable. On the other hand, tremendous performance gain can be achieved in applications that can bypass these downsides in most situations.

Comment: The MuSiC straightforward implementation described here (which uses, for example, as usual in distributed CO, voting (and transaction thread) blocking in atomic commitment protocol when needed) is for demonstration only, and has no connection to the implementation in H-Store or any other project.

In a MuSiC environment local schedules are "serial". Thus both local Optimistic CO (OCO; see below) and the "Global CO enforcement vote ordering strategy" condition for the atomic commitment protocol are met automatically. This results in both distributed CO compliance (and thus distributed serializability) and automatic global (voting) deadlock resolution.

Furthermore, also local "Strictness" follows automatically in a serial schedule. By Theorem 5.2 in (Raz 1992; page 307), when the CO vote ordering strategy is applied, also Global Strictness is guaranteed. Note that "serial" locally is the only mode that allows strictness and "optimistic" (no data access blocking) together.

The following is concluded:



Special case schedule property classes (e.g., SS2PL and SCO below) are strictly contained in the CO class. The generalizing classes (ECO and MVCO) strictly contain the CO class (i.e., include also schedules that are not CO compliant). The generalizing variants also guarantee global serializability without distributing local concurrency control information (each database has the "generalized autonomy" property: it uses only local information), while relaxing CO constraints and utilizing additional (local) information for better concurrency and performance: ECO uses knowledge about transactions being local (i.e., confined to a single database), and MVCO uses availability of data versions values. Like CO, both generalizing variants are "non-blocking", do not interfere with any transaction's operation scheduling, and can be seamlessly combined with any relevant concurrency control mechanism.

The term CO variant refers in general to CO, ECO, MVCO, or a combination of each of them with any relevant concurrency control mechanism or property (including Multi-version based ECO, MVECO). No other generalizing variants (which guarantee global serializability with no local concurrency control information distribution) are known, but may be discovered.

Strong Strict Two Phase Locking (SS2PL; also referred to as "Rigorousness" or "Rigorous scheduling") means that both read and write locks of a transaction are released only after the transaction has ended (either committed or aborted). The set of SS2PL schedules is a proper subset of the set of CO schedules.
This property is widely utilized in database systems, and since it implies CO, databases that use it and participate in global transactions generate together a serializable global schedule (when using any atomic commitment protocol, which is needed for atomicity in a multi-database environment). No database modification or addition is needed in this case to participate in a CO distributed solution: The set of undecided transactions to be aborted before committing in the local generic CO algorithm above is empty because of the locks, and hence such an algorithm is unnecessary in this case. A transaction can be voted on by a database system immediately after entering a "ready" state, i.e., completing running its task locally. Its locks are released by the database system only after it is decided by the atomic commitment protocol, and thus the condition in the "Global CO enforcing theorem" above is kept automatically. If a local timeout mechanism is used by a database system to resolve (local) SS2PL deadlocks, then aborting blocked transactions breaks not only potential local cycles in the global conflict graph (real cycles in the augmented conflict graph), but also database system's potential global cycles as a side effect, if the atomic commitment protocol's abort mechanism is relatively slow. Such independent aborts by several entities typically may result in unnecessary aborts for more than one transaction per global cycle. The situation is different for a local "wait-for graph" based mechanisms: Such cannot identify global cycles, and the atomic commitment protocol will break the global cycle, if the resulting voting deadlock is not resolved earlier in another database.

Local SS2PL together with atomic commitment implying global serializability can also be deduced directly: All transactions, including distributed, obey the 2PL (SS2PL) rules. The atomic commitment protocol mechanism is not needed here for consensus on commit, but rather for the end of phase-two synchronization point. Probably for this reason, without considering the atomic commitment voting mechanism, automatic global deadlock resolution has not been noticed before CO.

Strict Commitment Ordering (SCO; (Raz 1991c)) is the intersection of strictness (a special case of recoverability) and CO, and provides an upper bound for a schedule's concurrency when both properties exist. It can be implemented using blocking mechanisms (locking) similar to those used for the popular SS2PL with similar overheads.

Unlike SS2PL, SCO does not block on a read-write conflict but possibly blocks on commit instead. SCO and SS2PL have identical blocking behavior for the other two conflict types: write-read, and write-write. As a result, SCO has shorter average blocking periods, and more concurrency (e.g., performance simulations of a single database for the most significant variant of "locks with ordered sharing," which is identical to SCO, clearly show this, with approximately 100% gain for some transaction loads; also for identical transaction loads SCO can reach higher transaction rates than SS2PL before "lock thrashing" occurs). More concurrency means that with given computing resources more transactions are completed in time unit (higher transaction rate, throughput), and the average duration of a transaction is shorter (faster completion; see chart). The advantage of SCO is especially significant during lock contention.


SCO is as practical as SS2PL since as SS2PL it provides besides serializability also strictness, which is widely utilized as a basis for efficient recovery of databases from failure. An SS2PL mechanism can be converted to an SCO one for better performance in a straightforward way without changing recovery methods. A description of a SCO implementation can be found in (Perrizo and Tatarinov 1998). See also "Semi-optimistic database scheduler".

SS2PL is a proper subset of SCO (which is another explanation why SCO is less constraining and provides more concurrency than SS2PL).

For implementing Optimistic commitment ordering (OCO) the generic local CO algorithm is utilized without data access blocking, and thus without local deadlocks. OCO without transaction or operation scheduling constraints covers the entire CO class, and is not a special case of the CO class, but rather a useful CO variant and mechanism characterization.

Extended Commitment Ordering (ECO; (Raz 1993a)) generalizes CO. When local transactions (transactions confined to a single database) can be distinguished from global (distributed) transactions (transactions that span two databases or more), commitment order is applied to global transactions only. Thus, for a local (to a database) schedule to have the ECO property, the chronological (partial) order of commit events of global transactions only (unimportant for local transactions) is consistent with their order on the respective local conflict graph.


A distributed algorithm to guarantee global ECO exists. As for CO, the algorithm needs only (unmodified) atomic commitment protocol messages. In order to guarantee global serializability, each database needs to guarantee also the conflict serializability of its own transactions by any (local) concurrency control mechanism.



This condition (ECO with local serializability) is weaker than CO, and allows more concurrency at the cost of a little more complicated local algorithm (however, no practical overhead difference with CO exists).

When all the transactions are assumed to be global (e.g., if no information is available about transactions being local), ECO reduces to CO.

Before a global transaction is committed, a generic local (to a database) ECO algorithm aborts a minimal set of undecided transactions (neither committed, nor aborted; either local transactions, or global that run locally), that can cause later a cycle in the conflict graph. This set of aborted transactions (not unique, contrary to CO) can be optimized, if each transaction is assigned with a weight (that can be determined by transaction's importance and by the computing resources already invested in the running transaction; optimization can be carried out, for example, by a reduction from the "Max flow in networks" problem (Raz 1993a)). Like for CO such a set is time dependent, and becomes empty eventually. Practically, almost in all needed implementations a transaction should be committed only when the set is empty (and no set optimization is applicable). The local (to the database) concurrency control mechanism (separate from the ECO algorithm) ensures that local cycles are eliminated (unlike with CO, which implies serializability by itself; however, practically also for CO a local concurrency mechanism is utilized, at least to ensure Recoverability). Local transactions can be always committed concurrently (even if a precedence relation exists, unlike CO). When the overall transactions' local partial order (which is determined by the local conflict graph, now only with possible temporary local cycles, since cycles are eliminated by a local serializability mechanism) allows, also global transactions can be voted on to be committed concurrently (when all their transitively (indirect) preceding (via conflict) "global" transactions are committed, while transitively preceding local transactions can be at any state. This in analogy to the distributed CO algorithm's stronger concurrent voting condition, where all the transitively preceding transactions need to be committed).

The condition for guaranteeing "Global ECO" can be summarized similarly to CO:


Global ECO (all global cycles in the global conflict graph are eliminated by atomic commitment) together with Local serializability (i.e., each database system maintains serializability locally; all local cycles are eliminated) imply Global serializability (all cycles are eliminated). This means that if each database system in a multidatabase environment provides local serializability (by "any" mechanism) and enforces the "vote ordering strategy" in the theorem above (a generalization of CO's vote ordering strategy), then "Global serializability" is guaranteed (no local CO is needed anymore).

Similarly to CO as well, the ECO "voting-deadlock" situation can be summarized as follows:


As with CO this means that also global deadlocks due to data-access locking (with at least one lock blocking) are voting deadlocks, and are automatically resolved by atomic commitment.

Multi-version Commitment Ordering (MVCO; (Raz 1993b)) is a generalization of CO for databases with multi-version resources. With such resources "read-only transactions" do not block or being blocked for better performance. Utilizing such resources is a common way nowadays to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object). MVCO implies "One-copy-serializability" (1SER or 1SR) which is the generalization of serializability for multi-version resources. Like CO, MVCO is non-blocking, and can be combined with any relevant multi-version concurrency control mechanism without interfering with it. In the introduced underlying theory for MVCO conflicts are generalized for different versions of a same resource (differently from earlier multi-version theories). For different versions conflict chronological order is replaced by version order, and possibly reversed, while keeping the usual definitions for conflicting operations. Results for the regular and augmented conflict graphs remain unchanged, and similarly to CO a distributed MVCO enforcing algorithm exists, now for a mixed environment with both single-version and multi-version resources (now single-version is a special case of multi-version). As for CO, the MVCO algorithm needs only (unmodified) atomic commitment protocol messages with no additional communication overhead. Locking-based global deadlocks translate to voting deadlocks and are resolved automatically. In analogy to CO the following holds:



MVCO can be further generalized to employ the generalization of ECO (MVECO).

CO based snapshot isolation (COSI) is the intersection of "Snapshot isolation" (SI) with MVCO. SI is a multiversion concurrency control method widely utilized due to good performance and similarity to serializability (1SER) in several aspects. The theory in (Raz 1993b) for MVCO described above is utilized later in (Fekete et al. 2005) and other articles on SI, e.g., (Cahill et al. 2008); see also Making snapshot isolation serializable and the references there), for analyzing conflicts in SI in order to make it serializable. The method presented in (Cahill et al. 2008), "Serializable snapshot isolation" (SerializableSI), a low overhead modification of SI, provides good performance results versus SI, with only small penalty for enforcing serializability. A different method, by combining SI with MVCO (COSI), makes SI serializable as well, with a relatively low overhead, similarly to combining the generic CO algorithm with single-version mechanisms. Furthermore, the resulting combination, COSI, being MVCO compliant, allows COSI compliant database systems to inter-operate and transparently participate in a CO solution for distributed/global serializability (see below). Besides overheads also protocols' behaviors need to be compared quantitatively. On one hand, all serializable SI schedules can be made MVCO by COSI (by possible commit delays when needed) without aborting transactions. On the other hand, SerializableSI is known to unnecessarily abort and restart certain percentages of transactions also in serializable SI schedules.

With CO and its variants (e.g., SS2PL, SCO, OCO, ECO, and MVCO above) global serializability is achieved via "atomic commitment" protocol based distributed algorithms. For CO and all its variants atomic commitment protocol is the instrument to eliminate global cycles (cycles that span two or more databases) in the "global augmented" (and thus also regular) "conflict graph" (implicitly; no global data structure implementation is needed). In cases of either incompatible local commitment orders in two or more databases (when no global partial order can embed the respective local partial orders together), or a data-access locking related voting deadlock, both implying a global cycle in the global augmented conflict graph and missing votes, the atomic commitment protocol breaks such cycle by aborting an undecided transaction on it (see The distributed CO algorithm above). Differences between the various variants exist at the local level only (within the participating database systems). Each local CO instance of any variant has the same role, to determine the position of every global transaction (a transaction that spans two or more databases) within the local commitment order, i.e., to determine when it is the transaction's turn to be voted on locally in the atomic commitment protocol. Thus, all the CO variants exhibit the same behavior in regard to atomic commitment. This means that they are all interoperable via atomic commitment (using the same software interfaces, typically provided as services, some already standardized for atomic commitment, primarily for the two phase commit protocol, e.g., X/Open XA) and transparently can be utilized together in any distributed environment (while each CO variant instance is possibly associated with any relevant local concurrency control mechanism type).

In summary, any single global transaction can participate simultaneously in databases that may employ each any, possibly different, CO variant (while concurrently running processes in each such database, and running concurrently with local and other global transactions in each such database). The atomic commitment protocol is indifferent to CO, and does not distinguish between the various CO variants. Any "global cycle" generated in the augmented global conflict graph may span databases of different CO variants, and generate (if not broken by any local abort) a voting deadlock that is resolved by atomic commitment exactly the same way as in a single CO variant environment. "local cycles" (now possibly with mixed materialized and non-materialized conflicts, both serializability and data-access-locking deadlock related, e.g., SCO) are resolved locally (each by its respective variant instance's own local mechanisms).

Vote ordering (VO or Generalized CO (GCO); Raz 2009), the union of CO and all its above variants, is a useful concept and global serializability technique. To comply with VO, local serializability (in it most general form, commutativity based, and including multi-versioning) and the "vote order strategy" (voting by local precedence order) are needed.

Combining results for CO and its variants, the following is concluded:





</doc>
<doc id="28560285" url="https://en.wikipedia.org/wiki?curid=28560285" title="Data event">
Data event

A data event is a relevant state transition defined in an event schema. Typically, event schemata are described by pre- and post condition for a single or a set of data items. In contrast to ECA (Event condition action), which considers an event to be a signal, the data event not only refers to the change (signal), but describes specific state transitions, which are referred to in ECA as conditions.

Considering data events as relevant data item state transitions allows defining complex event-reaction schemata for a database. Defining data event schemata for relational databases is limited to attribute and instance events. Object-oriented databases also support collection properties, which allows defining changes in collections as data events, too.


</doc>
<doc id="20722131" url="https://en.wikipedia.org/wiki?curid=20722131" title="Log shipping">
Log shipping

Log shipping is the process of automating the backup of transaction log files on a primary (production) database server, and then restoring them onto a standby server. This technique is supported by Microsoft SQL Server, 4D Server, MySQL, and PostgreSQL. Similar to replication, the primary purpose of log shipping is to increase database availability by maintaining a backup server that can replace a production server quickly. Other databases such as Adaptive Server Enterprise and Oracle Database support the technique but require the Database Administrator to write code or scripts to perform the work.

Although the actual failover mechanism in log shipping is manual, this implementation is often chosen due to its low cost in human and server resources, and ease of implementation. In comparison, SQL server clusters enable automatic failover, but at the expense of much higher storage costs. Compared to database replication, log shipping does not provide as much in terms of reporting capabilities, but backs up system tables along with data tables, and locks the standby server from users' modifications. A replicated server can be modified (e.g. views) and is therefore unsuitable for failover purposes.



</doc>
<doc id="30055981" url="https://en.wikipedia.org/wiki?curid=30055981" title="Database application">
Database application

A database application is a computer program whose primary purpose is entering and retrieving information from a computerized database. Early examples of database applications were accounting systems and airline reservations systems, such as SABRE, developed starting in 1957.

A characteristic of modern database applications is that they facilitate simultaneous updates and queries from multiple users. Systems in the 1970s might have accomplished this by having each user in front of a 3270 terminal to a mainframe computer. By the mid-1980s it was becoming more common to give each user a personal computer and have a program running on that PC that is connected to a database server. Information would be pulled from the database, transmitted over a network, and then arranged, graphed, or otherwise formatted by the program running on the PC. Starting in the mid-1990s it became more common to build database applications with a Web interface. Rather than develop custom software to run on a user's PC, the user would use the same Web browser program for every application. A database application with a Web interface had the advantage that it could be used on devices of different sizes, with different hardware, and with different operating systems. Examples of early database applications with Web interfaces include amazon.com, which used the Oracle relational database management system, the photo.net online community, whose implementation on top of Oracle was described in the book Database-Backed Web Sites (Ziff-Davis Press; May 1997), and eBay, also running Oracle.

Electronic medical records are referred to on emrexperts.com, in December 2010, as "a software database application". A 2005 O'Reilly book uses the term in its title: Database Applications and the Web.

Some of the most complex database applications remain accounting systems, such as SAP, which may contain thousands of tables in only a single module. Many of today's most widely used computer systems are database applications, for example, Facebook, which was built on top of MySQL.

The etymology of the phrase "database application" comes from the practice of dividing computer software into systems programs, such as the operating system, compilers, the file system, and tools such as the database management system, and application programs, such as a payroll check processor. On a standard PC running Microsoft Windows, for example, the Windows operating system contains all of the systems programs while games, word processors, spreadsheet programs, photo editing programs, etc. would be application programs. As "application" is short for "application program", "database application" is short for "database application program".

Not every program that uses a database would typically be considered a "database application". For example, many physics experiments, e.g., the Large Hadron Collider, generate massive data sets that programs subsequently analyze. The data sets constitute a "database", though they are not typically managed with a standard relational database management system. The computer programs that analyze the data are primarily developed to answer hypotheses, not to put information back into the database and therefore the overall program would not be called a "database application".




</doc>
<doc id="26591446" url="https://en.wikipedia.org/wiki?curid=26591446" title="Universal IR Evaluation">
Universal IR Evaluation

In computer science, Universal IR Evaluation (information retrieval evaluation) aims to develop measures of database retrieval performance that shall be comparable across all information retrieval tasks.

IR (information retrieval) evaluation begins whenever a user submits a query (search term) to a database. If the user is able to determine the relevance of each document in the database (relevant or not relevant), then for each query, the complete set of documents is naturally divided into four distinct (mutually exclusive) subsets: relevant documents that are retrieved, not relevant documents that are retrieved, relevant documents that are not retrieved, and not relevant documents that are not retrieved. These four subsets (of documents) are denoted by the letters a,b,c,d respectively and are called Swets variables, named after their inventor.

In addition to the Swets definitions, four relevance metrics have also been defined: Precision refers to the fraction of relevant documents that are retrieved (a/(a+b)), and Recall refers to the fraction of retrieved documents that are relevant (a/(a+c)). These are the most commonly used and well-known relevance metrics found in the IR evaluation literature. Two less commonly used metrics include the Fallout, i.e., the fraction of not relevant documents that are retrieved (b/(b+d)), and the Miss, which refers to the fraction of relevant documents that are not retrieved (c/(c+d)) during any given search.

Universal IR evaluation addresses the mathematical possibilities and relationships among the four relevance metrics Precision, Recall, Fallout and Miss, denoted by P, R, F and M, respectively. One aspect of the problem involves finding a mathematical derivation of a complete set of universal IR evaluation points. The complete set of 16 points, each one a quadruple of the form (P,R,F,M), describes all the possible universal IR outcomes. For example, many of us have had the experience of querying a database and not retrieving any documents at all. In this case, the Precision would take on the undetermined form 0/0, the Recall and Fallout would both be zero, and the Miss would be any value greater than zero and less than one (assuming a mix of relevant and not relevant documents were in the database, none of which were retrieved). This universal IR evaluation point would thus be denoted by (0/0, 0, 0, M), which represents only one of the 16 possible universal IR outcomes.

The mathematics of universal IR evaluation is a fairly new subject since the relevance metrics P,R,F,M were not analyzed collectively until recently (within the past decade). A lot of the theoretical groundwork has already been formulated, but new insights in this area await discovery. For a detailed mathematical analysis, a query in the ScienceDirect database for "universal IR evaluation" retrieves several relevant peer-reviewed papers.




</doc>
<doc id="31271694" url="https://en.wikipedia.org/wiki?curid=31271694" title="Elasticity (data store)">
Elasticity (data store)

The elasticity of a data store relates to the flexibility of its data model and clustering capabilities. The greater the number of data model changes that can be tolerated, and the more easily the clustering can be managed, the more elastic the data store is considered to be.

Clustering elasticity is the ease of adding or removing nodes from the distributed data store. Usually, this is a difficult and delicate task to be done by an expert in a relational database system. Some NoSQL data stores, like Apache Cassandra have an easy solution, and a node can be added/removed with a few changes in the properties and by adding specifying at least one seed.

Relational databases are most often very inelastic, as they have a predefined data model that can only be adapted through redesign. Most NoSQL data stores, however, do not have a fixed schema. Each "row" can have a different number and even different type of columns. Concerning the data store, modifications in the schema are no problem. This makes this kind of data stores more elastic concerning the data model. The drawback is that the programmer has to take into account that the data model may change over time.



</doc>
<doc id="21852501" url="https://en.wikipedia.org/wiki?curid=21852501" title="Database virtualization">
Database virtualization

Database virtualization is the decoupling of the database layer, which lies between the storage and application layers within the application stack. Virtualization of the database layer enables a shift away from the physical, toward the logical or virtual. Virtualization enables compute and storage resources to be pooled and allocated on demand. This enables both the sharing of single server resources for multi-tenancy, as well as the pooling of server resources into a single logical database or cluster. In both cases, database virtualization provides increased flexibility, more granular and efficient allocation of pooled resources, and more scalable computing.

The act of partitioning data stores as a database grows has been in use for several decades. There are two primary ways that data has been partitioned inside legacy data management systems:



In virtual partitioning, logical data is abstracted from physical data by autonomously creating and managing large numbers of data partitions (100s to 1000s). Because they are autonomously maintained, resources required to manage the partitions are minimal. This kind of massive partitioning results in:


“Shared-data” and “shared-nothing” architectures allow scalability through multiple data partitions and cross-partition querying and transaction processing without full partition scanning.

Partitioning database sources from consumers is a fundamental concept. With greater numbers of database sources, inserting a horizontal data virtualization layer between the sources and consumers helps address this complexity. Rick van der Lans, author of multiple books on SQL and relational databases, has defined data virtualization as "the process of offering data consumers a data access interface that hides the technical aspects of stored data, such as location, storage structure, API, access language, and storage technology."




</doc>
<doc id="11861063" url="https://en.wikipedia.org/wiki?curid=11861063" title="Global serializability">
Global serializability

In concurrency control of "databases", "transaction processing" ("transaction management"), and other transactional distributed applications, global serializability (or modular serializability) is a property of a "global schedule" of transactions. A global schedule is the unified schedule of all the individual database (and other transactional object) schedules in a multidatabase environment (e.g., federated database). Complying with global serializability means that the global schedule is "serializable", has the "serializability" property, while each component database (module) has a serializable schedule as well. In other words, a collection of serializable components provides overall system serializability, which is usually incorrect. A need in correctness across databases in multidatabase systems makes global serializability a major goal for "global concurrency control" (or "modular concurrency control"). With the proliferation of the Internet, Cloud computing, Grid computing, and small, portable, powerful computing devices (e.g., smartphones), as well as increase in systems management sophistication, the need for atomic distributed transactions and thus effective global serializability techniques, to ensure correctness in and among distributed transactional applications, seems to increase.

In a federated database system or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple (and possibly distributed) databases. Enforcing global serializability in such system, where different databases may use different types of concurrency control, is problematic. Even if every local schedule of a single database is serializable, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach conflict serializability globally would lead to unacceptable performance, primarily due to computer and communication latency. Achieving global serializability effectively over different types of concurrency control has been open for several years. "Commitment ordering" (or Commit ordering; CO), a serializability technique publicly introduced in 1991 by Yoav Raz from Digital Equipment Corporation (DEC), provides an effective general solution for global (conflict) serializability across any collection of database systems and other transactional objects, with possibly different concurrency control mechanisms. CO does not need the distribution of conflict information, but rather utilizes the already needed (unmodified) atomic commitment protocol messages without any further communication between databases. It also allows optimistic (non-blocking) implementations. CO generalizes "strong strict two phase locking" (SS2PL), which in conjunction with the "two-phase commit" (2PC) protocol is the de facto standard for achieving global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join existing SS2PL based solutions for global serializability. The same applies also to all other multiple (transactional) object systems that use atomic transactions and need global serializability for correctness (see examples above; nowadays such need is not smaller than with database systems, the origin of atomic transactions).

The most significant aspects of CO that make it a uniquely effective general solution for global serializability are the following:

All these aspects, except the first two, are also possessed by the popular SS2PL, which is a (constrained, blocking) special case of CO and inherits many of CO's qualities.

The difficulties described above translate into the following problem:

Lack of an appropriate solution for the global serializability problem has driven researchers to look for alternatives to serializability as a correctness criterion in a multidatabase environment (e.g., see "Relaxing global serializability" below), and the problem has been characterized as difficult and "open". The following two quotations demonstrate the mindset about it by the end of the year 1991, with similar quotations in numerous other articles:


Commitment ordering, publicly introduced in May 1991 (see below), provides an efficient elegant general solution, from both practical and theoretical points of view, to the global serializability problem across database systems with possibly different concurrency control mechanisms. It provides conflict serializability with no negative effect on availability, and with no worse performance than the de facto standard for global serializability, CO's special case strong strict two-phase locking (SS2PL). It requires knowledge about neither local nor global transactions.


The commitment ordering solution comprises effective integration of autonomous database management systems with possibly different concurrency control mechanisms. This while local and global transactions execute in parallel without restricting any read or write operation in either local or global transactions, and without compromising the systems' autonomy.

Even in later years, after the public introduction of the Commitment ordering general solution in 1991, the problem still has been considered by many unsolvable:


The quotation above is from a 1997 article proposing a relaxed global serializability solution (see "Relaxing global serializability" below), and referencing Commitment ordering (CO) articles. The CO solution supports effectively both full ACID properties and full local autonomy, as well as meeting the other requirements posed above in the "Problem statement" section, and apparently has been misunderstood.

Similar thinking we see also in the following quotation from a 1998 article:


Also the above quoted article proposes a relaxed global serializability solution, while referencing the CO work. The CO solution for global serializability both bridges between different concurrency control protocols with no substantial concurrency reduction (and typically minor, if at all), and maintains the autonomy of local DBMSs. Evidently also here CO has been misunderstood. This misunderstanding continues to 2010 in a textbook by some of the same authors, where the same relaxed global serializability technique, "Two level serializability", is emphasized and described in detail, and CO is not mentioned at all.

On the other hand, the following quotation on CO appears in a 2009 book:



The characteristics and properties of the CO solution are discussed below.

Several solutions, some partial, have been proposed for the global serializability problem. Among them:


The problem of global serializability has been a quite intensively researched subject in the late 1980s and early 1990s. "Commitment ordering" (CO) has provided an effective general solution to the problem, insight into it, and understanding about possible generalizations of "strong strict two phase locking" (SS2PL), which practically and almost exclusively has been utilized (in conjunction with the "Two-phase commit protocol" (2PC) ) since the 1980s to achieve global serializability across databases. An important side-benefit of CO is the automatic "global deadlock" resolution that it provides (this is applicable also to distributed SS2PL; though global deadlocks have been an important research subject for SS2PL, automatic resolution has been overlooked, except in the CO articles, until today (2009)). At that time quite many commercial database system types existed, many non-relational, and databases were relatively very small. Multi database systems were considered a key for database scalability by database systems interoperability, and global serializability was urgently needed. Since then the tremendous progress in computing power, storage, and communication networks, resulted in orders of magnitude increases in both centralized databases' sizes, transaction rates, and remote access to database capabilities, as well as blurring the boundaries between centralized computing and distributed one over fast, low-latency local networks (e.g., Infiniband). These, together with progress in database vendors' distributed solutions (primarily the popular SS2PL with 2PC based, a de facto standard that allows interoperability among different vendors' (SS2PL-based) databases; both SS2PL and 2PC technologies have gained substantial expertise and efficiency), workflow management systems, and database replication technology, in most cases have provided satisfactory and sometimes better information technology solutions without multi database atomic distributed transactions over databases with different concurrency control (bypassing the problem above). As a result, the sense of urgency that existed with the problem at that period, and in general with high-performance distributed atomic transactions over databases with different concurrency control types, has reduced. However, the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems, and so the need in global serializability as a fundamental correctness criterion for such transactional systems (see also Distributed serializability in Serializability). With the proliferation of the Internet, Cloud computing, Grid computing, small, portable, powerful computing devices (e.g., smartphones), and sophisticated systems management the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase, and thus also the need in Commitment ordering (including the popular for databases special case SS2PL; SS2PL, though, does not meet the requirements of many other transactional objects).

Commitment ordering (or Commit ordering; CO) is the only high-performance, fault tolerant, conflict serializability providing solution that has been proposed as a fully distributed (no central computing component or data-structure are needed), general mechanism that can be combined seamlessly with any local (to a database) concurrency control mechanism (see technical summary). Since the CO property of a schedule is a necessary condition for global serializability of "autonomous databases" (in the context of concurrency control), it provides the only general solution for autonomous databases (i.e., if autonomous databases do not comply with CO, then global serializability may be violated). Seemingly by sheer luck, the CO solution possesses many attractive properties: 

The only overhead incurred by the CO solution is locally detecting conflicts (which is already done by any known serializability mechanism, both pessimistic and optimistic) and locally ordering in each database system both the (local) commits of local transactions and the voting for atomic commitment of global transactions. Such overhead is low. The net effect of CO may be some delays of commit events (but never more delay than SS2PL, and on the average less). This makes CO instrumental for global concurrency control of multidatabase systems (e.g., federated database systems). The underlying "Theory of Commitment ordering", part of Serializability theory, is both sound and elegant (and even "mathematically beautiful"; referring to structure and dynamics of conflicts, graph cycles, and deadlocks), with interesting implications for transactional distributed applications.

All the qualities of CO in the list above, except the first three, are also possessed by SS2PL, which is a special case of CO, but blocking and constraining. This partially explains the popularity of SS2PL as a solution (practically, the only solution, for many years) for achieving global serializability. However, property 9 above, automatic resolution of global deadlocks, has not been noticed for SS2PL in the database research literature until today (2009; except in the CO publications). This, since the phenomenon of voting-deadlocks in such environments and their automatic resolution by the atomic commitment protocol has been overlooked.

Most existing database systems, including all major commercial database systems, are "strong strict two phase locking (SS2PL)" based and already CO compliant. Thus they can participate in a CO based solution for global serializability in multidatabase environments without any modification (except for the popular "multiversioning", where additional CO aspects should be considered). Achieving global serializability across SS2PL based databases using atomic commitment (primarily using "two phase commit, 2PC") has been employed for many years (i.e., using the same CO solution for a specific special case; however, no reference is known prior to CO, that notices this special case's automatic global deadlock resolution by the atomic commitment protocol's augmented-conflict-graph global cycle elimination process). Virtually all existing distributed transaction processing environments and supporting products rely on SS2PL and provide 2PC. As a matter of fact SS2PL together with 2PC have become a de facto standard. This solution is a homogeneous concurrency control one, suboptimal (when both Serializability and Strictness are needed; see Strict commitment ordering; SCO) but still quite effective in most cases, sometimes at the cost of increased computing power needed relatively to the optimum. (However, for better performance relaxed serializability is used whenever applications allow). It allows inter-operation among SS2PL-compliant different database system types, i.e., allows heterogeneity in aspects other than concurrency control. SS2PL is a very constraining schedule property, and "takes over" when combined with any other property. For example, when combined with any optimistic property, the result is not optimistic anymore, but rather characteristically SS2PL. On the other hand, CO does not change data-access scheduling patterns at all, and "any" combined property's characteristics remain unchanged. Since also CO uses atomic commitment (e.g., 2PC) for achieving global serializability, as SS2PL does, any CO compliant database system or transactional object can transparently join existing SS2PL based environments, use 2PC, and maintain global serializability without any environment change. This makes CO a straightforward, natural generalization of SS2PL for any conflict serializability based database system, for all practical purposes.

Commitment ordering has been quite widely known inside the "transaction processing" and "databases" communities at "Digital Equipment Corporation" (DEC) since 1990. It has been under "company confidentiality" due to patenting

which is an approach for transaction management in the converging Grid computing and Cloud computing. 
See more in "The History of Commitment Ordering".

Some techniques have been developed for relaxed global serializability (i.e., they do not guarantee global serializability; see also "Relaxing serializability"). Among them (with several publications each):


While local (to a database system) relaxed serializability methods compromise "serializability" for performance gain (and are utilized only when the application can tolerate possible resulting inaccuracies, or its integrity is unharmed), it is unclear that various proposed "relaxed global serializability" methods which compromise "global serializability", provide any performance gain over "commitment ordering" which guarantees global serializability. Typically, the declared intention of such methods has not been performance gain over effective global serializability methods (which apparently have been unknown to the inventors), but rather correctness criteria alternatives due to lack of a known effective global serializability method. Oddly, some of them were introduced years after CO had been introduced, and some even quote CO without realizing that it provides an effective global serializability solution, and thus without providing any performance comparison with CO to justify them as alternatives to global serializability for some applications (e.g., "Two-level serializability"). "Two-level serializability" is even presented as a major global concurrency control method in a 2010 edition of a text-book on databases (authored by two of the original authors of Two-level serializability, where one of them, Avi Silberschatz, is also an author of the original "Strong recoverability" articles). This book neither mentions CO nor references it, and strangely, apparently does not consider CO a valid "Global serializability" solution.

Another common reason nowadays for Global serializability relaxation is the requirement of availability of internet products and services. This requirement is typically answered by large scale data replication. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic distributed transaction. However, with many replicas such a transaction is very large, and may span several computers and networks that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.
Consequently, Optimistic replication (Lazy replication) is often utilized (e.g., in many products and services by Google, Amazon, Yahoo, and alike), while global serializability is relaxed and compromised for eventual consistency. In this case relaxation is done only for applications that are not expected to be harmed by it.

Classes of schedules defined by "relaxed global serializability" properties either contain the global serializability class, or are incomparable with it. What differentiates techniques for "relaxed global conflict serializability" (RGCSR) properties from those of "relaxed conflict serializability" (RCSR) properties that are not RGCSR is typically the different way "global cycles" (span two or more databases) in the "global conflict graph" are handled. No distinction between global and local cycles exists for RCSR properties that are not RGCSR. RCSR contains RGCSR. Typically RGCSR techniques eliminate local cycles, i.e., provide "local serializability" (which can be achieved effectively by regular, known concurrency control methods); however, obviously they do not eliminate all global cycles (which would achieve global serializability).


</doc>
<doc id="31384436" url="https://en.wikipedia.org/wiki?curid=31384436" title="Lossless-Join Decomposition">
Lossless-Join Decomposition

In computer science the concept of a Lossless-Join Decomposition is central in removing redundancy safely from databases while preserving the original data.

Can also be called Nonadditive.
If you decompose a relation formula_1 into relations formula_2 you will have a Lossless-Join if a natural join of the two smaller relations yields back the original relation, i .e.;

formula_3.

If formula_1 is split into formula_5 and formula_6, for this decomposition to be lossless then at least one of the two following criteria should be met.

Projecting on formula_5 and formula_6, and joining back, results in the relation you started with.

Let formula_1 be a relation schema.

Let be a set of functional dependencies on formula_1.

Let formula_5 and formula_6 form a decomposition of formula_1 .

The decomposition is a lossless-join decomposition of formula_1 if at least one of the following functional dependencies are in (where stands for the closure for every attribute or attribute sets in ):



</doc>
<doc id="25753044" url="https://en.wikipedia.org/wiki?curid=25753044" title="Synonym (database)">
Synonym (database)

A synonym is an alias or alternate name for a table, view, sequence, or other schema object. They are used mainly to make it easy for users to access database objects owned by other users. They hide the underlying object's identity and make it harder for a malicious program or user to target the underlying object. Because a synonym is just an alternate name for an object, it requires no storage other than its definition. When an application uses a synonym, the DBMS forwards the request to the synonym's underlying base object. By coding your programs to use synonyms instead of database object names, you insulate yourself from any changes in the name, ownership, or object locations. If you frequently refer to a database object that has a long name, you might appreciate being able to refer to it with a shorter name without having to rename it and alter the code referring to it.

Synonyms are very powerful from the point of view of allowing users access to objects that do not lie within their schema. All synonyms have to be created explicitly with the CREATE SYNONYM command and the underlying objects can be located in the same database or in other databases that are connected by .

There are two major uses of synonyms:

When you create a table or a procedure, it is created in your schema, and other users can access it only by using your schema name as a prefix to the object's name. The way around for this is for the schema owner creates a synonym with the same name as the table name.

Public synonyms are owned by special schema in the Oracle Database called PUBLIC. As mentioned earlier, public synonyms can be referenced by all users in the database. Public synonyms are usually created by the application owner for the tables and other objects such as procedures and packages so the users of the application can see the objects

The following code shows how to create a public synonym for the employee table:

Now any user can see the table by just typing the original table name. If you wish, you could provide a different table name for that table in the CREATE SYNONYM statement. Remember that the DBA must create public synonyms. Just because you can see a table through public (or private) synonym doesn’t mean that you can also perform SELECT, INSERT, UPDATE or DELETE operations on the table. To be able to perform those operations, a user needs specific privileges for the underlying object, either directly or through roles from the application owner.

A private synonym is a synonym within a database schema that a developer typically uses to mask the true name of a table, view stored procedure, or other database object in an application schema.

Private synonyms, unlike public synonyms, can be referenced only by the schema that owns the table or object. You may want to create private synonyms when you want to refer to the same table by different contexts. Private synonym overrides public synonym definitions. You create private synonyms the same way you create public synonyms, but you omit the PUBLIC keyword in the CREATE statement.

The following example shows how to create a private synonym called addresses for the locations table. Note that once you create the private synonym, you can refer to the synonym exactly as you would the original table name.

Synonyms, both private and public, are dropped in the same manner by using the DROP SYNONYM command, but there is one important difference. If you are dropping a public synonym; you need to add the keyword PUBLIC after the keyword DROP.

The ALL_SYNONYMS (or DBA_SYNONYMS) view provides information on all synonyms in your database.



</doc>
<doc id="4032622" url="https://en.wikipedia.org/wiki?curid=4032622" title="Materialized view">
Materialized view

In computing, a materialized view is a database object that contains the results of a query. For example, it may be a local copy of data located remotely, or may be a subset of the rows and/or columns of a table or join result, or may be a summary using an aggregate function.

The process of setting up a materialized view is sometimes called materialization. This is a form of caching the results of a query, similar to memoization of the value of a function in functional languages, and it is sometimes described as a form of precomputation. As with other forms of precomputation, database users typically use materialized views for performance reasons, i.e. as a form of optimization.

Materialized views which store data based on remote tables are also known as snapshots. (C. J. Date regards the phrase "materialized view" as a deprecated term for a "snapshot".)

In any database management system following the relational model, a view is a virtual table representing the result of a database query. Whenever a query or an update addresses an ordinary view's virtual table, the DBMS converts these into queries or updates against the underlying base tables. A materialized view takes a different approach: the query result is cached as a concrete ("materialized") table (rather than a view as such) that may be updated from the original base tables from time to time. This enables much more efficient access, at the cost of extra storage and of some data being potentially out-of-date. Materialized views find use especially in data warehousing scenarios, where frequent queries of the actual base tables can be expensive. 

In a materialized view, indexes can be built on any column. In contrast, in a normal view, it's typically only possible to exploit indexes on columns that come directly from (or have a mapping to) indexed columns in the base tables; often this functionality is not offered at all.

Materialized views were implemented first by the Oracle Database: the Query rewrite feature was added from version 8i.

Example syntax to create a materialized view in Oracle:

REFRESH FAST START WITH SYSDATE
In PostgreSQL, version 9.3 and newer natively support materialized views. In version 9.3, a materialized view is not auto-refreshed, and is populated only at time of creation (unless codice_1 is used). It may be refreshed later manually using codice_2. In version 9.4, the refresh may be concurrent with selects on the materialized view if codice_3 is used.

Example syntax to create a materialized view in PostgreSQL:

Microsoft SQL Server differs from other RDBMS by the way of implementing materialized view via a concept known as "Indexed Views". The main difference is that such views do not require a refresh because they are in fact always synchronized to the original data of the tables that compound the view. To achieve this, it is necessary that the lines of origin and destination are "deterministic" in their mapping which limits the types of possible queries to do this. This mechanism has been realised since the 2000 version of SQL Server.

Example syntax to create a materialized view in SQL Server:

CREATE VIEW MV_MY_VIEW
WITH SCHEMABINDING
AS 
SELECT COL1, SUM(COL2) AS TOTAL
FROM <table_name>
GROUP BY COL1;
GO
CREATE UNIQUE CLUSTERED INDEX XV 
Both Apache Kafka (since v0.10.2) and Apache Spark (since v2.0) support materialized views on streams of data.

Materialized views are also supported in Sybase SQL Anywhere. In IBM DB2, they are called "materialized query tables". ClickHouse supports materialized views that automatically refresh on merges. MySQL doesn't support materialized views natively, but workarounds can be implemented by using triggers or stored procedures or by using the open-source application Flexviews. Materialized views can be implemented in Amazon DynamoDB using data modification events captured by DynamoDB Streams.



</doc>
<doc id="33211278" url="https://en.wikipedia.org/wiki?curid=33211278" title="Prepared statement">
Prepared statement

In database management systems (DBMS), a prepared statement or parameterized statement is a feature used to execute the same or similar database statements repeatedly with high efficiency. Typically used with SQL statements such as queries or updates, the prepared statement takes the form of a template into which certain constant values are substituted during each execution.

The typical workflow of using a prepared statement is as follows:


As compared to executing statements directly, prepared statements offer two main advantages:


On the other hand, if a query is executed only once, server-side prepared statements can be slower because of the additional round-trip to the server. Implementation limitations may also lead to performance penalties; for example, some versions of MySQL did not cache results of prepared queries. 
A stored procedure, which is also precompiled and stored on the server for later execution, has similar advantages. Unlike a stored procedure, a prepared statement is not normally written in a procedural language and cannot use or modify variables or use control flow structures, relying instead on the declarative database query language. Due to their simplicity and client-side emulation, prepared statements are more portable across vendors.

Major DBMSs, including MySQL, Oracle, DB2, Microsoft SQL Server and PostgreSQL widely support prepared statements. Prepared statements are normally executed through a non-SQL binary protocol for efficiency and protection from SQL injection, but with some DBMSs such as MySQL prepared statements are also available using a SQL syntax for debugging purposes.

A number of programming languages support prepared statements in their standard libraries and will emulate them on the client side even if the underlying DBMS does not support them, including Java's JDBC, Perl's DBI, PHP's PDO and Python's DB-API. Client-side emulation can be faster for queries which are executed only once, by reducing the number of round trips to the server, but is usually slower for queries executed many times. It resists SQL injection attacks equally effectively.

Many types of SQL injection attacks can be eliminated by "disabling literals", effectively requiring the use of prepared statements; only H2 supports this feature.

This example uses Java and JDBC:

Java codice_1 provides "setters" (codice_2 etc.) for all major built-in data types.

This example uses PHP and PDO:

This example uses Perl and DBI:

This example uses C# and ADO.NET:

ADO.NET codice_3 will accept any type for the codice_4 parameter of codice_5, and type conversion occurs automatically. Note the use of "named parameters" (i.e. codice_6) rather than codice_7—this allows you to use a parameter multiple times and in any arbitrary order within the query command text.

However, the AddWithValue method should not be used with variable length data types, like varchar and nvarchar. This is because .NET assumes the length of the parameter to be the length of the given value, rather than getting the actual length from the database via reflection. The consequence of this is that a different query plan is compiled and stored for each different length. In general, the maximum number of "duplicate" plans is the product of the lengths of the variable length columns as specified in the database. For this reason, it is important to use the standard Add method for variable length columns:

, where ParamLength is the length as specified in the database.

Since the standard Add method needs to be used for variable length data types, it is a good habit to use it for all parameter types.

This example uses Python and DB-API:

This example uses Direct SQL from Fourth generation language like eDeveloper, uniPaaS and magic XPA from Magic Software Enterprises

PureBasic (since v5.40 LTS) can manage 7 types of link with the following commands

SetDatabaseBlob, SetDatabaseDouble, SetDatabaseFloat, SetDatabaseLong, SetDatabaseNull, SetDatabaseQuad, SetDatabaseString

There are 2 different methods depending on the type of database

For SQLite, ODBC, MariaDB/Mysql use: ? 
For PostgreSQL use: $1, $2, $3, ... 


</doc>
<doc id="815760" url="https://en.wikipedia.org/wiki?curid=815760" title="Database server">
Database server

A database server is a server which uses a database application that provides database services to other computer programs or to computers, as defined by the client–server model. Database management systems (DBMSs) frequently provide database-server functionality, and some database management systems (such as MySQL) rely exclusively on the client–server model for database access (while others e.g. SQLite are meant for using as an embedded database).

Users access a database server either through a "front end" running on the user's computerwhich displays requested dataor through the "back end", which runs on the server and handles tasks such as data analysis and storage.

In a master-slave model, database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as proxies.

Most database applications respond to a query language. Each database understands its query language and converts each submitted query to server-readable form and executes it to retrieve results.

Examples of proprietary database applications include Oracle, DB2, Informix, and Microsoft SQL Server. Examples of free software database applications include PostgreSQL; and under the GNU General Public Licence include Ingres and MySQL. Every server uses its own query logic and structure. The SQL (Structured Query Language) query language is more or less the same on all relational database applications.

For clarification, a database server is simply a server that maintains services related to clients via database applications.

DB-Engines lists over 300 DBMSs in its ranking.

The foundations for modeling large sets of data were first introduced by Charles Bachman in 1969. Bachman introduced Data Structure Diagrams (DSDs) as a means to graphically represent data. DSDs provided a means to represent the relationships between different data entities. In 1970, Codd introduced the concept that users of a database should be ignorant of the "inner workings" of the database. Codd proposed the "relational view" of data which later evolved into the Relational Model which most databases use today. In 1971, the Database Task Report Group of CODASYL (the driving force behind the development of the programming language COBOL) first proposed a "data description language for describing a database, a data description language for describing that part of the data base known to a program, and a data manipulation language." Most of the research and development of databases focused on the relational model during the 1970s.

In 1975 Bachman demonstrated how the relational model and the data structure set were similar and "congruent" ways of structuring data while working for the Honeywell. The Entity-relationship model was first proposed in its current form by Peter Chen in 1976 while he was conducting research at MIT. This model became the most frequently used model to describe relational databases. Chen was able to propose a model that was superior to the navigational model and was more applicable to the "real world" than the relational model proposed by Codd.



</doc>
<doc id="23170217" url="https://en.wikipedia.org/wiki?curid=23170217" title="Database-as-IPC">
Database-as-IPC

In computer programming, Database-as-IPC is an anti-pattern where a database is used as the message queue for routine interprocess communication in a situation where a lightweight IPC mechanism such as sockets would be more suitable. British computer scientist, Junade Ali, defined the Database-as-IPC Anti-Pattern as using a database to "schedule jobs or queue up tasks to be completed", noting that this anti-pattern centres around using a database for temporary messages instead of persistent data.

Using a database for this kind of message passing is extremely inefficient compared to other IPC methods and often introduces serious long-term maintenance issues, but this method enjoys a measure of popularity because the database operations are more widely understood than "proper" IPC mechanisms.


</doc>
<doc id="7514525" url="https://en.wikipedia.org/wiki?curid=7514525" title="Database refactoring">
Database refactoring

A database refactoring is a simple change to a database schema that improves its design while retaining both its behavioral and informational semantics. Database refactoring does not change the way data is interpreted or used and does not fix bugs or add new functionality. Every refactoring to a database leaves the system in a working state, thus not causing maintenance lags, provided the meaningful data exists in the production environment. 

A database refactoring is conceptually more difficult than a code refactoring; code refactorings only need to maintain behavioral semantics while database refactorings also must maintain informational semantics.

You refactor a database schema for one of several reasons:

Examples of database refactoring:

The process of database refactoring is the act of applying database refactorings to evolve an existing database schema (database refactoring is a core practice of evolutionary database design). There are three considerations that need to be taken into account:




</doc>
<doc id="4916098" url="https://en.wikipedia.org/wiki?curid=4916098" title="Database machine">
Database machine

A database machines or back end processor is a computer or special hardware that stores and retrieves data from a database. It is specially designed for database access and is coupled to the main (front-end) computer(s) by a high-speed channel. The database machine is tightly coupled to the main CPU, whereas the database server is loosely coupled via the network. Database machines can transfer large packets of data to the mainframe using hundreds to thousands of microprocessors with database software. The front end processor receives the data and displays it. The back end processor on the other hand analyzes and stores the data from the front end processor. Back end processors result in higher performance, increasing host main memory, increasing database recovery and security, and decrease of cost to manufacture. The database machine contrasts with a database server, which is a computer in a local area network that holds a database.

According to Julie A. McCann, "Currently a DBMS controls the organisation, storage and retrieval of data whilst regulating the security and integrity of the database, it accepts requests for data from the application programs and instructs the operating system (OS) to transfer the appropriate data."

An example is the IBM System/38.


</doc>
<doc id="27594512" url="https://en.wikipedia.org/wiki?curid=27594512" title="Database (journal)">
Database (journal)

Database: The Journal of Biological Databases and Curation is an online peer-reviewed open access scientific journal that covers research on databases and biocuration. The journal was established in 2009 with David Landsman as the editor-in-chief. DATABASE is the official journal of the International Society for Biocuration. The journal has published the proceedings of the International Biocuration Conferences since 2009.

The journal is abstracted and indexed in MEDLINE/PubMed, Asian Science Citation Index, and Chemical Abstracts.


</doc>
<doc id="32288490" url="https://en.wikipedia.org/wiki?curid=32288490" title="Open Database License">
Open Database License

The Open Database License (ODbL) is a copyleft ("share alike") license agreement intended to allow users to freely share, modify, and use a database while maintaining this same freedom for others.

ODbL is published by Open Data Commons (see also Open Data), part of Open Knowledge International (was Foundation).

The ODbL was created with the goal of allowing users to share their data freely without worrying about problems relating to copyright or ownership. It allows users to make free use of the data in the database without worrying about copyright of the creators, and add to the data or use in other databases. The license establishes the rights of users of the database, as well as the correct procedure for attributing credit where credit is due for the data, and how to make changes or improvements in the data, thus simplifying the sharing and comparison of data. Users no longer need to worry of repercussions of violations of copyright law or stolen information when using an Open Database License.



The OpenStreetMap (OSM) project completed the move from a Creative Commons license to ODbL in September 2012 in an attempt to have more legal security and a more specific license for databases rather than creative works.

Other projects using ODbL include OpenCorporates, Open Food Facts, and Paris OpenData.




</doc>
<doc id="10904287" url="https://en.wikipedia.org/wiki?curid=10904287" title="Termcap">
Termcap

Termcap ("terminal capability") is a software library and database used on Unix-like computers. It enables programs to use display computer terminals in a device-independent manner, which greatly simplifies the process of writing portable text mode applications. Bill Joy wrote the first termcap library in 1978 for the Berkeley Unix operating system; it has since been ported to most Unix and Unix-like environments. Joy's design was reportedly influenced by the design of the terminal data store in the earlier Incompatible Timesharing System.

A termcap database can describe the capabilities of hundreds of different display terminals. This allows programs to have character-based display output, independent of the type of terminal. On-screen text editors such as vi and emacs are examples of programs that may use termcap. Other programs are listed in the category.

Examples of what the database describes:

Termcap databases consist of one or more descriptions of terminals.

Each description must contain the canonical name of the terminal. It may also contain one or more aliases for the name of the terminal. The canonical name or aliases are the keys by which the library searches the termcap database.

The description contains one or more capabilities, which have conventional names. The capabilities are typed: "boolean", "numeric" and "string". The termcap library has no predetermined type for each capability name. It determines the types of each capability by the syntax:

Applications which use termcap do expect specific types for the commonly used capabilities, and obtain the values of capabilities from the termcap database using library calls that return successfully only when the database contents matches the assumed type.

Termcap descriptions can be constructed by including the contents of one description in another, suppressing capabilities from the included description or overriding or adding capabilities. No matter what storage model is used, the termcap library constructs the terminal description from the requested description, including, suppressing or overriding at the time of the request.

Termcap data is stored as text, making it simple to modify. The text can be retrieved by the termcap library from files or environment variables.

The TERM environment variable contains the terminal type name.

The TERMCAP environment variable may contain a termcap database. It is most often used to store a single termcap description, set by a terminal emulator to provide the terminal's characteristics to the shell and dependent programs.

The TERMPATH environment variable is supported by newer termcap implementations and defines a search path for termcap files.

The original (and most common) implementation of the termcap library retrieves data from a flat text file. Searching a large termcap file, e.g., 500 kB, can be slow. To aid performance, a utility such as reorder is used to put the most frequently used entries near the beginning of the file.

BSD-4.4 based implementations of termcap store the terminal description in a hashed database (e.g., something like Berkeley DB version 1.85). These store two types of records: aliases which point to the canonical entry, and the canonical entry itself. The text of the termcap entry is stored literally.

The original termcap implementation was designed to use little memory:

Newer implementations of the termcap interface generally do not require the two-character name at the beginning of the entry.

Capability names are still two characters in all implementations.

The tgetent function used to read the terminal description uses a buffer whose size must be large enough for the data, and is assumed to be 1024 characters. Newer implementations of the termcap interface may relax this constraint by allowing a null pointer in place of the fixed buffer, or by hiding the data which would not fit, e.g., via the ZZ capability in NetBSD termcap. The terminfo library interface also emulates the termcap interface, and does not actually use the fixed-size buffer.

The terminfo library's emulation of termcap allows multiple other entries to be included without restricting the position. A few other newer implementations of the termcap library may also provide this ability, though it is not well documented.

A special capability, the "hz" capability, was defined specifically to support the Hazeltine 1500 terminal, which had the unfortunate characteristic of using the ASCII tilde character ('~') as a control sequence introducer. In order to support that terminal, not only did code that used the database have to know about using the tilde to introduce certain control sequences, but it also had to know to substitute another printable character for any tildes in the displayed text, since a tilde in the text would be interpreted by the terminal as the start of a control sequence, resulting in missing text and screen garbling. Additionally, attribute markers (such as start and end of underlining) themselves took up space on the screen. Comments in the database source code often referred to this as "Hazeltine braindamage". Since the Hazeltine 1500 was a widely used terminal in the late 1970s, it was important for applications to be able to deal with its limitations.




</doc>
<doc id="33815178" url="https://en.wikipedia.org/wiki?curid=33815178" title="Metadata repository">
Metadata repository

A metadata repository is a database created to store metadata. Metadata is information about the structures that contain the actual data. Metadata is often said to be "data about data", but this is misleading. Data profiles are an example of actual "data about data". Metadata adds one layer of abstraction to this definition– it is data about the structures that contain data. Metadata may describe the structure of any data, of any subject, stored in any format.

A well-designed metadata repository typically contains data far beyond simple definitions of the various data structures. Typical repositories store dozens to hundreds of separate pieces of information about each data structure.

Comparing the metadata of a couple data items - one digital and one physical - will help one understand what metadata really is:

First, digital: for data stored in a database one may have a table called "Patient" with many columns, each containing data which describes a different attribute of each patient. One of these columns may be named "Patient_Last_Name". What is some of the metadata about the column that contains the actual surnames of patients in the database? We have already used two items: the name of the column that contains the data (Patient_Last_Name) and the name of the table that contains the column (Patient). Other metadata might include the maximum length of last name that may be entered, whether or not last name is required (can we have a patient without Patient_Last_Name?), and whether the database converts any surnames entered in lower case to upper case. Metadata of a security nature may show the restrictions which limit who may view these names.

Second, physical: for data stored in a brick and mortar library, one have many volumes and may have various media, including books. Metadata about books would include ISBN, Binding_Type, Page_Count, Author, etc. Within Binding_Type, metadata would include possible bindings, material, etc.

This contextual information of business data include meaning and content, policies that govern, technical attributes, specifications that transform, and programs that manipulate.

The metadata repository is responsible for physically storing and cataloging metadata. Data in a metadata repository should be generic, integrated, current, and historical. Generic : meta model should store the metadata by generic terms instead of storing it by an applications-specific defined way, so that if your data base standard changes from one product to another the physical meta model of the metadata repository would not need to change. 
Integration of the metadata repository allows all business areas' metadata to be in an integrated fashion: covering all domains and subject areas of the organization. 
The metadata repository should have accessible current and historical metadata. Metadata repositories used to be referred to as a data dictionary.

With the transition of needs for the metadata usage for business intelligence has increased so is the scope of the metadata repository increased. Earlier data dictionaries are the closest place to interact technology with business. Data dictionaries are the universe of metadata repository in the initial stages but as the scope increased Business glossary and their tags to variety of status flags emerged in the business side while consumption of the technology metadata, their lineage and linkages made the repository, the source for valuable reports to bring business and technology together and helped data management decisions easier as well as assess the cost of the changes.

Metadata repository explores the enterprise wide data governance, data quality and master data management (includes master data and reference data) and integrates this wealth of information with integrated metadata across the organization to provide decision support system for data structures, even though it only reflects the structures consumed from various systems.

Repository has additional functionalities compared with registry. Metadata repository not only stores metadata like Metadata registry but also adds relationships with related metadata types. Metadata when related in a flow from its point of entry into organization up to the deliverables is considered as the lineage of that data point. Metadata when related across other related metadata types is called linkages. By providing the relationships to all the metadata points across the organization and maintaining its integrity with an architecture to handle the changes, metadata repository provides the basic material for understanding the complete data flow and their definitions and their impact. Also the important feature is to maintain the version control though this statement for contrasting is open for discussion. These definitions are still evolving, so the accuracy of the definitions needs refinement.

Purpose of registry is to define the metadata element and maintained across the organization. and data models and other data management teams refers to the registry for any changes to follow. While Metadata repository sources metadata from various metadata systems in the organizations and reflects what is in the upstream. Repository never acts as an upstream while registry is used as an upstream for metadata changes.

Metadata repository enables all the structure of the organizations data containers to one integrated place. This opens plethora of resourceful information for making calculated business decisions. This tool uses one generic form of data model to integrate all the models thus brings all the applications and programs of the organization into one format. And on top of it applying the business definitions and business processes brings the business and technology closer that will help organizations make reliable roadmaps with definite goals. With one stop information, business will have more control on the changes, and can do impact analysis of the tool. Usually business spends lots of time and money to make decisions based on discovery and research on impacts to make changes or to add new data structures or remove structures in data management of the organization. With a structured and well maintained repository, moving the product from ideation to delivery takes the least amount of time (considering other variables are constant). 
To sum it up:

Each database management system (DBMS) and database tools have their own language for the metadata components within. Database applications already have their own repositories or registries that are expected to provide all of the necessary functionality to access the data stored within. Vendors do not want other companies to be capable of easily migrating data away from their products and into competitors products, so they are proprietary with the way they handle metadata. CASE tools, DBMS dictionaries, ETL tools, data cleansing tools, OLAP tools, and data mining tools all handle and store metadata differently. Only a metadata repository can be designed to store the metadata components from all of these tools.

Metadata repositories should store metadata in four classifications: ownership, descriptive characteristics, rules and policies, and physical characteristics. Ownership, showing the data owner and the application owner. The descriptive characteristics, define the names, types and lengths, and definitions describing business data or business processes. Rules and policies, will define security, data cleanliness, timelines for data, and relationships. Physical characteristics define the origin or source, and physical location. Like building a logical data model for creating a database, a logical meta model can help identify the metadata requirements for business data. The metadata repository will be centralized, decentralized, or distributed. A centralized design means that there is one database for the metadata repository that stores metadata for all applications business wide. A centralized metadata repository has the same advantages and disadvantages of a centralized database. Easier to manage because all the data is in one database, but the disadvantage is that bottlenecks may occur.

A decentralized metadata repository stores metadata in multiple databases, either separated by location and or departments of the business. This makes management of the repository more involved than a centralized metadata repository, but the advantage is that the metadata can be broken down into individual departments.

A distributed metadata repository uses a decentralized method, but unlike a decentralized metadata repository the metadata remains in its original application. An XML gateway is created that acts as a directory for accessing the metadata within each different application. The advantages and disadvantages for a distributed metadata repository mirror that of a distributed database.

Design of information model should include various layers of metadata types to be overlapped to create an integrated view of the data. Various metadata types should be stitched with related metadata elements in a top down model linking to business glossary.

Layers of Metadata:

Metadata repositories can be designed as either an Entity-relationship model, or an Object-oriented design.



</doc>
<doc id="32543701" url="https://en.wikipedia.org/wiki?curid=32543701" title="Pseudocolumn">
Pseudocolumn

A Pseudocolumn is a "column" that yields a value when selected, but which is not an actual column of the table. An example is RowID or SysDate. It is often used in combination with the DUAL table.


</doc>
<doc id="1419779" url="https://en.wikipedia.org/wiki?curid=1419779" title="IDMS">
IDMS

CA IDMS (Integrated Database Management System) is primarily a network model (CODASYL) database management system for mainframes. It was first developed at B.F. Goodrich and later marketed by Cullinane Database Systems (renamed Cullinet in 1983). Since 1989 the product has been owned by Computer Associates (now CA Technologies), who renamed it Advantage CA-IDMS and later simply to CA IDMS.

The roots of IDMS go back to the pioneering database management system called Integrated Data Store (IDS), developed at General Electric by a team led by Charles Bachman and first released in 1964.

In the early 1960s IDS was taken from its original form, by the computer group of the B.F. Goodrich Chemical Division, and re-written in a language called Intermediate System Language (ISL). ISL was designed as a portable system programming language able to produce code for a variety of target machines. Since ISL was actually written in ISL, it was able to be ported to other machine architectures with relative ease, and then to produce code that would execute on them.

The Chemical Division computer group had given some thought to selling copies of IDMS to other companies, but was told by management that they were not in the software products business. Eventually a deal was struck with John Cullinane to buy the rights and market the product.

Because Cullinane was required to remit royalties back to B.F. Goodrich, all add-on products were listed and billed as separate products - even if they were mandatory for the core IDMS product to work. This sometimes confused customers.

The original platforms were the GE 235 computer and GE DATANET-30 message switching computer: later the product was ported to IBM mainframes and to DEC and ICL hardware.

The IBM-ported version runs on IBM mainframe systems (System/360, System/370, System/390, zSeries, System z9). In the mid-1980s, it was claimed that some 2,500 IDMS licenses had been sold. Users included the Strategic Air Command, Ford of Canada, Ford of Europe, Jaguar Cars, Clarks Shoes UK, AXA/PPP, MAPFRE, Royal Insurance, Tesco, Manulife, Hudson's Bay Company, Cleveland Clinic, Bank of Canada, General Electric, Aetna and BT in the UK.

A version for use on the DECSYSTEM series of computers was sold to DEC and was marketed as DBMS10 and later DBMS20.

In 1976 the source code was licensed to ICL, who ported the software to run on their 2900 series mainframes, and subsequently also on the older 1900 range. ICL continued development of the software independently of Cullinane, selling the original ported product under the name ICL 2900 IDMS and an enhanced version as IDMSX. In this form it was used by many large UK users, an example being the Pay-As-You-Earn system operated by Inland Revenue. Many of these IDMSX systems for UK Government were still running in 2013.

In the early to mid-1980s, relational database management systems started to become more popular, encouraged by increasing hardware power and the move to minicomputers and client–server architecture. Relational databases offered improved development productivity over CODASYL systems, and the traditional objections based on poor performance were slowly diminishing.

Cullinet attempted to continue competing against IBM's DB2 and other relational databases by developing a relational front-end and a range of productivity tools. These included Automatic System Facility (ASF), which made use of a pre-existing IDMS feature called LRF (Logical Record Facility). ASF was a fill-in-the-blanks database generator that would also develop a mini-application to maintain the tables.

It is difficult to judge whether such features may have been successful in extending the selling life of the product, but they made little impact in the long term. Those users who stayed with IDMS were primarily interested in its high performance, not in its relational capabilities. It was widely recognized (helped by a high-profile campaign by E. F. Codd, the father of the relational model) that there was a significant difference between a relational database and a network database with a relational veneer.

In 1989 Computer Associates continued after Cullinet acquisition with the development and released Release 12.0 with full SQL in 1992-93.

Nowadays, CA Technologies actively markets and supports the CA IDMS and enhanced IDMS in subsequent releases by TCP/IP support, Two-Phase commit support, XML publishing, zIIP specialty processor support, Web-enabled access in combination with CA IDMS Server, SQL Option and GUI database administration via CA IDMS Visual DBA tool.

CA-IDMS systems are today still running businesses worldwide. Many customers have opted to web-enable their applications via the CA-IDMS SQL Option which is part of CA Technologies' Dual Database Strategy.

One of the sophisticated features of IDMS was its built-in Integrated Data Dictionary (IDD). The IDD was primarily developed to maintain database definitions. It was itself an IDMS database.

DBAs (database administrators) and other users interfaced with the IDD using a language called Data Dictionary Definition Language (DDDL).

IDD was also used to store definitions and code for other products in the IDMS family such as ADS/Online and IDMS-DC.

IDD's power was that it was extensible and could be used to create definitions of just about anything. Some companies used it to develop in-house documentation.

The data model offered to users is the CODASYL network model. The main structuring concepts in this model are records and sets. Records essentially follow the COBOL pattern, consisting of fields of different types: this allows complex internal structure such as repeating items and repeating groups.

The most distinctive structuring concept in the Codasyl model is the set. Not to be confused with a mathematical set, a Codasyl set represents a one-to-many relationship between records: one owner, many members. The fact that a record can be a member in many different sets is the key factor that distinguishes the network model from the earlier hierarchical model. As with records, each set belongs to a named set type (different set types model different logical relationships). Sets are in fact ordered, and the sequence of records in a set can be used to convey information. A record can participate as an owner and member of any number of sets.

Records have identity, the identity being represented by a value known as a database key. In IDMS, as in most other Codasyl implementations, the database key is directly related to the physical address of the record on disk. Database keys are also used as pointers to implement sets in the form of linked lists and trees. This close correspondence between the logical model and the physical implementation (which is not a strictly necessary part of the Codasyl model, but was a characteristic of all successful implementations) is responsible for the efficiency of database retrieval, but also makes operations such as database loading and restructuring rather expensive.

Records can be accessed directly by database key, by following set relationships, or by direct access using key values. Initially the only direct access was through hashing, a mechanism known in the Codasyl model as CALC access. In IDMS, CALC access is implemented through an internal set, linking all records that share the same hash value to an owner record that occupies the first few bytes of every disk page.

In subsequent years, some versions of IDMS added the ability to access records using BTree-like indexes.

IDMS organizes its databases as a series of files. These files are mapped and pre-formatted into so-called areas. The areas are subdivided into pages which correspond to physical blocks on the disk. The database records are stored within these blocks.

The DBA allocates a fixed number of pages in a file for each area. The DBA then defines which records are to be stored in each area, and details of how they are to be stored.

IDMS intersperses special space-allocation pages throughout the database. These pages are used to keep track of the free space available in each page in the database. To reduce I/O requirements, the free space is only tracked for all pages when the free space for the area falls below 30%.

Four methods are available for storing records in an IDMS database: Direct, Sequential, CALC, and VIA. The Fujitsu/ICL IDMSX version extends this with two more methods, Page Direct, and Random.

In direct mode the target database key is specified by the user and is stored as close as possible to that DB key, with the actual DB key on which the record is stored being returned to the application program.

Sequential placement (not to be confused with indexed sequential), simply places each new record at the end of the area. This option is rarely used.

CALC uses a hashing algorithm to decide where to place the record; the hash key then provides efficient retrieval of the record. The entire CALC area is preformatted each with a header consisting of a special CALC "owner" record. The hashing algorithm determines a page number (from which the physical disk address can be determined), and the record is then stored on this page, or as near as possible to it, and is linked to the header record on that page using the CALC set. The CALC records are linked to the page's CALC Owner record using a single link-list (pointers). The CALC Owner located in the page header thus owns the set of all records which target to its particular page (whether the records are stored on that page or, in the case of an overflow, on another page ).

CALC provides extremely efficient storage and retrieval: IDMS can retrieve a CALC record in 1.1 I/O operations. However, the method does not cope well with changes to the value of the primary key, and expensive reorganization is needed if the number of pages needs to be expanded. A work-around is to expand the area, and then run an application program which scans the area sequentially for each CALC record, and then uses the MODIFY verb to update each record. This results in each CALC record being connected to the CALC Set for the correct target page as calculated for the Area's new page range. The downside to this method is that vanishingly few CALC records will now be on their target pages, and navigating each page's CALC set is likely to involve many IO operations. As a result, it is recommended only to use this work-around in extreme circumstances as performance will suffer.

VIA placement attempts to store a record near its owner in a particular set. Usually the records are clustered on the same physical page as the owner. This leads to efficient navigation when the record is accessed by following that set relationship. (VIA allows records to be stored in a different IDMS area so that they can be stored separately from the owner, yet remain clustered together for efficiency. Within IDMSX they may also be offset from the owner by a set number of pages).

Page Direct (IDMSX only) is similar to Direct mode, however a target Database page number is specified and the record is connected to the CALC chain for that page.

Random (IDMSX only) allocates a target page number to the record occurrence when it is stored using the CALC algorithm (this either uses a Key within the record or in the case of un-keyed random, uses the date & time of storage as a seed for the CALC algorithm).

Sets are generally maintained as linked lists, using the database key as a pointer. Every record includes a forward link to the next record; the database designer can choose whether to include owner pointers and prior pointers (if not provided, navigation in those directions will be slower).

Some versions of IDMS subsequently included the ability to define indexes: either record indexes, allowing records to be located from knowledge of a secondary key, or set indexes, allowing the members of a set to be retrieved by key value.

The IDMSX Page Direct and Random placement records are typically used in conjunction with Record Indexes as described above. The Indexes themselves are subject to placement rules, either Direct (which really means "CALC using the Index ID as the key") or CALC.

IDMS has many non-profit user associations across the globe composed of information technology professionals who use and/or support CA IDMS or related products. They include:




</doc>
<doc id="36085842" url="https://en.wikipedia.org/wiki?curid=36085842" title="Free license">
Free license

A free license or open license is a license agreement which contains provisions that allow other individuals to reuse another creator's work, giving them four major freedoms. Without a special license, these uses are normally prohibited by copyright law or commercial license. Most free licenses are worldwide, royalty-free, non-exclusive, and perpetual (see copyright durations). Free licenses are often the basis of crowdsourcing and crowdfunding projects.

The invention of the term "free license" and the focus on the rights of users were connected to the sharing traditions of the hacker culture of the 1970s public domain software ecosystem, the social and political free software movement (since 1980) and the open source movement (since the 1990s). These rights were codified by different groups and organizations for different domains in Free Software Definition, Open Source Definition, Debian Free Software Guidelines, Definition of Free Cultural Works and The Open Definition. These definitions were then transformed into licenses, using the copyright as legal mechanism. Since then, ideas of free/open licenses spread into different spheres of society.

Open source, free culture (unified as free and open-source movement), anticopyright, Wikimedia Foundation projects, public domain advocacy groups and pirate parties are connected with free and open licenses.





Creative Commons has affiliates in more than 100 jurisdictions all over the world.

EUPL was created in the European Union.

Harald Welte created gpl-violations.org



</doc>
<doc id="34793825" url="https://en.wikipedia.org/wiki?curid=34793825" title="Schema migration">
Schema migration

In software engineering, schema migration (also database migration, database change management) refers to the management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that database's schema to some newer or older version.

Migrations are performed programmatically by using a "schema migration tool". When invoked with a specified desired schema version, the tool automates the successive application or reversal of an appropriate sequence of schema changes until it is brought to the desired state.

Most schema migration tools aim to minimize the impact of schema changes on any existing data in the database. Despite this, preservation of data in general is not guaranteed because schema changes such as the deletion of a database column can destroy data (i.e. all values stored under that column for all rows in that table are deleted). Instead, the tools help to preserve the meaning of the data or to reorganize existing data to meet new requirements. Since meaning of the data often cannot be encoded, the configuration of the tools usually needs manual intervention.

Schema migration allows for fixing mistakes and adapting the data as requirements change. They are an essential part of software evolution, especially in agile environments (see below).

Applying a schema migration to a production database is always a risk. Development and test databases tend to be smaller and cleaner. The data in them is better understood or, if everything else fails, the amount of data is small enough for a human to process. Production databases are usually huge, old and full of surprises. The surprises can come from many sources:


For these reasons, the migration process needs a high level of discipline, thorough testing and a sound backup strategy.

When developing software applications backed by a database, developers typically develop the application source code in tandem with an evolving database schema. The code typically has rigid expectations of what columns, tables and constraints are present in the database schema whenever it needs to interact with one, so only the version of database schema against which the code was developed is considered fully compatible with that version of source code.

In software testing, while developers may mock the presence of a compatible database system for unit testing, any level of testing higher than this (e.g. integration testing or system testing) it is common for developers to test their application against a local or remote test database schematically compatible with the version of source code under test. In advanced applications, the migration itself can be subject to migration testing.

With schema migration technology, data models no longer need to be fully designed up-front, and are more capable of being adapted with changing project requirements throughout the software development lifecycle.

Teams of software developers usually use version control systems to manage and collaborate on changes made to versions of source code.
Different developers can develop on divergent, relatively older or newer branches of the same source code to make changes and additions during development.

Supposing that the software under development interacts with a database, every version of the source code can be associated with at least one database schema with which it is compatible.

Under good software testing practice, schema migrations can be performed on test databases to ensure that their schema is compatible to the source code. To streamline this process, a schema migration tool is usually invoked as a part of an automated software build as a prerequisite of the automated testing phase.

Schema migration tools can be said to solve versioning problems for database schemas just as version control systems solve versioning problems for source code. In practice, many schema migration tools actually rely on a textual representation of schema changes (such as files containing SQL statements) such that the version history of schema changes can effectively be stored alongside program source code within VCS. This approach ensures that the information necessary to recover a compatible database schema for a particular code branch is recoverable from the source tree itself. Another benefit of this approach is the handling of concurrent conflicting schema changes; developers may simply use their usual text-based conflict resolution tools to reconcile differences.

Schema migration tooling could be seen as a facility to track the history of an evolving schema.

Developers no longer need to remove the entire test database in order to create a new test database from scratch (e.g. using schema creation scripts from DDL generation tools). Further, if generation of test data costs a lot of time, developers can avoid regenerating test data for small, non-destructive changes to the schema.



</doc>
<doc id="37049649" url="https://en.wikipedia.org/wiki?curid=37049649" title="Spanner (database)">
Spanner (database)

Spanner is a "NewSQL" database developed by Google. Spanner is a globally distributed database service and storage solution. It provides features such as global transactions, strongly consistent reads, and automatic multi-site replication and failover.

Spanner stores large amounts of mutable structured data. Spanner allows users to perform arbitrary queries using SQL with relational data while maintaining strong consistency and high availability for that data with synchronous replication.

Key features of Spanner:


Spanner joined the Google platform in February 2017. It is available as part of Google Cloud Platform. 

Spanner's SQL capability was added in 2017 and documented in a SIGMOD 2017 paper 
Spanner uses the Paxos algorithm as part of its operation to shard (partition) data across hundreds of servers. It makes heavy use of hardware-assisted clock synchronization using GPS clocks and atomic clocks to ensure global consistency.

Google's F1 SQL database management system (DBMS) is built on top of Spanner, replacing Google's custom MySQL variant.




</doc>
<doc id="259065" url="https://en.wikipedia.org/wiki?curid=259065" title="Foreign key">
Foreign key

In the context of relational databases, a foreign key is a set of attributes subject to a certain kind of inclusion dependency constraint, specifically a constraint that the tuples consisting of the foreign key attributes in one relation, R, must also exist in some other (not necessarily distinct) relation, S, and furthermore that those attributes must also be a candidate key in S. In simpler words, a foreign key is a set of attributes that "references" a candidate key. For example, a table called TEAM may have an attribute, MEMBER_NAME, which is a foreign key referencing a candidate key, EMPLOYEE_NAME, in the EMPLOYEE table. Since MEMBER_NAME is a foreign key, any value existing as the name of a member in TEAM must also exist as an employee name in the EMPLOYEE table; every member of a TEAM is also an EMPLOYEE..

The table containing the foreign key is called the child table, and the table containing the candidate key is called the referenced or parent table. In database relational modeling and implementation, a candidate key is a set of zero or more attributes, the values of which are guaranteed to be unique for each tuple (row) in a relation. The value or combination of values of candidate key attributes for any tuple cannot be duplicated for any other tuple in that relation.

Since the purpose of the foreign key is to identify a particular row of referenced table, it is generally required that the foreign key is equal to the candidate key in some row of the primary table, or else have no value (the NULL value.). This rule is called a referential integrity constraint between the two tables.
Because violations of these constraints can be the source of many database problems, most database management systems provide mechanisms to ensure that every non-null foreign key corresponds to a row of the referenced table.

For example, consider a database with two tables: a CUSTOMER table that includes all customer data and an ORDER table that includes all customer orders. Suppose the business requires that each order must refer to a single customer. To reflect this in the database, a foreign key column is added to the ORDER table (e.g., CUSTOMERID), which references the primary key of CUSTOMER (e.g. ID). Because the primary key of a table must be unique, and because CUSTOMERID only contains values from that primary key field, we may assume that, when it has a value, CUSTOMERID will identify the particular customer which placed the order. However, this can no longer be assumed if the ORDER table is not kept up to date when rows of the CUSTOMER table are deleted or the ID column altered, and working with these tables may become more difficult. Many real world databases work around this problem by 'inactivating' rather than physically deleting master table foreign keys, or by complex update programs that modify all references to a foreign key when a change is needed.

Foreign keys play an essential role in database design. One important part of database design is making sure that relationships between real-world entities are reflected in the database by references, using foreign keys to refer from one table to another.
Another important part of database design is database normalization, in which tables are broken apart and foreign keys make it possible for them to be reconstructed.

Multiple rows in the referencing (or child) table may refer to the same row in the referenced (or parent) table. In this case, the relationship between the two tables is called a one to many relationship between the referenced table and the referencing table.

In addition, the child and parent table may, in fact, be the same table, i.e. the foreign key refers back to the same table. Such a foreign key is known in as a self-referencing or recursive foreign key. In database management systems, this is often accomplished by linking a first and second reference to the same table.

A table may have multiple foreign keys, and each foreign key can have a different parent table. Each foreign key is enforced independently by the database system. Therefore, cascading relationships between tables can be established using foreign keys.

Likewise, foreign keys can be defined as part of the codice_1 SQL statement.

If the foreign key is a single column only, the column can be marked as such using the following syntax:Foreign keys can be defined with a stored procedure statement.


Because the database management system enforces referential constraints, it must ensure data integrity if rows in a referenced table are to be deleted (or updated). If dependent rows in referencing tables still exist, those references have to be considered. specifies 5 different referential actions that shall take place in such occurrences:

Whenever rows in the master (referenced) table are deleted (or updated), the respective rows of the child (referencing) table with a matching foreign key column will be deleted (or updated) as well. This is called a cascade delete (or update).

A value cannot be updated or deleted when a row exists in a referencing or child table that references the value in the referenced table.

Similarly, a row cannot be deleted as long as there is a reference to it from a referencing or child table.

To understand RESTRICT (and CASCADE) better, it may be helpful to notice the following difference, which might not be immediately clear. The referential action CASCADE modifies the "behavior" of the (child) table itself where the word CASCADE is used. For example, ON DELETE CASCADE effectively says "When the referenced row is deleted from the other table (master table), then delete "also from me"". However, the referential action RESTRICT modifies the "behavior" of the master table, "not" the child table, although the word RESTRICT appears in the child table and not in the master table! So, ON DELETE RESTRICT effectively says: "When someone tries to delete the row from the other table (master table), prevent deletion "from that other table" (and of course, also don't delete from me, but that's not the main point here)."

RESTRICT is not supported by Microsoft SQL 2012 and earlier.

NO ACTION and RESTRICT are very much alike. The main difference between NO ACTION and RESTRICT is that with NO ACTION the referential integrity check is done after trying to alter the table. RESTRICT does the check before trying to execute the UPDATE or DELETE statement. Both referential actions act the same if the referential integrity check fails: the UPDATE or DELETE statement will result in an error.

In other words, when an UPDATE or DELETE statement is executed on the referenced table using the referential action NO ACTION, the DBMS verifies at the end of the statement execution that none of the referential relationships are violated. This is different from RESTRICT, which assumes at the outset that the operation will violate the constraint. Using NO ACTION, the triggers or the semantics of the statement itself may yield an end state in which no foreign key relationships are violated by the time the constraint is finally checked, thus allowing the statement to complete successfully.

In general, the action taken by the DBMS for SET NULL or SET DEFAULT is the same for
both ON DELETE or ON UPDATE: The value of the affected referencing attributes is
changed to NULL for SET NULL, and to the specified default value for SET DEFAULT.

Referential actions are generally implemented as implied triggers (i.e. triggers with system-generated names, often hidden.) As such, they are subject to the same limitations as user-defined triggers, and their order of execution relative to other triggers may need to be considered; in some cases it may become necessary to replace the referential action with its equivalent user-defined trigger to ensure proper execution order, or to work around mutating-table limitations.

Another important limitation appears with transaction isolation: your changes to a row may not be able to fully cascade because the row is referenced by data your transaction cannot "see", and therefore cannot cascade onto. An example: while your transaction is attempting to renumber a customer account, a simultaneous transaction is attempting to create a new invoice for that same customer; while a CASCADE rule may fix all the invoice rows your transaction can see to keep them consistent with the renumbered customer row, it won't reach into another transaction to fix the data there; because the database cannot guarantee consistent data when the two transactions commit, one of them will be forced to roll back (often on a first-come-first-served basis.)

As a first example to illustrate foreign keys, suppose an accounts database has a table with invoices and each invoice is associated with a particular supplier. Supplier details (such as name and address) are kept in a separate table; each supplier is given a 'supplier number' to identify it. Each invoice record has an attribute containing the supplier number for that invoice. Then, the 'supplier number' is the primary key in the Supplier table. The foreign key in the Invoices table points to that primary key. The relational schema is the following. Primary keys are marked in bold, and foreign keys are marked in italics.

The corresponding Data Definition Language statement is as follows.



</doc>
<doc id="11658036" url="https://en.wikipedia.org/wiki?curid=11658036" title="Data administration">
Data administration

Data administration or data resource management is an organizational function working in the areas of information systems and computer science that plans, organizes, describes and controls data resources. Data resources are usually as stored in databases under a database management system or other software such as electronic spreadsheets. In many smaller organizations, data administration is performed occasionally, or is a small component of the database administrator’s work.

In the context of information systems development, data administration ideally begins at system conception, ensuring there is a data dictionary to help maintain consistency, avoid redundancy, and model the database so as to make it logical and usable, by means of data modeling, including database normalization techniques.

According to the Data Management Association (DAMA), data resource management is "the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise".
Data Resource management may be thought of as a managerial activity that applies information system and other data management tools to the task of managing an organization’s data resource to meet a company’s business needs, and the information they provide to their shareholders.

Since the beginning of the information age, businesses need all types of data on their business activity. With each data created, when a business transaction is made, need data is created. With these data, new direction is needed that focuses on managing data as a critical resource of the organization to directly support its business activities. The data resource must be managed with the same intensity and formality that other critical resources are managed. Organizations must emphasize the information aspect of information technology, determine the data needed to support the business, and then use appropriate technology to build and maintain a high-quality data resource that provides that support.

Data resource quality is a measure of how well the organization's data resource supports the current and the future business information demand of the organization. The data resource cannot support just the current business information demand while sacrificing the future business information demand. It must support both the current and the future business information demand. The ultimate data resource quality is stability across changing business needs and changing technology.

A corporate data resource must be developed within single, organization-wide common data architecture. A data architecture is the science and method of designing and constructing a data resource that is business driven, based on real-world objects and events as perceived by the organization, and implemented into appropriate operating environments. It is the overall structure of a data resource that provides a consistent foundation across organizational boundaries to provide easily identifiable, readily available, high-quality data to support the business information demand.

The common data architecture is a formal, comprehensive data architecture that provides a common context within which all data at an organization's disposal are understood and integrated. It is subject oriented, meaning that it is built from data subjects that represent business objects and business events in the real world that are of interest to the organization and about which data are captured and maintained.


</doc>
<doc id="39084002" url="https://en.wikipedia.org/wiki?curid=39084002" title="FoundationDB">
FoundationDB

FoundationDB is a free and open-source multi-model distributed NoSQL database developed by Apple Inc. with a shared-nothing architecture. The product was designed around a "core" database, with additional features supplied in "layers." The core database exposes an ordered key-value store with transactions. The transactions are able to read or write multiple keys stored on any machine in the cluster while fully supporting ACID properties. Transactions are used to implement a variety of data models via layers.

The FoundationDB Alpha program began in January 2012 and concluded on March 4, 2013 with their public Beta release. Their 1.0 version was released for general availability on August 20, 2013. On March 24, 2015 it was reported that Apple has acquired the company. A notice on the FoundationDB web site indicated that the company has "evolved" its mission and would no longer offer downloads of the software.

On April 19, 2018, Apple open sourced the software, releasing it under the Apache 2.0 license.

The main features of FoundationDB included the following:










The design of FoundationDB results in several limitations:




FoundationDB, headquartered in Vienna, VA, was started in 2009 by Nick Lavezzo, Dave Rosenthal, and Dave Scherer, drawing on their experience in executive and technology roles at their previous company, Visual Sciences.

In March 2015 the FoundationDB Community site was updated to state that the company had changed directions and would no longer be offering downloads of its product. The company was acquired by Apple Inc., which was confirmed March 25, 2015.

On April 19, 2018, Apple open sourced the software, releasing it under the Apache 2.0 license.




</doc>
<doc id="38890725" url="https://en.wikipedia.org/wiki?curid=38890725" title="Database System Concepts">
Database System Concepts

Database System Concepts, by Abraham Silberschatz, Hank Korth, and S. Sudarshan is a best-selling textbook on database systems.

It is often called the sailboat book, because its cover has had sailboats since its first edition. The first edition cover had a number of sailboats, labelled with the names of various database models. The boats are sailing from a desert island towards a tropical island, while the wind pushes them away and prevents their arrival.

The book is currently in its 7th edition, released in March 2019 (copyright year 2020), with previous editions being released in 2010 (6th edition), 2005 (5th edition), 2001 (4th edition), 1997 (3rd edition), 1991 (2nd edition) and 1986 (1st edition). The 1st and 2nd editions of the book were authored by Hank Korth and Abraham Silberschatz. S. Sudarshan has been a co-author since the third edition. Various editions of the book have been translated into numerous languages, including Chinese, Korean, Portuguese and Spanish.



</doc>
<doc id="313755" url="https://en.wikipedia.org/wiki?curid=313755" title="Binary large object">
Binary large object

A Binary Large OBject (BLOB) is a collection of binary data stored as a single entity in a database management system. Blobs are typically images, audio or other multimedia objects, though sometimes binary executable code is stored as a blob. Database support for blobs is not universal.

Blobs were originally just big amorphous chunks of data invented by Jim Starkey at DEC, who describes them as "the thing that ate Cincinnati, Cleveland, or whatever" from "the 1958 Steve McQueen movie", referring to "The Blob". Later, Terry McKiever, a marketing person for Apollo, felt that it needed to be an acronym and invented the backronym "Basic Large Object". Then Informix invented an alternative backronym, "Binary Large Object".

The data type and definition was introduced to describe data not originally defined in traditional computer database systems, particularly because it was too large to store practically at the time the field of database systems was first being defined in the 1970s and 1980s. The data type became practical when disk space became cheap. This definition gained popularity with IBM's DB2.

The term is used in NoSQL databases, especially in Key-value store databases such as Redis.

The name "blob" is further borrowed by the deep learning software Caffe to represent multi-dimensional arrays.

In the world of free and open-source software, the term is also borrowed to refer to proprietary device drivers, which are distributed without their source code, exclusively through binary code; in such use, the term "binary blob" is common, even though the first letter in the "blob" abbreviation already stands for "binary".

Depending on the implementation and culture around usage, the concept might be alternately referred to as a "basic large object" or "binary data type".



</doc>
<doc id="8423925" url="https://en.wikipedia.org/wiki?curid=8423925" title="Database connection">
Database connection

A Database connection is a facility in computer science that allows client software to talk to database server software, whether on the same machine or not. A connection is required to send commands and receive answers, usually in the form of a result set.

Connections are a key concept in data-centric programming. Since some DBMS engines require considerable time to connect, connection pooling was invented to improve performance. No command can be performed against a database without an "open and available" connection to it.

Connections are built by supplying an underlying driver or provider with a connection string, which is a way of addressing a specific database or server and instance as well as user authentication credentials (for example, "Server=sql_box;Database=Common;User ID=uid;Pwd=password;"). Once a connection has been built it can be opened and closed at will, and properties (such as the command time-out length, or transaction, if one exists) can be set. The Connection String is composed of a set of key/value pairs as dictated by the data access interface and data provider being used.

Many databases (such as PostgreSQL) only allow one operation to be performed at a time on each connection. If a request for data (a SQL Select statement) is sent to the database and a result set is returned, the connection is open but not available for other operations until the client finishes consuming the result set. Other databases, like SQL Server 2005 (and later), do not impose this limitation. However, databases that provide multiple operations per connection usually incur far more overhead than those that permit only a single operation task at a time.

Database connections are finite and expensive and can take a disproportionately long time to create relative to the operations performed on them. It is very inefficient for an application to create, use, and close a database connection whenever it needs to update a database.

Connection pooling is a technique designed to alleviate this problem. A pool of database connections can be created and then shared among the applications that need to access the database.

The connection object obtained from the connection pool is often a wrapper around the actual database connection. The wrapper understands its relationship with the pool, and hides the details of the pool from the application. For example, the wrapper object can implement a "close" method that can be called just like the "close" method on the database connection. Unlike the method on the database connection, the method on the wrapper may not actually close the database connection, but instead return it to the pool. The application need not be aware of the connection pooling when it calls the methods on the wrapper object.

This approach encourages the practice of opening a connection in an application only when needed, and closing it as soon as the work is done, rather than holding a connection open for the entire life of the application. In this manner, a relatively small number of connections can service a large number of requests. This is also called multiplexing.

In a client/server architecture, on the other hand, a persistent connection is typically used so that server state can be managed. This "state" includes server-side cursors, temporary products, connection-specific functional settings, and so on.

An application failure occurs when the connection pool overflows. This can occur if all of the connection in the pool are in use when an application requests a connection. For example, the application may use a connection for too long when too many clients attempt to access the web site or one or more operations are blocked or simply inefficient.





</doc>
<doc id="39083496" url="https://en.wikipedia.org/wiki?curid=39083496" title="Hekaton (database)">
Hekaton (database)

Hekaton (also known as SQL Server In-Memory OLTP) is an in-memory database for OLTP workloads built into Microsoft SQL Server. Hekaton was designed in collaboration with Microsoft Research and was released in SQL Server 2014.

Traditional RDBMS systems were designed when memory resources were expensive, and were optimized for disk storage. Hekaton is instead optimized for a working set stored entirely in main memory, but is still accessible via T-SQL like normal tables. It is fundamentally different from the "DBCC PINTABLE" feature in earlier SQL Server versions.

Hekaton was announced at the Professional Association for SQL Server (PASS) conference 2012.


</doc>
<doc id="41433308" url="https://en.wikipedia.org/wiki?curid=41433308" title="Project 6">
Project 6

Project 6, or simply P6, is a global surveillance project jointly operated by U.S. Central Intelligence Agency (CIA) in close cooperation with the German intelligence agencies Bundesnachrichtendienst (BND) and Bundesamt für Verfassungsschutz (BfV). As part of efforts to combat terrorism, the project includes a massive database containing personal information such as photos, license plate numbers, Internet search histories and telephone metadata of presumed jihadists. The headquarters of the project is located in Neuss, Germany.



</doc>
<doc id="23957935" url="https://en.wikipedia.org/wiki?curid=23957935" title="Soterml">
Soterml

SoTerML (Soil and Terrain Markup Language) is a XML-based markup language for storing and exchanging soil and terrain related data. SoTerML development is being done within The e-SoTer Platform. GEOSS plans a global Earth Observation System and, within this framework, the e-SOTER project addresses the felt need for a global soil and terrain database.
The Centre for Geospatial Science (Currently Nottingham Gepospatial Institute) at the University of Nottingham has initiated the development since January 2009. Further development and maintenance is currently handled in National Soil Resources Institurte (NSRI) at Cranfield University, UK. The role of CGS is within the development of the e-SOTER dissemination platform, which is based on INSPIRE principles. The SoTerML development included:

1. Development of a data dictionary for nomenclatures and various data sources (data and metadata).

2. Development of an exchange format/procedures from the World Reference Base 2006. 



</doc>
<doc id="42171777" url="https://en.wikipedia.org/wiki?curid=42171777" title="Multimedia database">
Multimedia database

A Multimedia database (MMDB) is a collection of related for multimedia data. The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.

A Multimedia Database Management System (MMDBMS) is a framework that manages different types of data potentially represented in a wide diversity of formats on a wide array of media sources. It provides support for multimedia data types, and facilitate for creation, storage, access, query and control of a multimedia database.

A Multimedia Database (MMDB) hosts one or more multimedia data types (i.e. text, images, graphic objects, audio, video, animation sequences). 
These data types are broadly categorized into three classes: 

Additionally, a Multimedia Database (MMDB) needs to manage additional information pertaining to the actual multimedia data. 
The information is about the following: 
The last three types are called metadata as they describe several different aspects of the media data. The media keyword data and media feature data are used as indices for searching purpose. The media format data is used to present the retrieved information.

Like the traditional databases, Multimedia databases should address the following requirements:

Multimedia databases should have the ability to uniformly query data (media data, textual data) represented in different formats and have the ability to simultaneously query different media sources and conduct classical database operations across them. "(Query support)"

They should have the ability to retrieve media objects from a local storage device in a good manner. "(Storage support)"

They should have the ability to take the response generated by a query and develop a presentation of that response in terms of audio-visual media and have the ability to deliver this presentation. "(Presentation and delivery support)"


Examples of multimedia database application areas:



</doc>
<doc id="1423177" url="https://en.wikipedia.org/wiki?curid=1423177" title="Data store">
Data store

A data store is a repository for persistently storing and managing collections of data which include not just repositories like databases, but also simpler store types such as simple files, emails etc.

A database is a series of bytes that is managed by a database management system (DBMS). A file is a series of bytes that is managed by a file system. Thus, any database or file is a series of bytes that, once stored, is called a data store.

MATLAB and Cloud Storage systems like VMware, Firefox OS use "datastore" as a term for abstracting collections of data inside their respective applications.

Data store can refer to a broad class of storage systems including:



</doc>
<doc id="43856964" url="https://en.wikipedia.org/wiki?curid=43856964" title="Database seeding">
Database seeding

Database seeding is the initial seeding of a database with data.

Seeding a database is a process in which an initial set of data is provided to a database when it is being installed . 

It is especially useful when we want to populate the database with data we want to develop in future.

This is often an automated process that is executed upon the initial setup of an application.

The data can be dummy data or necessary data such as an initial administrator account.

\Migrations\Configuration.cs

AppBundle/DataFixtures/ORM/customer.yml (as in Version 1 of hautelook/AliceBundle )

app/database/seeds/users.php


</doc>
<doc id="4159307" url="https://en.wikipedia.org/wiki?curid=4159307" title="Data access layer">
Data access layer

A data access layer (DAL) in computer software is a layer of a computer program which provides simplified access to data stored in persistent storage of some kind, such as an entity-relational database. This acronym is prevalently used in Microsoft environments.

For example, the DAL might return a reference to an object (in terms of object-oriented programming) complete with its attributes instead of a row of fields from a database table. This allows the client (or user) modules to be created with a higher level of abstraction. This kind of model could be implemented by creating a class of data access methods that directly reference a corresponding set of database stored procedures. Another implementation could potentially retrieve or write records to or from a file system. The DAL hides this complexity of the underlying data store from the external world.

For example, instead of using commands such as "insert", "delete", and "update" to access a specific table in a database, a class and a few stored procedures could be created in the database. The procedures would be called from a method inside the class, which would return an object containing the requested values. Or, the insert, delete and update commands could be executed within simple functions like "registeruser" or "loginuser" stored within the data access layer.

Also, business logic methods from an application can be mapped to the Data Access Layer. So, for example, instead of making a query into a database to fetch all users from several tables the application can call a single method from a DAL which abstracts those database calls.

Applications using a data access layer can be either database server dependent or independent. If the data access layer supports multiple database types, the application becomes able to use whatever databases the DAL can talk to. In either circumstance, having a data access layer provides a centralized location for all calls into the database, and thus makes it easier to port the application to other database systems (assuming that 100% of the database interaction is done in the DAL for a given application).

Object-Relational Mapping tools provide data layers in this fashion, following the Active Record or Data Mapper patterns. The ORM/active-record model is popular with web frameworks.




</doc>
<doc id="44971098" url="https://en.wikipedia.org/wiki?curid=44971098" title="Multi-model database">
Multi-model database

Most database management systems are organized around a single data model that determines how data can be organized, stored, and manipulated. In contrast, a multi-model database is designed to support multiple data models against a single, integrated backend. Document, graph, relational, and key-value models are examples of data models that may be supported by a multi-model database.

The relational data model became popular after its publication by Edgar F. Codd in 1970. Due to increasing requirements for horizontal scalability and fault tolerance, NoSQL databases became prominent after 2009. NoSQL databases use a variety of data models, with document, graph, and key-value models being popular.

A Multi-model database is a database that can store, index and query data in more than one model. For some time, databases have primarily supported only one model, such as: relational database, document-oriented database, graph database or triplestore. A database that combines many of these is multi-model.

For some time, it was all but forgotten (or considered irrelevant) that there were any other database models besides Relational. The Relational model and notion of third normal form were the de facto standard for all data storage. However, prior to the dominance of Relational data modeling from about 1980 to 2005 the hierarchical database model was commonly used, and since 2000 or 2010, many NoSQL models that are non-relational including Documents, triples, key-value stores and graphs are popular. Arguably, geospatial data, temporal data and text data are also separate models, though indexed, queryable text data is generally termed a "search engine" rather than a database.

The first time the word "multi-model" has been associated to the databases was on May 30, 2012 in Cologne, Germany, during the Luca Garulli's key note "NoSQL Adoption – What’s the Next Step?". Luca Garulli envisioned the evolution of the 1st generation NoSQL products into new products with more features able to be used by multiple use cases.

The idea of multi-model databases can be traced back to Object-Relational Data Management Systems (ORDBMS) in the early 1990s and in a more broader scope even to federated and integrated DBMSs in the early 1980s. An ORDBMS system manages different types of data such as relational, object, text and spatial by plugging domain specific data types, functions and index implementations into the DBMS kernels. A Multi-model database is most directly a response to the "polyglot persistence" approach of knitting together multiple database products, each handing a different model, to achieve a multi-model capability as described by Martin Fowler. This strategy has two major disadvantages: it leads to a significant increase in operational complexity, and there is no support for maintaining data consistency across the separate data stores, so multi-model databases have begun to fill in this gap.

Multi-model databases are intended to offer the data modeling advantages of polyglot persistence, without its disadvantages. Operational complexity, in particular, is reduced through the use of a single data store. In general, there are two solutions to directly manage multi-model data currently: a single integrated multi-model database system or a tightly-integrated middleware over multiple single-model data stores.

Multi-model databases include (in alphabetic order):

As more and more platforms are proposed to deal with multi-model data, there are a few works on benchmarking multi-model databases. For instance, Pluciennik , Oliveira, and UniBench reviewed existing multi-model databases and made an evaluation effort towards comparing multi-model databases and other SQL and NoSQL databases respectively. They pointed out that the advantages of multi-model databases over single-model databases are as follows : (i) they are able to ingest a variety of data formats such as CSV( including Graph, Relational), JSON into storage without any additional efforts, (ii) they can employ a unified query language such as AQL, Orient SQL, SQL/XML, SQL/JSON to retrieve correlated multi-model data, such as graph-JSON-key/value, XML-relational, and JSON-relational in a single platform. (iii) they are able to support multi-model ACID transactions in the stand-alone mode.

The main difference between the available multi-model databases is related to their architectures. Multi-model databases can support different models either within the engine or via different layers on top of the engine. Some products may provide an engine which supports documents and graphs while others provide layers on top of a key-key store. With a layered architecture, each data model is provided via its own component.

In addition to offering multiple data models in a single data store, some databases allow developers to easily define custom data models. This capability is enabled by ACID transactions with high performance and scalability. In order for a custom data model to support concurrent updates, the database must be able to synchronize updates across multiple keys. ACID transactions, if they are sufficiently performant, allow such synchronization. JSON documents, graphs, and relational tables can all be implemented in a manner that inherits the horizontal scalability and fault-tolerance of the underlying data store.




</doc>
<doc id="44439976" url="https://en.wikipedia.org/wiki?curid=44439976" title="Composite index (database)">
Composite index (database)

A database composite index or multi-column index is an index that is based on several columns.


</doc>
<doc id="46456424" url="https://en.wikipedia.org/wiki?curid=46456424" title="PastPerfect Museum Software">
PastPerfect Museum Software

PastPerfect Museum Software is an application for collections archiving. It is designed for museums, but may be used by various institutions including libraries, archives, and natural history collections. PastPerfect allows for the database storage of artifacts, documents, photographs, and library books. PastPerfect is utilized by over 9000 museums nationwide.

PastPerfect was introduced in 1998 as the primary product of the then Pastime Software Company Inc. It was released in version 2 later the same year, updated to version three in 2001, and then to version four in 2004. It updated to version five in 2010, which remains the most current edition of the program. Pastime Software Company eventually changed its name to PastPerfect Software Inc. after 2007.

PastPerfect operates with four basic catalogs for sorting collections material. There is the "archive" catalog for storing documents, the "photograph" catalog for storing photos, tintypes, paintings, etc.; the "objects" catalog for storing three-dimensional artifacts, and the "library" catalog for storing books that would be included in the institution's library. Images can be uploaded into the catalog, with multiple photos per record. It also stores donor information, and automatically generates Deed of Gift forms and thank you letters upon completing certain steps of the accessioning process. Certain extensions allow for the inclusion of Oral History records and transcripts and there is an additional online function that allows institutions to digitize their collections and make them free to browse on the internet.

PastPerfect also contains a contact list function, for storing donors, members, and employee contact information in a similar catalog format as the rest of the program.


</doc>
<doc id="2255218" url="https://en.wikipedia.org/wiki?curid=2255218" title="Catalog server">
Catalog server

A catalog server provides a single point of access that allows users to centrally search for information across a distributed network. In other words, it indexes databases, files and information across large network and allows keywords, Boolean and other searches. If you need to provide a comprehensive searching service for your intranet, extranet or even the Internet, a catalog server is a standard solution.


</doc>
<doc id="46580631" url="https://en.wikipedia.org/wiki?curid=46580631" title="Text Database and Dictionary of Classic Mayan">
Text Database and Dictionary of Classic Mayan

The project Text Database and Dictionary of Classic Mayan (abbr. TWKM) promotes research on the writing and language of pre-Hispanic Maya culture. It is housed in the Faculty of Arts at the University of Bonn and was established with funding from the North Rhine-Westphalian Academy of Sciences, Humanities and the Arts. The project has a projected run-time of fifteen years and is directed by Nikolai Grube from the Department of Anthropology of the Americas at the University of Bonn. The goal of the project is to conduct computer-based studies of all extant Maya hieroglyphic texts from an epigraphic and cultural-historical standpoint, and to produce and publish a database and a comprehensive dictionary of the Classic Mayan language.

The text database, as well as the dictionary that will be compiled by the conclusion of the project, will be assembled based on all known texts from the pre-Hispanic Maya culture. These texts were produced and used between approximately the third century B.C. through A.D. 1500, in a region that today includes parts of the countries of Mexico, Guatemala, Belize, and Honduras. The thousands of hieroglyphic inscriptions on monuments, ceramics, or daily objects that have survived into the present offer insight into the language's vocabulary and structure. The project's database and dictionary will digitally represent original spellings using the logo-syllabic Maya hieroglyphs, as well as their transcription and transliteration in the Roman alphabet. The data will be additionally annotated with various epigraphic analyses, translations, and further object-specific information.

TWKM will employ digital technologies in order to compile and make available the data and metadata, as well as to publish the project's research results. The project thereby methodologically positions itself in the field of the digital humanities. The project will be conducted in cooperation with the project partners (below), the research association for the eHumanities TextGrid, as well as the University and Regional Library of Bonn (ULB). The working environment that is currently under construction, in which the data and metadata will be compiled and annotated, will be realized in theTextGrid Laboratory, a software of the virtual research environment. A further component of this software, the TextGrid Repository, will make the data that are authorized for publication freely available online and ensure their long-term storage. 
The tools for data compilation and annotation attained from the modularly constructed and extended TextGrid lab thereby provide all the necessary materials for facilitating the research team's the typical epigraphic workflow. The workflow usually begins by documenting the texts and the objects on which they are preserved, and by compiling descriptive data. It then continues with the various levels of epigraphic and linguistic analysis, and concludes in the best case scenario with a translation of the analyzed inscription and a corresponding publication. In cooperation with the ULB, selected data will additionally be made available. The project's Virtual Inscription Archive will present online, in the Digital Collections of the ULB, hieroglyphic inscriptions selected from the published data in the repository, including an image of and brief information about the texts and the objects on which they are written, epigraphic analysis, and translation.

One of the project's goals is to produce a dictionary of Classic Mayan, in both digital and print form, towards the end of the project run-time. Additionally, a database with a corpus of inscriptions, including their translations and epigraphic analyses, will be made freely available online. The database furthermore will provide an ontology-like link of the contextual object data with the inscriptions and with each other, thereby allowing a cultural-historical arrangement of all contents within the periods of pre-Hispanic Maya culture. The contents of the database are additionally linked to citations of relevant literature. As a result, the database will also make freely available to both the scientific community and other interested parties a bibliography representing the research history and a base of knowledge concerning ancient Maya culture and script.
In addition, the Classic Maya script, in its temporally defined stages of language development, will be gathered into and documented in a comprehensive language corpus with the aid of the information gathered by the project. In collaboration with all project participants, the corpus data can be used, together with the aid of various comparable analyses and also computational linguistic methods, such as inference-based methods, to confirm readings of some hieroglyphs that are currently only partially confirmed, and to eventually completely decipher the Classic Maya script.




</doc>
<doc id="23454460" url="https://en.wikipedia.org/wiki?curid=23454460" title="Database preservation">
Database preservation

Database preservation usually involves converting the information stored in a database to a form likely to be accessible in the long term as technology changes, without losing the initial characteristics (context, content, structure, appearance and behaviour) of the data.

Version 1.0 of the Software Independent Archiving of Relational Databases (SIARD) format was developed by the Swiss Federal Archives in 2007. It was designed for archiving relational databases in a vendor-neutral form. A SIARD archive is a ZIP-based package of files based on XML and SQL:1999. A SIARD file incorporates both the database content and also machine-processable structural metadata that records the structure of database tables and their relationships. The ZIP file contains an XML file describing the database structure (metadata.xml) as well as a collection of XML files, one per table, capturing the table content. The SIARD archive may also contain text files and binary files representing database large objects (BLOBs and CLOBs). SIARD permits direct access to individual tables by exploring with ZIP tools. A SIARD archive is not an operational database but supports re-integration of the archived database into another relational database management system (RDBMS) that supports SQL:1999. In addition, SIARD supports the addition of descriptive and contextual metadata that is not recorded in the database itself and the embedding of documentation files in the archive. SIARD Version 1.0 was homologized as standard eCH-0165 in 2013. 

Version 2.0 of the SIARD preservation format was designed and developed by the Swiss Federal Archives under the auspices of the E-ARK project. Version 2.0 is based on version 1.0 and defines a format that is backwards-compatible with version 1.0. New features in version 2.0 include:


A XML schema was created by researcher José Carlos Ramalho from the University of Minho to capture table information and data from a relational database. It was published in 2007.

The Database Preservation Toolkit (DBPTK) allows conversion between database formats, including connection to live systems, for purposes of digitally preserving databases. The toolkit allows conversion of live or backed-up databases into preservation formats such as SIARD, an XML-based format created for the purpose of database preservation. The toolkit also allows conversion of the preservation formats back into live systems to allow the full functionality of databases. For example, it supports a specialized export into MySQL, optimized for PhpMyAdmin, so the database can be fully experimented using a web interface.

This toolkit was originally part of the RODA project and then released on its own. It has been further developed in the E-ARK project together with a new version of the SIARD preservation format.

The toolkit uses input and output modules. Each module supports read and/or write to a particular database format or live system. New modules can easily be added by implementation of a new interface and adding new drivers.

Research projects this regard include:



</doc>
<doc id="47533724" url="https://en.wikipedia.org/wiki?curid=47533724" title="Common Database on Designated Areas">
Common Database on Designated Areas

The Common Database on Designated Areas or CDDA is a data bank for officially designated protected areas such as nature reserves, protected landscapes, national parks etc. in Europe.

The data bank, which went live in 1999, is a community project of the European Environment Agency (EEA) of the Council of Europe and the United Nations Environment Programme World Conservation Monitoring Centre (UNEP-WCMC).

The data bank is divided into a national area and an international area. The national area is for member states of the EU or EEA about the European Environment Information and Observation Network or "EIONET". Data cleansing for the national area of non-EEA members and the international area is carried out by UNEP-WCMC systems.

The data bank follows the system of the International Union for Conservation of Nature and Natural Resources (IUCN) and the standards of the United Nations in order to ensure compatibility with similar data banks worldwide, especially the World Database on Protected Areas (WDPA).

The data bank can be accessed from the Internet using the website of the European Nature Information System (EUNIS).

To date, true marine protected areas such as the Marine Protected Areas in the Atlantic Arc (MAIA) have not been included in the data bank. This is being pursued.


</doc>
<doc id="6306937" url="https://en.wikipedia.org/wiki?curid=6306937" title="International Medical Education Directory">
International Medical Education Directory

The International Medical Education Directory (IMED) was a public database of worldwide medical schools. The IMED was published as a joint collaboration of the Educational Commission for Foreign Medical Graduates (ECFMG) and the Foundation for Advancement of International Medical Education and Research (FAIMER).

The information available in IMED was derived from data collected by the Educational Commission for Foreign Medical Graduates (ECFMG) throughout its history of evaluating the medical education credentials of international medical graduates. Using these data as a starting point, Foundation for Advancement of International Medical Education and Research (FAIMER) began developing "IMED" in 2001 and made it publicly available in April 2002.

In April 2014, IMED was merged with the Avicenna Directory to create the World Directory of Medical Schools. The World Directory is now the definitive list of medical schools in the world, as IMED and Avicenna were discontinued in 2015.




</doc>
<doc id="47617027" url="https://en.wikipedia.org/wiki?curid=47617027" title="Event store">
Event store

An event store is a type of database optimized for storage of events.

Conceptually, in an event store, only the "events" of a dossier or policy are stored. The idea behind it is that the dossier or policy can be derived from these events. The events (and their corresponding data) are the only "real" facts that should be stored in the database. The instantiation of all other objects can be derived from these events. The code instantiates these objects in memory. In an event store database, this means that all objects that should be instantiated, are "not" stored in the database. Instead these objects are instantiated 'on the fly' in memory by the code based on the events. After usage of these objects (e.g. shown in a user interface), the instantiated objects are removed from memory.

For example, the event store concept of a database can be applied to insurance policies or pension dossiers. In these policies or dossiers the instantiation of each object that make up the dossier or policy (the person, partner(s), employments, etc.) can be derived and can be instantiated in memory based on the real world events.

A crucial part of an event store database is that each event has a double timeline: This enables event stores to correct errors of events that have been entered into the event store database before. 

Another crucial part of an event store database is that events that are stored are not allowed to be changed. Once stored, also erroneous events are not changed anymore. The only way to change (or better: correct) these events is to instantiate a new event with the new values and using the double timeline. A correcting event would have the new values of the original event, with an event data of that corrected event, but a different transaction date. This mechanism ensures reproducibility at each moment in the time, even in the time period before the correction has taken place. It also allows to reproduce situations based on erroneous events (if required).

One advantage of the event store concept is that handling the effects of back dated events (events that take effect before previous events and that may even invalidate them) is much easier. In regular databases, handling backdated events to correct previous, erroneous events can be painful as it often results in rolling back all previous, erroneous transactions and objects and rolling up the new, correct transactions and objects. In an event store, only the new event (and it's corresponding facts) are stored. The code will then redetermine the transactions and objects based on the new facts in memory.

An event store will simplify the code in that rolling back erroneous situations and rolling up the new, correct situations is not needed anymore.

Disadvantage may be that the code needs to re-instantiate all objects in memory based on the events each time a service call is received for a specific dossier of policy.




</doc>
<doc id="48214291" url="https://en.wikipedia.org/wiki?curid=48214291" title="Polyglot persistence">
Polyglot persistence

Polyglot persistence is the concept of using different data storage technologies to handle different data storage needs within a given software application. Polyglot programming, a term coined by Neal Ford in 2006, expresses the idea that computer applications should be written in a mix of different programming languages, in order to take advantage of the fact that different languages are suitable for tackling different problems. Complex applications combine different types of problems, so picking the right language for each job may be more productive than trying to solve all aspects of the problem using a single language. This same concept can be applied to databases, that an application can communicate with different databases, using each for what it is best at to achieve an end goal, hence the term "polyglot persistence".

There are numerous databases available to solve different problems. Using a single database to satisfy all of a program's requirements can result in a non-performant, "jack of all trades, master of none" solution. Relational databases, for example, are good at enforcing relationships that exist between various data tables. To discover a relationship or to find data from different tables that belong to the same object, an SQL join operation can be used. This might work when the data is smaller in size, but becomes problematic when the data involved grows larger. A graph database might solve the problem of relationships in case of Big Data, but it might not solve the problem of database transactions, which are provided by RDBM systems. Instead, a NoSQL document database might be used to store unstructured data for that particular part of the problem. Thus different problems are solved by different database systems, all within the same application.



</doc>
<doc id="28366048" url="https://en.wikipedia.org/wiki?curid=28366048" title="Couchbase Server">
Couchbase Server

Couchbase Server, originally known as Membase, is an open-source, distributed (shared-nothing architecture) multi-model NoSQL document-oriented database software package that is optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating and presenting data. In support of these kinds of application needs, Couchbase Server is designed to provide easy-to-scale key-value or JSON document access with low latency and high sustained throughput. It is designed to be clustered from a single machine to very large-scale deployments spanning many machines.
A version originally called Couchbase Lite was later marketed as Couchbase Mobile combined with other software.

Couchbase Server provided client protocol compatibility with memcached, but added disk persistence, data replication, live cluster reconfiguration, rebalancing and multitenancy with data partitioning.

Membase was developed by several leaders of the memcached project, who had founded a company, NorthScale, to develop a key-value store with the simplicity, speed, and scalability of memcached, but also the storage, persistence and querying capabilities of a database. The original membase source code was contributed by NorthScale, and project co-sponsors Zynga and Naver Corporation (then known as NHN) to a new project on membase.org in June 2010.

On February 8, 2011, the Membase project founders and Membase, Inc. announced a merger with CouchOne (a company with many of the principal players behind CouchDB) with an associated project merger. The merged company was called Couchbase, Inc. In January 2012, Couchbase released Couchbase Server 1.8. 
In September, 2012, Orbitz said it had changed some of its systems to use Couchbase.
On December 2012, 
Couchbase Server 2.0 (announced in July 2011) was released and included a new JSON document store, indexing and querying, incremental MapReduce and replication across data centers.

Every Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.
In the parlance of Eric Brewer’s CAP theorem, Couchbase is normally a CP type system meaning it provides consistency and partition tolerance, or it can be set up as an AP system with multiple clusters.

The cluster manager supervises the configuration and behavior of all the servers in a Couchbase cluster. It configures and supervises inter-node behavior like managing replication streams and re-balancing operations. It also provides metric aggregation and consensus functions for the cluster, and a RESTful cluster management interface. The cluster manager uses the Erlang programming language and the Open Telecom Platform.

Data replication within the nodes of a cluster can be controlled with several parameters.
In December 2012, support was added for replication between different data centers.

The data manager stores and retrieves documents in response to data operations from applications.
It asynchronously writes data to disk after acknowledging to the client. In version 1.7 and later, applications can optionally ensure data is written to more than one server or to disk before acknowledging a write to the client.
Parameters define item ages that affect when data is persisted, and how max memory and migration from main-memory to disk is handled.
It supports working sets greater than a memory quota per "node" or "bucket".
External systems can subscribe to filtered data streams, supporting, for example, full text search indexing, data analytics or archiving.

A document is the most basic unit of data manipulation in Couchbase Server. Documents are stored in JSON document format with no predefined schemas.

Couchbase Server includes a built-in multi-threaded object-managed cache that implements memcached compatible APIs such as get, set, delete, append, prepend etc.

Couchbase Server has a tail-append storage design that is immune to data corruption, OOM killers or sudden loss of power. Data is written to the data file in an append-only manner, which enables Couchbase to do mostly sequential writes for update, and provide an optimized access patterns for disk I/O.

A performance benchmark done by Altoros in 2012, compared Couchbase Server with other technologies.
Cisco Systems published a benchmark that measured the latency and throughput of Couchbase Server with a mixed workload in 2012.

Couchbase Server is a packaged version of Couchbase's open source software technology and is available in a community edition without recent bug fixes with Apache 2.0 license. and an edition for commercial use. 
Couchbase Server builds are available for Ubuntu, Debian, Red Hat, SUSE, Oracle Linux, Microsoft Windows and macOS operating systems.

Couchbase has supported software developers' kits for the programming languages .Net, PHP, Ruby, Python, C, Node.js, Java, and Go.

A query language called the non-first normal form query language, N1QL (pronounced nickel), is used for manipulating the JSON data in Couchbase, just like SQL manipulates data in RDBMS. It has SELECT, INSERT, UPDATE, DELETE, MERGE statements to operate on JSON data.
It was announced in March 2015 as "SQL for documents".

The N1QL data model is non-first normal form (N1NF) with support for nested attributes and domain-oriented normalization. The N1QL data model is also a proper superset and generalization of the relational model.





</doc>
<doc id="49241631" url="https://en.wikipedia.org/wiki?curid=49241631" title="Enterprise Information Technology Data Repository (EITDR)">
Enterprise Information Technology Data Repository (EITDR)

The Enterprise Information Technology Data Repository (EITDR) is the United States Air Force official database, presented as a webservice, for registering information technology (IT) systems and maintaining portfolio management data. This database provides IT portfolio managers and senior leaders with investment decision support and the ability to track and report compliance with federal laws and regulations. EITDR provides automated IT management processes, a common access point to gather, view, load, update, query, report and store pertinent data from disparate systems. EITDR serves as a single point of user entry for AF IT data and has electronic interface to other DoD/AF systems requiring the data.

United States Code, Title 40, Subtitle III, Chapter 113, requires the Air Force Chief Information Officer to implement a portfolio management process for maximizing the value and assessing and managing risks associated with information technology acquisition and use. EITDR was the result of this law.

The Office of the Secretary of the Air Force, Warfighting Integration and Chief Information Officer (SAF/XC) has primary responsibility. As of November 30, 2006, EITDR recorded 146 systems with operating budgets exceeding $1 million. In addition, FY06 EITDR operations and maintenance costs totaled $4.3 million.

On May 1, 2017, EITDR was replaced by ITIPS as the system of record for IT compliance.



</doc>
<doc id="49303115" url="https://en.wikipedia.org/wiki?curid=49303115" title="Digital Notam">
Digital Notam

A Digital NOTAM (DIGITAM), is a data set made available through digital services containing information concerning the establishment, condition or change in any aeronautical facility, service, procedure or hazard, the timely knowledge of which is essential to systems and automated equipment used by personnel concerned with flight operations.

The Digital NOTAM encoding is based on the Aeronautical Information Exchange Model (AIXM) version 5, which has been developed in cooperation between the European Organisation for the Safety of Air Navigation (EUROCONTROL) and the United States Federal Aviation Administration (FAA), with the support of the international AIS community. The DIGITAM eliminates the free form text contained within a NOTAM and replaces the text with a series of structured facts, which obtain to the aeronautical entity concerned.

In 2012 the Federal Aviation Administration (FAA) successfully launched the first global digital NOTAM (Notice to Airmen) system to all airports and airplanes in the United States.


</doc>
<doc id="1040387" url="https://en.wikipedia.org/wiki?curid=1040387" title="Database design">
Database design

Database design is the organization of data according to a database model. The designer determines what data must be stored and how the data elements interrelate. With this information, they can begin to fit the data to the database model.
Database management system manages the data accordingly.

Database design involves classifying data and identifying interrelationships. This theoretical representation of the data is called an "ontology". The ontology is the theory behind the database's design.

In a majority of cases, a person who is doing the design of a database is a person with expertise in the area of database design, rather than expertise in the domain from which the data to be stored is drawn e.g. financial information, biological information etc. Therefore, the data to be stored in the database must be determined in cooperation with a person who does have expertise in that domain, and who is aware of what data must be stored within the system.

This process is one which is generally considered part of requirements analysis, and requires skill on the part of the database designer to elicit the needed information from those with the domain knowledge. This is because those with the necessary domain knowledge frequently cannot express clearly what their system requirements for the database are as they are unaccustomed to thinking in terms of the discrete data elements which must be stored. Data to be stored can be determined by Requirement Specification.

Once a database designer is aware of the data which is to be stored within the database, they must then determine where dependency is within the data. Sometimes when data is changed you can be changing other data that is not visible. For example, in a list of names and addresses, assuming a situation where multiple people can have the same address, but one person cannot have more than one address, the address is dependent upon the name. When provided a name and the list the address can be uniquely determined; however, the inverse does not hold - when given an address and the list, a name cannot be uniquely determined because multiple people can reside at an address. Because an address is determined by a name, an address is considered dependent on a name.

Once the relationships and dependencies amongst the various pieces of information have been determined, it is possible to arrange the data into a logical structure which can then be mapped into the storage objects supported by the database management system. In the case of relational databases the storage objects are tables which store data in rows and columns. In an Object database the storage objects correspond directly to the objects used by the Object-oriented programming language used to write the applications that will manage and access the data. The relationships may be defined as attributes of the object classes involved or as methods that operate on the object classes.

The way this mapping is generally performed is such that each set of related data which depends upon a single object, whether real or abstract, is placed in a table. Relationships between these dependent objects is then stored as links between the various objects.

Each table may represent an implementation of either a logical object or a relationship joining one or more instances of one or more logical objects. Relationships between tables may then be stored as links connecting child tables with parents. Since complex logical relationships are themselves tables they will probably have links to more than one parent.

Database designs also include ER (entity-relationship model) diagrams. An ER diagram is a diagram that helps to design databases in an efficient way.

Attributes in ER diagrams are usually modeled as an oval with the name of the attribute, linked to the entity or relationship that contains the attribute.


In the field of relational database design, "normalization" is a systematic way of ensuring that a database structure is suitable for general-purpose querying and free of certain undesirable characteristics—insertion, update, and deletion anomalies that could lead to loss of data integrity.

A standard piece of database design guidance is that the designer should create a fully normalized design; selective denormalization can subsequently be performed, but only for performance reasons. The trade-off is storage space vs performance. The more normalized the design is, the less data redundancy there is (and therefore, it takes up less space to store), however, common data retrieval patterns may now need complex joins, merges, and sorts to occur - which takes up more data read, and compute cycles. Some modeling disciplines, such as the dimensional modeling approach to data warehouse design, explicitly recommend non-normalized designs, i.e. designs that in large part do not adhere to 3NF.
Normalization consists of normal forms that are 1NF,2NF,3NF,BOYCE-CODD NF (3.5NF),4NF and 5NF

Document databases take a different approach. A document that is stored in such a database, typically would contain more than one normalized data unit and often the relationships between the units as well. If all the data units and the relationships in question are often retrieved together, then this approach optimizes the number of retrieves. It also simplifies how data gets replicated, because now there is a clearly identifiable unit of data whose consistency is self-contained. Another consideration is that reading and writing a single document in such databases will require a single transaction - which can be an important consideration in a Microservices architecture. In such situations, often, portions of the document are retrieved from other services via an API and stored locally for efficiency reasons. If the data units were to be split out across the services, then a read (or write) to support a service consumer might require more than one service calls, and this could result in management of multiple transactions, which may not be preferred.

The physical design of the database specifies the physical configuration of the database on the storage media. This includes detailed specification of data elements, data types, indexing options and other parameters residing in the DBMS data dictionary. It is the detailed design of a system that includes modules & the database's hardware & software specifications of the system. Some aspects that are addressed at the physical layer:

At the application level, other aspects of the physical design can include the need to define stored procedures, or materialized query views, OLAP cubes, etc.




</doc>
<doc id="49584683" url="https://en.wikipedia.org/wiki?curid=49584683" title="Standard database management analysis">
Standard database management analysis

Standard database management analysis


</doc>
<doc id="47530715" url="https://en.wikipedia.org/wiki?curid=47530715" title="Cockroach Labs">
Cockroach Labs

Cockroach Labs is a computer software company that develops commercial database management systems. It is best known for CockroachDB, which has been compared to Google Spanner. CockroachDB is a project that is designed to store copies of data in multiple locations in order to deliver speed access. It is described as a scalable, consistently-replicated, transactional datastore.

Cockroach Labs was founded in 2015 by ex-Google employees Spencer Kimball, Peter Mattis, and Ben Darnell. Prior to Cockroach Labs, Kimball and Mattis were key members of the Google File System team while Darnell was a key member of the Google Reader team. While at Google, all three had previously used Bigtable and were acquainted with its successor, Spanner. After leaving Google, they wanted to design and build something similar for companies outside of Google. By June 2015, the company had nine CockroachDB engineers.

Spencer Kimball wrote the first iteration of the design in January 2014 and began the open-source project on GitHub in February 2014, allowing outside access and contributions. It attracted a community of experienced contributors, with the co-founders actively supporting the project with conferences, networking and meet-ups. Its collaborations on GitHub earned it Open Source Rookie of the Year, a title awarded by Black Duck Software to open-source projects.

In June 2015, the company closed $6.25 million in funding from Benchmark, Sequoia, Google Ventures and FirstMark Capital. Benchmark's general partner Peter Fenton was named to the company's board of directors. Additional investors included Hortonworks chief executive Rob Bearden, CoreOS CEO Alex Polvi, and Cloudera co-founder Jeff Hammerbacher.

In June 2019, Cockroach Labs announced that CockroachDB would change its license from the free software license Apache License 2.0 to a proprietary license.

Cockroach Labs raised $55 million in a Series C round in August 2019, led by Altimeter Capital.

The database is scalable in that a single instance can scale from a single laptop to thousands of servers.

CockroachDB is designed to run in the cloud and be resilient to failures. The result is a database that is described as "almost impossible" to take down. Even if multiple servers or an entire datacenter were to go offline, CockroachDB would keep services online.



</doc>
<doc id="50675732" url="https://en.wikipedia.org/wiki?curid=50675732" title="CrocBITE">
CrocBITE

CrocBITE is an online database of crocodile attacks reported on humans. The non-profit online research tool helps to scientifically analyze crocodile behavior via complex models. Users are encouraged to feed information in a crowdsourcing manner. The uploaded information needs to be verifiable. The database provides key insights into crocodile attack patterns and draws inferences to save human lives.

The online database was established in by Dr Adam Britton, a researcher at Charles Darwin University, his student Brandon Sideleau and Erin Britton. It is a compilation of government records, individual reports, registered contributors and historical data. Dr Simon Pooley, Junior Research fellow, Imperial College London joined hands to further the studies. The collaboration culminated when Dr Pooley met Dr Britton at the IUCN Crocodile Specialist Group, in Louisiana in 2014. Brandon was instrumental in designing the database and work on the IT infrastructure. The program received funds from Economic and Social Research Council, United Kingdom to the tune of and unspecified resourced plus amount from Big Gecko Crocodilian Research, Crocodillian.com and Charles Darwin University.

The research already has yielded pertinent observations that provide inside into crocodile attacks. It was observed that most attacks on humans occur from bites of Saltwater crocodile as against the popular understanding of Nile crocodiles taking the top spot. The broad category of Nile crocodile attacks were segmented into West African crocodile and "Crocodylus niloticus" (the Nile Crocodile) species to get a clear understanding of their respective attack zones.

The objective is that the information will used by communities and conservation managers to help inform and educate people about how to keep safe. The information is vital for Australia and Africa where such attacks are more likely than in other parts of the world. This is the only database of its kind with such comprehensive collection of information made available online.




</doc>
<doc id="31332189" url="https://en.wikipedia.org/wiki?curid=31332189" title="Digital Scriptorium">
Digital Scriptorium

Digital Scriptorium (DS) is a non-profit, tax-exempt consortium of American libraries with collections of pre-modern manuscripts, or manuscripts made in the tradition of books before printing. The DS database represents these manuscript collections in a web-based union catalog for teaching and scholarly research in medieval and Renaissance studies. It provides access to illuminated and textual manuscripts through online cataloging records, supported by high resolution digital images, retrievable by various topic searches. The DS database is an open access resource that enables users to study rare and valuable materials of academic, research, and public libraries. It makes available collections that are often restricted from public access and includes not only famous masterpieces of book illumination but also understudied manuscripts that have been previously overlooked for publication or study.

Funded by grants from the Andrew W. Mellon Foundation, the National Endowment for the Humanities, and the Gladys Krieble Delmas Foundation, DS at its inception in 1997 was a joint project of the Bancroft Library at the University of California, Berkeley (under Prof. Charles Faulhaber) and the Rare Book & Manuscript Library of Columbia University (under Dr. Consuelo W. Dutschke). The plan was to digitize and make available on the World Wide Web catalog records and selected images from the two universities' medieval and early Renaissance manuscript collections. The decision in favor of sample images rather than the complete imaging of manuscripts was originally practical, but today DS includes some records with sample and some with complete imaging. Records with sample images offer various pathways of entrance to the growing corpus of medieval and Renaissance manuscripts now available online. Because of patterns of collecting in the 19th and early 20th century, moreover, many manuscripts in American collections comprise partial texts or detached single leaves. Cataloging as many of these fragmentary works as possible increases the chance that some manuscripts could be reconstituted, if only virtually. Thus as a philosophical principle, DS includes large and small collections, complete bound books and single leaves.

Between 1999 and 2002, additional holdings from Huntington Library, the University of Texas, Austin, and the New York Public Library were incorporated, along with those of a number of smaller collections. The database has continued to grow and represents the collections of over thirty member institutions, including not only those with substantial repositories, such as Harvard University's Houghton Library, Yale University's Beinecke Library, and the University of Pennsylvania, but also libraries with few but rare works such as the Providence Public Library, which owns an unusual 15th century Bible (Wetmore Ms 1) in rebus format. As of September 2015, DS counts catalog records for 8,390 manuscripts and 47,624 digitized images.

The Digital Scriptorium database enables public viewing of non-circulating materials normally available only to specialists with restricted access. As a visual catalog, DS allows scholars to verify cataloguing information about places and dates of origin, scripts, artists, and quality. Special emphasis is placed on touchstone materials such as manuscripts signed and dated by scribes, thus beginning the American contribution to the goal established in 1953 by the "Comité international de paléographie latine" (International Committee of Latin Paleography): to document photographically the proportionately small number of codices of certain origin that will serve stylistically to localize and date the vast quantities of unsigned manuscripts. DS publishes not only manuscripts of firm attribution but also ones that need the attention of further scholarship that traditionally would have been unlikely candidates for reproduction. Because it is web-based, it also allows for updates and corrections, and as a matter of form individual records in DS acknowledge contributions from outside scholars. Because the DS consortium consists of academic, public, and rare book libraries, it encourages a broad audience that benefits from a reciprocally beneficial body of knowledge. While attending to the needs of community of specialists: medievalists, classicists, musicologists, paleographers, diplomatists, literary scholars and art historians, DS also recognizes a public user community that values rare and unique works of historical, literary and artistic significance.

Working together the DS consortium has expanded the resources of libraries to manage collections of rare materials by providing a digital cataloging standard for pre-modern manuscripts. A DS record includes extensive and granular descriptive metadata supported by high resolution digital images. Medieval and Renaissance manuscripts are best served by format-specific cataloging that is sensitive to their complexity as hand-made books and historical artifacts. They differ from modern manuscripts, such as letters or personal records, because although they are unique objects, they are usually not unique texts. They may be written, bound, and decorated by hand but most pre-modern manuscripts are books. The text of a fourteenth-century missal, for example, can be identified using an early twentieth-century printed version of the same text. Or several collections may own more than one manuscript of St. Augustine’s De Civitate Dei (The City of God). A DS search by title in fact retrieves fourteen copies of this work, all unique manuscripts dating from the 12th to the 15th centuries, owned by eight different libraries. The potential for relationships among manuscripts means that item-level, bibliographic cataloging rather than collection-based, archival cataloging best identifies and describes the content of medieval and Renaissance manuscripts. But these works also have historical and artifactual significance, so catalog records for medieval and Renaissance manuscripts need to describe not only their contents but also the complete provenance (chain of ownership), binding (which may be from a later period than the rest of the book), marks of ownership (such as coats-of-arms), physical condition, material support (usually parchment), foliation and construction of the book, layout of the text, attribution of script, date (often estimated), style and localization of decoration and painted illustration, and names, not just of authors, but craftsmen, scribes, artists and owners.

Some scholars of medieval and Renaissance manuscripts are less interested in the text of the manuscript than in some other aspect of its production. The field of art history offers the most striking example; a scholar studying the art of the illuminator Mariano del Buono will care equally for the Harvard manuscript (a copy of Plato) and for the manuscript held by New York Public Library (a copy of Livy). DS records factor for such interests and allow for searching on physical features only, whether by artist name (such as in the above example) or by place and date of origin ("Italy," and "1450-1499" would produce the same result).

Before digitization, most records of medieval and Renaissance manuscripts were excluded from library public access catalogs but instead were published in printed books written by scholar-specialists for a similarly trained audience. Compared to these DS records are similar to library cataloging records in that they are simplified, standardized and concise. Specialists may notice that a DS record omits some information (such as collation) available in scholarly catalogs, in order to avoid descriptions that would take too much time and discourage library catalogers from attempting the task. In agreement with Green and Meisner, the goal has been "more product, less process," or as the Spanish proverb goes, "lo mejor es enemigo de lo bueno." A DS record is thus not intended to serve as the ideal and thorough description of a manuscript, but rather a practical surrogate that still provides a better representation of it than a typical content-based OPAC (online public access catalog) record.

Compared to MARC (MARC standards are the bibliographic standards used by most American libraries), DS is better designed to describe not only the content but also the historical context and physical characteristics, while using sample images to support these descriptions. The sample images contain inherently descriptive information, and most DS records include digital images, although there are some exceptions. Although DS records can be adapted to MARC formats, DS better captures the complexity of medieval and Renaissance manuscripts than MARC. For example, DS nests the sections of descriptive metadata in order to catalog a manuscript with multiple texts of multiple origins all in one binding, while MARC provides a flat file that can only deal with a coterminous text + physical unit. DS also indexes specific information pertinent to this format, with a separate field for "Artist," for example, rather than one for "personal names." DS also creates more focused records compared to archival cataloging based on EAD (Encoded Archival Description), because the latter is collections-based and not designed to be descriptive and searched at the item level. Thus the DS cataloging method has become the unofficial library standard for the online cataloging of medieval and Renaissance manuscripts in the United States.

The University of California, Berkeley provided the first home to the DS database, both in terms of managing the project and devising its initial technology. For an interim period of time (2003–2011) DS was hosted at Columbia University but is now returned to its original home at Berkeley. The technical innovations produced by the teams of both originating universities created a digital product based on a progressive, standards-based digitization policy. Originally using Microsoft Access to serve as a cross-institutional data collection tool, the DS database used SGML and later XML to aggregate and query the combined information. The present platform managed by U.C. Berkeley depends upon software known as WebGenDB. WebGenDB is a non-proprietary, web-based interface for the underlying control database GenDB.

GenDB houses the descriptive, structural and administrative metadata for the materials being digitized for web presentation, and outputs the metadata using the Metadata Encoding and Transmission Standard (METS) format. METS provides an XML schema-based specification for encoding "hub" documents for materials whose content is digital. A "hub" document draws together potentially dispersed but related files and data. METS uses XML to provide a flexible vocabulary and syntax for identifying the digital components that together comprise a digital object, for specifying the location of these components, and for expressing their structural relationships. The digital components comprising a digital object could include the content files, the descriptive metadata, and the administrative metadata. METS can be used for the transfer, dissemination and/or archiving of digital objects, all in compliance with the OAIS (Open Archival Information System) reference model developed at OCLC. The DS reliance on OAIS (ISO 14721: 2003) promises secure digital preservation policy, supported by the California Digital Library digital curation services and the CDL "Merritt" digital archive.




</doc>
<doc id="50761156" url="https://en.wikipedia.org/wiki?curid=50761156" title="Kdb+">
Kdb+

kdb+ is a column-based relational time series database (TSDB) with in-memory (IMDB) abilities, developed and marketed by Kx Systems. The database is commonly used in high-frequency trading (HFT) to store, analyze, process, and retrieve large data sets at high speed. kdb+ has the ability to handle billions of records and analyzes data within a database. The database is available in 32-bit and 64-bit versions for several operating systems. Financial institutions use kdb+ to analyze time series data such as stock or commodity exchange data. The database has also been used for other time-sensitive data applications including commodity markets such as energy trading, telecommunications, sensor data, log data, and machine and computer network usage monitoring.

kdb+ is a high-performance column-store database that was designed to process and store large amounts of data. Commonly accessed data is pushed into random-access memory (RAM), which is faster to access than data in disk storage. Created with financial institutions in mind, the database was developed as a central repository to store time series data that supports real-time analysis of billions of records. kdb+ has the ability to analyze data over time and responds to queries similar to Structured Query Language (SQL).

Columnar databases return answers to some queries in a more efficient way than row-based database management systems. kdb+ dictionaries, tables and nanosecond time stamps are native data types and are used to store time series data. 

At the core of kdb+ is the built-in programming language, q, a concise, expressive query array language, and dialect of the language APL. Q can manipulate streaming, real-time, and historical data. kdb+ uses q to aggregate and analyze data, perform statistical functions, and join data sets and supports SQL queries The vector language q was built for speed and expressiveness and eliminates most need for looping structures. kdb+ includes interfaces in C, C++, Java, C#, and Python.

In 1998, Kx Systems released kdb, a database built on the language K written by Arthur Whitney. In 2003, kdb+ was released as a 64-bit version of kdb. In 2004, the kdb+ tick market database framework was released along with kdb+ taq, a loader for the New York Stock Exchange (NYSE) taq data. kdb+ was created by Arthur Whitney, building on his prior work with array languages.

In April 2007, Kx Systems announced that it was releasing a version of kdb+ for Mac OS X. At that time, kdb+ was also available on the operating systems Linux, Windows, and Solaris.

In September 2012, version 3.0 was released. It was optimized for Intel's upgraded processors with support for WebSockets, Globally unique identifiers (GUID)s, and Universally unique identifiers (UUID). Intel's Advanced Vector Extensions (AVX) and Streaming SIMD Extensions 4 (SSE4) 4.2 on the Sandy Bridge processors of the time allowed for enhanced support of the kdb+ system. In June 2013, version 3.1 was released, with benchmarks up to 8 times faster than for older versions.



</doc>
<doc id="5380838" url="https://en.wikipedia.org/wiki?curid=5380838" title="Data redundancy">
Data redundancy

In computer main memory, auxiliary storage and computer buses, data redundancy is the existence of data that is additional to the actual data and permits correction of errors in stored or transmitted data. The additional data can simply be a complete copy of the actual data, or only select pieces of data that allow detection of errors and reconstruction of lost or damaged data up to a certain level.

For example, by including additional data checksums, ECC memory is capable of detecting and correcting single-bit errors within each memory word, while RAID 1 combines two hard disk drives (HDDs) into a logical storage unit that allows stored data to survive a complete failure of one drive. Data redundancy can also be used as a measure against silent data corruption; for example, file systems such as Btrfs and ZFS use data and metadata checksumming in combination with copies of stored data to detect silent data corruption and repair its effects.

While different in nature, data redundancy also occurs in database systems that have values repeated unnecessarily in one or more records or fields, within a table, or where the field is replicated/repeated in two or more tables. Often this is found in Unnormalized database designs and results in the complication of database management, introducing the risk of corrupting the data, and increasing the required amount of storage. When done on purpose from a previously normalized database schema, it "may" be considered a form of database denormalization; used to improve performance of database queries (shorten the database response time).

For instance, when customer data are duplicated and attached with each product bought, then redundancy of data is a known source of inconsistency since a given customer might appear with different values for one or more of their attributes. Data redundancy leads to data anomalies and corruption and generally should be avoided by design; applying database normalization prevents redundancy and makes the best possible usage of storage.



</doc>
<doc id="1246562" url="https://en.wikipedia.org/wiki?curid=1246562" title="IPUMS">
IPUMS

Integrated Public Use Microdata Series (IPUMS) is the world's largest individual-level population database. IPUMS consists of microdata samples from United States "(IPUMS-USA)" and international "(IPUMS-International)" census records, as well as data from U.S. and international surveys. The records are converted into a consistent format and made available to researchers through a web-based data dissemination system.

IPUMS is housed at the Institute for Social Research and Data Innovation, an interdisciplinary research center at the University of Minnesota, under the direction of Professor Steven Ruggles.

IPUMS includes all persons enumerated in the United States Censuses from 1790 to 2010 (the 1890 census is missing because it was destroyed in a fire) and from the American Community Survey since 2000 and the Current Population Survey since 1962. The IPUMS provides consistent variable names, coding schemes, and documentation across all the samples, facilitating the analysis of long-term change.

IPUMS-International includes countries from Africa, Asia, Europe, and Latin America for 1960 forward. The database currently includes more than a billion individuals enumerated in 365 censuses from 94 countries around the world. IPUMS-International converts census microdata for multiple countries into a consistent format, allowing for comparisons across countries and time periods. Special efforts are made to simplify use of the data while losing no meaningful information. Comprehensive documentation is provided in a coherent form to facilitate comparative analyses of social and economic change.

Additional databases in the IPUMS family include the:


The "Journal of American History" described the effort as "One of the great archival projects of the past two decades." Liens Socio, the French portal for the social sciences, gave IPUMS the only “best site” designation that has gone to any non-French website, writing “IPUMS est un projet absolument extraordinaire...époustouflante [mind-blowing]!” 

The official motto of IPUMS is "use it for good, never for evil." All IPUMS data and documentation are available online free of charge.



</doc>
<doc id="23231259" url="https://en.wikipedia.org/wiki?curid=23231259" title="North Atlantic Population Project">
North Atlantic Population Project

The North Atlantic Population Project (NAPP) is a collaboration of historical demographers in Britain, Canada, Denmark, Germany, Iceland, Norway, and Sweden to produce a massive census microdata collection for the North Atlantic Region in the late-nineteenth century. The database includes complete individual-level census enumerations for each country, and provides information on over 110 million people. This large scale allows detailed analysis of small geographic areas and population subgroups.

The NAPP database is designed to be compatible with the Integrated Public Use Microdata Series (IPUMS), and is disseminated through the IPUMS data-access system at the Minnesota Population Center, University of Minnesota. Major collaborators on the project include Lisa Dillon, University of Montreal; Chad Gaffield, University of Ottawa; Ólöf Garðarsdóttir, Statistics Iceland; Marianne Jarnes Erikstad, University of Tromsø; Jan Oldervall University of Bergen; Evan Roberts, University of Minnesota; Steven Ruggles, University of Minnesota; Kevin Schürer, UK Data Archive; Gunnar Thorvaldsen, University of Tromsø; and Matthew Woollard, UK Data Archive. The project is also coordinated by the Minnesota Population Center at the University of Minnesota. 




</doc>
<doc id="6192642" url="https://en.wikipedia.org/wiki?curid=6192642" title="PISCES">
PISCES

PISCES ("Personal Identification Secure Comparison and Evaluation System") is a border control database system largely based on biometrics developed by Booz Allen Hamilton Inc..

The PISCES-project was initiated by the Department of State, Terrorist Interdiction Program (TIP) in 1997, initially as a system for countries in improving their watchlisting capabilities by providing a mainframe computer system to facilitate immigration processing in half a dozen countries. Foreign authorities used the technology to watchlist and exchange information with the United States Department of State about suspected terrorists appearing at their borders. The information is used to track and apprehend individual terrorists, not for wide-ranging analysis of terrorist travel methods"", according to US-government reports. It matches passengers inbound for the United States against facial images, fingerprints and biographical information at airports in high-risk countries. A high-speed data network permits U.S. authorities to be informed of problems with inbound passengers.

PISCES workstations installed throughout a country are linked by wide area network to the participating nation’s immigration, police or intelligence headquarters. The headquarters is provided with the automated capability to monitor activities at immigration points, evaluate traveler information and conduct real time data analysis.

Currently the PISCES-project falls under "The Terrorist Interdiction Program" (TIP), an ongoing programme of the United States Department of State. TIP provides all necessary software and hardware (mostly commercial and off-the-shelf, such as cameras and passport scanners), full installation, operator training, and system sustainment. Additionally, TIP assists with immigration business process improvement at ports of entry chosen for PISCES installation.

For FY 2007, funds will be used to support enhancements to the existing watch listing system software in order to provide a fraudulent document detection capability, a biometrics search capability, and improved name-searching effectiveness.

Starting in FY 2010 and onward to FY 2011, PISCES funding will be increased in what the United States Department of State considers "high risk" countries such as Afghanistan, Iraq, and Pakistan. A project to verify US visas via limited access to a US government database will go under trial in select outposts.

Although PISCES was operational in the months prior to September 11, 2001 it apparently failed to detect any of the terrorists involved in the attack.

According to the US Department of State,



In 2003: "is currently being deployed in five countries and is scheduled for deployment in 12 more countries this calendar year. Arrests and detentions have occurred in all five countries where the system has been deployed."

In 2005: "Since 2001, twenty nations have been provided this capability"

Expected 2011: 31 

On 27 March 2018 the country and the US government signed a Memorandum of Intention on the donation of a System for Personal Identification Secure Comparison and Evaluation (PISCES). In cooperation with Bosnia's Council of Ministers, the US government aims to install and maintain all the hardware and software required for the PISCES system to operate - starting with the International airport Sarajevo and later expanding to other airports and border crossings in the country.






</doc>
<doc id="3867126" url="https://en.wikipedia.org/wiki?curid=3867126" title="International Road Traffic and Accident Database">
International Road Traffic and Accident Database

The International Road Traffic and Accident Database(IRTAD) is a data collection maintained by the Organisation for Economic Co-operation and Development (OECD) and the International Transport Forum (ITF) in Paris, covering safety data in countries within and outside of Europe.

The database was started in 1988 in Federal Institution for Roads (BASt) in Bergisch Gladbach, Germany, in response to demands for international comparative data. It has grown to be an important resource in comparing road safety metrics between various developed countries.

At present 37 countries are included: Argentina, Australia, Austria, Belgium, Cambodia, Canada, Chile, Colombia, Czech Republic, Denmark, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Israel, Italy, Japan, Korea (South Korea), Lithuania, Luxembourg, Malaysia, the Netherlands, New Zealand, Nigeria, Norway, Poland, Portugal, Serbia, Slovenia, Spain, Sweden, Switzerland, United Kingdom and United States.

The following statistics are available for general use:



</doc>
<doc id="40248244" url="https://en.wikipedia.org/wiki?curid=40248244" title="World Database of Happiness">
World Database of Happiness

The World Database of Happiness is a web-based archive of research findings on subjective appreciation of life. The database contains both an overview of scientific publications on happiness and a digest of research findings. The database contains information on how happy people are in a wide range of circumstances and in 165 different nations. Happiness is defined as the degree to which an individual judges the quality of his or her life as a whole favorably. Two 'components' of happiness are distinguished: hedonic level of affect (the degree to which pleasant affect dominates) and contentment (perceived realization of wants).

The World Database of Happiness is a tool to quickly acquire an overview on the ever-growing stream of research findings on happiness Medio 2019 the database covered some 30,000 scientific findings, of which about 12,500 are distributional findings (on how happy people are) and another 17,500 correlational findings (on factors associated with more and less happiness). The first findings date from 1915.
The World Database of Happiness is a ‘findings archive’, which consists of electronic ‘finding pages’ on which separate research results are described in a standard format and terminology. These finding pages can be selected on various characteristics, such as population studies, the measure of happiness used and observed co-variates. All finding-pages have a specific internet address to which links can be made in scientific review papers or policy recommendations. This allows a concise presentation of many findings in a table, while providing readers with access to detail.
The Database has been used in hundreds scientific studies, for example to access under what conditions economic growth enhances average happiness

The World Database of Happiness is often used by popular media to make lists of the happiest countries around the globe. An example is the Happy Planet Index, which aims to chart sustainable happiness all over the world by combining data on longevity, happiness and the size of the ecological footprint of citizens.
The database has a clear conceptual focus, it includes only research findings on subjective enjoyment of one’s life as a whole. Thereby it evades the Babel that has haunted the study of happiness for ages. The other side of that coin is that much interesting research is left out. The findings are reported with technical details about measurement and statistical analysis. This detail is welcomed by scholars, but makes the information difficult to digest for lay-persons. Still another limitation is that the available findings are often contradictory, which makes it hard to draw firm conclusions about the causes of happiness. What is clear is that poor health, separation, unemployment and lack of social contact are all strongly negatively associated with happiness. Another problem for the World database of happiness is that the number of studies on happiness increases with such a high rate that it gets increasingly difficult to offer a complete overview of all research findings.

A further concern is that the Database of Happiness is exclusively focused on hedonic happiness (feeling good) and not on mature or noetic happiness, that is characterized by a sense of acceptance, inner serenity and being at peace with self, others, and the world. Paul Wong of the INPM put it like this: "The World Database of Happiness does not even include research findings of happiness in suffering. The reality is that all those suffering from a variety of misfortunes—such as poverty, sickness, accidents, or discrimination and oppression—also want to know how they can experience happiness; therefore, psychologists and policymakers need to be concerned about their needs."



</doc>
<doc id="1033971" url="https://en.wikipedia.org/wiki?curid=1033971" title="Profit impact of marketing strategy">
Profit impact of marketing strategy

The Profit Impact of Market Strategy (PIMS®) is a project that uses empirical data to determine which business strategies make the difference between success and failure. It is used to develop strategies for resource allocation and marketing. Some of the most important strategic metrics are market share, product quality, investment intensity and service quality (all measured by PIMS® and strongly correlated with financial performance). One of the most surprising findings is that the same factors work identically across different industries. PIMS® "yields solid evidence in support of both common sense and counter-intuitive principles for gaining and sustaining competitive advantage" Tom Peters and Nancy Austin. 

The PIMS project was originally initiated by senior managers of General Electric who wanted to know why some of their business units were more profitable than others. Under the direction of Sidney Schoeffler, an Economics Professor hired by GE for the purpose, the PIMS project was launched in the 1960s as an internal empirical study. The aim was to make GE's different strategic business units (SBUs) comparable.

Since GE was highly diversified at the time, key factors were sought that would have an impact on economic success regardless of the product. In particular, the return on investment (ROI), i.e. the profit per unit of tied capital, was used as the measure of success. In 1972, the project was transferred to the Marketing Sciences Institute (then under the wing of Harvard Business School, which extended it to other companies. In 1976, the American Strategic Planning Institute in Cambridge, Massachusetts, took charge of the project. 

Between 1970 and 1983, roughly 2600 strategic business units (SBUs) from around 200 companies took part in the surveys and provided key figures for the project. Today there are around 12,570 observations for 4200 SBUs. PIMS Associates in London has been the worldwide competence and design centre for PIMS® since the 1990s and has been part of Malik Management (Fredmund Malik) in St. Gallen (Switzerland) since 2005.

The PIMS project analysed the data they had gathered to identify the options, problems, resources and opportunities faced by each SBU. Based on the spread of each business across different industries, it was hoped that the data could be drawn upon to provide other business, in the same industry, with empirical evidence of which strategies lead to increased profitability. The database continues to be updated and drawn upon by academics and companies today.

The PIMS® databases currently comprise over 25,000 years of business experience at the SBU level (i.e. where the customer interface takes place and where marketing and investment decisions are made). Each SBU is characterized by hundreds of factors over a period of 3+ years, including market share of itself and its competitors, customer preference, relative prices, service quality, innovation rate, vertical integration, etc., as well as a number of market attractiveness factors and fairly detailed income statement, balance sheet and employee data.

In the PIMS study, more than 50 different core metrics were regularly surveyed. The most important of these are presented below: 

Characteristics of the business environment (market attractiveness):

Competitive strength:

Supply chain fitness:

Dynamics of change

Economic success factors (as variables to be explained):

While most of the variables appear obvious, PIMS® has the advantage of providing empirical data that defines quantitative relationships and attributes them to what some would consider reasonable.

Companies wishing to use the service will provide detailed information, for each of their main strategic business units including:
Based on the data provided, PIMS® provides four reports (Lancaster, Massingham and Ashford):


The following factors correlate particularly strongly with the ROI and ROS success factors:

Investment intensity correlates negatively (explains approx. 15 %):

On the one hand, this has the formal-analytical reason that with increasing investment intensity, i.e. the investment volume in relation to sales, the depreciation volume in relation to sales, the depreciation intensity, also increases and thus the profit decreases.
On the other hand, if the investment intensity is high, the fixed assets increase and there is an urge to also use these capacities, i.e. to increase the output volume and under certain circumstances to lower the prices and thus the profit margin.

Relative market share correlates positively (explains approx. 12 %):

The main reason for the positive influence of the relative market share is the economies of scale: The higher the market share, the larger the production volume and the lower the unit costs; this can also be explained by the experience curve.
Moreover, as the market share increases, so does the power vis-à-vis the suppliers, which means that better conditions can be achieved.

Relative product quality correlates positively (explains approx. 10 %):

Important reasons for the positive correlation are above all higher achievable prices for premium products, but also the higher willingness of consumers to buy high-quality services, so that the sales volume increases and thus positively influences the market share (see above).
Another reason is the lower complaint costs.
Overall, the factors surveyed explain about 70 percent of the differences in profitability between successful and unsuccessful business areas in the PIMS database (measured as variance).

Clearly, it could be argued that a database operating on information gathered in the period 1970 - 1983 is outdated. However data continues to be collected from participating companies and PIMS argues that it provides a unique source of time-series data, the conclusions from which have proven to be very stable over time.

It has also been suggested that PIMS is too heavily biased towards traditional, metal-bashing industries, such as car manufacturing; perhaps not surprising, considering the era in which the majority of the surveys were carried out. In reality, as of 2006, the 3,800+ businesses contained within the database includes data from the consumer, industrial and service sectors.

It is also heavily weighted towards large companies, at the expense of small entrepreneurial firms. This resulted from the data collection method used. Generally, only larger firms are prepared to pay the consulting fee, provide the survey data, and in return have access to the database in which they can compare their business with other large businesses or SBUs. Mintzberg (1998) claims that because the database is dominated by large established firms, it is more suitable as a technique for assessing the state of "being there rather than getting there". (page 99) This criticism is very important because if one is trying to get "average" results across industries to give us the "laws of the marketplace", a dubious enterprise as it is, the sampling strategy is important if one wants to obtain results that are representative.

→ "The PIMS master database at the heart of the PIMS program now includes more than 25,000 years of business experience across a broad spectrum of industries worldwide. These are more than 90% of the companies to be processed. About one-third of them manufacture consumer goods, 15% manufacture capital goods. The remaining business units are suppliers of raw materials and semi-finished products, components or accessories for industry and commerce. Trade and services companies account for less than 10% of total companies and yet represent a fairly large sample (over 250) of strategic business units in this category."
"About half of the business units in the PIMS database market their products or services nationally in the United States or Canada, while 11% serve regional markets in North America. European companies are also numerous today, with around 1,000 business units from continental European countries and 600 from the UK." 

The most important criticism leveled at PIMS is the fact that causation implies correlation but correlation does not imply causation. One of the most important "findings" of the PIMS program was to find a statistically significant relationship between profitability and market share (see Buzzell and Gale (1987)). The empirical work conducted by PIMS suggested that high market share yielded high profitability, but this correlation cannot be considered a "true" causal relationship because of the fact that correlation does not imply causation. In the multivariate correlation analysis, high market share was associated with high profits, but high profits could have been associated with high market share, or a third factor common to both could have caused the correlation. Many analysts believe that it is possible to use a statistical causality test to determine causation, but if the whole problem is that correlation is insufficient to determine causation in the first place, then how can using another correlation, which is what is used in the tests, determine causation.

→ "In connection with the market share, already indicated and frequent allegations that correlations are used in the PIMS investigations to draw conclusions about causal relationships, i.e. correlation is equated with causality. However, this problem is too obvious not to have been examined in detail during the development of the PIMS program. Backhaus et al. formulate this aptly: "The primary field of application of regression analysis is the investigation of causal relationships (cause-effect relationships), which we can also refer to as "The more the" relationships". Backhaus et al (2006), p. 46 (Emphasis in the original.) These authors then add the following: "It should be emphasized here that neither regression analysis nor other statistical methods can prove causalities beyond doubt. Rather, regression analysis can only prove correlations between variables. This is a necessary but not yet sufficient condition for causality." Backhaus et al. (2006), p.48 f. Within the framework of the PIMS studies, it was thus possible to determine causalities with the help of time series analyses due to the availability of data over longer periods. See, for example, Barilits (1994), p. 61. Correlations in this sense, including in the PIMS program, initially give nothing other than a reason to investigate possible causalities substantiated and intensively."" 

Another important criticism of PIMS is that it does not take into account heterogeneity in the data set. The presumption of PIMS analysis is that the same "laws of the marketplace" apply to all industries. However, the statistical assumptions employed in the econometric analysis make the assumption that all cross-sectional observations come from one statistical distribution that is the same for all cross-sectional observations. This tends to be the Achilles heel of virtually all cross-sectional analyses. If this homogeneous assumption is false, then cross-sectional observations are being drawn from different populations. While one can use estimation techniques such as fixed-effects to control for different population means, co-variances can also differ across populations (meaning behavior differs across populations) and the only way one can control for this aspect is to run regressions on each population separately. This means that the "laws of the marketplace" differ across populations, directly contradicting one of the main presumptions of using the PIMS data base for analysis.

Tellis and Golder (1996) claim that PIMS defines markets too narrowly. Respondents described their market very narrowly to give the appearance of high market share. They believe that this self reporting bias makes the conclusions suspect. They are also concerned that no defunct companies were included, leading to "survivor bias".




</doc>
<doc id="42882725" url="https://en.wikipedia.org/wiki?curid=42882725" title="GEPIR">
GEPIR

GEPIR is publicly available at no cost, but limits the number of requests per/day from an IP to 30. GS1 offers a similar paid service, the GS1 Data Hub, which provides additional capabilities without the traffic limitations imposed by GEPIR.

You can search by

All 111 GS1 Member Organizations are on GEPIR since 2013 
The first digits of the GTIN code are called GS1 prefix and are used to route the query to the corresponding GS1 Member Organization.

Results can be returned in HTML or XML for some countries.

The SOAP webservice provides a return code indicating if an error occurred:

GEPIR is accessible for free in almost all countries but the number of request per day is limited (from 20 to 30).
Since October 2013, GS1 France restricts access to GEPIR to companies (registration with [SIREN] number is required to use it).

A premium access have been created by GS1 France in January 2010 and allows companies to use GS1 web and SOAP interface without any limit. The price of this access is not public.



GEPIR is a lookup service coordinated by the GS1 GO that provides all end users with the ability to look up information about GS1 Identification Keys.

Depending on the service, systems are provided by GS1 Member Organisations (MOs) or 3rd party service providers, or both. Where a GS1 MO does not choose to provide the service directly to its end users, the GS1 Global Office may provide the service for that geography. Some services involve a technical component deployed by the GS1 Global Office that coordinates the systems provided by GS1 MOs and/or 3rd party service providers. 
The GEPIR service is provided by systems deployed by GS1 MOs, with the GS1 GO providing a central point of coordination to federate the local systems. The GS1 GO also provides the MO-level service for MOs that cannot or do not wish to deploy their own system.

The nature of synchronization with GEPIR requires users to know some quality issues or exceptions:


A Conformance Program has been Introduced by GS1 in 2013.



</doc>
<doc id="1500119" url="https://en.wikipedia.org/wiki?curid=1500119" title="Database publishing">
Database publishing

Database publishing is an area of automated media production in which specialized techniques are used to generate paginated documents from source data residing in traditional databases. Common examples are mail order catalogues, direct marketing, report generation, price lists and telephone directories. The database content can be in the form of text and pictures but can also contain metadata related to formatting and special rules that may apply to the document generation process. Database publishing can be incorporated into larger workflows as a component, where documents are created, approved, revised and released.

The basic idea is using database contents like article and price information to fill out pre-formatted template documents. Templates are typically created in a normal desktop layout application where certain boxes or text are designated as placeholders. These placeholders are then targeted with new content which flows in from the database. This allows for quick generation of final output and, in case of changes to the database, quickly perform updates, with limited or no manual intervention. 

Another model of database publishing is found in many web-to-print sites where users browse templates from an online catalog (such as business cards or brochures), personalize the selected template by filling in a form and then view the rendered result. In this case the initial source of data is from user input, but it is captured in a database so that if the same user revisits the site later, they can resume editing where they left off. The form is then pre-filled from the database-stored variables the user entered before.

The main layout applications for this workflow are: Datalogics Pager, Adobe FrameMaker / InDesign, QuarkXPress, Xyvision, Arbortext Advanced Print Publisher (formerly 3B2) and . Generally, these layout applications have a corresponding server version, which receives commands via web interfaces rather than desktop interaction. QuarkXPress Server and Adobe InDesign Server both take full advantage of the design features available in their respective desktop versions.

These applications make their broad spectrum of features available for extension and integration with vertical products, that can be developed either internally, through some form of scripting (e.g. JavaScript or AppleScript for InDesign), or externally, through some API and corresponding developer kits.

Other variants of database publishing are the rendering of content for direct PDF output. This approach prevents manual intervention on the final output, since PDF is not (comfortably) editable. This may not be perceived as a limitation in situations like report generation where manual editability is not needed or not desired.



</doc>
<doc id="47825086" url="https://en.wikipedia.org/wiki?curid=47825086" title="Dark data">
Dark data

Dark data is data which is acquired through various computer network operations but not used in any manner to derive insights or for decision making. The ability of an organisation to collect data can exceed the throughput at which it can analyse the data. In some cases the organisation may not even be aware that the data is being collected. IBM estimate that roughly 90 percent of data generated by sensors and analog-to-digital conversions never get used.

In an industrial context, dark data can include information gathered by sensors and telematics.

Organizations retain dark data for a multitude of reasons, and it is estimated that most companies are only analyzing 1% of their data. Often it is stored for regulatory compliance and record keeping. Some organizations believe that dark data could be useful to them in the future, once they have acquired better analytic and business intelligence technology to process the information. Because storage is inexpensive, storing data is easy. However, storing and securing the data usually entails greater expenses (or even risk) than the potential return profit.

A lot of dark data is unstructured, which means that the information is in formats that may be difficult to categorise, be read by the computer and thus analysed. Often the reason that business does not analyse their dark data is because of the amount of resources it would take and the difficulty of having that data analysed. According to Computer Weekly, 60% of organisations believe that their own business intelligence reporting capability is "inadequate" and 65% say that they have "somewhat disorganised content management approaches".

Useful data may become dark data after it becomes irrelevant, as it is not processed fast enough. This is called "perishable insights" in "live flowing data". For example, if the geolocation of a customer is known to a business, the business can make offer based on the location, however if this data is not processed immediately, it may be irrelevant in the future. According to IBM, about 60 percent of data loses its value immediately. Not analysing data immediately and letting it go 'dark' can lead to significant losses for an organisation in terms of not identifying fraud, for example, fast enough and then only addressing the issue when it is too late.

According to the New York Times, 90% of energy used by data centres is wasted. If data was not stored, energy costs could be saved. Furthermore, there are costs associated with the underutilisation of information and thus missed opportunities. According to Datamation, "the storage environments of EMEA organizations consist of 54 percent dark data, 32 percent Redundant, Obsolete and Trivial data and 14 percent business-critical data. By 2020, this can add up to $891 billion in storage and management costs that can otherwise be avoided."

The continuous storage of dark data can put an organisation at risk, especially if this data is sensitive. In the case of a breach, this can result in serious repercussions. These can be financial, legal and can seriously hurt an organisation's reputation. For example, a breach of private records of customers could result in the stealing of sensitive information, which could result in identity theft. Another example could be the breach of the company's own sensitive information, for example relating to research and development. These risks can be mitigated by assessing and auditing whether this data is useful to the organisation, employing strong encryption and security and finally, if it is determined to be discarded, then it should be discarded in a way that it becomes unretrievable.

It is generally considered that as more advanced computing systems for analysis of data are built, the higher the value of dark data will be. It has been noted that "data and analytics will be the foundation of the modern industrial revolution". Of course, this includes data that is currently considered "dark data" since there are not enough resources to process it. All this data that is being collected can be used in the future to bring maximum productivity and an ability for organisations to meet consumers' demand. Technology advancements are helping to leverage this dark data affordably, thanks to young and innovative companies such as Datumize, Veritas or Lucidworks. Furthermore, many organisations do not realise the value of dark data right now, for example in healthcare and education organisations deal with large amounts of data that could create a significant "potential to service students and patients in the manner in which the consumer and financial services pursue their target population".


</doc>
<doc id="50896194" url="https://en.wikipedia.org/wiki?curid=50896194" title="ImageNet">
ImageNet

The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories with a typical category, such as "balloon" or "strawberry", consisting of several hundred images. The database of annotations of third-party image URLs is freely available directly from ImageNet, though the actual images are not owned by ImageNet. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a "trimmed" list of one thousand non-overlapping classes.

On 30 September 2012, a convolutional neural network (CNN) called AlexNet achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than 10.8 percentage points lower than that of the runner up. This was made feasible due to the use of Graphics processing units (GPUs) during training, an essential ingredient of the deep learning revolution. According to "The Economist", "Suddenly people started to pay attention, not just within the AI community but across the technology industry as a whole."

In 2015, AlexNet was outperformed by Microsoft's very deep CNN with over 100 layers, which won the ImageNet 2015 contest.

AI researcher Fei-Fei Li began working on the idea for ImageNet in 2006. At a time when most AI research focused on models and algorithms, Li wanted to expand and improve the data available to train AI algorithms. In 2007, Li met with Princeton professor Christiane Fellbaum, one of the creators of WordNet to discuss the project. As a result of this meeting, Li went on to build ImageNet starting from the word-database of WordNet and using many of its features.

As an assistant professor at Princeton, Li assembled a team of researchers to work on the ImageNet project. They used Amazon Mechanical Turk to help with the classification of images.

They presented their database for the first time as a poster at the 2009 Conference on Computer Vision and Pattern Recognition (CVPR) in Florida.

ImageNet crowdsources its annotation process. Image-level annotations indicate the presence or absence of an object class in an image, such as "there are tigers in this image" or "there are no tigers in this image". Object-level annotations provide a bounding box around the (visible part of the) indicated object. ImageNet uses a variant of the broad WordNet schema to categorize objects, augmented with 120 categories of dog breeds to showcase fine-grained classification. One downside of WordNet use is the categories may be more "elevated" than would be optimal for ImageNet: "Most people are more interested in Lady Gaga or the iPod Mini than in this rare kind of diplodocus." In 2012 ImageNet was the world's largest academic user of Mechanical Turk. The average worker identified 50 images per minute.

The ILSVRC aims to "follow in the footsteps" of the smaller-scale PASCAL VOC challenge, established in 2005, which contained only about 20,000 images and twenty object classes. To "democratize" ImageNet, Fei-Fei Li proposed to the PASCAL VOC team a collaboration, beginning in 2010, where research teams would 
evaluate their algorithms on the given data set, and compete to achieve higher accuracy on several visual recognition tasks.

The resulting annual competition is now known as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The ILSVRC uses a "trimmed" list of only 1000 image categories or "classes", including 90 of the 120 dog breeds classified by the full ImageNet schema. The 2010s saw dramatic progress in image processing. Around 2011, a good ILSVRC classification error rate was 25%. In 2012, a deep convolutional neural net called AlexNet achieved 16%; in the next couple of years, error rates fell to a few percent. While the 2012 breakthrough "combined pieces that were all there before", the dramatic quantitative improvement marked the start of an industry-wide artificial intelligence boom. By 2015, researchers at Microsoft reported that their CNNs exceeded human ability at the narrow ILSVRC tasks. However, as one of the challenge's organizers, Olga Russakovsky, pointed out in 2015, the programs only have to identify images as belonging to one of a thousand categories; humans can recognize a larger number of categories, and also (unlike the programs) can judge the context of an image.

By 2014, more than fifty institutions participated in the ILSVRC. In 2015, Baidu scientists were banned for a year for using different accounts to greatly exceed the specified limit of two submissions per week. Baidu later stated that it fired the team leader involved and that it would establish a scientific advisory panel.

In 2017, 29 of 38 competing teams had greater than 95% accuracy. In 2017 ImageNet stated it would roll out a new, much more difficult, challenge in 2018 that involves classifying 3D objects using natural language. Because creating 3D data is more costly than annotating a pre-existing 2D image, the dataset is expected to be smaller. The applications of progress in this area would range from robotic navigation to augmented reality.

A study of the history of the multiple layers (taxonomy, object classes and labeling) of ImageNet and WordNet in 2019 described how bias is deeply embedded in most classification approaches for of all sorts of images. ImageNet is working to address various sources of bias.



</doc>
<doc id="53046682" url="https://en.wikipedia.org/wiki?curid=53046682" title="Imieliński-Lipski algebra">
Imieliński-Lipski algebra

An Imieliński-Lipski algebras is an extension of relational algebra onto tables with different types of null values. It is used to operate on relations with incomplete information.

Imieliński-Lipski algebras are defined to satisfy precise conditions for semantically meaningful extension of the usual relational operators, such as projection, selection, union, and join, from operators on relations to operators on relations with various kinds of "null values".

These conditions require that the system be "safe" in the sense that no incorrect conclusion is derivable by using a specified subset F of the relational operators; and that it be "complete" in the sense that all valid conclusions expressible by relational expressions using operators in F are in fact derivable in this system. 
For example, it is well known that the 3-valued logic approach to deal with null values, supported treatment of nulls values by SQL is not complete, see Ullman book.
To show this, let T be:
Take SQL query Q

SQL query Q will return empty set (no results) under 3-valued semantics currently adopted by all variants of SQL. This is the case because in SQL, NULL is never equal to any constant – in this case, neither to “Spring” nor “Fall” nor “Winter” (if there is Winter semester in this school). codice_1 will evaluate to MAYBE and so will codice_2. The disjunction MAYBE OR MAYBE evaluates to MAYBE (not TRUE). Thus Igor will not be part of the answer (and of course neither will Rohit). But Igor should be returned as the answer.

Indeed, regardless what semester Igor took the Networks class (no matter what was the unknown value of NULL), the selection condition will be true. This “Igor” will be missed by SQL and the SQL answer won’t be complete according to completeness requirements specified in Tomasz Imieliński, Witold Lipski, 'Incomplete Information in Relational Databases'. It is also argued there that 3-valued logic (TRUE, FALSE, MAYBE) can never provide guarantee of complete answer for tables with incomplete information.

Three algebras which satisfy conditions of safety and completeness are defined as Imielinski-Lipski algebras: the Codd-Tables algebra, the V-tables algebra and the Conditional tables (C-tables) algebra.

Codd-tables Algebra is based on the usual Codd's singe NULL values. The table T above is an example of Codd-table. Codd-table algebra supports projection and positive selections only. It is also demonstrated in [IL84 that it is not possible to correctly extend more relational operators over Codd-Tables. For example, such basic operation as join is not extendable over Codd-tables. It is not possible to define selections with Boolean conditions involving negation and preserve completeness. For example, queries like the above query Q cannot be supported.
In order to be able to extend more relational operators, more expressive form of null value representation is needed in tables which are called V-table.

V-tables algebra is based on many different ("marked") null values or variables allowed to appear in a table. V-tables allow to show that a value may be unknown but the same for different tuples. For example, in the table below Gaurav and Igor order the same (but unknown) beer in two unknown bars (which may, or may not be different – but remain unknown). Gaurav and Jane frequent the same unknown bad (Y1). Thus, instead one NULL value, we use indexed variables, or "Skolem constants" 

V-Tables algebra is shown to correctly support projection, positive selection (with no negation occurring in the selection condition), union, and renaming of attributes, which allows for processing arbitrary conjunctive queries.
A very desirable property enjoyed by the V-table algebra is that all relational operators on tables are performed in exactly the same way as in the case of the usual relations.

Example of conditional table (c-table) is shown below.

It has additional column “con” which is a Boolean condition involving variables, null values – same as in V-tables.

over the following table c-table

Conditional Tables algebra, mainly of theoretical interest, supports projection, selection, union, join, and renaming. Under closed world assumption, it can also handle the operator of difference, thus it can support all relational operators.

Imieliński-Lipski algebras were introduced by Tomasz Imieliński and Witold Lipski Jr. in 'Incomplete Information in Relational Databases' 



</doc>
<doc id="53000748" url="https://en.wikipedia.org/wiki?curid=53000748" title="KeyBase">
KeyBase

KeyBase is a database and web application for managing and deploying interactive taxonomic keys for plants and animals developed by the Royal Botanic Gardens Victoria. KeyBase provides a medium where pathway keys which where traditionally developed for print and other classical types of media, can be used more effectively in the internet environment. The platform uses a concept called "keys" which can be easily linked together, joined with other keys, or merged into larger other seamless keys groups, with each still available to be browsed independently. Keys in the KeyBase database can be filtered and displayed in a variety of ways, filters, and formats. 

Examples from Keybase database : 


</doc>
<doc id="10983365" url="https://en.wikipedia.org/wiki?curid=10983365" title="Database storage structures">
Database storage structures

Database tables and indexes may be stored on disk in one of a number of forms, including ordered/unordered flat files, ISAM, heap files, hash buckets, or B+ trees. Each form has its own particular advantages and disadvantages. The most commonly used forms are B+ trees and ISAM. Such forms or structures are one aspect of the overall schema used by a database engine to store information.

Unordered storage typically stores records in the order they are inserted. For example, a heap
Unordered storage typically stores the records in the order they are inserted. Such storage offers good insertion efficiency (formula_1), but inefficient retrieval times (formula_2). Typically these retrieval times are better, however, as most databases use indexes on the primary keys, resulting in retrieval times of formula_3 or formula_1 for keys that are the same as the database row offsets within the storage system.

Ordered storage typically stores the records in order and may have to rearrange or increase the file size when a new record is inserted, resulting in lower insertion efficiency. However, ordered storage provides more efficient retrieval as the records are pre-sorted, resulting in a complexity of formula_3.

Heap files are lists of unordered records of variable size. Although sharing a similar name, heap files are widely different from in-memory heaps. In-memory heaps are ordered, as opposed to heap files.



These are the most commonly used in practice.

Most conventional relational databases use "row-oriented" storage, meaning that all data associated with a given row is stored together. By contrast, column-oriented DBMS store all data from a given column together in order to more quickly serve data warehouse-style queries. Correlation databases are similar to row-based databases, but apply a layer of indirection to map multiple instances of the same value to the same numerical identifier.



</doc>
<doc id="44257105" url="https://en.wikipedia.org/wiki?curid=44257105" title="CrateDB">
CrateDB

CrateDB is a distributed SQL database management system that integrates a fully searchable document-oriented data store. It is open-source, written in Java, based on a shared-nothing architecture, and is designed for high scalability and includes components from Presto, Lucene, Elasticsearch and Netty.

The CrateDB project was started by Jodok Batlogg, an open source contributor and creator who has contributed to the Open Source Initiative Vorarlberg while at Lovely Systems in Dornbirn. The software is an open source, clustered database used for fast text search and analytics. The company, now called Crate.io, raised its first round of financing in April 2014, a $4M round in March 2016, and a $2.5M round in January 2017 from Dawn Capital, Draper Esprit, Speedinvest, and Sunstone Capital.

In June 2014, Crate.io won the judge's choice award at the GigaOm Structure Launchpad competition and in October 2014, they win the TechCrunch Disrupt Europe in London.

CrateDB 1.0 was released in December 2016. and was reported to have had more than one million downloads. CrateDB 2.0 and an Enterprise Edition was released in May 2017.

CrateDB's language is SQL but it uses the document-oriented approach of NoSQL style databases. The software uses the SQL parser from Presto, its own query analysis and distributed query engine. Elasticsearch and Lucene is used for the transport protocol and cluster discovery and Netty for asynchronous event driven network application framework.

CrateDB offers automatic data replication and self-healing clusters for high availability.

CrateDB includes a built-in Administration Interface. The Command Line interface (Crate Shell – CraSh) allows interactive queries. Its Python client is most advanced and features SQLAlchemy integration.


</doc>
<doc id="45567577" url="https://en.wikipedia.org/wiki?curid=45567577" title="NoSQLz">
NoSQLz

NoSQLz is a consistent key-value big data store (NoSQL database) for z/OS IBM systems. It was developed by systems programmer Thierry Falissard in 2013. The purpose was to provide a low-cost alternative to all proprietary mainframe DBMS (version 1 is free software).

NoSQLz only provides basic create, read, update and delete (CRUD) functions. It is designed to be very straightforward and easy to implement.

ACID properties are provided, so as to have "real transactions", through optimistic concurrency control, timestamp-based concurrency control and multiversion concurrency control (MVCC).

Unlike version 1, version 2 of NoSQLz is chargeable and supports IBM Parallel Sysplex. The NoSQLz DBMS can be interfaced in Rexx, Cobol, IBM High Level Assembler, etc.


</doc>
<doc id="182369" url="https://en.wikipedia.org/wiki?curid=182369" title="Whitelisting">
Whitelisting

Whitelisting is the practice of explicitly allowing some identified entities access to a particular privilege, service, mobility, access or recognition. It is the reverse of blacklisting.

Spam filters often include the ability to "whitelist" certain sender IP addresses, email addresses or domain names to protect their email from being rejected or sent to a junk mail folder. These can be manually maintained by the user or system administrator - but can also refer to externally maintained whitelist services. 

Non-commercial whitelists are operated by various non-profit organisations, ISPs and others interested in blocking spam. Rather than paying fees, the sender must pass a series of tests; for example, his email server must not be an open relay and have a static IP address. The operator of the whitelist may remove a server from the list if complaints are received.

Commercial whitelists are a system by which an Internet service provider allows someone to bypass spam filters when sending email messages to its subscribers, in return for a pre-paid fee, either an annual or a per-message fee. A sender can then be more confident that his messages have reached their recipients without being blocked, or having links or images stripped out of them, by spam filters. The purpose of commercial whitelists is to allow companies to reliably reach their customers by email.

Commercial providers include Return Path Certification, eco's Certified Senders Alliance, and the Spamhaus Whitelist.

Many websites rely on ads as a source of revenue, but the use of ad blockers is increasingly common. Websites that detect an adblocker in use often ask for it to be disabled - or their site to be "added to the whitelist" - a standard feature of most adblockers.

Another use for whitelists is local area network (LAN) security. Many network admins set up MAC address whitelists, or a MAC address filter, to control who is allowed on their networks. This is used when encryption is not a practical solution or in tandem with encryption. However, it's sometimes ineffective because a MAC address can be faked.

Some firewalls can be configured to only allow data-traffic from/ to certain (ranges of) IP-addresses.

One approach in combating viruses and malware is to whitelist software which is considered safe to run, blocking all others. This is particularly attractive in a corporate environment, where there are typically already restrictions on what software is approved. 

Leading providers of application whitelisting technology include Bit9, Velox, McAfee, Lumension and Airlock Digital.

On Microsoft Windows, recent versions include AppLocker, which allows administrators to control which executable files are denied or allowed to execute. With AppLocker, administrators are able to create rules based on file names, publishers or file location that will allow certain files to execute. Rules can apply to individuals or groups. Policies are used to group users into different enforcement levels. For example, some users can be added to report only policy that will allow administrators to understand the impact before moving that user to a higher enforcement level.

Linux system typically have AppArmor and SE Linux features available which can be used to effectively block all applications which are not explicitly whitelisted, and commercial products are also available.

On HP-UX introduced a feature called "HP-UX Whitelisting" on 11iv3 version.




</doc>
<doc id="46433353" url="https://en.wikipedia.org/wiki?curid=46433353" title="VOCEDplus">
VOCEDplus

VOCEDplus is a free international research database about tertiary education, maintained and developed by staff at the National Centre for Vocational Education Research (NCVER) in Adelaide, South Australia. The focus of the database content is the relation of post-compulsory education and training to workforce needs, skills development, and social inclusion.

The content of the VOCEDplus database encompasses vocational education and training (VET), higher education, lifelong learning, informal learning, VET in Schools, adult and community education, apprenticeships/traineeships, international education, providers of education and training, and workforce development. It is international in scope and contains over 77,000 English language records, many with links to full text documents.

VOCEDplus contains extensive Australian materials and includes a wide range of international information, covering outcomes of tertiary education in the shape of published research, practice, policy, and statistics.

Entries are included for the following types of publications: reports; annual reports; papers; discussion papers; occasional papers; working papers; books; book chapters; conference papers; conference proceedings; journals; journal articles; policy documents; published statistics; theses; podcasts; and teaching and training materials.

Each database entry contains standard bibliographic information and an abstract. Many entries include full text access via the publisher's website or a digitised copy.

In the early years VOCEDplus was known as VOCED. The original database was produced by a network of clearinghouses across Australia with the aim of sharing activities in the technical and further education (TAFE) sector. VOCED was produced in hardcopy and an electronic version was distributed on diskette.

1997 - the first web version of VOCED was made available from the National Centre for Vocational Education Research (NCVER) organisational website

1998 - a major project to upgrade the database and expand its international coverage commenced 

2001 - creation of VOCED's first own website

2001 - VOCED endorsed as the UNESCO international database for technical and vocational education and training (TVET) research information 

Many changes to the database and website occurred during this period with a focus on continuous improvement to meet the needs of users and utilise emerging technologies.

2006 - materials produced for two adult literacy and learning programs funded by the Australian Department of Education, Employment and Workplace Relations (DEEWR) - the Workplace English Language and Learning (WELL) Programme and the Adult Literacy National Project (ALNP) included in VOCED

2007 - the Australian clearinghouse network transferred most of the hardcopy collections to NCVER, to form a centralised repository of resources

2009 - materials produced by Reframing the Future (RTF) a vocational education and training workforce development initiative of the Australian, State and Territory Governments included in VOCED

A major rebuild of the database and website was undertaken during this period to take advantage of the potential of new technologies to provide improved services and incorporate Web 2.0 technologies (RSS feeds, and share and bookmark tools).

2009 - scope expanded to more fully encompass the higher education sector

2011 - launch of VOCEDplus 

2012 - a major retrospective digitisation project commenced and by the end of the 2012-2013 financial year a total of 9,328 publications (593,534 pages/microfiche frames) had been digitised, ensuring these publications are available electronically for free 

2015 - release of a refreshed look to adopt the new NCVER branding plus a number of search enhancements (Guided search, Expert search, and Glossary search) were added

2015 - first in the series of 'Focus on...' pages released

2016 - launch of the 'Pod Network', a convenient and efficient platform that allows instant access to research and a multitude of resources on a range of subjects 

2017 - completion of the 'Pod Network', consisting of 20 Pods (on broad subjects including Apprenticeships and traineeships, Foundation skills, Teaching and learning, Career development, and Students) and 74 Podlets (on narrow topics including Online learning, Social media, VET in schools, STEM skills, and Adult literacy)

2018 - launch of the 'Timeline of Australian VET policy initiatives 1998-2017' and the 'VET Knowledge Bank' which contains a suite of products capturing Australia's diverse, complex and ever-changing VET system 



</doc>
<doc id="18613380" url="https://en.wikipedia.org/wiki?curid=18613380" title="Altibase">
Altibase

Altibase is an open source relational database. Its creator is headquartered in Seoul, South Korea and New York, the US. Altibase scales vertically and horizontally. The company has offices in South Korea, United States and China.

Altibase is enterprise grade. Since 1999, Altibase has served over 600 enterprise clients including 8 Fortune Global 500 companies. Its major clients include Samsung, HP, Hyundai, China Mobile, China Telecom, China Unicom, E*TRADE, Toshiba Medical and various world-renowned finance companies. Altibase has been deployed in over 6000 use cases in industries ranging from telecommunications, finance, manufacturing and public service.

Altibase is an in-memory database. Since 1999, Altibase has pioneered the in-memory database space. Altibase has helped its clients with high performing database technologies that are mandatory for extremely demanding use cases such as billing and authentication for mega telecom companies and other applications for manufacturing and finance companies with very high throughput requirements.

Altibase is a hybrid database. Altibase was the first company in the world to develop and commercialize a hybrid database back in 2005. Altibase's hybrid architecture allows for utilization of memory tables for high performance and disk tables for economical storage in a single unified engine. As a result, data can be stored and manipulated in main memory alone, on physical disk alone or a combination of both. Consequently, Altibase eliminates the need to purchase an in-memory database and an on-disk database separately.

Altibase is relational and ACID and SQL compliant. Thus, Altibase can replace or interoperate with Oracle and other legacy databases. In addition, migration from other legacy databases to Altibase is exponentially less complex.

Altibase is currently among a very small subset of relational databases that provide sharding. In 2014, Altibase developed its scale-out technology, sharding. Since then, it has been adopted by various multi-national companies. Altibase's sharding is unique in that it is designed to allow for linear performance enhancement with almost no coordinator-related bottlenecks no matter how many servers are added. Another significant advantage of Altibase's sharding is that it does not require any changes to existing applications in order to implement Altibase if users' systems are built on relational databases. Consequently, existing DBAs will find it easy to learn, adopt and administrate Altibase when they opt to scale out their systems with Altibase.

From 1991 through 1997 the Mr.RT project was an in-memory database research project, conducted by the Electronics and Telecommunications Research Institute a government-funded research organization in South Korea. Altibase was incorporated in 1999.

The first version, called Spiner, was released in 2000 for commercial use. It took half of the in-memory DBMS market share in South Korea.

In 2002 the second version was released renamed to Altibase v2.0.
By 2003, Altibase v3.0 was released and it entered the Chinese market. 
Released version 4.0 with hybrid architecture, combining RAM and disk databases, was released in 2004.

In 2005 Altibase began working with Chinese telecommunications providers for billing systems,
and Twainese financial companies for home trading systems.
The software was certified by the Telecommunications Technology Association.
The Ministry of Government Administration and Home Affairs gave it an award in 2006.
Offices in China and United States opened in 2009.

In 2011, version 5.5.1 was renamed it to HDB (for "hybrid database"). The Altibase Data Stream product for complex event processing was renamed DSM.
The product received a Korean technology award.
Altibase introduced certification services.
In 2012, HDB Zeta and Extreme were announced, and DSM renamed to CEP

In 2013, yet another variant called XDB was announced,
and the company received ISO/IEC 20000 certification.
In 2018, Altibase went open source.

Altibase went open source in February, 2018.

According to marketing research, Altibase had 650 customers and thousands of installations.

Altibase's clients in the telecommunications, financial services, manufacturing, and utilities sectors include Bloomberg, LG U+, E*TRADE, HP, UAT Inc., SK Telecom, KT Corporation (formerly Korea Telecom), Hyundai Securities, Samsung Electronics, Shinhan Bank, Woori Bank, The South Korean Ministry of Defense, G-Market, and Chung-Ang University.

Altibase users in Japan include FX Prime, a foreign exchange services company, and Retela Crea Securities.

In China, the three major telecommunications companies, China Mobile, China Unicom, and China Telecom, utilize ALTIBASE HDB in 29 of 31 Chinese provinces.

ALTIBASE is a hybrid database, relational open source database management system manufactured by Altibase Corporation. The software comes with a hybrid architecture which allows it to access both memory-resident and disk-resident tables using single interface. It supports both synchronous and asynchronous replication and offers real-time ACID compliance. Support is also offered for a variety of SQL standards and programming languages. Other important capabilities include data import and export, data encryption for security, multiple data access command sets, materialized view and temporary tables, and others.

ALTIBASE is a so-called "hybrid DBMS", meaning that it simultaneously supports access to both memory-resident and disk-resident tables via a single interface. It is compatible with Solaris, HP-UX, AIX, Linux, and Windows.
It supports the complete SQL standard, features Multiversion concurrency control (MVCC), implements Fuzzy and Ping-Pong Checkpointing for periodically backing up memory-resident data, and ships with Replication and Database Link functionality.

Altibase acquired an in-memory database engine from the Electronics and Telecommunications Research Institute in February 2000, and commercialized the database in October of the same year.
In 2001, Altibase changed the name of the in-memory database product from "Spiner" to "Altibase" in 2001. In 2004, Altibase integrated the in-memory database with a disk-resident database to create a hybrid DBMS, released version 4.0 and renamed it as ALTIBASE HDB. Altibase released version 5.5.1 and 6.1.1 in 2012, version 6.3.1 in November 2013, and 6.5.1 in May 2015. Altibase claims that this is the world's first hybrid DBMS.

ALTIBASE HDB includes the following application development interfaces:



</doc>
<doc id="1885551" url="https://en.wikipedia.org/wiki?curid=1885551" title="Database dump">
Database dump

A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements. A database dump is most often used for backing up a database so that its contents can be restored in the event of data loss. Corrupted databases can often be recovered by analysis of the dump. Database dumps are often published by free software and free content projects, to allow reuse or forking of the database.




</doc>
<doc id="35855099" url="https://en.wikipedia.org/wiki?curid=35855099" title="Outline of databases">
Outline of databases

The following outline is provided as an overview of and topical guide to databases:

Database – organized collection of data, today typically in digital form. The data are typically organized to model relevant aspects of reality (for example, the availability of rooms in hotels), in a way that supports processes requiring this information (for example, finding a hotel with vacancies).

Databases can be described as all of the following:





Database languages –

Database security –

Database design –



Database management system –










Data warehouse –


















</doc>
<doc id="8377" url="https://en.wikipedia.org/wiki?curid=8377" title="Database">
Database

A database is an organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques.

The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a "database system". Often the term "database" is also used to loosely refer to any of the DBMS, the database system or an application associated with the database.

Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages.

Formally, a "database" refers to a set of related data and the way it is organized. Access to this data is usually provided by a "database management system" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.

Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.

Outside the world of professional information technology, the term "database" is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.

Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:


Both a database and its DBMS conform to the principles of a particular database model. "Database system" refers collectively to the database model, database management system, and database.

Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. RAID is used for recovery of data if any of the disks fail. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.

Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.

Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.

The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational.

The two main early navigational data models were the hierarchical model and the CODASYL model (network model)

The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and they remain dominant: IBM DB2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS. The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.

Object databases were developed in the 1980s to overcome the inconvenience of object-relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object-relational databases.

The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key-value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.

The introduction of the term "database" coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.

As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the "Database Task Group" within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the "CODASYL approach", and soon a number of commercial products based on this approach entered the market.

The CODASYL approach relied on the "manual" navigation of a linked data set which was formed into a large network. Applications could find records by one of three methods:

Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a very straightforward query language. However, in the final tally, CODASYL was very complex and required significant training and effort to produce useful applications.

IBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed, and Bachman's 1973 Turing Award presentation was "The Programmer as Navigator". IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL database are classified as network databases. IMS remains in use .

Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that was primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking "A Relational Model of Data for Large Shared Data Banks".

In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to use a "table" of fixed-length records, with each table used for a different type of entity. A linked-list system would be very inefficient when storing "sparse" databases where some of the data for any one record could be left empty. The relational model solved this by splitting the data into a series of normalized tables (or "relations"), with optional elements being moved out of the main table to where they would take up room only if needed. Data may be freely inserted, deleted and edited in these tables, with the DBMS doing whatever maintenance needed to present a table view to the application/user.
The relational model also allowed the content of the database to evolve without constant rewriting of links and pointers. The relational part comes from entities referencing other entities in what is known as one-to-many relationship, like a traditional hierarchical model, and many-to-many relationship, like a navigational (network) model. Thus, a relational model can express both hierarchical and navigational models, as well as its native tabular model, allowing for pure or combined modeling in terms of these three models, as the application requires.

For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single record, and unused items would simply not be placed in the database. In the relational approach, the data would be "normalized" into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.

Linking the information back together is the key to this system. In the relational model, some bit of information was used as a "key", uniquely defining a particular record. When information was being collected about a user, information stored in the optional tables would be found by searching for this key. For instance, if the login name of a user is unique, addresses and phone numbers for that user would be recorded with the login name as its key. This simple "re-linking" of related data back into a single collection is something that traditional computer languages are not designed for.

Just as the navigational approach would require programs to loop in order to collect records, the relational approach would require loops to collect information about any "one" record. Codd's suggestions was a set-oriented language, that would later spawn the ubiquitous SQL. Using a branch of mathematics known as tuple calculus, he demonstrated that such a system could support all the operations of normal databases (inserting, updating etc.) as well as providing a simple system for finding and returning "sets" of data in a single operation.

Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.

IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called "relational" are actually SQL DBMSs.

In 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs' Set-Theoretic Data model. MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998.

In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.

Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).

IBM started working on a prototype system loosely based on Codd's concepts as "System R" in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as "SQL/DS", and, later, "Database 2" (DB2).

Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it wasn't until Oracle Version 2 when Ellison beat IBM to market in 1979.

Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).

In Sweden, Codd's paper was also read and Mimer SQL was developed from the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise.

Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two have become irrelevant.

The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation." dBASE was one of the top selling software titles in the 1980s and early 1990s.

The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields. The term "object-relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object-relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object-relational mappings (ORMs) attempt to solve the same problem.

XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.

NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.

In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.

NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.

Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).

Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.

One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.




Connolly and Begg define Database Management System (DBMS) as a "software system that enables users to define, create, maintain and control access to the database".

The DBMS acronym is sometime extended to indicated the underlying database model, with RDBMS for relational, OODBMS or ORDBMS for the object (orientated) model and ORDBMS for Object-Relational. Other extensions can indicate some other characteristic, such as DDBMS for a distributed database management systems.

The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:

It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.

Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimise the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.

The large major enterprise DBMSs have tended to increase in size and functionality and can have involved thousands of human years of development effort through their lifetime.

Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.

A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with the database. A special purpose DBMS may use a private API and be specifically customised and linked to a single application. For example, an email system performing many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.

External interaction with the database will be via an application program that interfaces with the DBMS. This can range from a database tool that allows users to execute SQL queries textually or graphically, to a web site that happens to use a database to store and search information.

A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possible indirectly via a pre-processor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.

Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:


Database languages are specific to a particular data model. Notable examples include:


A database language may also incorporate features like:

Database storage is the container of the physical materialization of a database. It comprises the "internal" (physical) "level" in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the "conceptual level" and "external level" from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).

Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.

Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.

Often storage redundancy is employed to increase performance. A common example is storing "materialized views", which consist of frequently needed "external views" or query results. Storing such views saves the expensive computing of them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.

Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to a same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.

Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).

Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.

This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.

Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).

Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this to the database. Monitoring can be set up to attempt to detect security breaches.

Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).

The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.

A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help importing databases from other popular DBMSs.

After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).

When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.

After the database is created, initialised and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.

Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.

Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.

Other DBMS features might include:

Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".

The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity-relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.

Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.

Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms "data model" and "database model" are often used interchangeably, but in this article we use "data model" for the design of a specific database, and "database model" for the modeling notation used to express that design).

The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.

The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called "physical database design", and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.

Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.

A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.

Common logical data models for databases include:

An object-relational database combines the two related structures.

Physical data models include:

Other models include:

Specialized models are optimized for particular types of data:

A database management system provides three views of the database data:


While there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different "views" of the company's database.

The three-level database architecture relates to the concept of "data independence" which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.

The conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.

Separating the "external", "conceptual" and "internal" levels was a major feature of the relational database model implementations that dominate 21st century databases.

Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, and related concurrency control techniques, query languages and query optimization methods, RAID, and more.

The database research area has several dedicated academic journals (for example, "ACM Transactions on Database Systems"-TODS, "Data and Knowledge Engineering"-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).




</doc>
<doc id="59024285" url="https://en.wikipedia.org/wiki?curid=59024285" title="Jordan Antiquities Database and Information System">
Jordan Antiquities Database and Information System

The Jordan Antiquities Database and Information System (JADIS) was a computer database of antiquities in Jordan, the first of its kind in the Arab world. It was established by the Department of Antiquities in 1990, in cooperation with the American Center for Oriental Research in Amman and sponsored by the United States Agency for International Development. JADIS was in use until 2002, when it was superseded by a new system, MEGA-J. Over 10,841 antiquities were registered in the database.



</doc>
<doc id="8609174" url="https://en.wikipedia.org/wiki?curid=8609174" title="Single-instance storage">
Single-instance storage

Single-instance storage (SIS) is a system's ability to take multiple copies of content and replace them by a single shared copy. It is a means to eliminate data duplication and to increase efficiency. SIS is frequently implemented in file systems, e-mail server software, data backup and other storage-related computer software. Single-instance storage is a simple variant of data deduplication. While data deduplication may work at a segment or sub-block level, single instance storage works at the whole-file level and eliminates redundant copies of entire files or e-mail messages.

In the case of an e-mail server, single-instance storage would mean that a single copy of a message is held within its database whilst individual mailboxes access the content through a reference pointer. However, there is a common misconception that the primary benefit of single instance storage in mail servers is a reduction in disk space requirements. The truth is that its primary benefit is to greatly enhance delivery efficiency of messages sent to large distribution lists. In a mail server scenario disk space savings from single instance storage are transient and drop off very quickly over time.

When used in conjunction with backup software, single instance storage can reduce the quantity of archive media required since it avoids storing duplicate copies of the same file. Often identical files are installed on multiple computers, for example operating system files. With single instance storage, only one copy of a file is written to the backup media therefore reducing space. This becomes more important when the storage is offsite and on cloud storage such as Amazon S3. In such cases, it has been reported that deduplication can help reduce the costs of storage, costs of bandwidth and backup windows by up to 10:1.

Novell GroupWise was built on single-instance storage which accounts for its large capacity.

ISO CD/DVD image files can be optimized to use SIS to reduce the size of a CD/DVD compilation (if there are enough duplicated files) to make it fit into smaller media.

SIS is related to system wide file duplication search and multiple file instance detection tools such as the P2P application BearShare (5.n Versions and below) but differs in that SIS reduces storage utilization automatically and creates and retains symbolic linkages, whereas Bearshare allows for manual deletion of duplicates and associated user level file system, Windows Explorer type of icon links.

SIS was introduced with the Remote Installation Services feature of Windows 2000 Server. A typical server might hold ten or more unique installation configurations (perhaps with different device drivers or software suites) but perhaps only 20% of the data may be unique between configurations. Microsoft states that "SIS works by searching a hard disk volume to identify duplicate files. When SIS finds identical files, it saves one copy of the file to a central repository, called the SIS Common Store, and replaces other copies with pointers to the stored versions." Files are compared solely by their hash functions; files with different names or dates can be consolidated so long as the data itself is identical. Windows Server 2003 Standard Edition has SIS capabilities but is limited to OEM OS system installs.

The file-based Windows Imaging Format introduced in Windows Vista also supported single-instance storage. Single-instance storage was a feature of Microsoft Exchange Server since version 4.0 and is also present in Microsoft's Windows Home Server. It is deduplicating attachments only in Exchange 2007 and was dropped completely in Microsoft Exchange Server 2010.
Microsoft announced Windows Storage Server 2008 (WSS2008) with Single Instance Storage on June 1, 2009, and states this feature is not available on Windows Server 2008.

The feature is officially deprecated since Windows Server 2012, when a new，more powerful chunk-based data deduplication mechanism was introduced. It allows files with similar content to be deduplicated as long as they have stretches of identical data. This mechanism is more powerful than SIS. Since Windows Server 2019, the feature is fully supported on ReFS.



</doc>
<doc id="60478389" url="https://en.wikipedia.org/wiki?curid=60478389" title="African Origins">
African Origins

The African Origins project is a database run by researchers at Emory University which aims to document all the known facts about the African diaspora, including all documentary material pertaining to the transatlantic slave trade. It is a sister project to .


</doc>
<doc id="60478414" url="https://en.wikipedia.org/wiki?curid=60478414" title="Voyages: The Trans-Atlantic Slave Trade Database">
Voyages: The Trans-Atlantic Slave Trade Database

Voyages: The Trans-Atlantic Slave Trade Database is a database run by researchers at Emory University which aims to present all documentary material pertaining to the transatlantic slave trade. It is a sister project to African Origins.

The database breaks down the kingdoms or countries who engaged in the Atlantic trade, summarized in the following table:



</doc>
<doc id="60637208" url="https://en.wikipedia.org/wiki?curid=60637208" title="TiDB">
TiDB

TiDB is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and can provide horizontal scalability, strong consistency, and high availability. It is developed and supported primarily by PingCAP, Inc. and licensed under Apache 2.0. TiDB drew its initial design inspiration from Google’s Spanner and F1 papers.

TiDB was recognized by InfoWorld 2018 Bossie Award as one of the best open source software for data storage and analytics.

PingCAP Inc., a software company founded in April, 2015, began developing TiDB after its founding. The company is the primary developer, maintainer, and driver of TiDB and its associated open-source communities. PingCAP is a venture-backed company; it announced its 50 million USD Series C round financing in September, 2018.

TiDB can expand both SQL processing and storage capacity by adding new nodes. This makes infrastructure capacity scaling easier and more flexible compare to traditional relational database which only scale vertically.

TiDB acts like it is a MySQL 5.7 server to applications. A user can continue to use all of the existing MySQL client libraries. Because TiDB’s SQL processing layer is built from scratch, not a MySQL fork, its compatibility is not 100%, and there are known behavior differences between MySQL and TiDB.

TiDB internally shards table into small range-based chunks that are referred to as "regions". Each region defaults to approximately 100MB in size, and TiDB uses a two-phase commit internally to ensure that regions are maintained in a transactionally consistent way.

TiDB is designed to work in the cloud to make deployment, provisioning, operations, and maintenance flexible. The storage layer of TiDB, called TiKV, became a Cloud Native Computing Foundation member project in August, 2018, as a Sandbox level project. The architecture of the TiDB platform also allows SQL processing and storage to be scaled independently of each other.

TiDB can support both online transaction processing (OLTP) and online analytical processing (OLAP) workloads. This means that while a user may have traditionally transacted on MySQL and then Extracted, Transformed and Loaded (ETL) data into a column store for analytical processing, this step is no longer required.

TiDB uses the Raft consensus algorithm to ensure that data is highly available and safely replicated throughout storage in Raft groups. In the event of failure, a Raft group will automatically elect a new leader for the failed member, and self-heal the TiDB cluster without any required manual intervention. Failure and self-healing operations are transparent to the applications.

TiDB can be deployed in a Kubernetes-enabled cloud environment by using TiDB Operator. An Operator is a method of packaging, deploying, and managing a Kubernetes application. It is designed for running stateful workloads and was first introduced by CoreOS in 2016.TiDB Operator was originally developed by PingCAP and open-sourced in August, 2018. TiDB Operator can be used to deploy TiDB on a laptop, Google Cloud Platform’s Google Kubernetes Engine, and Amazon Web Services’ Elastic Container Service for Kubernetes. 

TiDB can be deployed using Ansible by using a TiDB-Ansible playbook. 

Docker can be used to deploy TiDB in a containerized environment on multiple nodes and multiple machines, and Docker Compose can be used to deploy TiDB with a single command for testing purposes.

TiDB has a series of open-source tools built around it to help with data replication and migration for existing MySQL and MariaDB users. 

Syncer is a tool that supports full data migration or incremental data replication from MySQL or MariaDB instances into a TiDB cluster. Data Migration (DM) is the second-generation iteration of Syncer that is suited for replicating data from already sharded MySQL or MariaDB tables to TiDB. A common use case of Syncer/DM is to connect MySQL or MariaDB tables to TiDB, treating TiDB almost as a slave, then directly run analytical workloads on this TiDB cluster in near real-time.

Lightning is a tool that supports high speed full-import of a large MySQL dump into a new TiDB cluster, providing a faster import experience than executing each SQL statement. This tool is used to populate an initially empty TiDB cluster with lots of data quickly, in order to speed up testing or production migration. The import speed improvement is achieved by parsing SQL statements into key-value pairs, then directly generate Sorted String Table (SST) files to RocksDB.

TiDB-Binlog is a tool used to collect the logical changes made to a TiDB cluster. It is used to provide incremental backup and replication, either between two TiDB clusters, or from a TiDB cluster to another downstream platform.

It is similar in functionality to MySQL master-slave replication. The main difference is that since TiDB is a distributed database, the binlog generated by each TiDB instance needs to be merged and sorted according to the time of the transaction commit before being consumed downstream. 

Currently, TiDB is used by more than 300 companies, including Shopee, BookMyShow, Xiaomi, Zhihu, Meituan-Dianping, iQiyi, Zhuan Zhuan, Mobike, Yiguo.com, and Yuanfudao.com. 


</doc>
<doc id="61186833" url="https://en.wikipedia.org/wiki?curid=61186833" title="Truth discovery">
Truth discovery

Truth discovery (also known as truth finding) is the process of choosing the actual "true value" for a data item when different data sources provide conflicting information on it.

Several algorithms have been proposed to tackle this problem, ranging from simple methods like majority voting to more complex ones able to estimate the trustworthiness of data sources.

Truth discovery problems can be divided into two sub-classes: single-truth and multi-truth. In the first case only one true value is allowed for a data item (e.g birthday of a person, capital city of a country). While in the second case multiple true values are allowed (e.g. cast of a movie, authors of a book).

Typically, truth discovery is the last step of a data integration pipeline, when the schemas of different data sources have been unified and the records referring to the same data item have been detected.

The abundance of data available on the web makes more and more probable to find that different sources provide (partially or completely) different values for the same data item. This, together with the fact that we are increasing our reliance on data to derive important decisions, motivates the need of developing good truth discovery algorithms.  

Many currently available methods rely on a voting strategy to define the true value of a data item. Nevertheless, recent studies, have shown that, if we rely only on majority voting, we could get wrong results even in 30% of the data items.

The solution to this problem is to assess the trustworthiness of the sources and give more importance to votes coming from trusted sources.

Ideally, supervised learning techniques could be exploited to assign a reliability score to sources after hand-crafted labeling of the provided values; unfortunately, this is not feasible since the number of needed labeled examples should be proportional to the number of sources, and in many applications the number of sources can be prohibitive.

Single-truth and multi-truth discovery are two very different problems.

Single-truth discovery is characterized by the following properties:


While in the multi-truth case the following properties hold:


Multi-truth discovery has unique features that make the problem more complex and should be taken into consideration when developing truth-discovery solutions.

The examples below point out the main differences of the two methods. Knowing that in both examples the truth is provided by source 1, in the single truth case (first table) we can say that sources 2 and 3 oppose to the truth and as a result provide wrong values. On the other hand, in the second case (second table), sources 2 and 3 are neither correct nor erroneous, they instead provide a subset of the true values and at the same time they do not oppose the truth. 

The vast majority of truth discovery methods are based on a voting approach: each source votes for a value of a certain data item and, at the end, the value with the highest vote is select as the true one. In the more sophisticated methods, votes do not have the same weight for all the data sources, more importance is indeed given to votes coming from trusted sources.

Source trustworthiness usually is not known "a" "priori" but estimated with an iterative approach. At each step of the truth discovery algorithm the trustworthiness score of each data source is refined, improving the assessment of the true values that in turn leads to a better estimation of the trustworthiness of the sources. This process usually ends when all the values reach a convergence state.

Source trustworthiness can be based on different metrics, such as accuracy of provided values, copying values from other sources and domain coverage.

Detecting copying behaviors is very important, in fact, copy allows to spread false values easily making truth discovery very hard, since many sources would vote for the wrong values. Usually systems decrease the weight of votes associated to copied values or even don’t count them at all.

Most of the currently available truth discovery methods have been designed to work well only in the single-truth case.

Below are reported some of the characteristics of the most relevant typologies of single-truth methods and how different systems model source trustworthiness.

Majority voting is the simplest method, the most popular value is selected as the true one. Majority voting is commonly used as a baseline when assessing the performances of more complex methods.

These methods estimate source trustworthiness exploiting a similar technique to the one used to measure authority of web pages based on web links. The vote assigned to a value is computed as the sum of the trustworthiness of the sources that provide that particular value, while the trustworthiness of a source is computed as the sum of the votes assigned to the values that the source provides.

These methods estimate source trustworthiness using similarity measures typically used in information retrieval. Source trustworthiness is computed as the cosine similarity (or other similarity measures) between the set of values provided by the source and the set of values considered true (either selected in a probabilistic way or obtained from a ground truth).

These methods use Bayesian inference to define the probability of a value being true conditioned on the values provided by all the sources.

formula_1

where formula_2 is a value provided for a data item formula_3 and formula_4 is the set of the observed values provided by all the sources for that specific data item.

The trustworthiness of a source is then computed based on the accuracy of the values that provides. Other more complex methods exploit Bayesian inference to detect copying behaviors and use these insights to better assess source trustworthiness.

Due to its complexity, less attention has been devoted to the study of the multi-truth discovery

Below are reported two typologies of multi-truth methods and their characteristics.

These methods use Bayesian inference to define the probability of a group of values being true conditioned on the values provided by all the data sources. In this case, since there could be multiple true values for each data item, and sources can provide multiple values for a single data item, it is not possible to consider values individually. An alternative is to consider mappings and relations between set of provided values and sources providing them. The trustworthiness of a source is then computed based on the accuracy of the values that provides.

More sophisticated methods also consider domain coverage and copying behaviors to better estimate source trustworthiness.

These methods use probabilistic graphical models to automatically define the set of true values of given data item and also to assess source quality without need of any supervision.

Many real-world applications can benefit from the use of truth discovery algorithms. Typical domains of application include: healthcare, crowd/social sensing, crowdsourcing aggregation, information extraction and knowledge base construction.

Truth discovery algorithms could be also used to revolutionize the way in which web pages are ranked in search engines, going form current methods based on link analysis like PageRank, to procedures that rank web pages based on the accuracy of the information they provide.



</doc>
<doc id="5321649" url="https://en.wikipedia.org/wiki?curid=5321649" title="Répertoire International de Littérature Musicale">
Répertoire International de Littérature Musicale

Répertoire International de Littérature Musicale (International Repertory of Music Literature; Internationales Repertorium der Musikliteratur), commonly known by its acronym RILM, is a nonprofit organization that offers digital collections and advanced tools for locating research on all topics related to music. Its mission is “to make this knowledge accessible to research and performance communities worldwide….to include the music scholarship of all countries, in all languages, and across all disciplinary and cultural boundaries, thereby fostering research in the arts, humanities, sciences, and social sciences.” Central to RILM's work and mission is the international bibliography of scholarship relating to all facets of music research.

RILM was founded in 1966 by the American musicologist Barry S. Brook (1918–1997) under the joint sponsorship of the International Musicological Society (IMS) and the International Association of Music Libraries, Archives and Documentation Centres (IAML). In 2007 the International Council for Traditional Music (ICTM) joined as a third sponsoring organization.

During 1967 and 1968, RILM has developed its first set of computer programs for the automated processing and sorting of bibliographic records and author/subject indexes. They ran first on the mainframe IBM computer IBM System/360 at the computing center of The City University of New York. The S/360 was delivered by IBM in 1964 and it was at that time the most advanced computing machine. The original IBM S/360 software was later migrated to IBM System/370 and used in the production of "RILM Abstracts" for twenty years, from 1969 to 1988. RILM's development of procedures for computerized data processing was immediately adopted by Répertoire International de Littérature d’Art (RILA), founded upon RILM's model, which started publishing abstracts in 1975.

In 1979 RILM entered an agreement with Lockheed Research Laboratory in Palo Alto, a division of Lockheed Missiles and Space Company, Inc., for the distribution of its data through the telephone lines. Later on, this agreement was transferred to DIALOG Information Retrieval Services. Although available online already before the advent of the Internet, until the end of the twentieth century the primary medium for distribution for its bibliographic records were printed volumes.

From 1993 onward RILM was no longer available on DIALOG Information Retrieval Service, but in 1989 the National Information Service Corporation (NISC) in Baltimore released "RILM Abstracts of Music Literature" on CD-ROM. During the 1990s "RILM Abstracts" became available online through NISC Muse (1993–2010), OCLC First Search (1994–2010), Ovid/SilverPlatter (2002–2010), and Cambridge Scientific Abstracts/ProQuest (2002–2010) platforms. RILM databases are currently exclusively available through EBSCO Information Services.

RILM's first editorial office was located at the Queens College, City University of New York (1967–68). The Graduate Center, CUNY has provided an institutional context for RILM's International Center since 1968.

RILM has a staff of over 40 employees: editors, technology experts, and administrators. It is governed and guided by a board of directors and an advisory committee, the Commission Mixte International, which consists of appointed members of the International Musicological Society (IMS), the International Association of Music Libraries, Archives and Documentation Centres (IAML), and the International Council for Traditional Music (ICTM). Barry Brook's second report, published in the March 1968 issue of "Notes", details the deliberations of the Commission Mixte as it worked to establish the procedures and functioning of RILM.

RILM Abstracts of Music Literature

"RILM Abstracts of Music Literature" covers significant international scholarship in both printed and digital media, and in any language. Published since August 1967, it consists of citations of articles, single-author books and collections of essays, bibliographies, catalogues, master's theses and doctoral dissertations, Festschriften, films, videos, technical drawings of instruments, facsimile editions, iconographies, commentaries included with critical editions of music, ethnographic recordings, conference proceedings, reviews, web resources as well as over 3500 periodicals that dovetail with the coverage of Répertoire International de la Presse Musicale. It differs from other music periodical indexes through its coverage of books, abstracts, indexing, and broad international yet selective coverage. Each entry provides the title in the original language, an English translation of the title, full bibliographic data, and an abstract with a detailed index, all of which help to convey the “aboutness” of the record (amplified online by the relevance-based order of display in the individual record. Many of the non-English entries also include an abstract in the language of the publication.

Following the UNESCO model, "RILM Abstracts" was conceived as a cooperative of national committees responsible for contributing bibliographic citations and abstracts for the publications issued in their respective countries to the International Office in New York. Today committees contribute about 15,000 records annually, which are edited, indexed, and added to the online database. Another 35,000 records per year are produced by editors at the International Office. Bibliographic information and abstracts—as well as journals that have not yet been covered by RILM—can also be submitted directly to the International Center in New York.

"RILM Abstracts of Music Literature" was the first abstracted bibliography in the humanities and designated by the American Council of Learned Societies (ACLS) as the pilot project for the development of a computerized, bibliographical system in the humanities to serve as a model for the more than 30 constituent scholarly societies of the ACLS. At the time when "RILM Abstracts" was published only in print, its subject thesaurus and name equivalencies, which led users to the preferred terms, were translated into seventeen languages and alphabetically integrated with the subject index. This practice allowed users to find the desired English-language term or the spelling of a personal name by initiating the search from the language most familiar to them. Gradually "RILM Abstracts" expanded its multilingual environment and the database now includes, besides standard English-language abstracts, also abstracts in the language of publication and in other languages whenever available. In the mid-2000s "RILM Abstracts" began to expand its coverage of Asian publications, with music scholarship published in Chinese periodicals. Concurrently, all elements of bibliographic records for publications issued in non-roman writing systems were offered bilingual.

"RILM Abstracts of Music Literature" appeared from 1967 to 1983 in triannual printed volumes with indexes corresponding to annual volumes as well as cumulative indexes corresponding to five-year periods; from 1984 to 1999 in annual volumes with corresponding indexes; and since 2000 it is available exclusively online through EBSCO"host".

RILM Abstracts of Music Literature with Full Text

In July 2016 "RILM Abstracts of Music Literature" expanded with the addition of music periodicals in full-text. "RILM Abstracts of Music Literature with Full Text (RAFT)" offers access to 240 music periodicals from 50 countries. Coverage also includes reviews as well as obituaries, editorials, correspondence, advertisements, and news, published from the early twentieth century to the present. In addition to metadata, abstracts, and indexing, "RAFT" offers searching and browsing tools for each full-text issue, cover to cover. The database is updated monthly. Details of each title's current coverage can be found in the title list at rilm.org/fulltext. New titles are added over time.

RILM Music Encyclopedias

In December 2015, RILM launched "RILM Music Encyclopedias (RME)" with 41 titles. Librarian Laurie Sampsel asserts that “cross searching the full text of so many titles yields results impossible (or highly unlikely) to find using the print versions of these encyclopaedias.” Stephen Henry mentions RME's “ability to provide access to some excellent European resources that might not otherwise be available to libraries with less than comprehensive collections.”

"RME"’s titles stem from different periods and countries: The earliest, Jean Jacques Rousseau’s "Dictionnaire de musique", was published in 1775. There is also the first edition of “The Grove,” in an edition published by Theodore Presser in 1895. The largest amount of titles date from 2000 onward. Among them are Ken Bloom's "Broadway", Lol Henderson and Lee Stacey's "Encyclopedia of Music in the 20th Century", Peter Matzke et al., "Das Gothic- und Dark Wave-Lexikon", and Richard Kostelanetz's "Dictionary of the Avant-Gardes". The comprehensive "", conceived between 1972 bis 2006, is also included. "RME" holds important titles for ethnomusicologists, among them "The Garland Encyclopedia of World Music" and Eileen Southern's "Biographical Dictionary of Afro-American and African Musicians", which is the first single comprehensive volume of its kind. The collection expands annually with additions of four titles in average. One of the titles, "", is being regularly updated with new articles or additions to existing articles. All titles are listed at RILM Music Encyclopedias.

Index to Printed Music

On 1 July 2018, RILM assumed ownership of "IPM". Previously, it was owned by the James Adrian Music Company, which was founded in 2000 by George R. Hill.

The "Index to Printed Music" ("IPM") is the only digital finding aid for searching specific musical works contained in printed collections, sets, and series. It indexes individual pieces of music printed in the complete works of composers, in anthologies containing pieces from disparate historical periods, and in other scholarly editions. It provides a granular level of detail about each piece, including performing forces, language, multiple clefs or figured bass, and more. "IPM" includes the complete contents of "Collected Editions, Historical Series & Sets & Monuments of Music: A Bibliography", by George R. Hill and Norris L. Stephens (Berkeley: Fallen Leaf Press, 1997), which, in turn, was based upon Anna H. Heyer's "Historical Sets, Collected Editions, and Monuments of Music: A Guide to Their Contents" (American Library Association, 1957–1980).

Since 2019 "IPM" offers new features, among them biographical facts identifying composers, editors, and lyricists; hyperlinks to open-access editions; music incipits for works that are otherwise difficult to distinguish from each other; easy toggling between collections and the individual works contained therein; and expanded search filters to enable refined searching by place and date of publication, document type, genre, and language of text.

Bibliolore

RILM hosts the blog "Bibliolore" whose posts have direct relationships to content found in "RILM Abstracts of Music Literature" and its enhancement, "RILM Abstracts of Music Literature with Full Text", as well as "RILM Music Encyclopedias", "MGG Online", and "Index to Printed Music". New posts appear every week, many of which celebrate round birthdays of musical figures and anniversaries. Since its inception in 2009, "Bibliolore" has published over 1300 posts and has been viewed over 500,000 times.

Between 1967 and 1999, RILM published "RILM Abstracts of Music Literature" in print, first quarterly and later annually. The 1999 volume, the last print volume, is the largest, with 19,619 records.

Since 1972, RILM has also published print volumes in the RILM Retrospectives series. These topical bibliographies commenced with the first edition of Barry S. Brook's "Thematic Catalogues in Music: An Annotated Bibliography" (Stuyvesant, NY: Pendragon Press, 1972). Recent volumes published in the series include "Speaking of Music: Music Conferences 1835–1966" (2004), an annotated bibliography of 5948 papers on musical topics presented at 447 conferences. While building on the previous efforts, the volume stays true to Brook's original vision while expanding upon it as well: it covers more than 130 years of conference proceedings and has a worldwide scope, though Western Europe remains in focus. The subsequent volume, "Liber Amicorum: Festschriften for Music Scholars and Nonmusicians, 1840–1966", is an annotated bibliography catalogue of 574 Festschriften, totaling 118 pages and 715 entries. The next part of the volume (totaling 355 pages and 3881 entries) documents all of the articles pertaining to music found in the listed Festschriften, preceded by an introductory history of Festschriften. All volumes are principally devoted to research materials published before RILM issued its first volume in 1967.

The RILM Perspectives series of conference proceedings explores topics related to the organization's global mission. The inaugural volume, "Music’s Intellectual History: Founders, Followers & Fads" of 2009 is based on papers presented at the conference on history of the music historiography, held at The Graduate Center, CUNY, 16–19 March 2005.

RILM has issued two editions of its comprehensive style guide, "How to Write About Music: The RILM Manual of Style". The second edition introduces material not included in the 2005 publication as well as revisions based on suggestions from readers. The manual differs significantly from the generalized style guides such as MLA or APA by explicating the matters of gender-neutral language, dead language, and punctuation (from the serial comma to the em dash) through the lens of music. The manual is specifically suited for students.

RILM has developed a stand-alone platform with the most advanced search and browse capabilities to host and distribute music reference works, beginning with the authoritative German-language music encyclopedia "Die Musik in Geschichte und Gegenwart" ("MGG"). The platform facilitates automatic translation of content to over 100 languages via Google Translate integration, user accounts where annotations and notes can be created, saved, and shared, cross references linking related content throughout "MGG Online", links to related content in "RILM Abstracts of Music Literature" and others, as well as an interface compatible with mobile and tablet devices. The search function is hailed as a powerful but easy-to-use tool, with different options available for limiting search results. Specific search results can be easily located in a preview section. "MGG Online" is based on the second edition of , but it includes continuous updates, revisions, and additions. The platform allows the user to access earlier versions of revised articles. The platform currently further developed for knowledge exchange.




</doc>
<doc id="60898724" url="https://en.wikipedia.org/wiki?curid=60898724" title="Census of Antique Works of Art and Architecture Known in the Renaissance">
Census of Antique Works of Art and Architecture Known in the Renaissance

The Census of Antique Works of Art and Architecture Known in the Renaissance (abbreviated Census) is an interdisciplinary research project dedicated to the study of the reception of antiquity in the Renaissance. At the heart of the project is its scholarly database recording the antique works of art and architecture known in the Renaissance in relation with the early-modern sources documenting them. 
The project is based at the "Institute of Art and Visual History" at the "Humboldt University of Berlin". 
The "Census" project came about as a means to acquire more clarity about the actual knowledge of antiquity of Renaissance artists. Since its inception, the project has therefore pursued the goal of registering all antique monuments known in the Renaissance and the Renaissance documents relating to them. After focusing on figurative art and its reception up until 1527 in the early phase, the temporal limit was later moved to around 1600 and other classes of art, mainly architecture, were included. In 2015, the "Census" database contained approximately 15,000 records of antique monuments as well as approximately 36,000 visual and written sources from the Renaissance and it is still being extended today. The ancient monuments cover sculpture, architecture, epigraphs, coins, paintings and mosaics. The Renaissance documents include drawings, prints, sculptures, paintings and medals as well as collection inventories, travelogues, artists' biographies and archival documents.

Through collaborations with neighbouring projects such as "Corpus Winckelmann" and "Corpus Medii Aevi", the time span covered in the Census database today extends beyond the Renaissance to the Middle Ages on the one hand, and to the 18th century on the other.

The "Census" was founded in 1946 at the "Warburg Institute" in London as a cooperation project with the "Institute of Fine Arts" at "New York University". The project was initiated by the art historians Fritz Saxl and Richard Krautheimer and the archaeologist Karl Lehmann who sought after a reliable research tool for a better understanding of the afterlife of antiquity in the Renaissance. From 1947 the archaeologist Phyllis Pray Bober helped with the realization of this idea by developing an index card system. In addition to information on dating, authorship, iconography, etc., the ancient monuments were recorded together with the corresponding Renaissance documents and sorted alphabetically and by genre. Initially, the compilation was restricted to antique sculpture and its early-modern documentation. Since 1954 the handwritten card entries were supplemented by reproductions from the Warburg Institute’s Photographic Collection.

In 1957, Ruth Rubinstein joined the "Census" at the "Warburg Institute" and became the second long-standing protagonist besides Bober, who had always worked for the project in New York. Since then, two corresponding index card systems and photographic collections were filed in New York and London. Over time, a number of critical editions of Renaissance sketchbooks after the antique based on research done with the "Census" were produced. One of the most important publications that arose within the "Census" project was Bober’s and Rubinstein’s "Renaissance Artists and Antique Sculpture: A Handbook of Sources", first published in 1986. 

In the late 1970s, the idea was born to convert the analogue index card system into a computer application. Since 1981, the first "Census" database was designed and created within the newly launched "Art History Information Program" of the "J. Paul Getty Trust".
Together, German art historian and archaeologist Arnold Nesselrath, newly appointed director of the "Census" project, and American programmer Rick Holt developed an object-relational data model and a database software for UNIX systems, which allowed to access data not only from the monuments file but from the documents file as well, as from authority files for places, persons, etc. At the same time, the "Bibliotheca Hertziana" ("Max Planck Institute for Art History" in Rome) became a host institution of the "Census" and the project’s scope was extended to the reception of ancient architecture in the Renaissance. 

After a productive decade of data entry both in London and in Rome, a newly developed retrieval system for the "Census" database was publicly introduced at the "Warburg Institute" in 1992. In 1995, after Horst Bredekamp had successfully campaigned for the project’s integration into the art history department of "Humboldt University", the "Census" moved to Berlin. Additional funding for the project was obtained from the "Federal Ministry of Education and Research". In the following years, the database was converted to the MS-DOS-based database system Dyabola. This allowed for data entry using several personal computers within a local area network, and, in 1998, for the first time publication of the database as a PC application distributed on CD-ROM (later on DVD), supplemented by annual updates. The first rudimentary internet version of the "Census" database was available to subscribers from 2000 onwards. 

After the financial support from the "Federal Ministry of Education and Research" had ended, additional funding was obtained through the Academies Programme of the "Union of German Academies of Sciences and Humanities" in 2003 and the "Census" became a long-term project of the "Berlin-Brandenburg Academy of Sciences and Humanities", who cooperated with the Humboldt University’s project until the end of 2017. During this time, the database software was completely renewed once again, this time into a browser-based web application. Using a highly individualised version of the digital asset management system Easydb, the "Census" has been accessible online with open access since 2007. 
Since 1999, the "Census" annually publishes the multilingual periodical "Pegasus – Berliner Beiträge zum Nachleben der Antike". The journal serves as a forum for all disciplines concerned with the reception of antiquity and broadens the view to all post-antique periods. In addition, research results emerging directly from the work with the "Census" database are presented here.
Furthermore, the book series "Cyriacus – Studien zur Rezeption der Antike" is edited by the "Census" together with the "Winckelmann Society" (Stendal) and the "Winckelmann Institute of Classical Archeology" at "Humboldt University of Berlin". It serves as a publication platform for conference proceedings and monographs. 
The "Census" is cooperating with the projects "Corpus Winckelmann" and "Corpus Medii Aevi", which document the reception and transformations of antiquity of other periods within the same database.
The "Corpus Winckelmann" or "Corpus of Antique Works of Art Known by Johann Joachim Winckelmann and his Contemporaries" is a database curated by the "Winckelmann Society" (Stendal) and collects the written and visual documents of the 17th and 18th centuries and especially Johann Joachim Winckelmann’s text quotations referring to specific ancient monuments.

The "Corpus Medii Aevi" is a project of the "Adolph Goldschmidt Centre for the Study of Romanesque Sculpture". Here lies the focus on medieval works of art, that show the adaptation and transformation of antique imagery and motives during the Middle Ages.


</doc>
<doc id="61526729" url="https://en.wikipedia.org/wiki?curid=61526729" title="Sensorvault">
Sensorvault

Sensorvault is the name of an internal Google database that contains records of users' historical geo-location data. 
It has been used by law enforcement to search for all devices within the vicinity of a crime, 

Members of the United States House Committee on Energy and Commerce sent a letter to Google CEO Sundar Pichai inquiring about many aspects of this database, such as what data is stored and how it is used; which affiliates and subsidiaries have access to the data or derived analytics; whether Google has other similar databases; who can access it; and whether Google sells, licenses or otherwise discloses the data to third parties in addition to law enforcement.


</doc>
<doc id="62453648" url="https://en.wikipedia.org/wiki?curid=62453648" title="Babelscores">
Babelscores

BabelScores is a digital library of musical scores devoted to contemporary music . The company was established in 2009 and provides a platform for the circulation and promotion of music written in the last 40 years. With an international catalogue with composers from all over the world, BabelScores' collection is entirely dedicated to contemporary music. 

During the year 2009, a group of students in Musical composition at the Conservatoire de Paris, came up with the idea of creating a digital music library for the most recent contemporary music. The creation of BabelScores and its digital platform is based on the wish of promoting the work of living composers. With a global approach, the main objective of the platform is to provide a circulation tool for musicians, universities, conservatories, ensembles, orchestras, musicologists and festivals throughout the world. The online platform was officially launched in 2010. 

Throughout the years 2009 and 2019, BabelScores has gathered a collection of over 210.000 pages of music, thus becoming a relevant digital platform for contemporary music today.

BabelScores library collects selected music from more than 350 composers. It offers completely digitized scores (issued by partner publishing houses or by BabelScores) displayed in a booklet-style reader, with a zoom and full-screen options, and streamed audio recordings.

The search engine of the library provides several search categories such as composer, genre (vocal music, instrumental music, etc.), date, instrument, geographical region or difficulty level. The library also features a bio-page for each composer. It presents a biography that includes education, career, music influences as well as other pieces.

BabelScores' interface is available in French, English and Spanish.

Access to Babelscores’ library is offered by subscription individually or through partners such as universities and music centres. BabelScores is licensed to several academic institutions such as Conservatoire Superieur National de Paris, Bibliothèque Nationale de France, Columbia University, Bayerische Staatsbibliothek, Dartmouth College, Harvard University, Juilliard School of Music, IRCAM Centre Pompidou, The Hong Kong Academy for Performing Arts, The University of Edinburgh, Musikhochschule Lübeck, Hochschule für Musik und Theater Hamburg, Yale University.



</doc>
