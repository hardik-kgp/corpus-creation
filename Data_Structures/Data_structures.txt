<doc id="2119394" url="https://en.wikipedia.org/wiki?curid=2119394" title="Introduction">
Introduction

Introduction, The Introduction, Intro, or The Intro may refer to:








</doc>
<doc id="8519" url="https://en.wikipedia.org/wiki?curid=8519" title="Data structure">
Data structure

In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.

Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.

Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers.

Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.

Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. 

The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).

There are numerous types of data structures, generally built upon simpler primitive data types:

In addition, "graphs" and "binary trees" are other commonly used data structures.

Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.

Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.

Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.

Many known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.





</doc>
<doc id="22474945" url="https://en.wikipedia.org/wiki?curid=22474945" title="Linked data structure">
Linked data structure

In computer science, a linked data structure is a data structure which consists of a set of data records ("nodes") linked together and organized by references ("links" or "pointers"). The link between data can also be called a connector.

In linked data structures, the links are usually treated as special data types that can only be dereferenced or compared for equality. Linked data structures are thus contrasted with arrays and other data structures that require performing arithmetic operations on pointers. This distinction holds even when the nodes are actually implemented as elements of a single array, and the references are actually array indices: as long as no arithmetic is done on those indices, the data structure is essentially a linked one.

Linking can be done in two ways using dynamic allocation and using array index linking.

Linked data structures include linked lists, search trees, expression trees, and many other widely used data structures. They are also key building blocks for many efficient algorithms, such as topological sort and set union-find.

A linked list is a collection of structures ordered not by their physical placement in memory but by logical links that are stored as part of the data in the structure itself. It is not necessary that it should be stored in the adjacent memory locations. Every structure has a data field and an address field. The Address field contains the address of its successor.

Linked list can be singly, doubly or multiply linked and can either be linear or circular.



This is an example of the node class used to store integers in a Java implementation of a linked list:

This is an example of the node structure used for implementation of linked list in C:

This is an example using typedefs:

Note: A structure like this which contains a member that points to the same structure is called a self-referential structure.

This is an example of the node class structure used for implementation of linked list in C++:

A search tree is a tree data structure in whose nodes data values can be stored from some ordered set, which is such that in an in-order traversal of the tree the nodes are visited in ascending order of the stored values.



Compared to arrays, linked data structures allow more flexibility in organizing the data and in allocating space for it. In arrays, the size of the array must be specified precisely at the beginning, which can be a potential waste of memory, or an arbitrary limitation which would later hinder functionality in some way. A linked data structure is built dynamically and never needs to be bigger than the program requires. It also requires no guessing at creation time, in terms of how much space must be allocated. This is a feature that is key in avoiding wastes of memory.

In an array, the array elements have to be in a contiguous (connected and sequential) portion of memory. But in a linked data structure, the reference to each node gives users the information needed to find the next one. The nodes of a linked data structure can also be moved individually to different locations within physical memory without affecting the logical connections between them, unlike arrays. With due care, a certain process or thread can add or delete nodes in one part of a data structure even while other processes or threads are working on other parts.

On the other hand, access to any particular node in a linked data structure requires following a chain of references that are stored in each node. If the structure has "n" nodes, and each node contains at most "b" links, there will be some nodes that cannot be reached in less than log "n" steps, slowing down the process of accessing these nodes - this sometimes represents a considerable slowdown, especially in the case of structures containing large numbers of nodes. For many structures, some nodes may require worst case up to "n"−1 steps. In contrast, many array data structures allow access to any element with a constant number of operations, independent of the number of entries.

Broadly the implementation of these linked data structure is through dynamic data structures. It gives us the chance to use particular space again. Memory can be utilized more efficiently by using these data structures. Memory is allocated as per the need and when memory is not further needed, deallocation is done.

Linked data structures may also incur in substantial memory allocation overhead (if nodes are allocated individually) and frustrate memory paging and processor caching algorithms (since they generally have poor locality of reference). In some cases, linked data structures may also use more memory (for the link fields) than competing array structures. This is because linked data structures are not contiguous. Instances of data can be found all over in memory, unlike arrays.

In arrays, nth element can be accessed immediately, while in a linked data structure we have to follow multiple pointers so element access time varies according to where in the structure the element is.

In some theoretical models of computation that enforce the constraints of linked structures, such as the pointer machine, many problems require more steps than in the unconstrained random access machine model.



</doc>
<doc id="10122951" url="https://en.wikipedia.org/wiki?curid=10122951" title="Succinct data structure">
Succinct data structure

In computer science, a succinct data structure is a data structure which uses an amount of space that is "close" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations. The concept was originally introduced by Jacobson to encode bit vectors, (unlabeled) trees, and planar graphs. Unlike general lossless data compression algorithms, succinct data structures retain the ability to use them in-place, without decompressing them first. A related notion is that of a compressed data structure, in which the size of the data structure depends upon the particular data being represented.

Suppose that formula_1 is the information-theoretical optimal number of bits needed to store some data. A representation of this data is called:


For example, a data structure that uses formula_5 bits of storage is compact, formula_6 bits is succinct, formula_7 bits is also succinct, and formula_8 bits is implicit.

Implicit structures are thus usually reduced to storing information using some permutation of the input data; the most well-known example of this is the heap.

Succinct indexable dictionaries, also called "rank/select" dictionaries, form the basis of a number of succinct representation techniques, including binary trees, formula_9-ary trees and multisets, as well as suffix trees and arrays. The basic problem is to store a subset formula_10 of a universe formula_11, usually represented as a bit array formula_12 where formula_13 iff formula_14 An indexable dictionary supports the usual methods on dictionaries (queries, and insertions/deletions in the dynamic case) as well as the following operations:



for formula_17.

In other words, formula_18 returns the number of elements equal to formula_19 up to position formula_20 while formula_21 returns the position of the formula_22-th occurrence of formula_23.

There is a simple representation which uses formula_24 bits of storage space (the original bit array and an formula_25 auxiliary structure) and supports rank and select in constant time. It uses an idea similar to that for range-minimum queries; there are a constant number of recursions before stopping at a subproblem of a limited size. The bit array formula_26 is partitioned into "large blocks" of size formula_27 bits and "small blocks" of size formula_28 bits. For each large block, the rank of its first bit is stored in a separate table formula_29; each such entry takes formula_30 bits for a total of formula_31 bits of storage. Within a large block, another directory formula_32 stores the rank of each of the formula_33 small blocks it contains. The difference here is that it only needs formula_34 bits for each entry, since only the differences from the rank of the first bit in the containing large block need to be stored. Thus, this table takes a total of formula_35 bits. A lookup table formula_36 can then be used that stores the answer to every possible rank query on a bit string of length formula_37 for formula_38; this requires formula_39 bits of storage space. Thus, since each of these auxiliary tables take formula_25 space, this data structure supports rank queries in formula_41 time and formula_24 bits of space.

To answer a query for formula_43 in constant time, a constant time algorithm computes:


In practice, the lookup table formula_36 can be replaced by bitwise operations and smaller tables that can be used to find the number of bits set in the small blocks. This is often beneficial, since succinct data structures find their uses in large data sets, in which case cache misses become much more frequent and the chances of the lookup table being evicted from closer CPU caches becomes higher. Select queries can be easily supported by doing a binary search on the same auxiliary structure used for rank; however, this takes formula_46 time in the worst case. A more complicated structure using formula_47 bits of additional storage can be used to support select in constant time. In practice, many of these solutions have hidden constants in the formula_48 notation which dominate before any asymptotic advantage becomes apparent; implementations using broadword operations and word-aligned blocks often perform better in practice.

The formula_24 space approach can be improved by noting that there are formula_50 distinct formula_51-subsets of formula_52 (or binary strings of length formula_53 with exactly formula_51 1’s), and thus formula_55 is an information theoretic lower bound on the number of bits needed to store formula_26. There is a succinct (static) dictionary which attains this bound, namely using formula_57 space. This structure can be extended to support rank and select queries and takes formula_58 space. Correct rank queries in this structure are however limited to elements contained in the set, analogous to how minimal perfect hashing functions work. This bound can be reduced to a space/time tradeoff by reducing the storage space of the dictionary to formula_59 with queries taking formula_60 time.

A null-terminated string (C string) takes "Z" + 1 space, and is thus implicit. A string with an arbitrary length (Pascal string) takes "Z" + log("Z") space, and is thus succinct. If there is a maximum length – which is the case in practice, since 2 = 4 GiB of data is a very long string, and 2 = 16 EiB of data is larger than any string in practice – then a string with a length is also implicit, taking "Z" + "k" space, where "k" is the number of data to represent the maximum length (e.g., 64 bits).

When a sequence of variable-length items (such as strings) needs to be encoded, there are various possibilities. A direct approach is to store a length and an item in each record – these can then be placed one after another. This allows efficient next, but not finding the "k"th item. An alternative is to place the items in order with a delimiter (e.g., null-terminated string). This uses a delimiter instead of a length, and is substantially slower, since the entire sequence must be scanned for delimiters. Both of these are space-efficient. An alternative approach is out-of-band separation: the items can simply be placed one after another, with no delimiters. Item bounds can then be stored as a sequence of length, or better, offsets into this sequence. Alternatively, a separate binary string consisting of 1s in the positions where an item begins, and 0s everywhere else is encoded along with it. Given this string, the formula_61 function can quickly determine where each item begins, given its index. This is "compact" but not "succinct," as it takes 2"Z" space, which is O("Z").

Another example is the representation of a binary tree: an arbitrary binary tree on formula_53 nodes can be represented in formula_63 bits while supporting a variety of operations on any node, which includes finding its parent, its left and right child, and returning the size of its subtree, each in constant time. The number of different binary trees on formula_53 nodes is formula_65formula_66. For large formula_53, this is about formula_68; thus we need at least about formula_69 bits to encode it. A succinct binary tree therefore would occupy only formula_70 bits per node.



</doc>
<doc id="3669635" url="https://en.wikipedia.org/wiki?curid=3669635" title="Implicit data structure">
Implicit data structure

In computer science, an implicit data structure or space-efficient data structure is a data structure that stores very little information other than the main or required data: a data structure that requires low overhead. They are called "implicit" because the position of the elements carries meaning and relationship between elements; this is contrasted with the use of pointers to give an "explicit" relationship between elements. Definitions of "low overhead" vary, but generally means constant overhead; in big O notation, "O"(1) overhead. A less restrictive definition is a succinct data structure, which allows greater overhead.

Formally, an implicit data structure is one with constant space overhead (above the information-theoretic lower bound).

Historically, defined an implicit data structure (and algorithms acting on one) as one "in which structural information is implicit in the way data are stored, rather than explicit in pointers." They are somewhat vague in the definition, defining it most strictly as a single array, with only the size retained (a single number of overhead), or more loosely as a data structure with constant overhead (). This latter definition is today more standard, and the still-looser notion of a data structure with non-constant but small overhead is today known as a succinct data structure, as defined by ; it was referred to as semi-implicit by .

A fundamental distinction is between "static" data structures (read-only) and "dynamic" data structures (which can be modified). Simple implicit data structures, such as representing a sorted list as an array, may be very efficient as a static data structure, but inefficient as a dynamic data structure, due to modification operations (such as insertion in the case of a sorted list) being inefficient.

A trivial example of an implicit data structure is an "array data structure", which is an implicit data structure for a list, and requires only the constant overhead of the length; unlike a linked list, which has a pointer associated with each data element, which "explicitly" gives the relationship from one element to the next. Similarly, a "null-terminated string" is an implicit data structure for a string (list of characters). These are considered very simple because they are static data structures (read-only), and only admit the simple operation of iteration over the elements.

Similarly simple is representing a multi-dimensional array as a single 1-dimensional array, together with its dimensions. For example, representing an "m" × "n" array as a single list of length "m·n", together with the numbers "m" and "n" (instead of as a 1-dimensional array of pointers to each 1-dimensional subarray). The elements need not be of the same type, and a table of data (a list of records) may similarly be represented implicitly as a flat (1-dimensional) list, together with the length of each field, so long as each field has uniform size (so a single size can be used per field, not per record).

A less trivial example is representing a sorted list by a "sorted array", which allows search in logarithmic time by binary search. Contrast with a search tree, specifically a binary search tree, which also allows logarithmic-time search, but requires pointers. A sorted array is only efficient as a static data structure, as modifying the list is slow – unlike a binary search tree – but does not require the space overhead of a tree.

An important example of an implicit data structure is representing a perfect binary tree as a list, in increasing order of depth, so root, first left child, first right child, first left child of first left child, etc. Such a tree occurs notably for an ancestry chart to a give depth, and the implicit representation is known as an "Ahnentafel" (ancestor table).

This can be generalized to a complete binary tree (where the last level may be incomplete), which yields the best-known example of an implicit data structure, namely the "binary heap", which is an implicit data structure for a priority queue. This is more sophisticated than earlier examples because it allows multiple operations, and is an efficient "dynamic" data structure (it allows efficient modification of the data): not only top, but also insert and pop.

More sophisticated implicit data structures include the beap (bi-parental heap).

The trivial examples of lists or tables of values date to prehistory, while historically non-trivial implicit data structures date at least to the Ahnentafel, which was introduced by Michaël Eytzinger in 1590 for use in genealogy. In formal computer science, the first implicit data structure is generally considered to be the sorted list, used for binary search, which was introduced by John Mauchly in 1946, in the Moore School Lectures, the first ever set of lectures regarding any computer-related topic. The binary heap was introduced in to implement the heapsort. The notion of an implicit data structure was formalized in , as part of introducing and analyzing the beap.



See publications of Hervé Brönnimann, J. Ian Munro, and Greg Frederickson.


</doc>
<doc id="24757213" url="https://en.wikipedia.org/wiki?curid=24757213" title="Compressed data structure">
Compressed data structure

The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science. It refers to a data structure whose operations are roughly as fast as those of a conventional data structure for the problem, but whose size can be substantially smaller. The size of the compressed data structure is typically highly dependent upon the entropy of the data being represented. 

Important examples of compressed data structures include the compressed suffix array and the FM-index, both of which can represent an arbitrary text of characters "T" for pattern matching. Given any input pattern "P", they support the operation of finding if and where "P" appears in "T". The search time is proportional to the sum of the length of pattern "P", a very slow-growing function of the length of the text "T", and the number of reported matches. The space they occupy is roughly equal to the size of the text "T" in entropy-compressed form, such as that obtained by Prediction by Partial Matching or gzip. Moreover, both data structures are self-indexing, in that they can reconstruct the text "T" in a random access manner, and thus the underlying text "T" can be discarded. In other words, they simultaneously provide a compressed and quickly searchable representation of the text "T". They represent a substantial space improvement over the conventional suffix tree and suffix array, which occupy many times more space than the size of "T". They also support searching for arbitrary patterns, as opposed to the inverted index, which can support only word-based searches. In addition, inverted indexes do not have the self-indexing feature. 

An important related notion is that of a succinct data structure, which uses space roughly equal to the information-theoretic minimum, which is a worst-case notion of the space needed to represent the data. In contrast, the size of a compressed data structure depends upon the particular data being represented. When the data are compressible, as is often the case in practice for natural language text, the compressed data structure can occupy space very close to the information-theoretic minimum, and significantly less space than most compression schemes.


</doc>
<doc id="24019691" url="https://en.wikipedia.org/wiki?curid=24019691" title="Search data structure">
Search data structure

In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database.

The simplest, most general, and least efficient search structure is merely an unordered sequential list of all the items. Locating the desired item in such a list, by the linear search method, inevitably requires a number of operations proportional to the number "n" of items, in the worst case as well as in the average case. Useful search data structures allow faster retrieval; however, they are limited to queries of some specific kind. Moreover, since the cost of building such structures is at least proportional to "n", they only pay off if several queries are to be performed on the same database (or on a database that changes little between queries).

Static search structures are designed for answering many queries on a fixed database; dynamic structures also allow insertion, deletion, or modification of items between successive queries. In the dynamic case, one must also consider the cost of fixing the search structure to account for the changes in the database.

The simplest kind of query is to locate a record that has a specific field (the "key") equal to a specified value "v". Other common kinds of query are "find the item with smallest (or largest) key value", "find the item with largest key value not exceeding "v"", "find all items with key values between specified bounds "v" and "v"".

In certain databases the key values may be points in some multi-dimensional space. For example, the key may be a geographic position (latitude and longitude) on the Earth. In that case, common kinds of queries are "find the record with a key closest to a given point "v"", or "find all items whose key lies at a given distance from "v"", or "find all items within a specified region "R" of the space".

A common special case of the latter are simultaneous range queries on two or more simple keys, such as "find all employee records with salary between 50,000 and 100,000 and hired between 1995 and 2007".



In this table, the asymptotic notation "O"("f"("n")) means "not exceeding some fixed multiple of "f"("n") in the worst case."

"Note": Insert on an unsorted array is sometimes quoted as being "O"("n") due to the assumption that the element to be inserted must be inserted at one particular location of the array, which would require shifting all the subsequent elements by one position. However, in a classic array, the array is used to store arbitrary unsorted elements, and hence the exact position of any given element is of no consequence, and insert is carried out by increasing the array size by 1 and storing the element at the end of the array, which is a "O"(1) operation. Likewise, the deletion operation is sometimes quoted as being "O"("n") due to the assumption that subsequent elements must be shifted, but in a classic unsorted array the order is unimportant (though elements are implicitly ordered by insert-time), so deletion can be carried out by swapping the element to be deleted with the last element in the array and then decrementing the array size by 1, which is a "O"(1) operation.

This table is only an approximate summary; for each data structure there are special situations and variants that may lead to different costs. Also two or more data structures can be combined to obtain lower costs.



</doc>
<doc id="662889" url="https://en.wikipedia.org/wiki?curid=662889" title="Persistent data structure">
Persistent data structure

In computing, a persistent data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. The term was introduced in Driscoll, Sarnak, Sleator, and Tarjans' 1986 article.

A data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent. Structures that are not persistent are called "ephemeral".

These types of data structures are particularly common in logical and functional programming.

In the partial persistence model, a programmer may query any previous version of a data structure, but may only update the latest version. This implies a linear ordering among each version of the data structure. In fully persistent model, both updates and queries are allowed on any version of the data structure. In some cases the performance characteristics of querying or updating older versions of a data structure may be allowed to degrade, as is true with the Rope data structure. In addition, a data structure can be referred to as confluently persistent if, in addition to being fully persistent, two versions of the same data structure can be combined to form a new version which is still fully persistent.

One method for creating a persistent data structure is to use a platform provided ephemeral data structure such as an array to store the data in the data structure and copy the entirety of that data structure using copy on write semantics for any updates to the data structure. This is an inefficient technique because the entire backing data structure must be copied for each write, leading to worst case O(n·m) performance characteristics for m modifications of an array of size n.

Fat node method is to record all changes made to node fields in the nodes themselves, without erasing old values of the fields. This requires that nodes be allowed to become arbitrarily “fat”. In other words, each fat node contains the same information and pointer fields as an ephemeral node, along with space for an arbitrary number of extra field values. Each extra field value has an associated field name and a version stamp which indicates the version in which the named field was changed to have the specified value. Besides, each fat node has its own version stamp, indicating the version in which the node was created. The only purpose of nodes having version stamps is to make sure that each node only contains one value per field name per version. In order to navigate through the structure, each original field value in a node has a version stamp of zero.

With using fat node method, it requires O(1) space for every modification: just store the new data. Each modification takes O(1) additional time to store the modification at the end of the modification history. This is an amortized time bound, assuming modification history is stored in a growable array. At access time, the right version at each node must be found as the structure is traversed. If "m" modifications were to be made, then each access operation would have O(log m) slowdown resulting from the cost of finding the nearest modification in the array.

With the path copying method a copy of all nodes is made on the path to any node which is about to be modified. These changes must then be cascaded back through the data structure: all nodes that pointed to the old node must be modified to point to the new node instead. These modifications cause more cascading changes, and so on, until the root node is reached.

With m modifications, this costs O(log m) additive lookup time. Modification time and space are bounded by the size of the longest path in the data structure and the cost of the update in the ephemeral data structure. In a Balanced Binary Search Tree without parent pointers the worst case modification time complexity is O(log n + update cost). However, in a linked list the worst case modification time complexity is O(n + update cost).

Sleator, Tarjan et al. came up with a way to combine the techniques of fat nodes and path copying, achieving O(1) access slowdown and O(1) modification space and time complexity.

In each node, one modification box is stored. This box can hold one modification to the node—either a modification to one of the pointers, or to the node's key, or to some other piece of node-specific data—and a timestamp for when that modification was applied. Initially, every node's modification box is empty.

Whenever a node is accessed, the modification box is checked, and its timestamp is compared against the access time. (The access time specifies the version of the data structure being considered.) If the modification box is empty, or the access time is before the modification time, then the modification box is ignored and only the normal part of the node is considered. On the other hand, if the access time is after the modification time, then the value in the modification box is used, overriding that value in the node.

Modifying a node works like this. (It is assumed that each modification touches one pointer or similar field.) If the node's modification box is empty, then it is filled with the modification. Otherwise, the modification box is full. A copy of the node is made, but using only the latest values. The modification is performed directly on the new node, without using the modification box. (One of the new node's fields overwritten and its modification box stays empty.) Finally, this change is cascaded to the node's parent, just like path copying. (This may involve filling the parent's modification box, or making a copy of the parent recursively. If the node has no parent—it's the root—it is added the new root to a sorted array of roots.)

With this algorithm, given any time t, at most one modification box exists in the data structure with time t. Thus, a modification at time t splits the tree into three parts: one part contains the data from before time t, one part contains the data from after time t, and one part was unaffected by the modification.

Time and space for modifications require amortized analysis. A modification takes O(1) amortized space, and O(1) amortized time. To see why, use a potential function ϕ, where ϕ(T) is the number of full live nodes in T . The live nodes of T are just the nodes that are reachable from the current root at the current time (that is, after the last modification). The full live nodes are the live nodes whose modification boxes are full.

Each modification involves some number of copies, say k, followed by 1 change to a modification box. Consider each of the k copies. Each costs O(1) space and time, but decreases the potential function by one. (First, the node to be copied must be full and live, so it contributes to the potential function. The potential function will only drop, however, if the old node isn't reachable in the new tree. But it is known that it isn't reachable in the new tree—the next step in the algorithm will be to modify the node's parent to point at the copy. Finally, it is known that the copy's modification box is empty. Thus, replaced a full live node has been replaced with an empty live node, and ϕ goes down by one.) The final step fills a modification box, which costs O(1) time and increases ϕ by one.

Putting it all together, the change in ϕ is Δϕ =1− k. Thus, the algorithm takes O(k +Δϕ)= O(1) space and O(k +Δϕ +1) = O(1) time

Perhaps the simplest persistent data structure is the singly linked list or "cons"-based list, a simple list of objects formed by each carrying a reference to the next in the list. This is persistent because the "tail" of the list can be taken, meaning the last "k" items for some "k", and new nodes can be added in front of it. The tail will not be duplicated, instead becoming shared between both the old list and the new list. So long as the contents of the tail are immutable, this sharing will be invisible to the program.

Many common reference-based data structures, such as red–black trees, stacks, and treaps, can easily be adapted to create a persistent version. Some others need slightly more effort, for example: queues, dequeues, and extensions including min-deques (which have an additional "O"(1) operation "min" returning the minimal element) and random access deques (which have an additional operation of random access with sub-linear, most often logarithmic, complexity).

There also exist persistent data structures which use destructive operations, making them impossible to implement efficiently in purely functional languages (like Haskell outside specialized monads like state or IO), but possible in languages like C or Java. These types of data structures can often be avoided with a different design. One primary advantage to using purely persistent data structures is that they often behave better in multi-threaded environments.

Singly linked lists are the bread-and-butter data structure in functional languages. ML-derived languages, like Haskell, that are purely functional because once a node in the list has been allocated, it cannot be modified, only copied, referenced or destroyed by the garbage collector when nothing refers to it. (Note that ML itself is not purely functional, but supports non-destructive list operations subset, that is also true in the Lisp (LISt Processing) functional language dialects like Scheme and Racket.)

Consider the two lists:
These would be represented in memory by:

where a circle indicates a node in the list (the arrow out representing the second element of the node which is a pointer to another node).

Now concatenating the two lists:
results in the following memory structure:

Notice that the nodes in list codice_1 have been copied, but the nodes in codice_2 are shared. As a result, the original lists (codice_1 and codice_2) persist and have not been modified.

The reason for the copy is that the last node in codice_1 (the node containing the original value codice_6) cannot be modified to point to the start of codice_2, because that would change the value of codice_1.

Consider a binary search tree, where every node in the tree has the recursive invariant that all subnodes contained in the left subtree have a value that is less than or equal to the value stored in the node, and subnodes contained in the right subtree have a value that is greater than the value stored in the node.

For instance, the set of data
might be represented by the following binary search tree:

A function which inserts data into the binary tree and maintains the invariant is:After executing
The following configuration is produced:

Notice two points: first, the original tree (codice_1) persists. Second, many common nodes are shared between the old tree and the new tree. Such persistence and sharing is difficult to manage without some form of garbage collection (GC) to automatically free up nodes which have no live references, and this is why GC is a feature commonly found in functional programming languages.

A persistent hash array mapped trie is a specialized variant of a hash array mapped trie that will preserve previous versions of itself on any updates. It is often used to implement a general purpose persistent map data structure.

Hash array mapped tries were originally described in a 2001 paper by Phil Bagwell entitled "Ideal Hash Trees". This paper presented a mutable Hash table where "Insert, search and delete times are small and constant, independent of key set size, operations are O(1). Small worst-case times for insert, search and removal operations can be guaranteed and misses cost less than successful searches". This data structure was then modified by Rich Hickey to be fully persistent for use in the Clojure programming language.

Conceptually, hash array mapped tries work similar to any generic tree in that they store nodes hierarchically and retrieve them by following a path down to a particular element. The key difference is that Hash Array Mapped Tries first use a hash function to transform their lookup key into a (usually 32 or 64 bit) integer. The path down the tree is then determined by using slices of the binary representation of that integer to index into a sparse array at each level of the tree. The leaf nodes of the tree behave similar to the buckets used to construct hash tables and may or may not contain multiple candidates depending on hash collisions.

Most implementations of persistent hash array mapped tries use a branching factor of 32 in their implementation. This means that in practice while insertions, deletions, and lookups into a persistent hash array mapped trie have a computational complexity of "O"(log "n"), for most applications they are effectively constant time, as it would require an extremely large number of entries to make any operation take more than a dozen steps.

Haskell is a pure functional language and therefore does not allow for mutation. Therefore, all data structures in the language are persistent, as it is impossible to not preserve the previous state of a data structure with functional semantics. This is because any change to a data structure that would render previous versions of a data structure invalid would violate referential transparency.

In its standard library Haskell has efficient persistent implementations for linked lists, Maps (implemented as size balanced trees), and Sets among others.

Like many programming languages in the Lisp family, Clojure contains an implementation of a linked list, but unlike other dialects its implementation of a Linked List has enforced persistence instead of being persistent by convention. Clojure also has syntax literals for efficient implementations of persistent vectors, maps, and sets based on persistent hash array mapped tries. These data structures implement the mandatory read-only parts of the Java collections framework.

The designers of the Clojure language advocate the use of persistent data structures over mutable data structures because they have value semantics which gives the benefit of making them freely shareable between threads with cheap aliases, easy to fabricate, and language independent.

These data structures form the basis of Clojure's support for parallel computing since they allow for easy retries of operations to sidestep data races and atomic compare and swap semantics.

The Elm programming language is purely functional like Haskell, which makes all of its data structures persistent by necessity. It contains persistent implementations of linked lists as well as persistent arrays, dictionaries, and sets.

Elm uses a custom virtual DOM implementation that takes advantage of the persistent nature of Elm data. As of 2016 it was reported by the developers of Elm that this virtual DOM allows the Elm language to render HTML faster than the popular JavaScript frameworks React, Ember, and Angular.

The popular JavaScript frontend framework React is frequently used along with a state management system that implements the Flux architecture, a popular implementation of which is the JavaScript library Redux. The Redux library is inspired by the state management pattern used in the Elm programming language, meaning that it mandates that users treat all data as persistent. As a result, the Redux project recommends that in certain cases users make use of libraries for enforced and efficient persistent data structures. This reportedly allows for greater performance than when comparing or making copies of regular JavaScript objects.

One such library of persistent data structures Immutable.js is based on the data structures made available and popularized by Clojure and Scala. It is mentioned by the documentation of Redux as being one of the possible libraries that can provide enforced immutability.

The Scala programming language promotes the use of persistent data structures for implementing programs using "Object-Functional Style". Scala contains implementations of many Persistent data structures including Linked Lists, Red–black trees, as well as persistent hash array mapped tries as introduced in Clojure.

Because persistent data structures are often implemented in such a way that successive versions of a data structure share underlying memory ergonomic use of such data structures generally requires some form of automatic garbage collection system such as reference counting or mark and sweep. In some platforms where persistent data structures are used it is an option to not use garbage collection which, while doing so can lead to memory leaks, can in some cases have a positive impact on the overall performance of an application.




</doc>
<doc id="25175778" url="https://en.wikipedia.org/wiki?curid=25175778" title="Concurrent data structure">
Concurrent data structure

In computer science, a concurrent data structure is a
particular way of storing and organizing data for access by
multiple computing threads (or processes) on a computer.

Historically, such data structures were used on uniprocessor
machines with operating systems that supported multiple
computing threads (or processes). The term concurrency captured the
multiplexing/interleaving of the threads' operations on the
data by the operating system, even though the processors never
issued two operations that accessed the data simultaneously.

Today, as multiprocessor computer architectures that provide
parallelism become the dominant computing platform (through the
proliferation of multi-core processors), the term has come to
stand mainly for data structures that can be accessed by multiple
threads which may actually access the data simultaneously because
they run on different processors that communicate with one another.
The concurrent data structure (sometimes also called a "shared data structure") is usually considered to reside in an abstract storage
environment called shared memory, though this memory may be
physically implemented as either a "tightly coupled" or a
distributed collection of storage modules.

Concurrent data structures, intended for use in
parallel or distributed computing environments, differ from
"sequential" data structures, intended for use on a uni-processor
machine, in several ways. Most notably, in a sequential environment
one specifies the data structure's properties and checks that they
are implemented correctly, by providing safety properties. In
a concurrent environment, the specification must also describe
liveness properties which an implementation must provide.
Safety properties usually state that something bad never happens,
while liveness properties state that something good keeps happening.
These properties can be expressed, for example, using Linear Temporal Logic.

The type of liveness requirements tend to define the data structure.
The method calls can be blocking or non-blocking. Data structures are not
restricted to one type or the other, and can allow combinations
where some method calls are blocking and others are non-blocking
(examples can be found in the Java concurrency software
library).

The safety properties of concurrent data structures must capture their 
behavior given the many possible interleavings of methods
called by different threads. It is quite
intuitive to specify how abstract data structures
behave in a sequential setting in which there are no interleavings.
Therefore, many mainstream approaches for arguing the safety properties of a
concurrent data structure (such as serializability, linearizability, sequential consistency, and
quiescent consistency ) specify the structures properties
sequentially, and map its concurrent executions to
a collection of sequential ones. 

In order to guarantee the safety and liveness properties, concurrent
data structures must typically (though not always) allow threads to
reach consensus as to the results
of their simultaneous data access and modification requests. To
support such agreement, concurrent data structures are implemented
using special primitive synchronization operations (see synchronization primitives)
available on modern multiprocessor machines
that allow multiple threads to reach consensus. This consensus can be achieved in a blocking manner by using locks, or without locks, in which case it is non-blocking. There is a wide body
of theory on the design of concurrent data structures (see
bibliographical references).

Concurrent data structures are significantly more difficult to design
and to verify as being correct than their sequential counterparts.

The primary source of this additional difficulty is concurrency, exacerbated by the fact that 
threads must be thought of as being completely asynchronous:
they are subject to operating system preemption, page faults,
interrupts, and so on. 

On today's machines, the layout of processors and
memory, the layout of data in memory, the communication load on the
various elements of the multiprocessor architecture all influence performance.
Furthermore, there is a tension between correctness and performance: algorithmic enhancements that seek to improve performance often make it more difficult to design and verify a correct
data structure implementation.

A key measure for performance is scalability, captured by the speedup of the implementation. Speedup is a measure of how
effectively the application is utilizing the machine it is running
on. On a machine with P processors, the speedup is the ratio of the structures execution time on a single processor to its execution time on T processors. Ideally, we want linear speedup: we would like to achieve a
speedup of P when using P processors. Data structures whose
speedup grows with P are called scalable. The extent to which one can scale the performance of a concurrent data structure is captured by a formula known as Amdahl's law and 
more refined versions of it such as Gustafson's law.

A key issue with the performance of concurrent data structures is the level of memory contention: the overhead in traffic to and from memory as a
result of multiple threads concurrently attempting to access the same
locations in memory. This issue is most acute with blocking implementations 
in which locks control access to memory. In order to
acquire a lock, a thread must repeatedly attempt to modify that
location. On a cache-coherent
multiprocessor (one in which processors have
local caches that are updated by hardware in order to keep them
consistent with the latest values stored) this results in long
waiting times for each attempt to modify the location, and is 
exacerbated by the additional memory traffic associated with
unsuccessful attempts to acquire the lock.





</doc>
<doc id="2349" url="https://en.wikipedia.org/wiki?curid=2349" title="Abstract data type">
Abstract data type

In computer science, an abstract data type (ADT) is a mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a "user" of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.

Formally, an ADT may be defined as a "class of objects whose logical behavior is defined by a set of values and a set of operations"; this is analogous to an algebraic structure in mathematics. What is meant by "behavior" varies by author, with the two main types of formal specifications for behavior being "axiomatic (algebraic) specification" and an "abstract model;" these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity ("cost"), both in terms of time (for computing operations) and space (for representing values). In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.

ADTs are a theoretical concept in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.

For example, integers are an ADT, defined as the values ..., −2, −1, 0, 1, 2, ..., and by the operations of addition, subtraction, multiplication, and division, together with greater than, less than, etc., which behave according to familiar mathematics (with care for integer division), independently of how the integers are represented by the computer. Explicitly, "behavior" includes obeying various axioms (associativity and commutativity of addition etc.), and preconditions on operations (cannot divide by zero). Typically integers are represented in a data structure as binary numbers, most often as two's complement, but might be binary-coded decimal or in ones' complement, but the user is abstracted from the concrete choice of representation, and can simply use the data as data types.

An ADT consists not only of operations, but also of values of the underlying data and of constraints on the operations. An "interface" typically refers only to the operations, and perhaps some of the constraints on the operations, notably pre-conditions and post-conditions, but not other constraints, such as relations between the operations.

For example, an abstract stack, which is a last-in-first-out structure, could be defined by three operations: push, that inserts a data item onto the stack; pop, that removes a data item from it; and peek or top, that accesses a data item on top of the stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a data item into the queue; dequeue, that removes the first data item from it; and front, that accesses and serves the first data item in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many data items have been pushed into the stack, and that the stack uses a constant amount of storage for each element.

Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages. However, an ADT may be implemented by specific data types or data structures, in many ways and in many programming languages; or described in a formal specification language. ADTs are often implemented as modules: the module's interface declares procedures that correspond to the ADT operations, sometimes with comments that describe the constraints. This information hiding strategy allows the implementation of the module to be changed without disturbing the client programs.

The term abstract data type can also be regarded as a generalized approach of a number of algebraic structures, such as lattices, groups, and rings. The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development.

An abstract data type is defined as a mathematical model of the data objects that make up a data type as well as the functions that operate on these objects.
There are no standard conventions for defining them. A broad division may be drawn between "imperative" and "functional" definition styles.

In the philosophy of imperative programming languages, an abstract data structure is conceived as an entity that is "mutable"—meaning that it may be in different "states" at different times. Some operations may change the state of the ADT; therefore, the order in which operations are evaluated is important, and the same operation on the same entities may have different effects if executed at different times—just like the instructions of a computer, or the commands and procedures of an imperative language. To underscore this view, it is customary to say that the operations are "executed" or "applied", rather than "evaluated". The imperative style is often used when describing abstract algorithms. (See The Art of Computer Programming by Donald Knuth for more details)

Imperative-style definitions of ADT often depend on the concept of an "abstract variable", which may be regarded as the simplest non-trivial ADT. An abstract variable "V" is a mutable entity that admits two operations:
with the constraint that

As in so many programming languages, the operation store("V", "x") is often written "V" ← "x" (or some similar notation), and fetch("V") is implied whenever a variable "V" is used in a context where a value is required. Thus, for example, "V" ← "V" + 1 is commonly understood to be a shorthand for store("V",fetch("V") + 1).

In this definition, it is implicitly assumed that storing a value into a variable "U" has no effect on the state of a distinct variable "V". To make this assumption explicit, one could add the constraint that

More generally, ADT definitions often assume that any operation that changes the state of one ADT instance has no effect on the state of any other instance (including other instances of the same ADT) — unless the ADT axioms imply that the two instances are connected (aliased) in that sense. For example, when extending the definition of abstract variable to include abstract records, the operation that selects a field from a record variable "R" must yield a variable "V" that is aliased to that part of "R".

The definition of an abstract variable "V" may also restrict the stored values "x" to members of a specific set "X", called the "range" or "type" of "V". As in programming languages, such restrictions may simplify the description and analysis of algorithms, and improve their readability.

Note that this definition does not imply anything about the result of evaluating fetch("V") when "V" is "un-initialized", that is, before performing any store operation on "V". An algorithm that does so is usually considered invalid, because its effect is not defined. (However, there are some important algorithms whose efficiency strongly depends on the assumption that such a fetch is legal, and returns some arbitrary value in the variable's range.)

Some algorithms need to create new instances of some ADT (such as new variables, or new stacks). To describe such algorithms, one usually includes in the ADT definition a create() operation that yields an instance of the ADT, usually with axioms equivalent to
This axiom may be strengthened to exclude also partial aliasing with other instances. On the other hand, this axiom still allows implementations of create() to yield a previously created instance that has become inaccessible to the program.

As another example, an imperative-style definition of an abstract stack could specify that the state of a stack "S" can be modified only by the operations
with the constraint that

Since the assignment "V" ← "x", by definition, cannot change the state of "S", this condition implies that "V" ← pop("S") restores "S" to the state it had before the push("S", "x"). From this condition and from the properties of abstract variables, it follows, for example, that the sequence
where "x", "y", and "z" are any values, and "U", "V", "W" are pairwise distinct variables, is equivalent to

Here it is implicitly assumed that operations on a stack instance do not modify the state of any other ADT instance, including other stacks; that is,

An abstract stack definition usually includes also a Boolean-valued function empty("S") and a create() operation that returns a stack instance, with axioms equivalent to

Sometimes an ADT is defined as if only one instance of it existed during the execution of the algorithm, and all operations were applied to that instance, which is not explicitly notated. For example, the abstract stack above could have been defined with operations push("x") and pop(), that operate on "the" only existing stack. ADT definitions in this style can be easily rewritten to admit multiple coexisting instances of the ADT, by adding an explicit instance parameter (like "S" in the previous example) to every operation that uses or modifies the implicit instance.

On the other hand, some ADTs cannot be meaningfully defined without assuming multiple instances. This is the case when a single operation takes two distinct instances of the ADT as parameters. For an example, consider augmenting the definition of the abstract stack with an operation compare("S", "T") that checks whether the stacks "S" and "T" contain the same items in the same order.

Another way to define an ADT, closer to the spirit of functional programming, is to consider each state of the structure as a separate entity. In this view, any operation that modifies the ADT is modeled as a mathematical function that takes the old state as an argument, and returns the new state as part of the result. Unlike the imperative operations, these functions have no side effects. Therefore, the order in which they are evaluated is immaterial, and the same operation applied to the same arguments (including the same input states) will always return the same results (and output states).

In the functional view, in particular, there is no way (or need) to define an "abstract variable" with the semantics of imperative variables (namely, with fetch and store operations). Instead of storing values into variables, one passes them as arguments to functions.

For example, a complete functional-style definition of an abstract stack could use the three operations:

In a functional-style definition there is no need for a create operation. Indeed, there is no notion of "stack instance". The stack states can be thought of as being potential states of a single stack structure, and two stack states that contain the same values in the same order are considered to be identical states. This view actually mirrors the behavior of some concrete implementations, such as linked lists with hash cons.

Instead of create(), a functional-style definition of an abstract stack may assume the existence of a special stack state, the "empty stack", designated by a special symbol like Λ or "()"; or define a bottom() operation that takes no arguments and returns this special stack state. Note that the axioms imply that
In a functional-style definition of a stack one does not need an empty predicate: instead, one can test whether a stack is empty by testing whether it is equal to Λ.

Note that these axioms do not define the effect of top("s") or pop("s"), unless "s" is a stack state returned by a push. Since push leaves the stack non-empty, those two operations are undefined (hence invalid) when "s" = Λ. On the other hand, the axioms (and the lack of side effects) imply that push("s", "x") = push("t", "y") if and only if "x" = "y" and "s" = "t".

As in some other branches of mathematics, it is customary to assume also that the stack states are only those whose existence can be proved from the axioms in a finite number of steps. In the abstract stack example above, this rule means that every stack is a "finite" sequence of values, that becomes the empty stack (Λ) after a finite number of pops. By themselves, the axioms above do not exclude the existence of infinite stacks (that can be poped forever, each time yielding a different state) or circular stacks (that return to the same state after a finite number of pops). In particular, they do not exclude states "s" such that pop("s") = "s" or push("s", "x") = "s" for some "x". However, since one cannot obtain such stack states with the given operations, they are assumed "not to exist".

Aside from the behavior in terms of axioms, it is also possible to include, in the definition of an ADT operation, their algorithmic complexity. Alexander Stepanov, designer of the C++ Standard Template Library, included complexity guarantees in the STL specification, arguing:

Abstraction provides a promise that any implementation of the ADT has certain properties and abilities; knowing these is all that is required to make use of an ADT object. The user does not need any technical knowledge of how the implementation works to use the ADT. In this way, the implementation may be complex but will be encapsulated in a simple interface when it is actually used.

Code that uses an ADT object will not need to be edited if the implementation of the ADT is changed. Since any changes to the implementation must still comply with the interface, and since code using an ADT object may only refer to properties and abilities specified in the interface, changes may be made to the implementation without requiring any changes in code where the ADT is used.

Different implementations of the ADT, having all the same properties and abilities, are equivalent and may be used somewhat interchangeably in code that uses the ADT. This gives a great deal of flexibility when using ADT objects in different situations. For example, different implementations of the ADT may be more efficient in different situations; it is possible to use each in the situation where they are preferable, thus increasing overall efficiency.

Some operations that are often specified for ADTs (possibly under other names) are

In imperative-style ADT definitions, one often finds also

The free operation is not normally relevant or meaningful, since ADTs are theoretical entities that do not "use memory". However, it may be necessary when one needs to analyze the storage used by an algorithm that uses the ADT. In that case one needs additional axioms that specify how much memory each ADT instance uses, as a function of its state, and how much of it is returned to the pool by free.

Some common ADTs, which have proved useful in a great variety of applications, are

Each of these ADTs may be defined in many ways and variants, not necessarily equivalent. For example, an abstract stack may or may not have a count operation that tells how many items have been pushed and not yet popped. This choice makes a difference not only for its clients but also for the implementation.

An extension of ADT for computer graphics was proposed in 1979: an abstract graphical data type (AGDT). It was introduced by Nadia Magnenat Thalmann, and Daniel Thalmann. AGDTs provide the advantages of ADTs with facilities to build graphical objects in a structured way.

Implementing an ADT means providing one procedure or function for each abstract operation. The ADT instances are represented by some concrete data structure that is manipulated by those procedures, according to the ADT's specifications.

Usually there are many ways to implement the same ADT, using several different concrete data structures. Thus, for example, an abstract stack can be implemented by a linked list or by an array.

In order to prevent clients from depending on the implementation, an ADT is often packaged as an "opaque data type" in one or more modules, whose interface contains only the signature (number and types of the parameters and results) of the operations. The implementation of the module—namely, the bodies of the procedures and the concrete data structure used—can then be hidden from most clients of the module. This makes it possible to change the implementation without affecting the clients. If the implementation is exposed, it is known instead as a "transparent data type."

When implementing an ADT, each instance (in imperative-style definitions) or each state (in functional-style definitions) is usually represented by a handle of some sort.

Modern object-oriented languages, such as C++ and Java, support a form of abstract data types. When a class is used as a type, it is an abstract type that refers to a hidden representation. In this model an ADT is typically implemented as a class, and each instance of the ADT is usually an object of that class. The module's interface typically declares the constructors as ordinary procedures, and most of the other ADT operations as methods of that class. However, such an approach does not easily encapsulate multiple representational variants found in an ADT. It also can undermine the extensibility of object-oriented programs.
In a pure object-oriented program that uses interfaces as types, types refer to behaviors not representations.

As an example, here is an implementation of the abstract stack above in the C programming language.
An imperative-style interface might be:
This interface could be used in the following manner:
This interface can be implemented in many ways. The implementation may be arbitrarily inefficient, since the formal definition of the ADT, above, does not specify how much space the stack may use, nor how long each operation should take. It also does not specify whether the stack state "s" continues to exist after a call "x" ← pop("s").

In practice the formal definition should specify that the space is proportional to the number of items pushed and not yet popped; and that every one of the operations above must finish in a constant amount of time, independently of that number. To comply with these additional specifications, the implementation could use a linked list, or an array (with dynamic resizing) together with two integers (an item count and the array size).

Functional-style ADT definitions are more appropriate for functional programming languages, and vice versa. However, one can provide a functional-style interface even in an imperative language like C. For example:
Many modern programming languages, such as C++ and Java, come with standard libraries that implement several common ADTs, such as those listed above.

The specification of some programming languages is intentionally vague about the representation of certain built-in data types, defining only the operations that can be done on them. Therefore, those types can be viewed as "built-in ADTs". Examples are the arrays in many scripting languages, such as Awk, Lua, and Perl, which can be regarded as an implementation of the abstract list.




</doc>
<doc id="43695328" url="https://en.wikipedia.org/wiki?curid=43695328" title="List">
List

A "list" is any enumeration of a set of items. List or lists may also refer to:










</doc>
<doc id="21788730" url="https://en.wikipedia.org/wiki?curid=21788730" title="Stack">
Stack

Stack may refer to:









</doc>
<doc id="4106977" url="https://en.wikipedia.org/wiki?curid=4106977" title="Queue">
Queue

Queue () may refer to:





</doc>
<doc id="24485" url="https://en.wikipedia.org/wiki?curid=24485" title="Priority queue">
Priority queue

In computer science, a priority queue is an abstract data type which is like a regular queue or stack data structure, but where additionally each element has a "priority" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.

While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is a concept like "a list" or "a map"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.

A priority queue must at least support the following operations:


In addition, "peek" (in this context often called "find-max" or "find-min"), which returns the highest-priority element but does not modify the queue, is very frequently implemented, and nearly always executes in "O"(1) time. This operation and its "O"(1) performance is crucial to many applications of priority queues.

More advanced implementations may support more complicated operations, such as "pull_lowest_priority_element", inspecting the first few highest- or lowest-priority elements, clearing the queue, clearing subsets of the queue, performing a batch insert, merging two or more queues into one, incrementing priority of any element, etc.

One can imagine a priority queue as a modified queue, but when one would get the next element off the queue, the highest-priority element is retrieved first.

Stacks and queues may be modeled as particular kinds of priority queues. As a reminder, here is how stacks and queues behave:


In a stack, the priority of each inserted element is monotonically increasing; thus, the last element inserted is always the first retrieved. In a queue, the priority of each inserted element is monotonically decreasing; thus, the first element inserted is always the first retrieved.

There are a variety of simple, usually inefficient, ways to implement a priority queue. They provide an analogy to help one understand what a priority queue is. For instance, one can keep all the elements in an unsorted list. Whenever the highest-priority element is requested, search through all elements for the one with the highest priority. (In big "O" notation: "O"(1) insertion time, "O"("n") pull time due to search.)

To improve performance, priority queues typically use a heap as their backbone, giving "O"(log "n") performance for inserts and removals, and "O"("n") to build initially. Variants of the basic heap data structure such as pairing heaps or Fibonacci heaps can provide better bounds for some operations.

Alternatively, when a self-balancing binary search tree is used, insertion and removal also take "O"(log "n") time, although building trees from existing sequences of elements takes "O"("n" log "n") time; this is typical where one might already have access to these data structures, such as with third-party or standard libraries.

From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. The section on the equivalence of priority queues and sorting algorithms, below, describes how efficient sorting algorithms can create efficient priority queues.

There are several specialized heap data structures that either supply additional operations or outperform heap-based implementations for specific types of keys, specifically integer keys.


For applications that do many "peek" operations for every "extract-min" operation, the time complexity for peek actions can be reduced to "O"(1) in all tree and heap implementations by caching the highest priority element after every insertion and removal. For insertion, this adds at most a constant cost, since the newly inserted element is compared only to the previously cached minimum element. For deletion, this at most adds an additional "peek" cost, which is typically cheaper than the deletion cost, so overall time complexity is not significantly impacted.

Monotone priority queues are specialized queues that are optimized for the case where no item is ever inserted that has a lower priority (in the case of min-heap) than any item previously extracted. This restriction is met by several practical applications of priority queues.

The semantics of priority queues naturally suggest a sorting method: insert all the elements to be sorted into a priority queue, and sequentially remove them; they will come out in sorted order. This is actually the procedure used by several sorting algorithms, once the layer of abstraction provided by the priority queue is removed. This sorting method is equivalent to the following sorting algorithms:

A sorting algorithm can also be used to implement a priority queue. Specifically, Thorup says:
We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to "n" keys in "S"("n") time per key, then there is a priority queue supporting "delete" and "insert" in "O"("S"("n")) time and "find-min" in constant time.
That is, if there is a sorting algorithm which can sort in "O"("S") time per key, where "S" is some function of "n" and word size, then one can use the given procedure to create a priority queue where pulling the highest-priority element is "O"(1) time, and inserting new elements (and deleting elements) is "O"("S") time. For example, if one has an "O"("n" log log "n") sort algorithm, one can create a priority queue with "O"(1) pulling and "O"(log log "n") insertion.

A priority queue is often considered to be a "container data structure".

The Standard Template Library (STL), and the C++ 1998 standard, specifies codice_1 as one of the STL container adaptor class templates. However, it does not specify how two elements with same priority should be served, and indeed, common implementations will not return them according to their order in the queue. It implements a max-priority-queue, and has three parameters: a comparison object for sorting such as a function object (defaults to less<T> if unspecified), the underlying container for storing the data structures (defaults to std::vector<T>), and two iterators to the beginning and end of a sequence. Unlike actual STL containers, it does not allow iteration of its elements (it strictly adheres to its abstract data type definition). STL also has utility functions for manipulating another random-access container as a binary max-heap. The Boost libraries also have an implementation in the library heap.

Python's heapq module implements a binary min-heap on top of a list.

Java's library contains a class, which implements a min-priority-queue.

Go's library contains a container/heap module, which implements a min-heap on top of any compatible data structure.

The Standard PHP Library extension contains the class SplPriorityQueue.

Apple's Core Foundation framework contains a CFBinaryHeap structure, which implements a min-heap.

Priority queuing can be used to manage limited resources such as bandwidth on a transmission line from a network router. In the event of outgoing traffic queuing due to insufficient bandwidth, all other queues can be halted to send the traffic from the highest priority queue upon arrival. This ensures that the prioritized traffic (such as real-time traffic, e.g. an RTP stream of a VoIP connection) is forwarded with the least delay and the least likelihood of being rejected due to a queue reaching its maximum capacity. All other traffic can be handled when the highest priority queue is empty. Another approach used is to send disproportionately more traffic from higher priority queues.

Many modern protocols for local area networks also include the concept of priority queues at the media access control (MAC) sub-layer to ensure that high-priority applications (such as VoIP or IPTV) experience lower latency than other applications which can be served with best effort service. Examples include IEEE 802.11e (an amendment to IEEE 802.11 which provides quality of service) and ITU-T G.hn (a standard for high-speed local area network using existing home wiring (power lines, phone lines and coaxial cables).

Usually a limitation (policer) is set to limit the bandwidth that traffic from the highest priority queue can take, in order to prevent high priority packets from choking off all other traffic. This limit is usually never reached due to high level control instances such as the Cisco Callmanager, which can be programmed to inhibit calls which would exceed the programmed bandwidth limit.

Another use of a priority queue is to manage the events in a discrete event simulation. The events are added to the queue with their simulation time used as the priority. The execution of the simulation proceeds by repeatedly pulling the top of the queue and executing the event thereon.

"See also": Scheduling (computing), queueing theory

When the graph is stored in the form of adjacency list or matrix, priority queue can be used to extract minimum efficiently when implementing Dijkstra's algorithm, although one also needs the ability to alter the priority of a particular vertex in the priority queue efficiently.

Huffman coding requires one to repeatedly obtain the two lowest-frequency trees. A priority queue is one method of doing this.

Best-first search algorithms, like the A* search algorithm, find the shortest path between two vertices or nodes of a weighted graph, trying out the most promising routes first. A priority queue (also known as the "fringe") is used to keep track of unexplored routes; the one for which the estimate (a lower bound in the case of A*) of the total path length is smallest is given highest priority. If memory limitations make best-first search impractical, variants like the SMA* algorithm can be used instead, with a double-ended priority queue to allow removal of low-priority items.

The Real-time Optimally Adapting Meshes (ROAM) algorithm computes a dynamically changing triangulation of a terrain. It works by splitting triangles where more detail is needed and merging them where less detail is needed. The algorithm assigns each triangle in the terrain a priority, usually related to the error decrease if that triangle would be split. The algorithm uses two priority queues, one for triangles that can be split and another for triangles that can be merged. In each step the triangle from the split queue with the highest priority is split, or the triangle from the merge queue with the lowest priority is merged with its neighbours.

Using min heap priority queue in Prim's algorithm to find the minimum spanning tree of a connected and undirected graph, one can achieve a good running time. This min heap priority queue uses the min heap data structure which supports operations such as "insert", "minimum", "extract-min", "decrease-key". In this implementation, the weight of the edges is used to decide the priority of the vertices. Lower the weight, higher the priority and higher the weight, lower the priority.




</doc>
<doc id="19877" url="https://en.wikipedia.org/wiki?curid=19877" title="Map">
Map

A map is a symbolic depiction emphasizing relationships between elements of some space, such as objects, regions, or themes.

Many maps are static, fixed to paper or some other durable medium, while others are dynamic or interactive. Although most commonly used to depict geography, maps may represent any space, real or fictional, without regard to context or scale, such as in brain mapping, DNA mapping, or computer network topology mapping. The space being mapped may be two dimensional, such as the surface of the earth, three dimensional, such as the interior of the earth, or even more abstract spaces of any dimension, such as arise in modeling phenomena having many independent variables.

Although the earliest maps known are of the heavens, geographic maps of territory have a very long tradition and exist from ancient times. The word "map" comes from the medieval Latin "Mappa mundi", wherein "mappa" meant napkin or cloth and "mundi" the world. Thus, "map" became the shortened term referring to a two-dimensional representation of the surface of the world.

Cartography or "map-making" is the study and practice of crafting representations of the Earth upon a flat surface (see History of cartography), and one who makes maps is called a cartographer.

Road maps are perhaps the most widely used maps today, and form a subset of navigational maps, which also include aeronautical and nautical charts, railroad network maps, and hiking and bicycling maps. In terms of quantity, the largest number of drawn map sheets is probably made up by local surveys, carried out by municipalities, utilities, tax assessors, emergency services providers, and other local agencies. Many national surveying projects have been carried out by the military, such as the British Ordnance Survey: a civilian government agency, internationally renowned for its comprehensively detailed work.

In addition to location information, maps may also be used to portray contour lines indicating constant values of elevation, temperature, rainfall, etc.

The orientation of a map is the relationship between the directions on the map and the corresponding compass directions in reality. The word "orient" is derived from Latin , meaning east. In the Middle Ages many maps, including the T and O maps, were drawn with east at the top (meaning that the direction "up" on the map corresponds to East on the compass). The most common cartographic convention is that north is at the top of a map.

Maps not oriented with north at the top:

Many maps are drawn to a scale expressed as a ratio, such as 1:10,000, which means that 1 unit of measurement on the map corresponds to 10,000 of that same unit on the ground. The scale statement can be accurate when the region mapped is small enough for the curvature of the Earth to be neglected, such as a city map. Mapping larger regions, where curvature cannot be ignored, requires projections to map from the curved surface of the Earth to the plane. The impossibility of flattening the sphere to the plane without distortion means that the map cannot have constant scale. Rather, on most projections the best that can be attained is accurate scale along one or two paths on the projection. Because scale differs everywhere, it can only be measured meaningfully as point scale per location. Most maps strive to keep point scale variation within narrow bounds. Although the scale statement is nominal it is usually accurate enough for most purposes unless the map covers a large fraction of the earth. At the scope of a world map, scale as a single number is practically meaningless throughout most of the map. Instead, it usually refers to the scale along the equator.
Some maps, called cartograms, have the scale deliberately distorted to reflect information other than land area or distance. For example, this map (at the right) of Europe has been distorted to show population distribution, while the rough shape of the continent is still discernible.

Another example of distorted scale is the famous London Underground map. The basic geographical structure is respected but the tube lines (and the River Thames) are smoothed to clarify the relationships between stations. Near the center of the map stations are spaced out more than near the edges of map.

Further inaccuracies may be deliberate. For example, cartographers may simply omit military installations or remove features solely in order to enhance the clarity of the map. For example, a road map may not show railroads, smaller waterways or other prominent non-road objects, and even if it does, it may show them less clearly (e.g. dashed or dotted lines/outlines) than the main roads. Known as decluttering, the practice makes the subject matter that the user is interested in easier to read, usually without sacrificing overall accuracy. Software-based maps often allow the user to toggle decluttering between ON, OFF and AUTO as needed. In AUTO the degree of decluttering is adjusted as the user changes the scale being displayed.

Geographic maps use a projection to translating the three-dimensional real surface of the geoid to a two-dimensional picture. Projection always distorts the surface. There are many ways to apportion the distortion, and so there are many map projections. Which projection to use depends on the purpose of the map.

The various features shown on a map are represented by conventional signs or symbols. For example, colors can be used to indicate a classification of roads. Those signs are usually explained in the margin of the map, or on a separately published characteristic sheet.

Some cartographers prefer to make the map cover practically the entire screen or sheet of paper, leaving no room "outside" the map for information about the map as a whole.
These cartographers typically place such information in an otherwise "blank" region "inside" the mapcartouche, map legend, title, compass rose, bar scale, etc.
In particular, some maps contain smaller "sub-maps" in otherwise blank regions—often one at a much smaller scale showing the whole globe and where the whole map fits on that globe, and a few showing "regions of interest" at a larger scale in order to show details that wouldn't otherwise fit.
Occasionally sub-maps use the same scale as the large map—a few maps of the contiguous United States include a sub-map to the same scale for each of the two non-contiguous states.

To communicate spatial information effectively, features such as rivers, lakes, and cities need to be labeled. Over centuries cartographers have developed the art of placing names on even the densest of maps. Text placement or name placement can get mathematically very complex as the number of labels and map density increases. Therefore, text placement is time-consuming and labor-intensive, so cartographers and GIS users have developed automatic label placement to ease this process.

Maps of the world or large areas are often either 'political' or 'physical'. The most important purpose of the political map is to show territorial borders; the purpose of the physical is to show features of geography such as mountains, soil type or land use including infrastructure such as roads, railroads and buildings. Topographic maps show elevations and relief with contour lines or shading. Geological maps show not only the physical surface, but characteristics of the underlying rock, fault lines, and subsurface structures.

From the last quarter of the 20th century, the indispensable tool of the cartographer has been the computer. Much of cartography, especially at the data-gathering survey level, has been subsumed by Geographic Information Systems (GIS). The functionality of maps has been greatly advanced by technology simplifying the superimposition of spatially located variables onto existing geographical maps. Having local information such as rainfall level, distribution of wildlife, or demographic data integrated within the map allows more efficient analysis and better decision making. In the pre-electronic age such superimposition of data led Dr. John Snow to identify the location of an outbreak of cholera. Today, it is used by agencies of the human kind, as diverse as wildlife conservationists and militaries around the world.

Even when GIS is not involved, most cartographers now use a variety of computer graphics programs to generate new maps.

Interactive, computerised maps are commercially available, allowing users to "zoom in" or "zoom out" (respectively meaning to increase or decrease the scale), sometimes by replacing one map with another of different scale, centered where possible on the same point. In-car global navigation satellite systems are computerised maps with route-planning and advice facilities which monitor the user's position with the help of satellites. From the computer scientist's point of view, zooming in entails one or a combination of:


For example:

"See also: Webpage (Graphics), PDF (Layers), MapQuest, Google Maps, Google Earth, OpenStreetMap or Yahoo! Maps."

The maps that reflect the territorial distribution of climatic conditions based on the results of long-term observations are called climatic maps. These maps can be compiled both for individual climatic features (temperature, precipitation, humidity) and for combinations of them at the earth's surface and in the upper layers of the atmosphere. Climatic maps show climatic features across a large region and permit values of climatic features to be compared in different parts of the region. When generating the map, interpolation can be used to synthesize values where there are no measurements, under the assumption that conditions change smoothly.

Climatic maps generally apply to individual months and to the year as a whole, sometimes to the four seasons, to the growing period, and so forth. On maps compiled from the observations of ground meteorological stations, atmospheric pressure is converted to sea level. Air temperature maps are compiled both from the actual values observed on the surface of the earth and from values converted to sea level. The pressure field in free atmosphere is represented either by maps of the distribution of pressure at different standard altitudes—for example, at every kilometer above sea level—or by maps of baric topography on which altitudes (more precisely geopotentials) of the main isobaric surfaces (for example, 900, 800, and 700 millibars) counted off from sea level are plotted. The temperature, humidity, and wind on aeroclimatic maps may apply either to standard altitudes or to the main isobaric surfaces.

Isolines are drawn on maps of such climatic features as the long-term mean values (of atmospheric pressure, temperature, humidity, total precipitation, and so forth) to connect points with equal values of the feature in question—for example, isobars for pressure, isotherms for temperature, and isohyets for precipitation. Isoamplitudes are drawn on maps of amplitudes (for example, annual amplitudes of air temperature—that is, the differences between the mean temperatures of the warmest and coldest month). Isanomals are drawn on maps of anomalies (for example, deviations of the mean temperature of each place from the mean temperature of the entire latitudinal zone). Isolines of frequency are drawn on maps showing the frequency of a particular phenomenon (for example, annual number of days with a thunderstorm or snow cover). Isochrones are drawn on maps showing the dates of onset of a given phenomenon (for example, the first frost and appearance or disappearance of the snow cover) or the date of a particular value of a meteorological element in the course of a year (for example, passing of the mean daily air temperature through zero). Isolines of the mean numerical value of wind velocity or isotachs are drawn on wind maps (charts); the wind resultants and directions of prevailing winds are indicated by arrows of different length or arrows with different plumes; lines of flow are often drawn. Maps of the zonal and meridional components of wind are frequently compiled for the free atmosphere. Atmospheric pressure and wind are usually combined on climatic maps. Wind roses, curves showing the distribution of other meteorological elements, diagrams of the annual course of elements at individual stations, and the like are also plotted on climatic maps.

Maps of climatic regionalization, that is, division of the earth's surface into climatic zones and regions according to some classification of climates, are a special kind of climatic map.

Climatic maps are often incorporated into climatic atlases of varying geographic range (globe, hemispheres, continents, countries, oceans) or included in comprehensive atlases. Besides general climatic maps, applied climatic maps and atlases have great practical value. Aeroclimatic maps, aeroclimatic atlases, and agroclimatic maps are the most numerous.

Maps exist of the Solar System, and other cosmological features such as star maps. In addition maps of other bodies such as the Moon and other planets are technically not "geo"graphical maps.

Diagrams such as schematic diagrams and Gantt charts and treemaps display logical relationships between items, rather than geographical relationships. Topological in nature, only the connectivity is significant. The London Underground map and similar subway maps around the world are a common example of these maps.

General-purpose maps provide many types of information on one map. Most atlas maps, wall maps, and road maps fall into this category. The following are some features that might be shown on general-purpose maps: bodies of water, roads, railway lines, parks, elevations, towns and cities, political boundaries, latitude and longitude, national and provincial parks. These maps give a broad understanding of location and features of an area. The reader may gain an understanding of the type of landscape, the location of urban places, and the location of major transportation routes all at once.



Some countries required that all published maps represent their national claims regarding border disputes. For example:

In 2010, the People's Republic of China began requiring that all online maps served from within China be hosted there, making them subject to Chinese laws.









</doc>
<doc id="10070867" url="https://en.wikipedia.org/wiki?curid=10070867" title="Bidirectional map">
Bidirectional map

In computer science, a bidirectional map, or hash bag, is an associative data structure in which the formula_1 pairs form a one-to-one correspondence. Thus the binary relation is functional in each direction: each formula_2 can also be mapped to a unique formula_3. A pair formula_4 thus provides a unique coupling between formula_5 and formula_6 so that formula_6 can be found when formula_5 is used as a key and formula_5 can be found when formula_6 is used as a key.

Mathematically, a bidirectional map can be defined a bijection formula_11 between two different sets of keys formula_12 and formula_13 of equal cardinality, thus constituting a injective and surjective function:

formula_14



</doc>
<doc id="1663543" url="https://en.wikipedia.org/wiki?curid=1663543" title="Multimap">
Multimap

In computer science, a multimap (sometimes also multihash or multidict) is a generalization of a map or associative array abstract data type in which more than one value may be associated with and returned for a given key. Both map and multimap are particular cases of containers (for example, see C++ Standard Template Library containers). Often the multimap is implemented as a map with lists or sets as the map values.


C++'s Standard Template Library provides the codice_1 container for the sorted multimap using a self-balancing binary search tree, and SGI's STL extension provides the codice_2 container, which implements a multimap using a hash table.

As of C++11, the Standard Template Library provides the codice_3 for the unordered multimap.

Quiver provides a Multimap for Dart.

Apache Commons Collections provides a MultiMap interface for Java. It also provides a MultiValueMap implementing class that makes a MultiMap out of a Map object and a type of Collection.

Google Guava provides a Multimap interface and implementations of it.
Python provides a codice_4 class that can be used to create a multimap. You can instantiate the class as codice_5.

OCaml's standard library module codice_6 implements a hash table where it's possible to store multiple values for a key.

The Scala programming language's API also provides Multimap and implementations



</doc>
<doc id="454886" url="https://en.wikipedia.org/wiki?curid=454886" title="Set">
Set

Set or The Set may refer to:
















</doc>
<doc id="18955875" url="https://en.wikipedia.org/wiki?curid=18955875" title="Tree">
Tree

In botany, a tree is a perennial plant with an elongated stem, or trunk, supporting branches and leaves in most species. In some usages, the definition of a tree may be narrower, including only woody plants with secondary growth, plants that are usable as lumber or plants above a specified height. In wider definitions, the taller palms, tree ferns, bananas, and bamboos are also trees. Trees are not a taxonomic group but include a variety of plant species that have independently evolved a trunk and branches as a way to tower above other plants to compete for sunlight. Trees tend to be long-lived, some reaching several thousand years old. Trees have been in existence for 370 million years. It is estimated that there are just over 3 trillion mature trees in the world.

A tree typically has many secondary branches supported clear of the ground by the trunk. This trunk typically contains woody tissue for strength, and vascular tissue to carry materials from one part of the tree to another. For most trees it is surrounded by a layer of bark which serves as a protective barrier. Below the ground, the roots branch and spread out widely; they serve to anchor the tree and extract moisture and nutrients from the soil. Above ground, the branches divide into smaller branches and shoots. The shoots typically bear leaves, which capture light energy and convert it into sugars by photosynthesis, providing the food for the tree's growth and development.

Trees usually reproduce using seeds. Flowers and fruit may be present, but some trees, such as conifers, instead have pollen cones and seed cones. Palms, bananas, and bamboos also produce seeds, but tree ferns produce spores instead.

Trees play a significant role in reducing erosion and moderating the climate. They remove carbon dioxide from the atmosphere and store large quantities of carbon in their tissues. Trees and forests provide a habitat for many species of animals and plants. Tropical rainforests are among the most biodiverse habitats in the world. Trees provide shade and shelter, timber for construction, fuel for cooking and heating, and fruit for food as well as having many other uses. In parts of the world, forests are shrinking as trees are cleared to increase the amount of land available for agriculture. Because of their longevity and usefulness, trees have always been revered, with sacred groves in various cultures, and they play a role in many of the world's mythologies.

Although "tree" is a term of common parlance, there is no universally recognised precise definition of what a tree is, either botanically or in common language. In its broadest sense, a tree is any plant with the general form of an elongated stem, or trunk, which supports the photosynthetic leaves or branches at some distance above the ground. Trees are also typically defined by height, with smaller plants from being called shrubs, so the minimum height of a tree is only loosely defined. Large herbaceous plants such as papaya and bananas are trees in this broad sense.

A commonly applied narrower definition is that a tree has a woody trunk formed by secondary growth, meaning that the trunk thickens each year by growing outwards, in addition to the primary upwards growth from the growing tip. Under such a definition, herbaceous plants such as palms, bananas and papayas are not considered trees regardless of their height, growth form or stem girth. Certain monocots may be considered trees under a slightly looser definition; while the Joshua tree, bamboos and palms do not have secondary growth and never produce true wood with growth rings, they may produce "pseudo-wood" by lignifying cells formed by primary growth. Tree species in the genus "Dracaena", despite also being monocots, do have secondary growth caused by meristem in their trunk, but it is different from the thickening meristem found in dicotyledonous trees.

Aside from structural definitions, trees are commonly defined by use; for instance, as those plants which yield lumber.

The tree growth habit is an evolutionary adaptation found in different groups of plants: by growing taller, trees are able to compete better for sunlight. Trees tend to be tall and long-lived, some reaching several thousand years old. Several trees are among the oldest organisms now living. Trees have modified structures such as thicker stems composed of specialised cells that add structural strength and durability, allowing them to grow taller than many other plants and to spread out their foliage. They differ from shrubs, which have a similar growth form, by usually growing larger and having a single main stem; but there is no consistent distinction between a tree and a shrub, made more confusing by the fact that trees may be reduced in size under harsher environmental conditions such as on mountains and subarctic areas. The tree form has evolved separately in unrelated classes of plants in response to similar environmental challenges, making it a classic example of parallel evolution. With an estimated 60,000-100,000 species, the number of trees worldwide might total twenty-five per cent of all living plant species. The greatest number of these grow in tropical regions and many of these areas have not yet been fully surveyed by botanists, making tree diversity and ranges poorly known.

The majority of tree species are angiosperms. There are about 1000 species of gymnosperm trees, including conifers, cycads, ginkgophytes and gnetales; they produce seeds which are not enclosed in fruits, but in open structures such as pine cones, and many have tough waxy leaves, such as pine needles. Most angiosperm trees are eudicots, the "true dicotyledons", so named because the seeds contain two cotyledons or seed leaves. There are also some trees among the old lineages of flowering plants called basal angiosperms or paleodicots; these include "Amborella", "Magnolia", nutmeg and avocado, while trees such as bamboo, palms and bananas are monocots.

Wood gives structural strength to the trunk of most types of tree; this supports the plant as it grows larger. The vascular system of trees allows water, nutrients and other chemicals to be distributed around the plant, and without it trees would not be able to grow as large as they do. Trees, as relatively tall plants, need to draw water up the stem through the xylem from the roots by the suction produced as water evaporates from the leaves. If insufficient water is available the leaves will die. The three main parts of trees include the root, stem, and leaves; they are integral parts of the vascular system which interconnects all the living cells. In trees and other plants that develop wood, the vascular cambium allows the expansion of vascular tissue that produces woody growth. Because this growth ruptures the epidermis of the stem, woody plants also have a cork cambium that develops among the phloem. The cork cambium gives rise to thickened cork cells to protect the surface of the plant and reduce water loss. Both the production of wood and the production of cork are forms of secondary growth.

Trees are either evergreen, having foliage that persists and remains green throughout the year, or deciduous, shedding their leaves at the end of the growing season and then having a dormant period without foliage. Most conifers are evergreens, but larches ("Larix" and "Pseudolarix") are deciduous, dropping their needles each autumn, and some species of cypress ("Glyptostrobus", "Metasequoia" and "Taxodium") shed small leafy shoots annually in a process known as cladoptosis. The crown is the spreading top of a tree including the branches and leaves, while the uppermost layer in a forest, formed by the crowns of the trees, is known as the canopy. A sapling is a young tree.

Many tall palms are herbaceous monocots; these do not undergo secondary growth and never produce wood. In many tall palms, the terminal bud on the main stem is the only one to develop, so they have unbranched trunks with large spirally arranged leaves. Some of the tree ferns, order Cyatheales, have tall straight trunks, growing up to , but these are composed not of wood but of rhizomes which grow vertically and are covered by numerous adventitious roots.

The number of trees in the world, according to a 2015 estimate, is 3.04 trillion, of which 1.39 trillion (46%) are in the tropics or sub-tropics, 0.61 trillion (20%) in the temperate zones, and 0.74 trillion (24%) in the coniferous boreal forests. The estimate is about eight times higher than previous estimates, and is based on tree densities measured on over 400,000 plots. It remains subject to a wide margin of error, not least because the samples are mainly from Europe and North America. The estimate suggests that about 15 billion trees are cut down annually and about 5 billion are planted. In the 12,000 years since the start of human agriculture, the number of trees worldwide has decreased by 46%.

In suitable environments, such as the Daintree Rainforest in Queensland, or the mixed podocarp and broadleaf forest of Ulva Island, New Zealand, forest is the more-or-less stable climatic climax community at the end of a plant succession, where open areas such as grassland are colonised by taller plants, which in turn give way to trees that eventually form a forest canopy.

In cool temperate regions, conifers often predominate; a widely distributed climax community in the far north of the northern hemisphere is moist taiga or northern coniferous forest (also called boreal forest). Taiga is the world's largest land biome, forming 29% of the world's forest cover. The long cold winter of the far north is unsuitable for plant growth and trees must grow rapidly in the short summer season when the temperature rises and the days are long. Light is very limited under their dense cover and there may be little plant life on the forest floor, although fungi may abound. Similar woodland is found on mountains where the altitude causes the average temperature to be lower thus reducing the length of the growing season.

Where rainfall is relatively evenly spread across the seasons in temperate regions, temperate broadleaf and mixed forest typified by species like oak, beech, birch and maple is found. Temperate forest is also found in the southern hemisphere, as for example in the Eastern Australia temperate forest, characterised by "Eucalyptus" forest and open acacia woodland.

In tropical regions with a monsoon or monsoon-like climate, where a drier part of the year alternates with a wet period as in the Amazon rainforest, different species of broad-leaved trees dominate the forest, some of them being deciduous. In tropical regions with a drier savanna climate and insufficient rainfall to support dense forests, the canopy is not closed, and plenty of sunshine reaches the ground which is covered with grass and scrub. "Acacia" and baobab are well adapted to living in such areas.

The roots of a tree serve to anchor it to the ground and gather water and nutrients to transfer to all parts of the tree. They are also used for reproduction, defence, survival, energy storage and many other purposes. The radicle or embryonic root is the first part of a seedling to emerge from the seed during the process of germination. This develops into a taproot which goes straight downwards. Within a few weeks lateral roots branch out of the side of this and grow horizontally through the upper layers of the soil. In most trees, the taproot eventually withers away and the wide-spreading laterals remain. Near the tip of the finer roots are single cell root hairs. These are in immediate contact with the soil particles and can absorb water and nutrients such as potassium in solution. The roots require oxygen to respire and only a few species such as mangroves and the pond cypress ("Taxodium ascendens") can live in permanently waterlogged soil.

In the soil, the roots encounter the hyphae of fungi. Many of these are known as mycorrhiza and form a mutualistic relationship with the tree roots. Some are specific to a single tree species, which will not flourish in the absence of its mycorrhizal associate. Others are generalists and associate with many species. The tree acquires minerals such as phosphorus from the fungus, while the fungus obtains the carbohydrate products of photosynthesis from the tree. The hyphae of the fungus can link different trees and a network is formed, transferring nutrients and signals from one place to another. The fungus promotes growth of the roots and helps protect the trees against predators and pathogens. It can also limit damage done to a tree by pollution as the fungus accumulate heavy metals within its tissues. Fossil evidence shows that roots have been associated with mycorrhizal fungi since the early Paleozoic, four hundred million years ago, when the first vascular plants colonised dry land.
Some trees such as the alders ("Alnus" species) have a symbiotic relationship with "Frankia" species, a filamentous bacterium that can fix nitrogen from the air, converting it into ammonia. They have actinorhizal root nodules on their roots in which the bacteria live. This process enables the tree to live in low nitrogen habitats where they would otherwise be unable to thrive. The plant hormones called cytokinins initiate root nodule formation, in a process closely related to mycorrhizal association.

It has been demonstrated that some trees are interconnected through their root system, forming a colony. The interconnections are made by the inosculation process, a kind of natural grafting or welding of vegetal tissues. The tests to demonstrate this networking are performed by injecting chemicals, sometimes radioactive, into a tree, and then checking for its presence in neighbouring trees.

The roots are, generally, an underground part of the tree, but some tree species have evolved roots that are aerial. The common purposes for aerial roots may be of two kinds, to contribute to the mechanical stability of the tree, and to obtain oxygen from air. An instance of mechanical stability enhancement is the red mangrove that develops prop roots that loop out of the trunk and branches and descend vertically into the mud. A similar structure is developed by the Indian banyan. Many large trees have buttress roots which flare out from the lower part of the trunk. These brace the tree rather like angle brackets and provide stability, reducing sway in high winds. They are particularly prevalent in tropical rainforests where the soil is poor and the roots are close to the surface.

Some tree species have developed root extensions that pop out of soil, in order to get oxygen, when it is not available in the soil because of excess water. These root extensions are called pneumatophores, and are present, among others, in black mangrove and pond cypress.

The main purpose of the trunk is to raise the leaves above the ground, enabling the tree to overtop other plants and outcompete them for light. It also transports water and nutrients from the roots to the aerial parts of the tree, and distributes the food produced by the leaves to all other parts, including the roots.

In the case of angiosperms and gymnosperms, the outermost layer of the trunk is the bark, mostly composed of dead cells of phellem (cork). It provides a thick, waterproof covering to the living inner tissue. It protects the trunk against the elements, disease, animal attack and fire. It is perforated by a large number of fine breathing pores called lenticels, through which oxygen diffuses. Bark is continually replaced by a living layer of cells called the cork cambium or phellogen. The London plane ("Platanus × acerifolia") periodically sheds its bark in large flakes. Similarly, the bark of the silver birch ("Betula pendula") peels off in strips. As the tree's girth expands, newer layers of bark are larger in circumference, and the older layers develop fissures in many species. In some trees such as the pine ("Pinus" species) the bark exudes sticky resin which deters attackers whereas in rubber trees ("Hevea brasiliensis") it is a milky latex that oozes out. The quinine bark tree ("Cinchona officinalis") contains bitter substances to make the bark unpalatable. Large tree-like plants with lignified trunks in the Pteridophyta, Arecales, Cycadophyta and Poales such as the tree ferns, palms, cycads and bamboos have different structures and outer coverings.

Although the bark functions as a protective barrier, it is itself attacked by boring insects such as beetles. These lay their eggs in crevices and the larvae chew their way through the cellulose tissues leaving a gallery of tunnels. This may allow fungal spores to gain admittance and attack the tree. Dutch elm disease is caused by a fungus ("Ophiostoma" species) carried from one elm tree to another by various beetles. The tree reacts to the growth of the fungus by blocking off the xylem tissue carrying sap upwards and the branch above, and eventually the whole tree, is deprived of nourishment and dies. In Britain in the 1990s, 25 million elm trees were killed by this disease.

The innermost layer of bark is known as the phloem and this is involved in the transport of the sap containing the sugars made by photosynthesis to other parts of the tree. It is a soft spongy layer of living cells, some of which are arranged end to end to form tubes. These are supported by parenchyma cells which provide padding and include fibres for strengthening the tissue. Inside the phloem is a layer of undifferentiated cells one cell thick called the vascular cambium layer. The cells are continually dividing, creating phloem cells on the outside and wood cells known as xylem on the inside.

The newly created xylem is the sapwood. It is composed of water-conducting cells and associated cells which are often living, and is usually pale in colour. It transports water and minerals from the roots to the upper parts of the tree. The oldest, inner part of the sapwood is progressively converted into heartwood as new sapwood is formed at the cambium. The conductive cells of the heartwood are blocked in some species. Heartwood is usually darker in colour than the sapwood. It is the dense central core of the trunk giving it rigidity. Three quarters of the dry mass of the xylem is cellulose, a polysaccharide, and most of the remainder is lignin, a complex polymer. A transverse section through a tree trunk or a horizontal core will show concentric circles or lighter or darker wood – tree rings. These rings are the annual growth rings There may also be rays running at right angles to growth rings. These are vascular rays which are thin sheets of living tissue permeating the wood. Many older trees may become hollow but may still stand upright for many years.

Trees do not usually grow continuously throughout the year but mostly have spurts of active expansion followed by periods of rest. This pattern of growth is related to climatic conditions; growth normally ceases when conditions are either too cold or too dry. In readiness for the inactive period, trees form buds to protect the meristem, the zone of active growth. Before the period of dormancy, the last few leaves produced at the tip of a twig form scales. These are thick, small and closely wrapped and enclose the growing point in a waterproof sheath. Inside this bud there is a rudimentary stalk and neatly folded miniature leaves, ready to expand when the next growing season arrives. Buds also form in the axils of the leaves ready to produce new side shoots. A few trees, such as the eucalyptus, have "naked buds" with no protective scales and some conifers, such as the Lawson's cypress, have no buds but instead have little pockets of meristem concealed among the scale-like leaves.

When growing conditions improve, such as the arrival of warmer weather and the longer days associated with spring in temperate regions, growth starts again. The expanding shoot pushes its way out, shedding the scales in the process. These leave behind scars on the surface of the twig. The whole year's growth may take place in just a few weeks. The new stem is unlignified at first and may be green and downy. The Arecaceae (palms) have their leaves spirally arranged on an unbranched trunk. In some tree species in temperate climates, a second spurt of growth, a Lammas growth may occur which is believed to be a strategy to compensate for loss of early foliage to insect predators.

Primary growth is the elongation of the stems and roots. Secondary growth consists of a progressive thickening and strengthening of the tissues as the outer layer of the epidermis is converted into bark and the cambium layer creates new phloem and xylem cells. The bark is inelastic. Eventually the growth of a tree slows down and stops and it gets no taller. If damage occurs the tree may in time become hollow.

Leaves are structures specialised for photosynthesis and are arranged on the tree in such a way as to maximise their exposure to light without shading each other. They are an important investment by the tree and may be thorny or contain phytoliths, lignins, tannins or poisons to discourage herbivory. Trees have evolved leaves in a wide range of shapes and sizes, in response to environmental pressures including climate and predation. They can be broad or needle-like, simple or compound, lobed or entire, smooth or hairy, delicate or tough, deciduous or evergreen. The needles of coniferous trees are compact but are structurally similar to those of broad-leaved trees. They are adapted for life in environments where resources are low or water is scarce. Frozen ground may limit water availability and conifers are often found in colder places at higher altitudes and higher latitudes than broad leaved trees. In conifers such as fir trees, the branches hang down at an angle to the trunk, enabling them to shed snow. In contrast, broad leaved trees in temperate regions deal with winter weather by shedding their leaves. When the days get shorter and the temperature begins to decrease, the leaves no longer make new chlorophyll and the red and yellow pigments already present in the blades become apparent. Synthesis in the leaf of a plant hormone called auxin also ceases. This causes the cells at the junction of the petiole and the twig to weaken until the joint breaks and the leaf floats to the ground. In tropical and subtropical regions, many trees keep their leaves all year round. Individual leaves may fall intermittently and be replaced by new growth but most leaves remain intact for some time. Other tropical species and those in arid regions may shed all their leaves annually, such as at the start of the dry season. Many deciduous trees flower before the new leaves emerge. A few trees do not have true leaves but instead have structures with similar external appearance such as Phylloclades – modified stem structures – as seen in the genus "Phyllocladus".

Trees can be pollinated either by wind or by animals, mostly insects. Many angiosperm trees are insect pollinated. Wind pollination may take advantage of increased wind speeds high above the ground. Trees use a variety of methods of seed dispersal. Some rely on wind, with winged or plumed seeds. Others rely on animals, for example with edible fruits. Others again eject their seeds (ballistic dispersal), or use gravity so that seeds fall and sometimes roll.

Seeds are the primary way that trees reproduce and their seeds vary greatly in size and shape. Some of the largest seeds come from trees, but the largest tree, "Sequoiadendron giganteum", produces one of the smallest tree seeds. The great diversity in tree fruits and seeds reflects the many different ways that tree species have evolved to disperse their offspring.

For a tree seedling to grow into an adult tree it needs light. If seeds only fell straight to the ground, competition among the concentrated saplings and the shade of the parent would likely prevent it from flourishing. Many seeds such as birch are small and have papery wings to aid dispersal by the wind. Ash trees and maples have larger seeds with blade shaped wings which spiral down to the ground when released. The kapok tree has cottony threads to catch the breeze.

The seeds of conifers, the largest group of gymnosperms, are enclosed in a cone and most species have seeds that are light and papery that can be blown considerable distances once free from the cone. Sometimes the seed remains in the cone for years waiting for a trigger event to liberate it. Fire stimulates release and germination of seeds of the jack pine, and also enriches the forest floor with wood ash and removes competing vegetation. Similarly, a number of angiosperms including "Acacia cyclops" and "Acacia mangium" have seeds that germinate better after exposure to high temperatures.

The flame tree "Delonix regia" does not rely on fire but shoots its seeds through the air when the two sides of its long pods crack apart explosively on drying. The miniature cone-like catkins of alder trees produce seeds that contain small droplets of oil that help disperse the seeds on the surface of water. Mangroves often grow in water and some species have propagules, which are buoyant fruits with seeds that start germinating before becoming detached from the parent tree. These float on the water and may become lodged on emerging mudbanks and successfully take root.

Other seeds, such as apple pips and plum stones, have fleshy receptacles and smaller fruits like hawthorns have seeds enclosed in edible tissue; animals including mammals and birds eat the fruits and either discard the seeds, or swallow them so they pass through the gut to be deposited in the animal's droppings well away from the parent tree. The germination of some seeds is improved when they are processed in this way. Nuts may be gathered by animals such as squirrels that cache any not immediately consumed. Many of these caches are never revisited, the nut-casing softens with rain and frost, and the seed germinates in the spring. Pine cones may similarly be hoarded by red squirrels, and grizzly bears may help to disperse the seed by raiding squirrel caches.

The single extant species of Ginkgophyta ("Ginkgo biloba") has fleshy seeds produced at the ends of short branches on female trees, and "Gnetum", a tropical and subtropical group of gymnosperms produce seeds at the tip of a shoot axis.

The earliest trees were tree ferns, horsetails and lycophytes, which grew in forests in the Carboniferous period. The first tree may have been "Wattieza", fossils of which have been found in New York State in 2007 dating back to the Middle Devonian (about 385 million years ago). Prior to this discovery, "Archaeopteris" was the earliest known tree. Both of these reproduced by spores rather than seeds and are considered to be links between ferns and the gymnosperms which evolved in the Triassic period. The gymnosperms include conifers, cycads, gnetales and ginkgos and these may have appeared as a result of a whole genome duplication event which took place about 319 million years ago. Ginkgophyta was once a widespread diverse group of which the only survivor is the maidenhair tree "Ginkgo biloba". This is considered to be a living fossil because it is virtually unchanged from the fossilised specimens found in Triassic deposits.

During the Mesozoic (245 to 66 million years ago) the conifers flourished and became adapted to live in all the major terrestrial habitats. Subsequently, the tree forms of flowering plants evolved during the Cretaceous period. These began to displace the conifers during the Tertiary era (66 to 2 million years ago) when forests covered the globe. When the climate cooled 1.5 million years ago and the first of four ice ages occurred, the forests retreated as the ice advanced. In the interglacials, trees recolonised the land that had been covered by ice, only to be driven back again in the next ice age.

Trees are an important part of the terrestrial ecosystem, providing essential habitats including many kinds of forest for communities of organisms. Epiphytic plants such as ferns, some mosses, liverworts, orchids and some species of parasitic plants (e.g., mistletoe) hang from branches; these along with arboreal lichens, algae, and fungi provide micro-habitats for themselves and for other organisms, including animals. Leaves, flowers and fruits are seasonally available. On the ground underneath trees there is shade, and often there is undergrowth, leaf litter, and decaying wood that provide other habitat. Trees stabilise the soil, prevent rapid run-off of rain water, help prevent desertification, have a role in climate control and help in the maintenance of biodiversity and ecosystem balance.

Many species of tree support their own specialised invertebrates. In their natural habitats, 284 different species of insect have been found on the English oak ("Quercus robur") and 306 species of invertebrate on the Tasmanian oak ("Eucalyptus obliqua"). Non-native tree species provide a less biodiverse community, for example in the United Kingdom the sycamore ("Acer pseudoplatanus"), which originates from southern Europe, has few associated invertebrate species, though its bark supports a wide range of lichens, bryophytes and other epiphytes.

In ecosystems such as mangrove swamps, trees play a role in developing the habitat, since the roots of the mangrove trees reduce the speed of flow of tidal currents and trap water-borne sediment, reducing the water depth and creating suitable conditions for further mangrove colonisation. Thus mangrove swamps tend to extend seawards in suitable locations. Mangrove swamps also provide an effective buffer against the more damaging effects of cyclones and tsunamis.

Silviculture is the practice of controlling the establishment, growth, composition, health, and quality of forests, which are areas that have a high density of trees. Cultivated trees are planted and tended by humans, usually because they provide food (fruits or nuts), ornamental beauty, or some type of wood product that benefits people. An area of land planted with fruit or nut trees is an orchard. A small wooded area, usually with no undergrowth, is called a grove and a small wood or thicket of trees and bushes is called a coppice or copse. A large area of land covered with trees and undergrowth is called woodland or forest. An area of woodland composed primarily of trees established by planting or artificial seeding is known as a plantation.
Trees are the source of many of the world's best known fleshy fruits. Apples, pears, plums, cherries and citrus are all grown commercially in temperate climates and a wide range of edible fruits are found in the tropics. Other commercially important fruit include dates, figs and olives. Palm oil is obtained from the fruits of the oil palm ("Elaeis guineensis"). The fruits of the cocoa tree ("Theobroma cacao") are used to make cocoa and chocolate and the berries of coffee trees, "Coffea arabica" and "Coffea canephora", are processed to extract the coffee beans. In many rural areas of the world, fruit is gathered from forest trees for consumption. Many trees bear edible nuts which can loosely be described as being large, oily kernels found inside a hard shell. These include coconuts ("Cocos nucifera"), Brazil nuts ("Bertholletia excelsa"), pecans ("Carya illinoinensis"), hazel nuts ("Corylus"), almonds ("Prunus dulcis"), walnuts ("Juglans regia"), pistachios ("Pistacia vera") and many others. They are high in nutritive value and contain high-quality protein, vitamins and minerals as well as dietary fibre. A variety of nut oils are extracted by pressing for culinary use; some such as walnut, pistachio and hazelnut oils are prized for their distinctive flavours, but they tend to spoil quickly.

In temperate climates there is a sudden movement of sap at the end of the winter as trees prepare to burst into growth. In North America, the sap of the sugar maple ("Acer saccharum") is most often used in the production of a sweet liquid, maple syrup. About 90% of the sap is water, the remaining 10% being a mixture of various sugars and certain minerals. The sap is harvested by drilling holes in the trunks of the trees and collecting the liquid that flows out of the inserted spigots. It is piped to a sugarhouse where it is heated to concentrate it and improve its flavour. Similarly in northern Europe the spring rise in the sap of the silver birch ("Betula pendula") is tapped and collected, either to be drunk fresh or fermented into an alcoholic drink. In Alaska, the sap of the sweet birch ("Betula lenta") is made into a syrup with a sugar content of 67%. Sweet birch sap is more dilute than maple sap; a hundred litres are required to make one litre of birch syrup.

Various parts of trees are used as spices. These include cinnamon, made from the bark of the cinnamon tree ("Cinnamomum zeylanicum") and allspice, the dried small fruits of the pimento tree ("Pimenta dioica"). Nutmeg is a seed found in the fleshy fruit of the nutmeg tree ("Myristica fragrans") and cloves are the unopened flower buds of the clove tree ("Syzygium aromaticum").

Many trees have flowers rich in nectar which are attractive to bees. The production of forest honey is an important industry in rural areas of the developing world where it is undertaken by small-scale beekeepers using traditional methods. The flowers of the elder ("Sambucus") are used to make elderflower cordial and petals of the plum ("Prunus spp.") can be candied. Sassafras oil is a flavouring obtained from distilling bark from the roots of the sassafras tree ("Sassafras albidum").

The leaves of trees are widely gathered as fodder for livestock and some can be eaten by humans but they tend to be high in tannins which makes them bitter. Leaves of the curry tree ("Murraya koenigii") are eaten, those of kaffir lime ("Citrus × hystrix") (in Thai food) and "Ailanthus" (in Korean dishes such as bugak) and those of the European bay tree ("Laurus nobilis") and the California bay tree ("Umbellularia californica") are used for flavouring food. "Camellia sinensis", the source of tea, is a small tree but seldom reaches its full height, being heavily pruned to make picking the leaves easier.

Wood smoke can be used to preserve food. In the hot smoking process the food is exposed to smoke and heat in a controlled environment. The food is ready to eat when the process is complete, having been tenderised and flavoured by the smoke it has absorbed. In the cold process, the temperature is not allowed to rise above . The flavour of the food is enhanced but raw food requires further cooking. If it is to be preserved, meat should be cured before cold smoking.

Wood has traditionally been used for fuel, especially in rural areas. In less developed nations it may be the only fuel available and collecting firewood is often a time consuming task as it becomes necessary to travel further and further afield in the search for fuel. It is often burned inefficiently on an open fire. In more developed countries other fuels are available and burning wood is a choice rather than a necessity. Modern wood-burning stoves are very fuel efficient and new products such as wood pellets are available to burn.

Charcoal can be made by slow pyrolysis of wood by heating it in the absence of air in a kiln. The carefully stacked branches, often oak, are burned with a very limited amount of air. The process of converting them into charcoal takes about fifteen hours. Charcoal is used as a fuel in barbecues and by blacksmiths and has many industrial and other uses.

Timber, "trees that are grown in order to produce wood" is cut into lumber (sawn wood) for use in construction. Wood has been an important, easily available material for construction since humans started building shelters. Engineered wood products are available which bind the particles, fibres or veneers of wood together with adhesives to form composite materials. Plastics have taken over from wood for some traditional uses.

Wood is used in the construction of buildings, bridges, trackways, piles, poles for power lines, masts for boats, pit props, railway sleepers, fencing, hurdles, shuttering for concrete, pipes, scaffolding and pallets. In housebuilding it is used in joinery, for making joists, roof trusses, roofing shingles, thatching, staircases, doors, window frames, floor boards, parquet flooring, panelling and cladding.

Wood is used to construct carts, farm implements, boats, dugout canoes and in shipbuilding. It is used for making furniture, tool handles, boxes, ladders, musical instruments, bows, weapons, matches, clothes pegs, brooms, shoes, baskets, turnery, carving, toys, pencils, rollers, cogs, wooden screws, barrels, coffins, skittles, veneers, artificial limbs, oars, skis, wooden spoons, sports equipment and wooden balls.

Wood is pulped for paper and used in the manufacture of cardboard and made into engineered wood products for use in construction such as fibreboard, hardboard, chipboard and plywood. The wood of conifers is known as softwood while that of broad-leaved trees is hardwood.

Besides inspiring artists down the centuries, trees have been used to create art. Living trees have been used in bonsai and in tree shaping, and both living and dead specimens have been sculpted into sometimes fantastic shapes.

 is the practice of "hòn non bộ" originated in China and spread to Japan more than a thousand years ago, there are similar practices in other cultures like the living miniature landscapes of Vietnam "hòn non bộ". The word "bonsai" is often used in English as an umbrella term for all miniature trees in containers or pots.

The purposes of bonsai are primarily contemplation (for the viewer) and the pleasant exercise of effort and ingenuity (for the grower). Bonsai practice focuses on long-term cultivation and shaping of one or more small trees growing in a container, beginning with a cutting, seedling, or small tree of a species suitable for bonsai development. Bonsai can be created from nearly any perennial woody-stemmed tree or shrub species that produces true branches and can be cultivated to remain small through pot confinement with crown and root pruning. Some species are popular as bonsai material because they have characteristics, such as small leaves or needles, that make them appropriate for the compact visual scope of bonsai and a miniature deciduous forest can even be created using such species as Japanese maple, Japanese zelkova or hornbeam.

Tree shaping is the practice of changing living trees and other woody plants into man made shapes for art and useful structures. There are a few different methods of shaping a tree. There is a gradual method and there is an instant method. The gradual method slowly guides the growing tip along predetermined pathways over time whereas the instant method bends and weaves saplings long into a shape that becomes more rigid as they thicken up. Most artists use grafting of living trunks, branches, and roots, for art or functional structures and there are plans to grow "living houses" with the branches of trees knitting together to give a solid, weatherproof exterior combined with an interior application of straw and clay to provide a stucco-like inner surface.

Tree shaping has been practised for at least several hundred years, the oldest known examples being the living root bridges built and maintained by the Khasi people of Meghalaya, India using the roots of the rubber tree ("Ficus elastica").

Cork is produced from the thick bark of the cork oak ("Quercus suber"). It is harvested from the living trees about once every ten years in an environmentally sustainable industry. More than half the world's cork comes from Portugal and is largely used to make stoppers for wine bottles. Other uses include floor tiles, bulletin boards, balls, footwear, cigarette tips, packaging, insulation and joints in woodwind instruments.

The bark of other varieties of oak has traditionally been used in Europe for the tanning of hides though bark from other species of tree has been used elsewhere. The active ingredient, tannin, is extracted and after various preliminary treatments, the skins are immersed in a series of vats containing solutions in increasing concentrations. The tannin causes the hide to become supple, less affected by water and more resistant to bacterial attack.

At least 120 drugs come from plant sources, many of them from the bark of trees. Quinine originates from the cinchona tree ("Cinchona") and was for a long time the remedy of choice for the treatment of malaria. Aspirin was synthesised to replace the sodium salicylate derived from the bark of willow trees ("Salix") which had unpleasant side effects. The anti-cancer drug Paclitaxel is derived from taxol, a substance found in the bark of the Pacific yew ("Taxus brevifolia"). Other tree based drugs come from the paw-paw ("Carica papaya"), the cassia ("Cassia spp."), the cocoa tree ("Theobroma cacao"), the tree of life ("Camptotheca acuminata") and the downy birch ("Betula pubescens").

The papery bark of the white birch tree ("Betula papyrifera") was used extensively by Native Americans. Wigwams were covered by it and canoes were constructed from it. Other uses included food containers, hunting and fishing equipment, musical instruments, toys and sledges. Nowadays, bark chips, a by-product of the timber industry, are used as a mulch and as a growing medium for epiphytic plants that need a soil-free compost.

Trees create a visual impact in the same way as do other landscape features and give a sense of maturity and permanence to park and garden. They are grown for the beauty of their forms, their foliage, flowers, fruit and bark and their siting is of major importance in creating a landscape. They can be grouped informally, often surrounded by plantings of bulbs, laid out in stately avenues or used as specimen trees. As living things, their appearance changes with the season and from year to year.

Trees are often planted in town environments where they are known as street trees or amenity trees. They can provide shade and cooling through evapotranspiration, absorb greenhouse gases and pollutants, intercept rainfall, and reduce the risk of flooding. It has been shown that they are beneficial to humans in creating a sense of well-being and reducing stress. Many towns have initiated tree-planting programmes. In London for example, there is an initiative to plant 20,000 new street trees and to have an increase in tree cover of 5% by 2025, equivalent to one tree for every resident.

Latex is a sticky defensive secretion that protects plants against herbivores. Many trees produce it when injured but the main source of the latex used to make natural rubber is the Pará rubber tree ("Hevea brasiliensis"). Originally used to create bouncy balls and for the waterproofing of cloth, natural rubber is now mainly used in tyres for which synthetic materials have proved less durable. The latex exuded by the balatá tree ("Manilkara bidentata") is used to make golf balls and is similar to gutta-percha, made from the latex of the "getah perca" tree "Palaquium". This is also used as an insulator, particularly of undersea cables, and in dentistry, walking sticks and gun butts. It has now largely been replaced by synthetic materials.

Resin is another plant exudate that may have a defensive purpose. It is a viscous liquid composed mainly of volatile terpenes and is produced mostly by coniferous trees. It is used in varnishes, for making small castings and in ten-pin bowling balls. When heated, the terpenes are driven off and the remaining product is called "rosin" and is used by stringed instrumentalists on their bows. Some resins contain essential oils and are used in incense and aromatherapy. Fossilised resin is known as amber and was mostly formed in the Cretaceous (145 to 66 million years ago) or more recently. The resin that oozed out of trees sometimes trapped insects or spiders and these are still visible in the interior of the amber.

The camphor tree ("Cinnamomum camphora") produces an essential oil and the eucalyptus tree ("Eucalyptus globulus") is the main source of eucalyptus oil which is used in medicine, as a fragrance and in industry.

Dead trees pose a safety risk, especially during high winds and severe storms, and removing dead trees involves a financial burden, whereas the presence of healthy trees can clean the air, increase property values, and reduce the temperature of the built environment and thereby reduce building cooling costs. During times of drought, trees can fall into water stress, which may cause a tree to become more susceptible to disease and insect problems, and ultimately may lead to a tree's death. Irrigating trees during dry periods can reduce the risk of water stress and death.

Trees have been venerated since time immemorial. To the ancient Celts, certain trees, especially the oak, ash and thorn, held special significance as providing fuel, building materials, ornamental objects and weaponry. Other cultures have similarly revered trees, often linking the lives and fortunes of individuals to them or using them as oracles. In Greek mythology, dryads were believed to be shy nymphs who inhabited trees.

The Oubangui people of west Africa plant a tree when a child is born. As the tree flourishes, so does the child but if the tree fails to thrive, the health of the child is considered at risk. When it flowers it is time for marriage. Gifts are left at the tree periodically and when the individual dies, their spirit is believed to live on in the tree.

Trees have their roots in the ground and their trunk and branches extended towards the sky. This concept is found in many of the world's religions as a tree which links the underworld and the earth and holds up the heavens. In Norse mythology, Yggdrasil is a central cosmic tree whose roots and branches extend to various worlds. Various creatures live on it. In India, Kalpavriksha is a wish-fulfilling tree, one of the nine jewels that emerged from the primitive ocean. Icons are placed beneath it to be worshipped, tree nymphs inhabit the branches and it grants favours to the devout who tie threads round the trunk. Democracy started in North America when the Great Peacemaker formed the Iroquois Confederacy, inspiring the warriors of the original five American nations to bury their weapons under the Tree of Peace, an eastern white pine ("Pinus strobus"). In the creation story in the Bible, the tree of life and the knowledge of good and evil was planted by God in the Garden of Eden.

Sacred groves exist in China, India, Africa and elsewhere. They are places where the deities live and where all the living things are either sacred or are companions of the gods. Folklore lays down the supernatural penalties that will result if desecration takes place for example by the felling of trees. Because of their protected status, sacred groves may be the only relicts of ancient forest and have a biodiversity much greater than the surrounding area.
Some Ancient Indian tree deities, such as Puliyidaivalaiyamman, the Tamil deity of the tamarind tree, or Kadambariyamman, associated with the kadamba tree were seen as manifestations of a goddess who offers her blessings by giving fruits in abundance.

Trees have a theoretical maximum height of , but the tallest known specimen on earth is believed to be a coast redwood ("Sequoia sempervirens") at Redwood National Park, California. It has been named Hyperion and is tall. In 2006, it was reported to be tall. The tallest known broad-leaved tree is a mountain ash ("Eucalyptus regnans") growing in Tasmania with a height of .

The largest tree by volume is believed to be a giant sequoia ("Sequoiadendron giganteum") known as the General Sherman Tree in the Sequoia National Park in Tulare County, California. Only the trunk is used in the calculation and the volume is estimated to be .

The oldest living tree with a verified age is also in California. It is a Great Basin bristlecone pine ("Pinus longaeva") growing in the White Mountains. It has been dated by drilling a core sample and counting the annual rings. It is estimated to currently be years old.

A little farther south, at Santa Maria del Tule, Oaxaca, Mexico, is the tree with the broadest trunk. It is a Montezuma cypress ("Taxodium mucronatum") known as Árbol del Tule and its diameter at breast height is giving it a girth of . The tree's trunk is far from round and the exact dimensions may be misleading as the circumference includes much empty space between the large buttress roots.


</doc>
<doc id="2052" url="https://en.wikipedia.org/wiki?curid=2052" title="Array data structure">
Array data structure

In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of "elements" (values or variables), each identified by at least one "array index" or "key". An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.

For example, an array of 10 32-bit (4 bytes) integer variables, with indices 0 through 9, may be stored as 10 words at memory addresses 2000, 2004, 2008, ... 2036, so that the element with index "i" has the address 2000 + ("i" × 4).

The memory address of the first element of an array is called first address, foundation address, or base address.

Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term "vector" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word "table" is sometimes used as a synonym of "array".

Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.

Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.

The term "array" is often used to mean array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.

The term is also used, especially in the description of algorithms, to mean associative array or "abstract array", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.

The first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. John von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer. Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.

Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), Lisp (1958), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.

Arrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.

Arrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient (implicit data structures), requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).

One or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate "dynamic memory" portably.

Arrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple codice_1 statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.

When data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indexes are also called subscripts. An index "maps" the array value to a stored object.

There are three ways in which the elements of an array can be indexed:


Using zero based indexing is the design choice of many influential programming languages, including C, Java and Lisp. This leads to simpler implementation where the subscript refers to an offset from the starting position of an array, so the first element has an offset of zero.

Arrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example, a two-dimensional array codice_2 with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression codice_3 in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and "n" for an "n"-dimensional array.

The number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.

In standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a "linear" formula on the indices.

A one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.

As an example consider the C declaration codice_4

Syntax : datatype anArrayName[sizeofArray];

In the given example the array can contain 10 elements of any value available to the codice_5 type. In C, the array element indices are 0-9 inclusive in this case. For example, the expressions codice_6 and codice_7 are the first and last elements respectively.

For a vector with linear addressing, the element with index "i" is located at the address "B" + "c" × "i", where "B" is a fixed "base address" and "c" a fixed constant, sometimes called the "address increment" or "stride".

If the valid element indices begin at 0, the constant "B" is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element "zeroth" rather than "first".

However, one can choose the index of the first element by an appropriate choice of the base address "B". For example, if the array has five elements, indexed 1 through 5, and the base address "B" is replaced by "B" + 30"c", then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant "B" may not be the address of any element.

For a multidimensional array, the element with indices "i","j" would have address "B" + "c" · "i" + "d" · "j", where the coefficients "c" and "d" are the "row" and "column address increments", respectively.

More generally, in a "k"-dimensional array, the address of an element with indices "i", "i", ..., "i" is

For example: int a[2][3];

This means that array a has 2 rows and 3 columns, and the array is of integer type. Here we can store 6 elements they will be 
stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a, a, a, a, a, a.

This formula requires only "k" multiplications and "k" additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.

The coefficients "c" must be chosen so that every valid index tuple maps to the address of a distinct element.

If the minimum legal value for every index is 0, then "B" is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address "B". Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing "B" by "B" + "c" - − 3 "c" will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.

The addressing formula is completely defined by the dimension "d", the base address "B", and the increments "c", "c", ..., "c". It is often useful to pack these parameters into a record called the array's "descriptor" or "stride vector" or "dope vector". The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.

Often the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.
There are two systematic compact layouts for a two-dimensional array. For example, consider the matrix
In the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:

In column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:

For arrays with three or more indices, "row major order" puts in consecutive positions any two elements whose index tuples differ only by one in the "last" index. "Column major order" is analogous with respect to the "first" index.

In systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product "A"·"B" of two matrices, it would be best to have "A" stored in row-major order, and "B" in column-major order.

Static arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a "dynamic" version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.

Some array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.

More complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.

Both "store" and "select" take (deterministic worst case) constant time. Arrays take linear (O("n")) space in the number of elements "n" that they hold.

In an array with element size "k" and on a machine with a cache line size of B bytes, iterating through an array of "n" elements requires the minimum of ceiling("nk"/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/"k" better than the number of cache misses needed to access "n" elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does "not" mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.

Memory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead, e.g. to store index bounds, but this is language-dependent. It can also happen that elements stored in an array require "less" memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called "packed" arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.

Array accesses with statically predictable access patterns are a major source of data parallelism.

Dynamic arrays or growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ("n")) additional storage, whereas arrays do not reserve additional storage.

Associative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.

Balanced trees require O(log "n") time for indexed access, but also permit inserting or deleting elements in O(log "n") time, whereas growable arrays require linear (Θ("n")) time to insert or delete elements at an arbitrary position.

Linked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.
An Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row(pointer on c or c++)
. Thus an element in row "i" and column "j" of an array "A" would be accessed by double indexing ("A"["i"]["j"] in typical notation). This alternative structure allows jagged arrays, where each row may have a different size — or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.

The dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array a rectangle of data, a three-dimensional array a block of data, etc.

This should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.


</doc>
<doc id="366016" url="https://en.wikipedia.org/wiki?curid=366016" title="Dope vector">
Dope vector

In computer programming, a dope vector is a data structure used to hold information about a data object, especially its memory layout.

Dope vectors are most commonly used to describe arrays, which commonly store multiple instances of a particular datatype as a contiguous block of memory. For example, an array containing 100 elements, each of which occupies 32 bytes, requires 100 × 32 bytes. By itself, such a memory block has no place to keep track of how large the array (or other object) is overall, how large each element within it is, or how many elements it contains. A dope vector is a place to store such information. Dope vectors can also describe structures which may contain arrays or variable elements.

If such an array is stored contiguously, with the first byte at memory location "M", then its last byte is at location . A major advantage of this arrangement is that locating item "N" is easy: it begins at location . Of course, the value 32 must be known (this value is commonly called the "stride" of the array or the "width" of the array's elements). Navigating an array data structure using an index is called dead reckoning.

This arrangement, however (without adding dope vectors) means that having the location of item N is not enough to discover the index N itself; or the stride; or whether there are elements at or . For example, a function or method may iterate over all the items in an array and pass each one to another function or method, which does not know the item is part of an array at all, much less where or how large the array is. 

Without a dope vector, even knowing the address of the entire array does not tell you how big it is. This is important because writing to the element in an array that only contains "N" elements, will likely destroy some other data. Because many programming languages treat character strings as a kind of array, this leads directly to the infamous Buffer overflow problem.

A dope vector reduces these problems by storing a small amount of metadata along with an array (or other object). With dope vectors, a compiler can easily (and optionally) insert code that prevents accidentally writing beyond the end of an array or other object. Alternatively, the programmer can access the dope vector when desired, for safety or other purposes.

The exact set of metadata included in a dope vector varies from one language and/or operating system to another, but a dope vector for an array might contain: 


A program then can refer to the array (or other dope-vector-using object) by referring to the dope vector. This is commonly automatic in high level languages. Getting to an element of the array costs a tiny bit more (commonly one instruction, which fetches the pointer to the actual data from out of the dope vector). On the other hand, doing many other common operations are easier and/or faster:




Even with a dope vector, having (only) a pointer to a particular member of an array does not enable finding the position in the array, or the location of the array or the dope vector itself. If that is desired, such information can be added to each element within the array. Such per-element information can be useful, but is not part of the dope vector.

Dope vectors can be a general facility, shared across multiple datatypes (not just arrays and/or strings)



</doc>
<doc id="1696737" url="https://en.wikipedia.org/wiki?curid=1696737" title="Iliffe vector">
Iliffe vector

In computer programming, an Iliffe vector, also known as a display, is a data structure used to implement multi-dimensional arrays. An Iliffe vector for an "n"-dimensional array (where "n" ≥ 2) consists of a vector (or 1-dimensional array) of pointers to an ("n" − 1)-dimensional array. They are often used to avoid the need for expensive multiplication operations when performing address calculation on an array element. They can also be used to implement jagged arrays, such as triangular arrays, triangular matrices and other kinds of irregularly shaped arrays. The data structure is named after John K. Iliffe.

Their disadvantages include the need for multiple chained pointer indirections to access an element, and the extra work required to determine the next row in an "n"-dimensional array to allow an optimising compiler to prefetch it. Both of these are a source of delays on systems where the CPU is significantly faster than main memory.

The Iliffe vector for a 2-dimensional array is simply a vector of pointers to vectors of data, i.e., the Iliffe vector represents the columns of an array where each column element is a pointer to a row vector.

Multidimensional arrays in languages such as Java, Python (multidimensional lists), Ruby, Visual Basic .NET, Perl, PHP, JavaScript, Objective-C (when using NSArray, not a row-major C-style array), Swift, and Atlas Autocode are implemented as Iliffe vectors. Iliffe vectors were used to implement sparse multidimensional arrays in the OLAP product Holos.

Iliffe vectors are contrasted with dope vectors in languages such as Fortran, which contain the stride factors and offset values for the subscripts in each dimension.



</doc>
<doc id="1456434" url="https://en.wikipedia.org/wiki?curid=1456434" title="Dynamic array">
Dynamic array

In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. It is supplied with standard libraries in many modern mainstream programming languages. Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation.

A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end.

A simple dynamic array can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. The elements of the dynamic array are stored contiguously at the start of the underlying array, and the remaining positions towards the end of the underlying array are reserved, or unused. Elements can be added at the end of a dynamic array in constant time by using the reserved space, until this space is completely consumed. When all space is consumed, and an additional element is to be added, then the underlying fixed-sized array needs to be increased in size. Typically resizing is expensive because it involves allocating a new underlying array and copying each element from the original array. Elements can be removed from the end of a dynamic array in constant time, as no resizing is required. The number of elements used by the dynamic array contents is its "logical size" or "size", while the size of the underlying array is called the dynamic array's "capacity" or "physical size", which is the maximum possible size without relocating data. 

A fixed-size array will suffice in applications where the maximum logical size is fixed (e.g. by specification), or can be calculated before the array is allocated. A dynamic array might be preferred if:

To avoid incurring the cost of resizing many times, dynamic arrays resize by a large amount, such as doubling in size, and use the reserved space for future expansion. The operation of adding an element to the end might work as follows:

function insertEnd(dynarray a, element e)

As "n" elements are inserted, the capacities form a geometric progression. Expanding the array by any constant proportion "a" ensures that inserting "n" elements takes "O"("n") time overall, meaning that each insertion takes amortized constant time. Many dynamic arrays also deallocate some of the underlying storage if its size drops below a certain threshold, such as 30% of the capacity. This threshold must be strictly smaller than 1/"a" in order to provide hysteresis (provide a stable band to avoid repeatedly growing and shrinking) and support mixed sequences of insertions and removals with amortized constant cost.

Dynamic arrays are a common example when teaching amortized analysis.

The growth factor for the dynamic array depends on several factors including a space-time trade-off and algorithms used in the memory allocator itself. For growth factor "a", the average time per insertion operation is . If memory allocator uses a first-fit allocation algorithm, then growth factor values such as "a=2" can cause dynamic array expansion to run out of memory even though a significant amount of memory may still be available. There have been various discussions on ideal growth factor values, including proposals for the golden ratio as well as the value 1.5. Many textbooks, however, use "a" = 2 for simplicity and analysis purposes.

Below are growth factors used by several popular implementations:
The dynamic array has performance similar to an array, with the addition of new operations to add and remove elements:


Dynamic arrays benefit from many of the advantages of arrays, including good locality of reference and data cache utilization, compactness (low memory use), and random access. They usually have only a small fixed additional overhead for storing information about the size and capacity. This makes dynamic arrays an attractive tool for building cache-friendly data structures. However, in languages like Python or Java that enforce reference semantics, the dynamic array generally will not store the actual data, but rather it will store references to the data that resides in other areas of memory. In this case, accessing items in the array sequentially will actually involve accessing multiple non-contiguous areas of memory, so the many advantages of the cache-friendliness of this data structure are lost.

Compared to linked lists, dynamic arrays have faster indexing (constant time versus linear time) and typically faster iteration due to improved locality of reference; however, dynamic arrays require linear time to insert or delete at an arbitrary location, since all following elements must be moved, while linked lists can do this in constant time. This disadvantage is mitigated by the gap buffer and "tiered vector" variants discussed under "Variants" below. Also, in a highly fragmented memory region, it may be expensive or impossible to find contiguous space for a large dynamic array, whereas linked lists do not require the whole data structure to be stored contiguously.

A balanced tree can store a list while providing all operations of both dynamic arrays and linked lists reasonably efficiently, but both insertion at the end and iteration over the list are slower than for a dynamic array, in theory and in practice, due to non-contiguous storage and tree traversal/manipulation overhead.

Gap buffers are similar to dynamic arrays but allow efficient insertion and deletion operations clustered near the same arbitrary location. Some deque implementations use array deques, which allow amortized constant time insertion/removal at both ends, instead of just one end.

Goodrich presented a dynamic array algorithm called "tiered vectors" that provided O(n) performance for order preserving insertions or deletions from the middle of the array.

Hashed array tree (HAT) is a dynamic array algorithm published by Sitarski in 1996. Hashed array tree wastes order n amount of storage space, where n is the number of elements in the array. The algorithm has O(1) amortized performance when appending a series of objects to the end of a hashed array tree.

In a 1999 paper, Brodnik et al. describe a tiered dynamic array data structure, which wastes only n space for "n" elements at any point in time, and they prove a lower bound showing that any dynamic array must waste this much space if the operations are to remain amortized constant time. Additionally, they present a variant where growing and shrinking the buffer has not only amortized but worst-case constant time.

Bagwell (2002) presented the VList algorithm, which can be adapted to implement a dynamic array.

C++'s codice_1 and Rust's codice_2 are implementations of dynamic arrays, as are the codice_3 classes supplied with the Java API and the .NET Framework. 

The generic codice_4 class supplied with version 2.0 of the .NET Framework is also implemented with dynamic arrays. Smalltalk's codice_5 is a dynamic array with dynamic start and end-index, making the removal of the first element also O(1). 

Python's codice_6 datatype implementation is a dynamic array. 

Delphi and D implement dynamic arrays at the language's core. 

Ada's generic package provides dynamic array implementation for a given subtype. 

Many scripting languages such as Perl and Ruby offer dynamic arrays as a built-in primitive data type. 

Several cross-platform frameworks provide dynamic array implementations for C, including codice_7 and codice_8 in Core Foundation, and codice_9 and codice_10 in GLib.



</doc>
<doc id="12673184" url="https://en.wikipedia.org/wiki?curid=12673184" title="Hashed array tree">
Hashed array tree

In computer science, a hashed array tree (HAT) is a dynamic array data-structure published by Edward Sitarski in 1996, maintaining an array of separate memory fragments (or "leaves") to store the data elements, unlike simple dynamic arrays which maintain their data in one contiguous memory area. Its primary objective is to reduce the amount of element copying due to automatic array resizing operations, and to improve memory usage patterns.

Whereas simple dynamic arrays based on geometric expansion waste linear (Ω("n")) space, where "n" is the number of elements in the array, hashed array trees waste only order "O"() storage space. An optimization of the algorithm allows elimination of data copying completely, at a cost of increasing the wasted space.

It can perform access in constant (O(1)) time, though slightly slower than simple dynamic arrays. The algorithm has O(1) amortized performance when appending a series of objects to the end of a hashed array tree. Contrary to its name, it does not use hash functions.

As defined by Sitarski, a hashed array tree has a top-level directory containing a power of two number of leaf arrays. All leaf arrays are the same size as the top-level directory. This structure superficially resembles a hash table with array-based collision chains, which is the basis for the name "hashed array tree". A full hashed array tree can hold "m" elements, where "m" is the size of the top-level directory. The use of powers of two enables faster physical addressing through bit operations instead of arithmetic operations of quotient and remainder and ensures the O(1) amortized performance of append operation in the presence of occasional global array copy while expanding.

In a usual dynamic array geometric expansion scheme, the array is reallocated as a whole sequential chunk of memory with the new size a double of its current size (and the whole data is then moved to the new location). This ensures O(1) amortized operations at a cost of O(n) wasted space, as the enlarged array is filled to the half of its new capacity.

When a hashed array tree is full, its directory and leaves must be restructured to twice their prior size to accommodate additional append operations. The data held in old structure is then moved into the new locations. Only one new leaf is then allocated and added into the top array which thus becomes filled only to a quarter of its new capacity. All the extra leaves are not allocated yet, and will only be allocated when needed, thus wasting only "O"() of storage.

There are multiple alternatives for reducing size: when a hashed array tree is one eighth full, it can be restructured to a smaller, half-full hashed array tree; another option is only freeing unused leaf arrays, without resizing the leaves. Further optimizations include adding new leaves without resizing while growing the directory array as needed, possibly through geometric expansion. This will eliminate the need for data copying completely at the cost of making the wasted space be "O"("n"), with a small constant, and only performing restructuring when a set threshold overhead is reached.

Brodnik et al. presented a dynamic array algorithm with a similar space wastage profile to hashed array trees. Brodnik's implementation retains previously allocated leaf arrays, with a more complicated address calculation function as compared to hashed array trees.



</doc>
<doc id="378974" url="https://en.wikipedia.org/wiki?curid=378974" title="Gap buffer">
Gap buffer

A gap buffer in computer science is a dynamic array that allows efficient insertion and deletion operations clustered near the same location. Gap buffers are especially common in text editors, where most changes to the text occur at or near the current location of the cursor. The text is stored in a large buffer in two contiguous segments, with a gap between them for inserting new text. Moving the cursor involves copying text from one side of the gap to the other (sometimes copying is delayed until the next operation that changes the text). Insertion adds new text at the end of the first segment; deletion deletes it.

Text in a gap buffer is represented as two strings, which take very little extra space and which can be searched and displayed very quickly, compared to more sophisticated data structures such as linked lists. However, operations at different locations in the text and ones that fill the gap (requiring a new gap to be created) may require copying most of the text, which is especially inefficient for large files. The use of gap buffers is based on the assumption that such recopying occurs rarely enough that its cost can be amortized over the more common cheap operations. This makes the gap buffer a simpler alternative to the rope for use in text editors such as Emacs.

Below are some examples of operations with buffer gaps. The gap is represented by the empty space between the square brackets. This representation is a bit misleading: in a typical implementation, the endpoints of the gap are tracked using pointers or array indices, and the contents of the gap are ignored; this allows, for example, deletions to be done by adjusting a pointer without changing the text in the buffer. It is a common programming practice to use a semi-open interval for the gap pointers, i.e. the start-of-gap points to the invalid character following the last character in the first buffer, and the end-of-gap points to the first valid character in the second buffer (or equivalently, the pointers are considered to point "between" characters).

Initial state:

User inserts some new text:

User moves the cursor before "started"; system moves "started " from the first buffer to the second buffer.

User adds text filling the gap; system creates new gap:




</doc>
<doc id="11891734" url="https://en.wikipedia.org/wiki?curid=11891734" title="Circular buffer">
Circular buffer

A circular buffer, circular queue, cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end.
This structure lends itself easily to buffering data streams.

The useful property of a circular buffer is that it does not need to have its elements shuffled around when one is consumed.
In other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer.

Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a linked list approach may be preferred instead.

In some situations, overwriting circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card) is unable to momentarily keep up. Also, the LZ77 family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer.

A circular buffer first starts empty and of some predefined length.
For example, this is a 7-element buffer:

Assume that a 1 is written into the middle of the buffer (exact starting location does not matter in a circular buffer):

Then assume that two more elements are added — 2 & 3 — which get appended after the 1:

If two elements are then removed from the buffer, the oldest values inside the buffer are removed.
The two elements removed, in this case, are 1 & 2, leaving the buffer with just a 3:

If the buffer has 7 elements, then it is completely full:

A property of the circular buffer is that when it is full and a subsequent write is performed, then it starts overwriting the oldest data.
In the current example, two more elements — A & B — are added and they "overwrite" the 3 & 4:

Alternatively, the routines that manage the buffer could prevent overwriting the data and return an error or raise an exception.
Whether or not data is overwritten is up to the semantics of the buffer routines or the application using the circular buffer.

Finally, if two elements are now removed then what would be returned is not 3 & 4 but 5 & 6 because A & B overwrote the 3 & the 4 yielding the buffer with:

A circular buffer can be implemented using four pointers, or two pointers and two integers:

This image shows a partially full buffer:

This image shows a full buffer with four elements (numbers 1 through 4) having been overwritten:

When an element is overwritten, the start pointer is incremented to the next element.

In utilizing full buffer capacity with pointer-based implementation strategy, the buffer's full or empty state could not be resolved directly from checking the positions of the start and end indexes. Therefore, an additional mechanism must be implemented for checking this. One common way to deal with this, when using 2 pointers, is to only allow the buffer to hold (size - 1) items. When both pointers are equal, the buffer is empty, and when the end pointer is one less than the start pointer, the buffer is full.

When the buffer is instead designed to track the number of inserted elements , checking for emptiness means checking and checking for fullness means checking whether equals the capacity.

Incrementing and Decrementing the circular buffer address pointers is accomplished in software using the following modulus formulas:

Note, the extra Length addition for the decrement by one operation is required to prevent negative results and to ensure proper rollover the end address of the circular buffer.

A circular-buffer implementation may be optimized by mapping the underlying buffer to two contiguous regions of virtual memory. (Naturally, the underlying buffer‘s length must then equal some multiple of the system’s page size.) Reading from and writing to the circular buffer may then be carried out with greater efficiency by means of direct memory access; those accesses which fall beyond the end of the first virtual-memory region will automatically wrap around to the beginning of the underlying buffer. When the read offset is advanced into the second virtual-memory region, both offsets—read and write—are decremented by the length of the underlying buffer.

Perhaps the most common version of the circular buffer uses 8-bit bytes as elements.

Some implementations of the circular buffer use fixed-length elements that are bigger than 8-bit bytes—16-bit integers for audio buffers,
53-byte ATM cells for telecom buffers, etc.
Each item is contiguous and has the correct data alignment,
so software reading and writing these values can be faster than software that handles non-contiguous and non-aligned values.

Ping-pong buffering can be considered a very specialized circular buffer with exactly two large fixed-length elements.

The Bip Buffer (bipartite buffer) is very similar to a circular buffer, except it always returns contiguous blocks which can be variable length. This offers nearly all the efficiency advantages of a circular buffer while maintaining the ability for the buffer to be used in APIs that only accept contiguous blocks.

Fixed-sized compressed circular buffers use an alternative indexing strategy based on elementary number theory to maintain a fixed-sized compressed representation of the entire data sequence.



</doc>
<doc id="1189937" url="https://en.wikipedia.org/wiki?curid=1189937" title="Bit array">
Bit array

A bit array (also known as bit map, bit set, bit string, or bit vector) is an array data structure that compactly stores bits. It can be used to implement a simple set data structure. A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores "kw" bits, where "w" is the number of bits in the unit of storage, such as a byte or word, and "k" is some nonnegative integer. If "w" does not divide the number of bits to be stored, some space is wasted due to internal fragmentation.

A bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. As with other arrays, the access to a single bit can be managed by applying an index to the array. Assuming its size (or length) to be "n" bits, the array can be used to specify a subset of the domain (e.g. {0, 1, 2, ..., "n"−1}), where a 1-bit indicates the presence and a 0-bit the absence of a number in the set. This set data structure uses about "n"/"w" words of space, where "w" is the number of bits in each machine word. Whether the least significant bit (of the word) or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred (on little-endian machines).

Although most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using bitwise operations. In particular:

To obtain the bit mask needed for these operations, we can use a bit shift operator to shift the number 1 to the left by the appropriate number of places, as well as bitwise negation if necessary.

Given two bit arrays of the same size representing sets, we can compute their union, intersection, and set-theoretic difference using "n"/"w" simple bit operations each (2"n"/"w" for difference), as well as the complement of either:

for i from 0 to n/w-1
If we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only "n"/"w" memory accesses are required:

for i from 0 to n/w-1

Both of these code samples exhibit ideal locality of reference, which will subsequently receive large performance boost from a data cache. If a cache line is "k" words, only about "n"/"wk" cache misses will occur.

As with character strings it is straightforward to define "length", "substring", lexicographical "compare", "concatenation", "reverse" operations. The implementation of some of these operations is sensitive to endianness.

If we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the Hamming weight article for examples of an efficient implementation.

Vertical flipping of a one-bit-per-pixel image, or some FFT algorithms, requires flipping the bits of individual words (so codice_1 becomes codice_2).
When this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits:
exchange two 16bit halfwords
exchange bytes by pairs (0xddccbbaa -> 0xccddaabb)
swap bits by pairs
swap bits (b31 b30 ... b1 b0 -> b30 b31 ... b0 b1)

The last operation can be written ((x&0x55555555)«1) | (x&0xaaaaaaaa)»1)).
The find first set or "find first one" operation identifies the index or position of the 1-bit with the smallest index in an array, and has widespread hardware support (for arrays not larger than a word) and efficient algorithms for its computation. When a priority queue is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size "find first one" to longer arrays, one can find the first nonzero word and then run "find first one" on that word. The related operations "find first zero", "count leading zeros", "count leading ones", "count trailing zeros", "count trailing ones", and "log base 2" (see find first set) can also be extended to a bit array in a straightforward manner.

A bit array is the most dense storage for "random" bits, that is, where each bit is equally likely to be 0 or 1, and each one is independent. But most data is not random, so it may be possible to store it more compactly. For example, the data of a typical fax image is not random and can be compressed. Run-length encoding is commonly used to compress these long streams. However, most compressed data formats are not so easy to access randomly; also by compressing bit arrays too aggressively we run the risk of losing the benefits due to bit-level parallelism (vectorization). Thus, instead of compressing bit arrays as streams of bits, we might compress them as streams of bytes or words (see Bitmap index (compression)).

Bit arrays, despite their simplicity, have a number of marked advantages over other data structures for the same problems:
However, bit arrays aren't the solution to everything. In particular:

Because of their compactness, bit arrays have a number of applications in areas where space or efficiency is at a premium. Most commonly, they are used to represent a simple group of boolean flags or an ordered sequence of boolean values.

Bit arrays are used for priority queues, where the bit at index "k" is set if and only if "k" is in the queue; this data structure is used, for example, by the Linux kernel, and benefits strongly from a find-first-zero operation in hardware.

Bit arrays can be used for the allocation of memory pages, inodes, disk sectors, etc. In such cases, the term "bitmap" may be used. However, this term is frequently used to refer to raster images, which may use multiple bits per pixel.

Another application of bit arrays is the Bloom filter, a probabilistic set data structure that can store large sets in a small space in exchange for a small probability of error. It is also possible to build probabilistic hash tables based on bit arrays that accept either false positives or false negatives.

Bit arrays and the operations on them are also important for constructing succinct data structures, which use close to the minimum possible space. In this context, operations like finding the "n"th 1 bit or counting the number of 1 bits up to a certain position become important.

Bit arrays are also a useful abstraction for examining streams of compressed data, which often contain elements that occupy portions of bytes or are not byte-aligned. For example, the compressed Huffman coding representation of a single 8-bit character can be anywhere from 1 to 255 bits long.

In information retrieval, bit arrays are a good representation for the posting lists of very frequent terms. If we compute the gaps between adjacent values in a list of strictly increasing integers and encode them using unary coding, the result is a bit array with a 1 bit in the "n"th position if and only if "n" is in the list. The implied probability of a gap of "n" is 1/2. This is also the special case of Golomb coding where the parameter M is 1; this parameter is only normally selected when -log(2-"p")/log(1-"p") ≤ 1, or roughly the term occurs in at least 38% of documents.

The APL programming language fully supports bit arrays of arbitrary shape and size as a Boolean datatype distinct from integers. All major implementations (Dyalog APL, APL2, APL Next, NARS2000, Gnu APL, etc.) pack the bits densely into whatever size the machine word is. Bits may be accessed individually via the usual indexing notation (A[3]) as well as through all of the usual primitive functions and operators where they are often operated on using a special case algorithm such as summing the bits via a table lookup of bytes.

The C programming language's "bit fields", pseudo-objects found in structs with size equal to some number of bits, are in fact small bit arrays; they are limited in that they cannot span words. Although they give a convenient syntax, the bits are still accessed using bitwise operators on most machines, and they can only be defined statically (like C's static arrays, their sizes are fixed at compile-time). It is also a common idiom for C programmers to use words as small bit arrays and access bits of them using bit operators. A widely available header file included in the X11 system, xtrapbits.h, is “a portable way for systems to define bit field manipulation of arrays of bits.” A more explanatory description of aforementioned approach can be found in the comp.lang.c faq.

In C++, although individual codice_3s typically occupy the same space as a byte or an integer, the STL type codice_4 is a partial template specialization in which bits are packed as a space efficiency optimization. Since bytes (and not bits) are the smallest addressable unit in C++, the [] operator does "not" return a reference to an element, but instead returns a proxy reference. This might seem a minor point, but it means that codice_4 is "not" a standard STL container, which is why the use of codice_4 is generally discouraged. Another unique STL class, codice_7, creates a vector of bits fixed at a particular size at compile-time, and in its interface and syntax more resembles the idiomatic use of words as bit sets by C programmers. It also has some additional power, such as the ability to efficiently count the number of bits that are set. The Boost C++ Libraries provide a codice_8 class whose size is specified at run-time.

The D programming language provides bit arrays in its standard library, Phobos, in codice_9. As in C++, the [] operator does not return a reference, since individual bits are not directly addressable on most hardware, but instead returns a codice_3.

In Java, the class creates a bit array that is then manipulated with functions named after bitwise operators familiar to C programmers. Unlike the codice_7 in C++, the Java codice_12 does not have a "size" state (it has an effectively infinite size, initialized with 0 bits); a bit can be set or tested at any index. In addition, there is a class , which represents a Set of values of an enumerated type internally as a bit vector, as a safer alternative to bit fields.

The .NET Framework supplies a codice_13 collection class. It stores boolean values, supports random access and bitwise operators, can be iterated over, and its codice_14 property can be changed to grow or truncate it.

Although Standard ML has no support for bit arrays, Standard ML of New Jersey has an extension, the codice_13 structure, in its SML/NJ Library. It is not fixed in size and supports set operations and bit operations, including, unusually, shift operations.

Haskell likewise currently lacks standard support for bitwise operations, but both GHC and Hugs provide a codice_16 module with assorted bitwise functions and operators, including shift and rotate operations and an "unboxed" array over boolean values may be used to model a Bit array, although this lacks support from the former module.

In Perl, strings can be used as expandable bit arrays. They can be manipulated using the usual bitwise operators (codice_17), and individual bits can be tested and set using the "vec" function.

In Ruby, you can access (but not set) a bit of an integer (codice_18 or codice_19) using the bracket operator (codice_20), as if it were an array of bits.

Apple's Core Foundation library contains CFBitVector and CFMutableBitVector structures.

PL/I supports arrays of "bit strings" of arbitrary length, which may be either fixed-length or varying. The array elements may be "aligned"— each element begins on a byte or word boundary— or "unaligned"— elements immediately follow each other with no padding.

PL/pgSQL and PostgreSQL's SQL support "bit strings" as native type. There are two SQL bit types: codice_21")</code> and codice_22")</code>, where "codice_23" is a positive integer.

Hardware description languages such as VHDL, Verilog, and SystemVerilog natively support bit vectors as these are used to model storage elements like flip-flops, hardware busses and hardware signals in general. In hardware verification languages such as OpenVera, "e" and SystemVerilog, bit vectors are used to sample values from the hardware models, and to represent data that is transferred to hardware during simulations.




</doc>
<doc id="284528" url="https://en.wikipedia.org/wiki?curid=284528" title="Bitboard">
Bitboard

A bitboard is a specialized bit array data structure commonly used in computer systems that play board games, where each bit corresponds to a game board space or piece. This allows parallel bitwise operations to set or query the game state, or determine moves or plays in the game.

Bits in the same bitboard relate to each other by the rules of the game, often forming a game position when taken together. Other bitboards are commonly used as masks to transform or answer queries about positions. Bitboards are applicable to any game whose progress is represented by the state of, or presence of pieces on, discrete spaces of a gameboard, by mapping of the space states to bits in the data structure. Bitboards are a more efficient alternative board representation to the traditional "mailbox" representation, where each piece or space on the board is an array element.

Bitboards are especially effective when the associated bits of various related states on the board fit into a single word or double word of the CPU architecture, so that single bitwise operators like AND and OR can be used to build or query game states.

Among the computer game implementations that use bitboards are chess, checkers, othello and word games. The scheme was first employed in checkers programs in the 1950s, and since the mid-1970s has been the de facto standard for game board representation in computer automatons. 

A bitboard, a specialized bit field, is a format that packs multiple related boolean variables into the same machine word, typically representing a position on a board game, or state of a game. Each bit represents a space; when the bit is positive, a property of that space is true. Bitboards allow the computer to answer some questions about game state with one bitwise operation. For example, if a chess program wants to know if the white player has any pawns in the center of the board (center four squares) it can just compare a bitboard for the player's pawns with one for the center of the board using a bitwise AND operation. If there are no center pawns then the result will be all zero bits (i.e. equal to zero). Multiple bitboards may represent different properties of spaces over the board, and special or temporary bitboards (like temporary variables) may represent local properties or hold intermediate collated results.

The efficacy of bitboards is augmented by two other properties of the implementation. First, bitboards are fast to incrementally update, such as flipping the bits at the source and destination positions in a bitboard for piece location when a piece is moved. Second, bitmaps representing static properties like all spaces attacked by each piece type for every position on a chessboard can be pre-collated and stored in a table, so that answering a question like "what are the legal moves of a knight on space e4?" can be answered by a single memory fetch.

Bitfield implementations take advantage of the presence of fullword (32-bit or 64-bit) bitwise logical operations like AND, OR, NOT and others on modern CPU architectures in order to be efficient. Bitboards may not be effective on earlier 8- and 16-bit minicomputer and microprocessor architectures.

As a result of necessary compression and encoding of the contents of massive tables and the probability of transcription or encoding errors, bitboard programs are tedious for software developers to either write or debug. Auxiliary generative methods not part of the application are usually required to build the tables.

Bitboard representations use parallel bitwise operations available on nearly all CPUs that complete in one cycle and are fully pipelined and cached etc. Nearly all CPUs have AND, OR, NOR, and XOR. 
Furthermore, modern CPUs have instruction pipelines that queue instructions for execution. A processor with multiple execution units can perform more than one instruction per cycle if more than one instruction is available in the pipeline. Normal instruction sequences with branches may cause the pipeline to empty if a branch is mispredicted. Many bitboard operations require fewer conditionals and therefore increase pipelining and make effective use of multiple execution units on many CPUs.

CPUs have a bit width which they are designed toward and can carry out bitwise operations in one cycle in this width. So, on a 64-bit or more CPU, 64-bit operations can occur in one instruction. There may be support for higher or lower width instructions. Many 32-bit CPUs may have some 64-bit instructions and those may take more than one cycle or otherwise be handicapped compared to their 32-bit instructions.

If the bitboard is larger than the width of the instruction set, multiple instructions will be required to perform a full-width operation on it. So a program using 64-bit bitboards would run faster on a 64-bit processor than on a 32-bit processor.

Bitboard representations have much longer code, both source and object code. Long bit-twiddling sequences are technically tricky to write and debug. The bitboards themselves are sparse, sometimes containing only a single one bit in 64, so bitboard implementations are memory-intensive. Both these issues may increase cache misses or cause cache threshing.

If the processor does not have hardware instructions for 'first one' (or 'leading zeros') and 'count ones' (or 'count zeros'), the implementation will be significantly handicapped, as these operations are extremely inefficient to code as assembly language loops.

Bitboards require more memory than piece-list board data structures, but are more execution efficient because many loop-and-compare operations are reduced to a single (or small number of) bitwise operation(s). For example, in mailbox, to determine whether <piece> attacks <space> requires generating and looping through legal moves of <piece> and comparing them to <space>. With bitboards, the legal moves of <piece> are stored in a bitmap, and that map is ANDed with the bitmap for <space>. A non-zero result means <piece> attacks <space>.

For some games, writing a bitboard engine requires a fair amount of source code including data tables that will be longer than the compact mailbox/enumeration implementation. For mobile devices (such as cell phones) with a limited number of registers or processor instruction cache, this can cause a problem. For full-sized computers, it may cause cache misses between level-one and level-two cache. This is only a potential problem, not a major drawback, as most machines will have enough instruction cache for this not to be an issue.

Some kinds of bitboards are derived from others by an elaborate process of cross-correlation, such as the attack maps in chess. Reforming all these maps at each change of game state (such as a move) can be prohibitively expensive, so derived bitmaps are incrementally updated, a process which requires intricate and precise code. This is much faster to execute, because only bitmaps associated with changed spaces, not all bitmaps over the board, need to change. Without incremental update, bitmapped representation may not be more efficient than the older mailbox representation where update is intrinsically local and incremental.

Some kinds of bitmaps that don't depend on board configurations can be precomputed and retrieved by table lookup rather than collated after a move or state change of the board, such as spaces attacked by a knight or king located on each of 64 spaces of a chessboard that would otherwise require an enumeration.

The obvious, and simplest representation of the configuration of pieces on a chessboard, is as a list (array) of pieces in a conveniently searchable order (such as smallest to largest in value) that maps each piece to its location on the board. Analogously, collating the spaces attacked by each piece requires a serial enumeration of such spaces for a piece. This scheme is called "mailbox addressing". Separate lists are maintained for white and black pieces, and often for white and black pawns. The maps are updated each move, which requires a linear search (or two if a piece was captured) through the piece list. The advantage of mailbox is simple code; the disadvantage is linear lookups are slow. Faster, but more elaborate data structures that map pieces to locations are called "bitboards".

In bitboard representations, each bit of a 64 bit word (or double word on 32-bit architectures) is associated with a square of the chessboard. Any mapping of bits to squares can be used, but by broad convention, bits are associated with squares from left to right and bottom to top, so that bit 0 represents square a1, bit 7 is square h1, bit 56 is square a8 and bit 63 is square h8.

Many different configurations of the board are usually represented by their own bitboards including the locations of the kings, all white pawns, all black pawns, as well as bitboards for each of the other piece types or combinations of pieces like all white pieces. Two attack bitboards are also universal: one bitboard per space for all pieces attacking space, and the inverse bitboard for all spaces attacked by piece for each space containing a piece. Bitboards can be also be constants like one representing the first rank, which would have one bits in positions 0 - 7. Other local or transitional bitboards like "all spaces adjacent to the king attacked by opposing pieces" may be collated as necessary or convenient.

An example of the use of the bitboards would be determining whether a piece is "enprise": bitboards for "all friendly pieces guarding <space>" and "all opposing pieces attacking <space>" would allow matching the pieces to readily determine whether a target piece on <space> is "enprise".

One of the drawbacks of standard bitboards is collating the attack vectors of the sliding pieces (rook, bishop, queen), because they have an indefinite number of attack spaces depending on other occupied spaces. This requires several lengthy sequences of masks, shifts and complements per piece.

In acknowledgement of the code size and computing complexity of generating bitboards for the attack vectors of sliding pieces, alternate bitboard data structures have been devised to collate them. The bitboard representations of knights, kings, pawns and other board configurations is unaffected by the use of auxiliary bitboards for the sliding pieces.
Rotated bitboards are complementary bitboard data structures that enable tabularizing of sliding piece attack vectors, one for file attack vectors of rooks, and one each for the diagonal and anti-diagonal attack vectors of bishops (rank attacks of rooks can be indexed from standard bitboards). With these bitboards, a single table lookup replaces lengthy sequences of bitwise operations.

These bitboards rotate the board occupancy configuration by 90 degrees, 45 degrees, and/or 315 degrees. A standard bitboard will have one byte per rank of the chess board. With this bitboard it's easy to determine rook attacks across a rank, using a table indexed by the occupied square and the occupied positions in the rank (because rook attacks stop at the first occupied square). By rotating the bitboard 90 degrees, rook attacks up and down a file can be examined the same way. Adding bitboards rotated 45 degrees and 315 degrees (-45 degrees) produces bitboards in which the diagonals are easy to examine. The queen can be examined by combining rook and bishop attacks. Actually rotating a bitboard is an inelegant transformation that can take dozens of instructions.

The rank and file attack vectors of rooks and the diagonal and anti-diagonal attack vectors of bishops can be separately masked and used as indices into a hash table of precomputed attack vectors depending on occupancy, 8-bits each for rooks and 2-8 bits each for bishops. The full attack vector of a piece is obtained as the union of each of the two unidirectional vectors indexed from the hash table. The number of entries in the hash table is modest, on the order of 8*2^8 or 2K bytes, but two hash function computations and two lookups per piece are required. For the hashing scheme employed, see 

Magic bitboards are an extrapolation of the time-space tradeoff of direct hashing lookup of attack vectors. These use a transmutation of the full attack vector as an index into the hash table. "Magic" is a misnomer, and simply refers to the generation and use of a perfect hash function in conjunction with tricks to reduce the potential size of the hash table that would have to be stored in memory, which is 8*2^64 or 144 exabytes. The first observation is that the "outer squares" or first and eighth ranks together with the 'a' and 'h' files are irrelevant to the occupancy of the attack vector: the piece attacks those squares or not (depending on other blocking pieces) regardless of occupancy, so these can be eliminated from consideration, leaving just 6x6 or 36 squares (~bits in the corresponding hash function). As with other schemes which require a perfect hashing function, an exhaustive process of enumeration, partly algorithmic and partly trial and error, is necessary to generate the hash function. But the intractable issue remains: these are very active tables, and their size (less than a million entries in most cases) is huge relative to the lower level cache sizes of modern chip architectures, resulting in cache flooding. So magic bitboards in many application provide no performance gain over more modest hashing schemes or rotated bitboards.

The bitboard method for representing a board game appears to have been invented in the mid-1950s, by Arthur Samuel and was used in his checkers program.
For the more complicated game of chess, it appears the method was independently rediscovered later by the Kaissa team in the Soviet Union in the late 1960s, and again by the authors of the U.S. Northwestern University program "Chess" in the early 1970s. The 64-bit word length of 1970s super computers like Amdahl and Cray machines, facilitated the development of bitboard representations which conveniently mapped the 64-squares of the chessboard to bits of a word.

Rotated bitboards for collating the moves of sliding pieces were invented by Professor Robert Hyatt, author of Cray Blitz and Crafty chess engines, sometime in the mid-1990s and shared with the Dark Thought programming team. They were later implemented in Crafty and Dark Thought, but the first published description wasn't until 1997. 

A decade later, direct lookup methods using masked ranks, files and diagonals to index a table of attack vectors depending on occupancy states of bits under the mask, were introduced. One such scheme utilizing a perfect hash function to eliminate hash collisions, was termed "magic bitboards". Nonetheless, the large size and high access rates of such tables caused memory occupancy and cache contention issues, and weren't necessarily more efficacious than the rotated bitboard approach.
Today, game programs remain divided, with the best scheme being application dependent.

Many other games besides chess benefit from bitboards.












</doc>
<doc id="991858" url="https://en.wikipedia.org/wiki?curid=991858" title="Parallel array">
Parallel array

In computing, a group of parallel arrays (also known as structure of arrays or SoA) is a form of implicit data structure that uses multiple arrays to represent a singular array of records. It keeps a separate, homogeneous data array for each field of the record, each having the same number of elements. Then, objects located at the same index in each array are implicitly the fields of a single record. Pointers from one object to another are replaced by array indices. This contrasts with the normal approach of storing all fields of each record together in memory (also known as array of structures or AoS). For example, one might declare an array of 100 names, each a string, and 100 ages, each an integer, associating each name with the age that has the same index.

An example in C using parallel arrays:
in Perl (using a hash of arrays to hold references to each array):

Or, in Python:
Parallel arrays have a number of practical advantages over the normal approach:

However, parallel arrays also have several strong disadvantages, which serves to explain why they are not generally preferred:


The bad locality of reference can be alleviated in some cases: if a structure can be divided into groups of fields that are generally accessed together, an array can be constructed for each group, and its elements are records containing only these subsets of the larger structure's fields. (see data oriented design). This is a valuable way of speeding up access to very large structures with many members, while keeping the portions of the structure tied together. An alternative to tying them together using array indexes is to use references to tie the portions together, but this can be less efficient in time and space. Another alternative is to mock up a record structure in a single-dimensional array by declaring an array of n*m size and referring to the r-th field in record i as element as array(m*i+r). Some compiler optimizations, particularly for vector processors, are able to perform this transformation automatically when arrays of structures are created in the program.




</doc>
<doc id="356457" url="https://en.wikipedia.org/wiki?curid=356457" title="Lookup table">
Lookup table

In computer science, a lookup table is an array that replaces runtime computation with a simpler array indexing operation. The savings in terms of processing time can be significant, since retrieving a value from memory is often faster than undergoing an "expensive" computation or input/output operation. The tables may be precalculated and stored in static program storage, calculated (or "pre-fetched") as part of a program's initialization phase (memoization), or even stored in hardware in application-specific platforms. Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. FPGAs also make extensive use of reconfigurable, hardware-implemented, lookup tables to provide programmable hardware functionality.

Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions.

In ancient (499 AD) India, Aryabhata created one of the first sine tables, which he encoded in a Sanskrit-letter-based number system. In 493 AD, Victorius of Aquitaine wrote a 98-column multiplication table which gave (in Roman numerals) the product of every number from 2 to 50 times and the rows were "a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144" Modern school children are often taught to memorize "times tables" to avoid calculations of the most commonly used numbers (up to 9 x 9 or 12 x 12).

Early in the history of computers, input/output operations were particularly slow – even in comparison to processor speeds of the time. It made sense to reduce expensive read operations by a form of manual caching by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items. Despite the introduction of systemwide caching that now automates this process, application level lookup tables can still improve performance for data items that rarely, if ever, change.

Lookup tables were one of the earliest functionalities implemented in computer spreadsheets, with the initial version of VisiCalc (1979) including a codice_1 function among its original 20 functions. This has been followed by subsequent spreadsheets, such as Microsoft Excel, and complemented by specialized codice_2 and codice_3 functions to simplify lookup in a vertical or horizontal table. In Microsoft Excel the codice_4 function has been rolled out starting 28 August 2019. 

This is known as a linear search or brute-force search, each element being checked for equality in turn and the associated value, if any, used as a result of the search. This is often the slowest search method unless frequently occurring values occur early in the list. For a one-dimensional array or linked list, the lookup is usually to determine whether or not there is a match with an 'input' data value.

An example of a "divide and conquer algorithm", binary search involves each element being found by determining which half of the table a match may be found in and repeating until either success or failure. This is only possible if the list is sorted but gives good performance even if the list is lengthy.

For a trivial hash function lookup, the unsigned raw data value is used "directly" as an index to a one-dimensional table to extract a result. For small ranges, this can be amongst the fastest lookup, even exceeding binary search speed with zero branches and executing in constant time.

One discrete problem that is expensive to solve on many computers is that of counting the number of bits which are set to 1 in a (binary) number, sometimes called the "population function". For example, the decimal number "37" is "00100101" in binary, so it contains three bits that are set to binary "1".

A simple example of C code, designed to count the 1 bits in a "int", might look like this:

This apparently simple algorithm can take potentially hundreds of cycles even on a modern architecture, because it makes many branches in the loop - and branching is slow. This can be ameliorated using loop unrolling and some other compiler optimizations. There is however a simple and much faster algorithmic solution - using a trivial hash function table lookup.

Simply construct a static table, "bits_set", with 256 entries giving the number of one bits set in each possible byte value (e.g. 0x00 = 0, 0x01 = 1, 0x02 = 1, and so on). Then use this table to find the number of ones in each byte of the integer using a trivial hash function lookup on each byte in turn, and sum them. This requires no branches, and just four indexed memory accesses, considerably faster than the earlier code.

The above source can be improved easily, (avoiding AND'ing, and shifting) by 'recasting' 'x' as a 4 byte unsigned char array and, preferably, coded in-line as a single statement instead of being a function.
Note that even this simple algorithm can be too slow now, because the original code might run faster from the cache of modern processors, and (large) lookup tables do not fit well in caches and can cause a slower access to memory (in addition, in the above example, it requires computing addresses within a table, to perform the four lookups needed).

In data analysis applications, such as image processing, a lookup table (LUT) is used to transform the input data into a more desirable output format. For example, a grayscale picture of the planet Saturn will be transformed into a color image to emphasize the differences in its rings.

A classic example of reducing run-time computations using lookup tables is to obtain the result of a trigonometry calculation, such as the sine of a value. Calculating trigonometric functions can substantially slow a computing application. The same application can finish much sooner when it first precalculates the sine of a number of values, for example for each whole number of degrees (The table can be defined as static variables at compile time, reducing repeated run time costs). 
When the program requires the sine of a value, it can use the lookup table to retrieve the closest sine value from a memory address, and may also take the step of interpolating to the sine of the desired value, instead of calculating by mathematical formula. Lookup tables are thus used by mathematics co-processors in computer systems. An error in a lookup table was responsible for Intel's infamous floating-point divide bug.

Functions of a single variable (such as sine and cosine) may be implemented by a simple array. Functions involving two or more variables require multidimensional array indexing techniques. The latter case may thus employ a two-dimensional array of power[x][y] to replace a function to calculate x for a limited range of x and y values. Functions that have more than one result may be implemented with lookup tables that are arrays of structures.

As mentioned, there are intermediate solutions that use tables in combination with a small amount of computation, often using interpolation. Pre-calculation combined with interpolation can produce higher accuracy for values that fall between two precomputed values. This technique requires slightly more time to be performed but can greatly enhance accuracy in applications that require the higher accuracy. Depending on the values being precomputed, pre-computation with interpolation can also be used to shrink the lookup table size while maintaining accuracy.

In image processing, lookup tables are often called LUTs (or 3DLUT), and give an output value for each of a range of index values. One common LUT, called the "colormap" or "palette", is used to determine the colors and intensity values with which a particular image will be displayed. In computed tomography, "windowing" refers to a related concept for determining how to display the intensity of measured radiation.

While often effective, employing a lookup table may nevertheless result in a severe penalty if the computation that the LUT replaces is relatively simple. Memory retrieval time and the complexity of memory requirements can increase application operation time and system complexity relative to what would be required by straight formula computation. The possibility of polluting the cache may also become a problem. Table accesses for large tables will almost certainly cause a cache miss. This phenomenon is increasingly becoming an issue as processors outpace memory. A similar issue appears in rematerialization, a compiler optimization. In some environments, such as the Java programming language, table lookups can be even more expensive due to mandatory bounds-checking involving an additional comparison and branch for each lookup.

There are two fundamental limitations on when it is possible to construct a lookup table for a required operation. One is the amount of memory that is available: one cannot construct a lookup table larger than the space available for the table, although it is possible to construct disk-based lookup tables at the expense of lookup time. The other is the time required to compute the table values in the first instance; although this usually needs to be done only once, if it takes a prohibitively long time, it may make the use of a lookup table an inappropriate solution. As previously stated however, tables can be statically defined in many cases.

Most computers only perform basic arithmetic operations and cannot directly calculate the sine of a given value. Instead, they use the CORDIC algorithm or a complex formula such as the following Taylor series to compute the value of sine to a high degree of precision:

However, this can be expensive to compute, especially on slow processors, and there are many applications, particularly in traditional computer graphics, that need to compute many thousands of sine values every second. A common solution is to initially compute the sine of many evenly distributed values, and then to find the sine of "x" we choose the sine of the value closest to "x". This will be close to the correct value because sine is a continuous function with a bounded rate of change. For example:

Unfortunately, the table requires quite a bit of space: if IEEE double-precision floating-point numbers are used, over 16,000 bytes would be required. We can use fewer samples, but then our precision will significantly worsen. One good solution is linear interpolation, which draws a line between the two points in the table on either side of the value and locates the answer on that line. This is still quick to compute, and much more accurate for smooth functions such as the sine function. Here is an example using linear interpolation:

Linear interpolation provides for an interpolated function that is continuous, but will not, in general, have continuous derivatives. For smoother interpolation of table lookup that is continuous and has continuous first derivative, one should use the cubic hermite spline.

Another solution that uses a quarter of the space but takes a bit longer to compute would be to take into account the relationships between sine and cosine along with their symmetry rules. In this case, the lookup table is calculated by using the sine function for the first quadrant (i.e. sin(0..pi/2)). When we need a value, we assign a variable to be the angle wrapped to the first quadrant. We then wrap the angle to the four quadrants (not needed if values are always between 0 and 2*pi) and return the correct value (i.e. first quadrant is a straight return, second quadrant is read from pi/2-x, third and fourth are negatives of the first and second respectively). For cosine, we only have to return the angle shifted by pi/2 (i.e. x+pi/2). For tangent, we divide the sine by the cosine (divide-by-zero handling may be needed depending on implementation):

When using interpolation, the size of the lookup table can be reduced by using "nonuniform sampling", which means that where the function is close to straight, we use few sample points, while where it changes value quickly we use more sample points to keep the approximation close to the real curve. For more information, see interpolation.

Storage caches (including disk caches for files, or processor caches for either code or data) work also like a lookup table. The table is built with very fast memory instead of being stored on slower external memory, and maintains two pieces of data for a sub-range of bits composing an external memory (or disk) address (notably the lowest bits of any possible external address):
A single (fast) lookup is performed to read the tag in the lookup table at the index specified by the lowest bits of the desired external storage address, and to determine if the memory address is hit by the cache. When a hit is found, no access to external memory is needed (except for write operations, where the cached value may need to be updated asynchronously to the slower memory after some time, or if the position in the cache must be replaced to cache another address).

In digital logic, a lookup table can be implemented with a multiplexer whose select lines are driven by the address signal and whose inputs are the values of the elements contained in the array. These values can either be hard-wired, as in an ASIC which purpose is specific to a function, or provided by D latches which allow for configurable values.

An "n"-bit LUT can encode any "n"-input boolean function by storing the truth table of the function in the LUT. This is an efficient way of encoding Boolean logic functions, and LUTs with 4-6 bits of input are in fact the key component of modern field-programmable gate arrays (FPGAs) which provide reconfigurable hardware logic capabilities.




</doc>
<doc id="18167" url="https://en.wikipedia.org/wiki?curid=18167" title="Linked list">
Linked list

In computer science, a linked list is a linear collection of data elements, whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence. In its most basic form, each node contains: data, and a reference (in other words, a "link") to the next node in the sequence. This structure allows for efficient insertion or removal of elements from any position in the sequence during iteration. More complex variants add additional links, allowing more efficient insertion or removal of nodes at arbitrary positions. A drawback of linked lists is that access time is linear (and difficult to pipeline). Faster access, such as random access, is not feasible. Arrays have better cache locality compared to linked lists.

Linked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis.

The principal benefit of a linked list over a conventional array is that the list elements can be easily inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while restructuring an array at run-time is a much more expensive operation. Linked lists allow insertion and removal of nodes at any point in the list, and allow doing so with a constant number of operations by keeping the link previous to the link being added or removed in memory during list traversal.

On the other hand, since simple linked lists by themselves do not allow random access to the data or any form of efficient indexing, many basic operations—such as obtaining the last node of the list, finding a node that contains a given datum, or locating the place where a new node should be inserted—may require iterating through most or all of the list elements. The advantages and disadvantages of using linked lists are given below. Linked list are dynamic, so the length of list can increase or decrease as necessary. Each node does not necessarily follow the previous one physically in the memory.


Linked lists were developed in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation as the primary data structure for their Information Processing Language. IPL was used by the authors to develop several early artificial intelligence programs, including the Logic Theory Machine, the General Problem Solver, and a computer chess program. Reports on their work appeared in IRE Transactions on Information Theory in 1956, and several conference proceedings from 1957 to 1959, including Proceedings of the Western Joint Computer Conference in 1957 and 1958, and Information Processing (Proceedings of the first UNESCO International Conference on Information Processing) in 1959. The now-classic diagram consisting of blocks representing list nodes with arrows pointing to successive list nodes appears in "Programming the Logic Theory Machine" by Newell and Shaw in Proc. WJCC, February 1957. Newell and Simon were recognized with the ACM Turing Award in 1975 for having "made basic contributions to artificial intelligence, the psychology of human cognition, and list processing".
The problem of machine translation for natural language processing led Victor Yngve at Massachusetts Institute of Technology (MIT) to use linked lists as data structures in his COMIT programming language for computer research in the field of linguistics. A report on this language entitled "A programming language for mechanical translation" appeared in Mechanical Translation in 1958.

LISP, standing for list processor, was created by John McCarthy in 1958 while he was at MIT and in 1960 he published its design in a paper in the Communications of the ACM, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I". One of LISP's major data structures is the linked list.

By the early 1960s, the utility of both linked lists and languages which use these structures as their primary data representation was well established. Bert Green of the MIT Lincoln Laboratory published a review article entitled "Computer languages for symbol manipulation" in IRE Transactions on Human Factors in Electronics in March 1961 which summarized the advantages of the linked list approach. A later review article, "A Comparison of list-processing computer languages" by Bobrow and Raphael, appeared in Communications of the ACM in April 1964.

Several operating systems developed by Technical Systems Consultants (originally of West Lafayette Indiana, and later of Chapel Hill, North Carolina) used singly linked lists as file structures. A directory entry pointed to the first sector of a file, and succeeding portions of the file were located by traversing pointers. Systems using this technique included Flex (for the Motorola 6800 CPU), mini-Flex (same CPU), and Flex9 (for the Motorola 6809 CPU). A variant developed by TSC for and marketed by Smoke Signal Broadcasting in California, used doubly linked lists in the same manner.

The TSS/360 operating system, developed by IBM for the System 360/370 machines, used a double linked list for their file system catalog. The directory structure was similar to Unix, where a directory could contain files and other directories and extend to any depth.

Each record of a linked list is often called an 'element' or 'node'.

The field of each node that contains the address of the next node is usually called the 'next link' or 'next pointer'. The remaining fields are known as the 'data', 'information', 'value', 'cargo', or 'payload' fields.

The 'head' of a list is its first node. The 'tail' of a list may refer either to the rest of the list after the head, or to the last node in the list. In Lisp and some derived languages, the next node may be called the 'cdr' (pronounced "could-er") of the list, while the payload of the head node may be called the 'car'.

Singly linked lists contain nodes which have a data field as well as 'next' field, which points to the next node in line of nodes. Operations that can be performed on singly linked lists include insertion, deletion and traversal.

The following code demonstrates how to add a new node with data "value" to the end of a singly linked list:
node addNode(node head, int value) {

In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. The two links may be called 'forward('s') and 'backwards', or 'next' and 'prev'('previous').

A technique known as XOR-linking allows a doubly linked list to be implemented using a single link field in each node. However, this technique requires the ability to do bit operations on addresses, and therefore may not be available in some high-level languages.
Many modern operating systems use doubly linked lists to maintain references to active processes, threads, and other dynamic objects. A common strategy for rootkits to evade detection is to unlink themselves from these lists.

In a 'multiply linked list', each node contains two or more link fields, each field being used to connect the same set of data records in a different order of same set(e.g., by name, by department, by date of birth, etc.). While doubly linked lists can be seen as special cases of multiply linked list, the fact that the two and more orders are opposite to each other leads to simpler and more efficient algorithms, so they are usually treated as a separate case.

In the last node of a list, the link field often contains a null reference, a special value is used to indicate the lack of further nodes. A less common convention is to make it point to the first node of the list; in that case, the list is said to be 'circular' or 'circularly linked'; otherwise, it is said to be 'open' or 'linear'. It is a list where the last pointer points to the first node.

In the case of a circular doubly linked list, the first node also points to the last node of the list.

In some implementations an extra 'sentinel' or 'dummy' node may be added before the first data record or after the last one. This convention simplifies and accelerates some list-handling algorithms, by ensuring that all links can be safely dereferenced and that every list (even one that contains no data elements) always has a "first" and "last" node.

An empty list is a list that contains no data records. This is usually the same as saying that it has zero nodes. If sentinel nodes are being used, the list is usually said to be empty when it has only sentinel nodes.

The link fields need not be physically part of the nodes. If the data records are stored in an array and referenced by their indices, the link field may be stored in a separate array with the same indices as the data records.

Since a reference to the first node gives access to the whole list, that reference is often called the 'address', 'pointer', or 'handle' of the list. Algorithms that manipulate linked lists usually get such handles to the input lists and return the handles to the resulting lists. In fact, in the context of such algorithms, the word "list" often means "list handle". In some situations, however, it may be convenient to refer to a list by a handle that consists of two links, pointing to its first and last nodes.

The alternatives listed above may be arbitrarily combined in almost every way, so one may have circular doubly linked lists without sentinels, circular singly linked lists with sentinels, etc.

As with most choices in computer programming and design, no method is well suited to all circumstances. A linked list data structure might work well in one case, but cause problems in another. This is a list of some of the common tradeoffs involving linked list structures.

A "dynamic array" is a data structure that allocates all elements contiguously in memory, and keeps a count of the current number of elements. If the space reserved for the dynamic array is exceeded, it is reallocated and (possibly) copied, which is an expensive operation.

Linked lists have several advantages over dynamic arrays. Insertion or deletion of an element at a specific point of a list, assuming that we have indexed a pointer to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. While one can "delete" an element from an array in constant time by somehow marking its slot as "vacant", this causes fragmentation that impedes the performance of iteration.

Moreover, arbitrarily many elements may be inserted into a linked list, limited only by the total memory available; while a dynamic array will eventually fill up its underlying array data structure and will have to reallocate—an expensive operation, one that may not even be possible if memory is fragmented, although the cost of reallocation can be averaged over insertions, and the cost of an insertion due to reallocation would still be amortized O(1). This helps with appending elements at the array's end, but inserting into (or removing from) middle positions still carries prohibitive costs due to data moving to maintain contiguity. An array from which many elements are removed may also have to be resized in order to avoid wasting too much space.

On the other hand, dynamic arrays (as well as fixed-size array data structures) allow constant-time random access, while linked lists allow only sequential access to elements. Singly linked lists, in fact, can be easily traversed in only one direction. This makes linked lists unsuitable for applications where it's useful to look up an element by its index quickly, such as heapsort. Sequential access on arrays and dynamic arrays is also faster than on linked lists on many machines, because they have optimal locality of reference and thus make good use of data caching.

Another disadvantage of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data items such as characters or boolean values, because the storage overhead for the links may exceed by a factor of two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data). It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.

Some hybrid solutions try to combine the advantages of the two representations. Unrolled linked lists store several elements in each list node, increasing cache performance while decreasing memory overhead for references. CDR coding does both these as well, by replacing references with the actual data referenced, which extends off the end of the referencing record.

A good example that highlights the pros and cons of using dynamic arrays vs. linked lists is by implementing a program that resolves the Josephus problem. The Josephus problem is an election method that works by having a group of people stand in a circle. Starting at a predetermined person, you count around the circle "n" times. Once you reach the "n"th person, take them out of the circle and have the members close the circle. Then count around the circle the same "n" times and repeat the process, until only one person is left. That person wins the election. This shows the strengths and weaknesses of a linked list vs. a dynamic array, because if you view the people as connected nodes in a circular linked list then it shows how easily the linked list is able to delete nodes (as it only has to rearrange the links to the different nodes). However, the linked list will be poor at finding the next person to remove and will need to search through the list until it finds that person. A dynamic array, on the other hand, will be poor at deleting nodes (or elements) as it cannot remove one node without individually shifting all the elements up the list by one. However, it is exceptionally easy to find the "n"th person in the circle by directly referencing them by their position in the array.

The list ranking problem concerns the efficient conversion of a linked list representation into an array. Although trivial for a conventional computer, solving this problem by a parallel algorithm is complicated and has been the subject of much research.

A balanced tree has similar memory access patterns and space overhead to a linked list while permitting much more efficient indexing, taking O(log n) time instead of O(n) for a random access. However, insertion and deletion operations are more expensive due to the overhead of tree manipulations to maintain balance. Schemes exist for trees to automatically maintain themselves in a balanced state: AVL trees or red-black trees.

While doubly linked and circular lists have advantages over singly linked linear lists, linear lists offer some advantages that make them preferable in some situations.

A singly linked linear list is a recursive data structure, because it contains a pointer to a "smaller" object of the same type. For that reason, many operations on singly linked linear lists (such as merging two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using iterative commands. While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.

Linear singly linked lists also allow tail-sharing, the use of a common final portion of sub-list as the terminal portion of two different lists. In particular, if a new node is added at the beginning of a list, the former list remains available as the tail of the new one—a simple example of a persistent data structure. Again, this is not true with the other variants: a node may never belong to two different circular or doubly linked lists.

In particular, end-sentinel nodes can be shared among singly linked non-circular lists. The same end-sentinel node may be used for "every" such list. In Lisp, for example, every proper list ends with a link to a special node, denoted by codice_1 or codice_2, whose codice_3 and codice_4 links point to itself. Thus a Lisp procedure can safely take the codice_3 or codice_4 of "any" list.

The advantages of the fancy variants are often limited to the complexity of the algorithms, not in their efficiency. A circular list, in particular, can usually be emulated by a linear list together with two variables that point to the first and last nodes, at no extra cost.

Double-linked lists require more space per node (unless one uses XOR-linking), and their elementary operations are more expensive; but they are often easier to manipulate because they allow fast and easy sequential access to the list in both directions. In a doubly linked list, one can insert or delete a node in a constant number of operations given only that node's address. To do the same in a singly linked list, one must have the "address of the pointer" to that node, which is either the handle for the whole list (in case of the first node) or the link field in the "previous" node. Some algorithms require access in both directions. On the other hand, doubly linked lists do not allow tail-sharing and cannot be used as persistent data structures

A circularly linked list may be a natural option to represent arrays that are naturally circular, e.g. the corners of a polygon, a pool of buffers that are used and released in FIFO ("first in, first out") order, or a set of processes that should be time-shared in round-robin order. In these applications, a pointer to any node serves as a handle to the whole list.

With a circular list, a pointer to the last node gives easy access also to the first node, by following one link. Thus, in applications that require access to both ends of the list (e.g., in the implementation of a queue), a circular structure allows one to handle the structure by a single pointer, instead of two.

A circular list can be split into two circular lists, in constant time, by giving the addresses of the last node of each piece. The operation consists in swapping the contents of the link fields of those two nodes. Applying the same operation to any two nodes in two distinct lists joins the two list into one. This property greatly simplifies some algorithms and data structures, such as the quad-edge and face-edge.

The simplest representation for an empty "circular" list (when such a thing makes sense) is a null pointer, indicating that the list has no nodes. Without this choice, many algorithms have to test for this special case, and handle it separately. By contrast, the use of null to denote an empty "linear" list is more natural and often creates fewer special cases.

Sentinel node may simplify certain list operations, by ensuring that the next or previous nodes exist for every element, and that even empty lists have at least one node. One may also use a sentinel node at the end of the list, with an appropriate data field, to eliminate some end-of-list tests. For example, when scanning the list looking for a node with a given value "x", setting the sentinel's data field to "x" makes it unnecessary to test for end-of-list inside the loop. Another example is the merging two sorted lists: if their sentinels have data fields set to +∞, the choice of the next output node does not need special handling for empty lists.

However, sentinel nodes use up extra space (especially in applications that use many short lists), and they may complicate other operations (such as the creation of a new empty list).

However, if the circular list is used merely to simulate a linear list, one may avoid some of this complexity by adding a single sentinel node to every list, between the last and the first data nodes. With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link. The list handle should then be a pointer to the last data node, before the sentinel, if the list is not empty; or to the sentinel itself, if the list is empty.

The same trick can be used to simplify the handling of a doubly linked linear list, by turning it into a circular doubly linked list with a single sentinel node. However, in this case, the handle should be a single pointer to the dummy node itself.

When manipulating linked lists in-place, care must be taken to not use values that you have invalidated in previous assignments. This makes algorithms for inserting or deleting linked list nodes somewhat subtle. This section gives pseudocode for adding or removing nodes from singly, doubly, and circularly linked lists in-place. Throughout we will use "null" to refer to an end-of-list marker or sentinel, which may be implemented in a number of ways.

Our node data structure will have two fields. We also keep a variable "firstNode" which always points to the first node in the list, or is "null" for an empty list.

Traversal of a singly linked list is simple, beginning at the first node and following each "next" link until we come to the end:

The following code inserts a node after an existing node in a singly linked list. The diagram shows how it works. Inserting a node before an existing one cannot be done directly; instead, one must keep track of the previous node and insert a node after it.

Inserting at the beginning of the list requires a separate function. This requires updating "firstNode".

Similarly, we have functions for removing the node "after" a given node, and for removing a node from the beginning of the list. The diagram demonstrates the former. To find and remove a particular node, one must again keep track of the previous element.

Notice that codice_7 sets codice_8 to codice_9 when removing the last node in the list.

Since we can't iterate backwards, efficient codice_10 or codice_11 operations are not possible. Inserting to a list before a specific node requires traversing the list, which would have a worst case running time of O(n). 

Appending one linked list to another can be inefficient unless a reference to the tail is kept as part of the List structure, because we must traverse the entire first list in order to find the tail, and then append the second list to this. Thus, if two linearly linked lists are each of length formula_1, list appending has asymptotic time complexity of formula_2. In the Lisp family of languages, list appending is provided by the codice_12 procedure.

Many of the special cases of linked list operations can be eliminated by including a dummy element at the front of the list. This ensures that there are no special cases for the beginning of the list and renders both codice_13 and codice_7 unnecessary. In this case, the first useful data in the list will be found at codice_15.

In a circularly linked list, all nodes are linked in a continuous circle, without using "null." For lists with a front and a back (such as a queue), one stores a reference to the last node in the list. The "next" node after the last node is the first node. Elements can be added to the back of the list and removed from the front in constant time.

Circularly linked lists can be either singly or doubly linked.

Both types of circularly linked lists benefit from the ability to traverse the full list beginning at any given node. This often allows us to avoid storing "firstNode" and "lastNode", although if the list may be empty we need a special representation for the empty list, such as a "lastNode" variable which points to some node in the list or is "null" if it's empty; we use such a "lastNode" here. This representation significantly simplifies adding and removing nodes with a non-empty list, but empty lists are then a special case.

Assuming that "someNode" is some node in a non-empty circular singly linked list, this code iterates through that list starting with "someNode":

Notice that the test "while node ≠ someNode" must be at the end of the loop. If the test was moved to the beginning of the loop, the procedure would fail whenever the list had only one node.

This function inserts a node "newNode" into a circular linked list after a given node "node". If "node" is null, it assumes that the list is empty.

Suppose that "L" is a variable pointing to the last node of a circular linked list (or null if the list is empty). To append "newNode" to the "end" of the list, one may do

To insert "newNode" at the "beginning" of the list, one may do

Languages that do not support any type of reference can still create links by replacing pointers with array indices. The approach is to keep an array of records, where each record has integer fields indicating the index of the next (and possibly previous) node in the array. Not all nodes in the array need be used. If records are also not supported, parallel arrays can often be used instead.

As an example, consider the following linked list record that uses arrays instead of pointers:

A linked list can be built by creating an array of these structures, and an integer variable to store the index of the first element.

Links between elements are formed by placing the array index of the next (or previous) cell into the Next or Prev field within a given element. For example:

In the above example, codice_16 would be set to 2, the location of the first entry in the list. Notice that entry 3 and 5 through 7 are not part of the list. These cells are available for any additions to the list. By creating a codice_17 integer variable, a free list could be created to keep track of what cells are available. If all entries are in use, the size of the array would have to be increased or some elements would have to be deleted before new entries could be stored in the list.

The following code would traverse the list and display names and account balance:

When faced with a choice, the advantages of this approach include:

This approach has one main disadvantage, however: it creates and manages a private memory space for its nodes. This leads to the following issues:
For these reasons, this approach is mainly used for languages that do not support dynamic memory allocation. These disadvantages are also mitigated if the maximum size of the list is known at the time the array is created.

Many programming languages such as Lisp and Scheme have singly linked lists built in. In many functional languages, these lists are constructed from nodes, each called a "cons" or "cons cell". The cons has two fields: the "car", a reference to the data for that node, and the "cdr", a reference to the next node. Although cons cells can be used to build other data structures, this is their primary purpose.

In languages that support abstract data types or templates, linked list ADTs or templates are available for building linked lists. In other languages, linked lists are typically built using references together with records.

When constructing a linked list, one is faced with the choice of whether to store the data of the list directly in the linked list nodes, called "internal storage", or merely to store a reference to the data, called "external storage". Internal storage has the advantage of making access to the data more efficient, requiring less storage overall, having better locality of reference, and simplifying memory management for the list (its data is allocated and deallocated at the same time as the list nodes).

External storage, on the other hand, has the advantage of being more generic, in that the same data structure and machine code can be used for a linked list no matter what the size of the data is. It also makes it easy to place the same data in multiple linked lists. Although with internal storage the same data can be placed in multiple lists by including multiple "next" references in the node data structure, it would then be necessary to create separate routines to add or delete cells based on each field. It is possible to create additional linked lists of elements that use internal storage by using external storage, and having the cells of the additional linked lists store references to the nodes of the linked list containing the data.

In general, if a set of data structures needs to be included in linked lists, external storage is the best approach. If a set of data structures need to be included in only one linked list, then internal storage is slightly better, unless a generic linked list package using external storage is available. Likewise, if different sets of data that can be stored in the same data structure are to be included in a single linked list, then internal storage would be fine.

Another approach that can be used with some languages involves having different data structures, but all have the initial fields, including the "next" (and "prev" if double linked list) references in the same location. After defining separate structures for each type of data, a generic structure can be defined that contains the minimum amount of data shared by all the other structures and contained at the top (beginning) of the structures. Then generic routines can be created that use the minimal structure to perform linked list type operations, but separate routines can then handle the specific data. This approach is often used in message parsing routines, where several types of messages are received, but all start with the same set of fields, usually including a field for message type. The generic routines are used to add new messages to a queue when they are received, and remove them from the queue in order to process the message. The message type field is then used to call the correct routine to process the specific type of message.

Suppose you wanted to create a linked list of families and their members. Using internal storage, the structure might look like the following:

To print a complete list of families and their members using internal storage, we could write:

Using external storage, we would create the following structures:

To print a complete list of families and their members using external storage, we could write:

Notice that when using external storage, an extra step is needed to extract the record from the node and cast it into the proper data type. This is because both the list of families and the list of members within the family are stored in two linked lists using the same data structure ("node"), and this language does not have parametric types.

As long as the number of families that a member can belong to is known at compile time, internal storage works fine. If, however, a member needed to be included in an arbitrary number of families, with the specific number known only at run time, external storage would be necessary.

Finding a specific element in a linked list, even if it is sorted, normally requires O("n") time (linear search). This is one of the primary disadvantages of linked lists over other data structures. In addition to the variants discussed above, below are two simple ways to improve search time.

In an unordered list, one simple heuristic for decreasing average search time is the "move-to-front heuristic", which simply moves an element to the beginning of the list once it is found. This scheme, handy for creating simple caches, ensures that the most recently used items are also the quickest to find again.

Another common approach is to "index" a linked list using a more efficient external data structure. For example, one can build a red-black tree or hash table whose elements are references to the linked list nodes. Multiple such indexes can be built on a single list. The disadvantage is that these indexes may need to be updated each time a node is added or removed (or at least, before that index is used again).

A random access list is a list with support for fast random access to read or modify any element in the list. One possible implementation is a skew binary random access list using the skew binary number system, which involves a list of trees with special properties; this allows worst-case constant time head/cons operations, and worst-case logarithmic time random access to an element by index. Random access lists can be implemented as persistent data structures.

Random access lists can be viewed as immutable linked lists in that they likewise support the same O(1) head and tail operations.

A simple extension to random access lists is the min-list, which provides an additional operation that yields the minimum element in the entire list in constant time (without mutation complexities).

Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported.

The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list.

A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node.

An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list.

A hash table may use linked lists to store the chains of items that hash to the same position in the hash table.

A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index.

A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.




</doc>
<doc id="291683" url="https://en.wikipedia.org/wiki?curid=291683" title="XOR linked list">
XOR linked list

An XOR linked list is a type of data structure used in computer programming. It takes advantage of the bitwise XOR operation to decrease storage requirements for doubly linked lists.

An ordinary doubly linked list stores addresses of the previous and next list items in each list node, requiring two address fields:

An XOR linked list compresses the same information into "one" address field by storing the bitwise XOR (here denoted by ⊕) of the address for "previous" and the address for "next" in one field:

More formally: 

When traversing the list from left to right: supposing the cursor is at C, the previous item, B may be XORed with the value in the link field (B⊕D). The address for D will then be obtained and list traversal may resume. The same pattern applies in the other direction.

To start traversing the list in either direction from some point, the address of two consecutive items is required. If the addresses of the two consecutive items are reversed, list traversal will occur in the opposite direction.

The key is the first operation, and the properties of XOR: 

The R2 register always contains the XOR of the address of current item C with the address of the predecessor item P: C⊕P. The Link fields in the records contain the XOR of the left and right successor addresses, say L⊕R. XOR of R2 (C⊕P) with the current link field (L⊕R) yields C⊕P⊕L⊕R. 

In each case, the result is the XOR of the current address with the next address. XOR of this with the current address in R1 leaves the next address. R2 is left with the requisite XOR pair of the (now) current address and the predecessor.


 X R2,Link R2 <- C⊕D ⊕ B⊕D (i.e. B⊕C, "Link" being the link field



Computer systems have increasingly cheap and plentiful memory, therefore storage overhead is not generally an overriding issue outside specialized embedded systems. Where it is still desirable to reduce the overhead of a linked list, unrolling provides a more practical approach (as well as other advantages, such as increasing cache performance and speeding random access).

The underlying principle of the XOR linked list can be applied to any reversible binary operation. Replacing XOR by addition or subtraction gives slightly different, but largely equivalent, formulations:

 ... A B C D E ...

This kind of list has exactly the same properties as the XOR linked list, except that a zero link field is not a "mirror". The address of the next node in the list is given by subtracting the previous node's address from the current node's link field.

 ... A B C D E ...

This kind of list differs from the standard "traditional" XOR linked list in that the instruction sequences needed to traverse the list forwards is different from the sequence needed to traverse the list in reverse. The address of the next node, going forwards, is given by "adding" the link field to the previous node's address; the address of the preceding node is given by "subtracting" the link field from the next node's address.

The subtraction linked list is also special in that the entire list can be relocated in memory without needing any patching of pointer values, since adding a constant offset to each address in the list will not require any changes to the values stored in the link fields. (See also serialization.) This is an advantage over both XOR linked lists and traditional linked lists.




</doc>
<doc id="1035267" url="https://en.wikipedia.org/wiki?curid=1035267" title="Unrolled linked list">
Unrolled linked list

In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. It can dramatically increase cache performance, while decreasing the memory overhead associated with storing list metadata such as references. It is related to the B-tree.

A typical unrolled linked list node looks like this:

Each node holds up to a certain maximum number of elements, typically just large enough so that the node fills a single cache line or a small multiple thereof. A position in the list is indicated by both a reference to the node and a position in the elements array. It is also possible to include a "previous" pointer for an unrolled doubly linked list.

To insert a new element, we simply find the node the element should be in and insert the element into the codice_1 array, incrementing codice_2. If the array is already full, we first insert a new node either preceding or following the current one and move half of the elements in the current node into it.

To remove an element, we simply find the node it is in and delete it from the codice_1 array, decrementing codice_2. If this reduces the node to less than half-full, then we move elements from the next node to fill it back up above half. If this leaves the next node less than half full, then we move all its remaining elements into the current node, then bypass and delete it.

One of the primary benefits of unrolled linked lists is decreased storage requirements. All nodes (except at most one) are at least half-full. If many random inserts and deletes are done, the average node will be about three-quarters full, and if inserts and deletes are only done at the beginning and end, almost all nodes will be full. Assume that:
Then, the space used for "n" elements varies between formula_1 and formula_2. For comparison, ordinary linked lists require formula_3 space, although "v" may be smaller, and arrays, one of the most compact data structures, require formula_4 space. Unrolled linked lists effectively spread the overhead "v" over a number of elements of the list. Thus, we see the most significant space gain when overhead is large, codice_5 is large, or elements are small.

If the elements are particularly small, such as bits, the overhead can be as much as 64 times larger than the data on many machines. Moreover, many popular memory allocators will keep a small amount of metadata for each node allocated, increasing the effective overhead "v". Both of these make unrolled linked lists more attractive.

Because unrolled linked list nodes each store a count next to the "next" field, retrieving the "k"th element of an unrolled linked list (indexing) can be done in "n"/"m" + 1 cache misses, up to a factor of "m" better than ordinary linked lists. Additionally, if the size of each element is small compared to the cache line size, the list can be traversed in order with fewer cache misses than ordinary linked lists. In either case, operation time still increases linearly with the size of the list.





</doc>
<doc id="336155" url="https://en.wikipedia.org/wiki?curid=336155" title="Skip list">
Skip list

In computer science, a skip list is a data structure that allows formula_1 search complexity as well as formula_1 insertion complexity within an ordered sequence of formula_3 elements. Thus it can get the best features of an array (for searching) while maintaining a linked list-like structure that allows insertion- which is not possible in an array. Fast search is made possible by maintaining a linked hierarchy of subsequences, with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right). Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically or deterministically, with the former being more common.

A skip list is built in layers. The bottom layer is an ordinary ordered linked list. Each higher layer acts as an "express lane" for the lists below, where an element in layer formula_4 appears in layer formula_5 with some fixed probability formula_6 (two commonly used values for formula_6 are formula_8 or formula_9). On average, each element appears in formula_10 lists, and the tallest element (usually a special head element at the front of the skip list) in all the lists. The skip list contains formula_11 (i.e. logarithm base formula_12 of formula_3) lists.

A search for a target element begins at the head element in the top list, and proceeds horizontally until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most formula_12, which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total "expected" cost of a search is formula_15 which is formula_16, when formula_6 is a constant. By choosing different values of formula_6, it is possible to trade search costs against storage costs.

The elements used for a skip list can contain more than one pointer since they can participate in more than one list.

Insertions and deletions are implemented much like the corresponding linked-list operations, except that "tall" elements must be inserted into or deleted from more than one linked list.

formula_19 operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to formula_1 search time. (Choose the level of the i'th finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd. Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them.

Alternatively, we could make the level structure quasi-random in the following way:

Like the derandomized version, quasi-randomization is only done when there is some other reason to be running an formula_19 operation (which visits every node).

The advantage of this quasi-randomness is that it doesn't give away nearly as much level-structure related information to an adversarial user as the de-randomized one. This is desirable because an adversarial user who is able to tell which nodes are not at the lowest level can pessimize performance by simply deleting higher-level nodes. (Bethea and Reiter however argue that nonetheless an adversary can use probabilistic and timing methods to force performance degradation.) The search performance is still guaranteed to be logarithmic.

It would be tempting to make the following "optimization": In the part which says "Next, for each "i"th...", forget about doing a coin-flip for each even-odd pair. Just flip a coin once to decide whether to promote only the even ones or only the odd ones. Instead of formula_22 coin flips, there would only be formula_1 of them. Unfortunately, this gives the adversarial user a 50/50 chance of being correct upon guessing that all of the even numbered nodes (among the ones at level 1 or higher) are higher than level one. This is despite the property that he has a very low probability of guessing that a particular node is at level "N" for some integer "N".

A skip list does not provide the same absolute worst-case performance guarantees as more traditional balanced tree data structures, because it is always possible (though with very low probability) that the coin-flips used to build the skip list will produce a badly balanced structure. However, they work well in practice, and the randomized balancing scheme has been argued to be easier to implement than the deterministic balancing schemes used in balanced binary search trees. Skip lists are also useful in parallel computing, where insertions can be done in different parts of the skip list in parallel without any global rebalancing of the data structure. Such parallelism can be especially advantageous for resource discovery in an ad-hoc wireless network because a randomized skip list can be made robust to the loss of any single node.

As described above, a skip list is capable of fast formula_1 insertion and removal of values from a sorted sequence, but it has only slow formula_19 lookups of values at a given position in the sequence (i.e. return the 500th value); however, with a minor modification the speed of random access indexed lookups can be improved to formula_1.

For every link, also store the width of the link. The width is defined as the number of bottom layer links being traversed by each of the higher layer "express lane" links.

For example, here are the widths of the links in the example at the top of the page:

Notice that the width of a higher level link is the sum of the component links below it (i.e. the width 10 link spans the links of widths 3, 2 and 5 immediately below it). Consequently, the sum of all widths is the same on every level (10 + 1 = 1 + 3 + 2 + 5 = 1 + 2 + 1 + 2 + 3 + 2).

To index the skip list and find the i'th value, traverse the skip list while counting down the widths of each traversed link. Descend a level whenever the upcoming width would be too large.

For example, to find the node in the fifth position (Node 5), traverse a link of width 1 at the top level. Now four more steps are needed but the next width on this level is ten which is too large, so drop one level. Traverse one link of width 3. Since another step of width 2 would be too far, drop down to the bottom level. Now traverse the final link of width 1 to reach the target running total of 5 (1+3+1). 

This method of implementing indexing is detailed in Section 3.4 Linear List Operations in "A skip list cookbook" by William Pugh.

Skip lists were first described in 1989 by William Pugh.

To quote the author:

List of applications and frameworks that use skip lists:
Skip lists are used for efficient statistical computations of running medians (also known as moving medians).
Skip lists are also used in distributed applications (where the nodes represent physical computers, and pointers represent network connections) and for implementing highly scalable concurrent priority queues with less lock contention, or even without locking, as well as lockless concurrent dictionaries. There are also several US patents for using skip lists to implement (lockless) priority queues and concurrent dictionaries.




</doc>
<doc id="6213252" url="https://en.wikipedia.org/wiki?curid=6213252" title="Self-organizing list">
Self-organizing list

A self-organizing list is a list that reorders its elements based on some self-organizing heuristic to improve average access time.
The aim of a self-organizing list is to improve efficiency of linear search by moving more frequently accessed items towards the head of the list. A self-organizing list achieves near constant time for element access in the best case. A self-organizing list uses a reorganizing algorithm to adapt to various query distributions at runtime.

The concept of self-organizing lists has its roots in the idea of activity organization
of records in files stored on disks or tapes. 
One frequently cited discussion of self-organizing files and lists is
Knuth
John McCabe has the first algorithmic complexity analyses of the Move-to-Front (MTF) strategy where an item
is moved to the front of the list after it is accessed.
He analyzed the average time needed for randomly ordered list to get in optimal order.
The optimal ordering of a list is the one in which items are ordered in the list by
the probability with which they will be needed, with the most accessed item first.
The optimal ordering may not be known in advance, and may also change over time.
McCabe introduced the transposition strategy in which an accessed item is exchanged with the
item in front of it in the list. He made the conjecture that transposition
worked at least as well in the average case as MTF in approaching the optimal ordering of records in the limit.
This conjecture was later proved by Rivest.
McCabe also noted that with either the transposition or MTF heuristic, the optimal ordering of records would
be approached even if the heuristic was only applied every Nth access, and that a value of N might be 
chosen that would reflect the relative cost of relocating records with the value of approaching the optimal ordering
more quickly.
Further improvements were made, and algorithms suggested by researchers including: Rivest, Tenenbaum and Nemes, Knuth, and 
Bentley and McGeoch (e.g. Worst-case analyses for self-organizing sequential search heuristics).

The simplest implementation of a self-organizing list is as a linked list and thus while being efficient in random node inserting and memory allocation, suffers from inefficient accesses to random nodes. A self-organizing list reduces the inefficiency by dynamically rearranging the nodes in the list based on access frequency.

If a particular node is to be searched for in the list, each node in the list must be sequentially compared till the desired node is reached. In a linked list, retrieving the nth element is an O(n) operation. This is highly inefficient when compared to an array for example, where accessing the n element is an O(1) operation.

A self organizing list rearranges the nodes keeping the most frequently accessed ones at the head of the list. Generally, in a particular query, the chances of accessing a node which has been accessed many times before are higher than the chances of accessing a node which historically has not been so frequently accessed. As a result, keeping the commonly accessed nodes at the head of the list results in reducing the number of comparisons required in an average case to reach the desired node. This leads to better efficiency and generally reduced query times.

The implementation and methods of a self-organizing list are identical to those for a standard linked list. The linked list and the self-organizing list differ only in terms of the organization of the nodes; the interface remains the same.

It can be shown that in the average case, the time required to a search on a self-organizing list of size n is
where p(i) is the probability of accessing the ith element in the list, thus also called the access probability.
If the access probability of each element is the same (i.e. p(1) = p(2) = p(3) = ... = p(n) = 1/n) then the ordering of the elements is irrelevant and the average time complexity is given by
and T(n) does not depend on the individual access probabilities of the elements in the list in this case.
However, in the case of searches on lists with non uniform record access probabilities (i.e. those lists in which the probability of accessing one element is different from another), the average time complexity can be reduced drastically by proper positioning of the elements contained in the list.
This is done by pairing smaller i with larger access probabilities so as to reduce the overall average time complexity.

This may be demonstrated as follows:
Given List: A(0.1), B(0.1), C(0.3), D(0.1), E(0.4)
Without rearranging, average search time required is:
Now suppose the nodes are rearranged so that those nodes with highest probability of access are placed closest to the front so that the rearranged list is now:
E(0.4), C(0.3), D(0.1), A(0.1), B(0.1)
Here, average search time is:
Thus the average time required for searching in an organized list is (in this case) around 40% less than the time required to search a randomly arranged list.

This is the concept of the self-organized list in that the average speed of data retrieval is increased by rearranging the nodes according to access frequency.

In the worst case, the element to be located is at the very end of the list be it a normal list or a self-organized one and thus n comparisons must be made to reach it. Therefore, the worst case running time of a linear search on the list is O(n) independent of the type of list used.
Note that the expression for the average search time in the previous section is a probabilistic one. Keeping the commonly accessed elements at the head of the list simply reduces the probability of the worst case occurring but does not eliminate it completely. Even in a self-organizing list, if a lowest access probability element (obviously located at the end of the list) is to be accessed, the entire list must be traversed completely to retrieve it. This is the worst case search.

In the best case, the node to be searched is one which has been commonly accessed and has thus been identified by the list and kept at the head. This will result in a near constant time operation. In big-oh notation, in the best case, accessing an element is an O(1) operation.

While ordering the elements in the list, the access probabilities of the elements are not generally known in advance. This has led to the development of various heuristics to approximate optimal behavior. The basic heuristics used to reorder the elements in the list are:

This technique moves the element which is accessed to the head of the list. This has the advantage of being easily implemented and requiring no extra memory. This heuristic also adapts quickly to rapid changes in the query distribution. On the other hand, this method may prioritize infrequently accessed nodes — for example, if an uncommon node is accessed even once, it is moved to the head of the list and given maximum priority even if it is not going to be accessed frequently in the future. These 'over rewarded' nodes destroy the optimal ordering of the list and lead to slower access times for commonly accessed elements. Another disadvantage is that this method may become too flexible leading to access patterns that change too rapidly. This means that due to the very short memories of access patterns even an optimal arrangement of the list can be disturbed immediately by accessing an infrequent node in the list.

In this technique, the number of times each node was searched for is counted i.e. every node keeps a separate counter variable which is incremented every time it is called. The nodes are then rearranged according to decreasing count. Thus, the nodes of highest count i.e. most frequently accessed are kept at the head of the list. The primary advantage of this technique is that it generally is more realistic in representing the actual access pattern. However, there is an added memory requirement, that of maintaining a counter variable for each node in the list. Also, this technique does not adapt quickly to rapid changes in the access patterns. For example: if the count of the head element say A is 100 and for any node after it say B is 40, then even if B becomes the new most commonly accessed element, it must still be accessed at least (100 - 40 = 60) times before it can become the head element and thus make the list ordering optimal.

This technique involves swapping an accessed node with its predecessor. Therefore, if any node is accessed, it is swapped with the node in front unless it is the head node, thereby increasing its priority. This algorithm is again easy to implement and space efficient and is more likely to keep frequently accessed nodes at the front of the list. However, the transpose method is more cautious. i.e. it will take many accesses to move the element to the head of the list. This method also does not allow for rapid response to changes in the query distributions on the nodes in the list.

Research has been focused on fusing the above algorithms to achieve better efficiency. Bitner's Algorithm uses MTF initially and then uses transpose method for finer rearrangements. Some algorithms are randomized and try to prevent the over-rewarding of infrequently accessed nodes that may occur in the MTF algorithm. Other techniques involve reorganizing the nodes based on the above algorithms after every n accesses on the list as a whole or after n accesses in a row on a particular node and so on. Some algorithms rearrange the nodes which are accessed based on their proximity to the head node, for example: Swap-With-Parent or Move-To-Parent algorithms.
Another class of algorithms are used when the search pattern exhibits a property called locality of reference whereby in a given interval of time, only a smaller subset of the list is probabilistically most likely to be accessed. This is also referred to as dependent access where the probability of the access of a particular element depends on the probability of access of its neighboring elements. Such models are common in real world applications such as database or file systems and memory management and caching. A common framework for algorithms dealing with such dependent environments is to rearrange the list not only based on the record accessed but also on the records near it. This effectively involves reorganizing a sublist of the list to which the record belongs.

Language translators like compilers and interpreters use self-organizing lists to maintain symbol tables during compilation or interpretation of program source code. Currently research is underway to incorporate the self-organizing list data structure in embedded systems to reduce bus transition activity which leads to power dissipation in those circuits. These lists are also used in artificial intelligence and neural networks as well as self-adjusting programs. The algorithms used in self-organizing lists are also used as caching algorithms as in the case of LFU algorithm.



</doc>
<doc id="4321" url="https://en.wikipedia.org/wiki?curid=4321" title="Binary tree">
Binary tree

In computer science, a binary tree is a tree data structure in which each node has at most two children, which are referred to as the ' and the '. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple ("L", "S", "R"), where "L" and "R" are binary trees or the empty set and "S" is a singleton set. Some authors allow the binary tree to be the empty set as well.

From a graph theory perspective, binary (and K-ary) trees as defined here are actually arborescences. A binary tree may thus be also called a bifurcating arborescence—a term which appears in some very old programming books, before the modern computer science terminology prevailed. It is also possible to interpret a binary tree as an undirected, rather than a directed graph, in which case a binary tree is an ordered, rooted tree. Some authors use rooted binary tree instead of "binary tree" to emphasize the fact that the tree is rooted, but as defined above, a binary tree is always rooted. A binary tree is a special case of an ordered K-ary tree, where "k" is 2.

In mathematics, what is termed "binary tree" can vary significantly from author to author. Some use the definition commonly used in computer science, but others define it as every non-leaf having exactly two children and don't necessarily order (as left/right) the children either.

In computing, binary trees are used in two very different ways:



To actually define a binary tree in general, we must allow for the possibility that only one of the children may be empty. An artifact, which in some textbooks is called an "extended binary tree" is needed for that purpose. An extended binary tree is thus recursively defined as:

Another way of imagining this construction (and understanding the terminology) is to consider instead of the empty set a different type of node—for instance square nodes if the regular ones are circles.

A binary tree is a rooted tree that is also an ordered tree (a.k.a. plane tree) in which every node has at most two children. A rooted tree naturally imparts a notion of levels (distance from the root), thus for every node a notion of children may be defined as the nodes connected to it a level below. Ordering of these children (e.g., by drawing them on a plane) makes possible to distinguish left child from right child. But this still doesn't distinguish between a node with left but not a right child from a one with right but no left child.

The necessary distinction can be made by first partitioning the edges, i.e., defining the binary tree as triplet (V, E, E), where (V, E ∪ E) is a rooted tree (equivalently arborescence) and E ∩ E is empty, and also requiring that for all "j" ∈ { 1, 2 } every node has at most one E child. A more informal way of making the distinction is to say, quoting the Encyclopedia of Mathematics, that "every node has a left child, a right child, neither, or both" and to specify that these "are all different" binary trees.

Tree terminology is not well-standardized and so varies in the literature.



In combinatorics one considers the problem of counting the number of full binary trees of a given size. Here the trees have no values attached to their nodes (this would just multiply the number of possible trees by an easily determined factor), and trees are distinguished only by their structure; however the left and right child of any node are distinguished (if they are different trees, then interchanging them will produce a tree distinct from the original one). The size of the tree is taken to be the number "n" of internal nodes (those with two children); the other nodes are leaf nodes and there are of them. The number of such binary trees of size "n" is equal to the number of ways of fully parenthesizing a string of symbols (representing leaves) separated by "n" binary operators (representing internal nodes), so as to determine the argument subexpressions of each operator. For instance for one has to parenthesize a string like , which is possible in five ways:
The correspondence to binary trees should be obvious, and the addition of redundant parentheses (around an already parenthesized expression or around the full expression) is disallowed (or at least not counted as producing a new possibility).

There is a unique binary tree of size 0 (consisting of a single leaf), and any other binary tree is characterized by the pair of its left and right children; if these have sizes "i" and "j" respectively, the full tree has size . Therefore, the number formula_15 of binary trees of size "n" has the following recursive description formula_16, and formula_17 for any positive integer "n". It follows that formula_15 is the Catalan number of index "n".

The above parenthesized strings should not be confused with the set of words of length 2"n" in the Dyck language, which consist only of parentheses in such a way that they are properly balanced. The number of such strings satisfies the same recursive description (each Dyck word of length 2"n" is determined by the Dyck subword enclosed by the initial '(' and its matching ')' together with the Dyck subword remaining after that closing parenthesis, whose lengths 2"i" and 2"j" satisfy ); this number is therefore also the Catalan number formula_15. So there are also five Dyck words of length 6:

These Dyck words do not correspond to binary trees in the same way. Instead, they are related by the following recursively defined bijection: the Dyck word equal to the empty string corresponds to the binary tree of size 0 with only one leaf. Any other Dyck word can be written as (formula_21)formula_22, where formula_21,formula_22 are themselves (possibly empty) Dyck words and where the two written parentheses are matched. The bijection is then defined by letting the words formula_21 and formula_22 correspond to the binary trees that are the left and right children of the root.

A bijective correspondence can also be defined as follows: enclose the Dyck word in an extra pair of parentheses, so that the result can be interpreted as a Lisp list expression (with the empty list () as only occurring atom); then the dotted-pair expression for that proper list is a fully parenthesized expression (with NIL as symbol and '.' as operator) describing the corresponding binary tree (which is in fact the internal representation of the proper list).

The ability to represent binary trees as strings of symbols and parentheses implies that binary trees can represent the elements of a free magma on a singleton set.

Binary trees can be constructed from programming language primitives in several ways.

In a language with records and references, binary trees are typically constructed by having a tree node structure which contains some data and references to its left child and its right child. Sometimes it also contains a reference to its unique parent. If a node has fewer than two children, some of the child pointers may be set to a special null value, or to a special sentinel node.

This method of storing binary trees wastes a fair bit of memory, as the pointers will be null (or point to the sentinel) more than half the time; a more conservative representation alternative is threaded binary tree.

In languages with tagged unions such as ML, a tree node is often a tagged union of two types of nodes, one of which is a 3-tuple of data, left child, and right child, and the other of which is a "leaf" node, which contains no data and functions much like the null value in a language with pointers. For example, the following line of code in OCaml (an ML dialect) defines a binary tree that stores a character in each node.

Binary trees can also be stored in breadth-first order as an implicit data structure in arrays, and if the tree is a complete binary tree, this method wastes no space. In this compact arrangement, if a node has an index "i", its children are found at indices formula_27 (for the left child) and formula_28 (for the right), while its parent (if any) is found at index "formula_29" (assuming the root has index zero). This method benefits from more compact storage and better locality of reference, particularly during a preorder traversal. However, it is expensive to grow and wastes space proportional to 2 - "n" for a tree of depth "h" with "n" nodes.

This method of storage is often used for binary heaps. No space is wasted because nodes are added in breadth-first order.

A succinct data structure is one which occupies close to minimum possible space, as established by information theoretical lower bounds. The number of different binary trees on formula_1 nodes is formula_31, the formula_1th Catalan number (assuming we view trees with identical "structure" as identical). For large formula_1, this is about formula_34; thus we need at least about formula_35 bits to encode it. A succinct binary tree therefore would occupy formula_36 bits.

One simple representation which meets this bound is to visit the nodes of the tree in preorder, outputting "1" for an internal node and "0" for a leaf. If the tree contains data, we can simply simultaneously store it in a consecutive array in preorder. This function accomplishes this:

The string "structure" has only formula_37 bits in the end, where formula_1 is the number of (internal) nodes; we don't even have to store its length. To show that no information is lost, we can convert the output back to the original tree like this:

More sophisticated succinct representations allow not only compact storage of trees but even useful operations on those trees directly while they're still in their succinct form.

There is a one-to-one mapping between general ordered trees and binary trees, which in particular is used by Lisp to represent general ordered trees as binary trees. To convert a general ordered tree to binary tree, we only need to represent the general tree in left-child right-sibling way. The result of this representation will automatically be a binary tree, if viewed from a different perspective. Each node "N" in the ordered tree corresponds to a node "N' " in the binary tree; the "left" child of "N' " is the node corresponding to the first child of "N", and the "right" child of "N' " is the node corresponding to "N" 's next sibling --- that is, the next node in order among the children of the parent of "N". This binary tree representation of a general order tree is sometimes also referred to as a left-child right-sibling binary tree (also known as LCRS tree, doubly chained tree, filial-heir chain).

One way of thinking about this is that each node's children are in a linked list, chained together with their "right" fields, and the node only has a pointer to the beginning or head of this list, through its "left" field.

For example, in the tree on the left, A has the 6 children {B,C,D,E,F,G}. It can be converted into the binary tree on the right.

The binary tree can be thought of as the original tree tilted sideways, with the black left edges representing "first child" and the blue right edges representing "next sibling". The leaves of the tree on the left would be written in Lisp as:

which would be implemented in memory as the binary tree on the right, without any letters on those nodes that have a left child.

There are a variety of different operations that can be performed on binary trees. Some are mutator operations, while others simply return useful information about the tree.

Nodes can be inserted into binary trees in between two other nodes or added after a leaf node. In binary trees, a node that is inserted is specified as to which child it is.

To add a new node after leaf node A, A assigns the new node as one of its children and the new node assigns node A as its parent.

Insertion on internal nodes is slightly more complex than on leaf nodes. Say that the internal node is node A and that node B is the child of A. (If the insertion is to insert a right child, then B is the right child of A, and similarly with a left child insertion.) A assigns its child to the new node and the new node assigns its parent to A. Then the new node assigns its child to B and B assigns its parent as the new node.

Deletion is the process whereby a node is removed from the tree. Only certain nodes in a binary tree can be removed unambiguously.

Suppose that the node to delete is node A. If A has no children, deletion is accomplished by setting the child of A's parent to null. If A has one child, set the parent of A's child to A's parent and set the child of A's parent to A's child.

In a binary tree, a node with two children cannot be deleted unambiguously. However, in certain binary trees (including binary search trees) these nodes "can" be deleted, though with a rearrangement of the tree structure.

Pre-order, in-order, and post-order traversal visit each node in a tree by recursively visiting each node in the left and right subtrees of the root.

In depth-first order, we always attempt to visit the node farthest from the root node that we can, but with the caveat that it must be a child of a node we have already visited. Unlike a depth-first search on graphs, there is no need to remember all the nodes we have visited, because a tree cannot contain cycles. Pre-order is a special case of this. See depth-first search for more information.

Contrasting with depth-first order is breadth-first order, which always attempts to visit the node closest to the root that it has not already visited. See breadth-first search for more information. Also called a "level-order traversal".

In a complete binary tree, a node's breadth-index ("i" − (2 − 1)) can be used as traversal instructions from the root. Reading bitwise from left to right, starting at bit "d" − 1, where "d" is the node's distance from the root ("d" = ⌊log2("i"+1)⌋) and the node in question is not the root itself ("d" > 0). When the breadth-index is masked at bit "d" − 1, the bit values 0 and 1 mean to step either left or right, respectively. The process continues by successively checking the next bit to the right until there are no more. The rightmost bit indicates the final traversal from the desired node's parent to the node itself. There is a time-space trade-off between iterating a complete binary tree this way versus each node having pointer/s to its sibling/s.




</doc>
<doc id="4320" url="https://en.wikipedia.org/wiki?curid=4320" title="Binary search tree">
Binary search tree

In computer science, binary search trees (BST), sometimes called ordered or sorted binary trees, are a particular type of container: a data structure that stores "items" (such as numbers, names etc.) in memory. They allow fast lookup, addition and removal of items, and can be used to implement either dynamic sets of items, or lookup tables that allow finding an item by its "key" (e.g., finding the phone number of a person by name).

Binary search trees keep their keys in sorted order, so that lookup and other operations can use the principle of binary search: when looking for a key in a tree (or a place to insert a new key), they traverse the tree from root to leaf, making comparisons to keys stored in the nodes of the tree and deciding, on the basis of the comparison, to continue searching in the left or right subtrees. On average, this means that each comparison allows the operations to skip about half of the tree, so that each lookup, insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree. This is much better than the linear time required to find items by key in an (unsorted) array, but slower than the corresponding operations on hash tables.

Several variants of the binary search tree have been studied in computer science; this article deals primarily with the basic type, making references to more advanced types when appropriate.

A binary search tree is a rooted binary tree, whose internal nodes each store a key (and optionally, an associated value) and each have two distinguished sub-trees, commonly denoted "left" and "right". The tree additionally satisfies the binary search property, which states that the key in each node must be greater than or equal to any key stored in the left sub-tree, and less than or equal to any key stored in the right sub-tree. The leaves (final nodes) of the tree contain no key and have no structure to distinguish them from one another. 

Frequently, the information represented by each node is a record rather than a single data element. However, for sequencing purposes, nodes are compared according to their keys rather than any part of their associated records. The major advantage of binary search trees over other data structures is that the related sorting algorithms and search algorithms such as in-order traversal can be very efficient; they are also easy to code.

Binary search trees are a fundamental data structure used to construct more abstract data structures such as sets, multisets, and associative arrays.

Binary search requires an order relation by which every element (item) can be compared with every other element in the sense of a total preorder. The part of the element which effectively takes place in the comparison is called its "key". Whether duplicates, i.e. different elements with same key, shall be allowed in the tree or not, does not depend on the order relation, but on the application only.

In the context of binary search trees a total preorder is realized most flexibly by means of a three-way comparison subroutine.

Binary search trees support three main operations: insertion of elements, deletion of elements, and lookup (checking whether a key is present).

Searching in a binary search tree for a specific key can be programmed recursively or iteratively.

We begin by examining the root node. If the tree is "null", the key we are searching for does not exist in the tree. Otherwise, if the key equals that of the root, the search is successful and we return the node. If the key is less than that of the root, we search the left subtree. Similarly, if the key is greater than that of the root, we search the right subtree. This process is repeated until the key is found or the remaining subtree is "null". If the searched key is not found after a "null" subtree is reached, then the key is not present in the tree. This is easily expressed as a recursive algorithm (implemented in Python):

The same algorithm can be implemented iteratively:
These two examples rely on the order relation being a total order.

If the order relation is only a total preorder, a reasonable extension of the functionality is the following: also in case of equality search down to the leaves in a direction specified by the user. A binary tree sort equipped with such a comparison function becomes stable.

Because in the worst case this algorithm must search from the root of the tree to the leaf farthest from the root, the search operation takes time proportional to the tree's "height" (see tree terminology). On average, binary search trees with nodes have height. However, in the worst case, binary search trees can have height, when the unbalanced tree resembles a linked list (degenerate tree).

Insertion begins as a search would begin; if the key is not equal to that of the root, we search the left or right subtrees as before. Eventually, we will reach an external node and add the new key-value pair (here encoded as a record 'newNode') as its right or left child, depending on the node's key. In other words, we examine the root and recursively insert the new node to the left subtree if its key is less than that of the root, or the right subtree if its key is greater than or equal to the root.

Here's how a typical binary search tree insertion might be performed in a binary tree in C++:

Alternatively, a non-recursive version might be implemented like this. Using a pointer-to-pointer to keep track of where we came from lets the code avoid explicit checking for and handling of the case where it needs to insert a node at the tree root:

The above "destructive" procedural variant modifies the tree in place. It uses only constant heap space (and the iterative version uses constant stack space as well), but the prior version of the tree is lost. Alternatively, as in the following Python example, we can reconstruct all ancestors of the inserted node; any reference to the original tree root remains valid, making the tree a persistent data structure:

The part that is rebuilt uses space in the average case and in the worst case.

In either version, this operation requires time proportional to the height of the tree in the worst case, which is time in the average case over all trees, but time in the worst case.

Another way to explain insertion is that in order to insert a new node in the tree, its key is first compared with that of the root. If its key is less than the root's, it is then compared with the key of the root's left child. If its key is greater, it is compared with the root's right child. This process continues, until the new node is compared with a leaf node, and then it is added as this node's right or left child, depending on its key: if the key is less than the leaf's key, then it is inserted as the leaf's left child, otherwise as the leaf's right child.

There are other ways of inserting nodes into a binary tree, but this is the only way of inserting nodes at the leaves and at the same time preserving the BST structure.

When removing a node from a binary "search" tree it is mandatory to maintain the in-order sequence of the nodes.
There are many possibilities to do this. However, the following method which has been proposed by T. Hibbard in 1962 guarantees that the heights of the subject subtrees are changed by at most one.
There are three possible cases to consider:
In all cases, when "D" happens to be the root, make the replacement node root again.

Nodes with two children are harder to delete. A node's in-order successor is its right subtree's left-most child, and a node's in-order predecessor is the left subtree's right-most child. In either case, this node will have only one or no child at all. Delete it according to one of the two simpler cases above.

Consistently using the in-order successor or the in-order predecessor for every instance of the two-child case can lead to an unbalanced tree, so some implementations select one or the other at different times.

Runtime analysis: Although this operation does not always traverse the tree down to a leaf, this is always a possibility; thus in the worst case it requires time proportional to the height of the tree. It does not require more even when the node has two children, since it still follows a single path and does not visit any node twice.

Once the binary search tree has been created, its elements can be retrieved in-order by recursively traversing the left subtree of the root node, accessing the node itself, then recursively traversing the right subtree of the node, continuing this pattern with each node in the tree as it's recursively accessed. As with all binary trees, one may conduct a pre-order traversal or a post-order traversal, but neither are likely to be useful for binary "search" trees. An in-order traversal of a binary search tree will always result in a sorted list of node items (numbers, strings or other comparable items).

The code for in-order traversal in Python is given below. It will call callback (some function the programmer wishes to call on the node's value, such as printing to the screen) for every node in the tree.

Traversal requires time, since it must visit every node. This algorithm is also , so it is asymptotically optimal.

Traversal can also be implemented iteratively. For certain applications, e.g. greater equal search, approximative search, an operation for "single step (iterative) traversal" can be very useful. This is, of course, implemented without the callback construct and takes on average and in the worst case.

Sometimes we already have a binary tree, and we need to determine whether it is a BST. This problem has a simple recursive solution.

The BST property—every node on the right subtree has to be larger than the current node and every node on the left subtree has to be smaller than the current node—is the key to figuring out whether a tree is a BST or not. The greedy algorithm—simply traverse the tree, at every node check whether the node contains a value larger than the value at the left child and smaller than the value on the right child—does not work for all cases. Consider the following tree:

In the tree above, each node meets the condition that the node contains a value larger than its left child and smaller than its right child hold, and yet it is not a BST: the value 5 is on the right subtree of the node containing 20, a violation of the BST property.

Instead of making a decision based solely on the values of a node and its children, we also need information flowing down from the parent as well. In the case of the tree above, if we could remember about the node containing the value 20, we would see that the node with value 5 is violating the BST property contract.

So the condition we need to check at each node is: 

A recursive solution in C++ can explain this further:
codice_1 and codice_2 are done to allow only distinct elements in BST.

If we want same elements to also be present, then we can use only codice_3 in both places.

The initial call to this function can be something like this:

Essentially we keep creating a valid range (starting from [MIN_VALUE, MAX_VALUE]) and keep shrinking it down for each node as we go down recursively.

As pointed out in section #Traversal, an in-order traversal of a binary "search" tree returns the nodes sorted. Thus we only need to keep the last visited node while traversing the tree and check whether its key is smaller (or smaller/equal, if duplicates are to be allowed in the tree) compared to the current key.

A binary search tree can be used to implement a simple sorting algorithm. Similar to heapsort, we insert all the values we wish to sort into a new ordered data structure—in this case a binary search tree—and then traverse it in order.

The worst-case time of codice_4 is —if you feed it a sorted list of values, it chains them into a linked list with no left subtrees. For example, codice_5 yields the tree codice_6.

There are several schemes for overcoming this flaw with simple binary trees; the most common is the self-balancing binary search tree. If this same procedure is done using such a tree, the overall worst-case time is , which is asymptotically optimal for a comparison sort. In practice, the added overhead in time and space for a tree-based sort (particularly for node allocation) make it inferior to other asymptotically optimal sorts such as heapsort for static list sorting. On the other hand, it is one of the most efficient methods of "incremental sorting", adding items to a list over time while keeping the list sorted at all times.

Binary search trees can serve as priority queues: structures that allow insertion of arbitrary key as well as lookup and deletion of the minimum (or maximum) key. Insertion works as previously explained. "Find-min" walks the tree, following left pointers as far as it can without hitting a leaf:

"Find-max" is analogous: follow right pointers as far as possible. "Delete-min" ("max") can simply look up the minimum (maximum), then delete it. This way, insertion and deletion both take logarithmic time, just as they do in a binary heap, but unlike a binary heap and most other priority queue implementations, a single tree can support all of "find-min", "find-max", "delete-min" and "delete-max" at the same time, making binary search trees suitable as double-ended priority queues.

There are many types of binary search trees. AVL trees and red-black trees are both forms of self-balancing binary search trees. A splay tree is a binary search tree that automatically moves frequently accessed elements nearer to the root. In a treap ("tree heap"), each node also holds a (randomly chosen) priority and the parent node has higher priority than its children. Tango trees are trees optimized for fast searches.
T-trees are binary search trees optimized to reduce storage space overhead, widely used for in-memory databases

A degenerate tree is a tree where for each parent node, there is only one associated child node. It is unbalanced and, in the worst case, performance degrades to that of a linked list. If your add node function does not handle re-balancing, then you can easily construct a degenerate tree by feeding it with data that is already sorted. What this means is that in a performance measurement, the tree will essentially behave like a linked list data structure.

D. A. Heger (2004) presented a performance comparison of binary search trees. Treap was found to have the best average performance, while red-black tree was found to have the smallest number of performance variations.

If we do not plan on modifying a search tree, and we know exactly how often each item will be accessed, we can construct an "optimal binary search tree", which is a search tree where the average cost of looking up an item (the "expected search cost") is minimized.

Even if we only have estimates of the search costs, such a system can considerably speed up lookups on average. For example, if we have a BST of English words used in a spell checker, we might balance the tree based on word frequency in text corpora, placing words like "the" near the root and words like "agerasia" near the leaves. Such a tree might be compared with Huffman trees, which similarly seek to place frequently used items near the root in order to produce a dense information encoding; however, Huffman trees store data elements only in leaves, and these elements need not be ordered.

If the sequence in which the elements in the tree will be accessed is unknown in advance, splay trees can be used which are asymptotically as good as any static search tree we can construct for any particular sequence of lookup operations.

"Alphabetic trees" are Huffman trees with the additional constraint on order, or, equivalently, search trees with the modification that all elements are stored in the leaves. Faster algorithms exist for "optimal alphabetic binary trees" (OABTs).




</doc>
<doc id="378310" url="https://en.wikipedia.org/wiki?curid=378310" title="Self-balancing binary search tree">
Self-balancing binary search tree

In computer science, a self-balancing (or height-balanced) binary search tree is any node-based binary search tree that automatically keeps its height (maximal number of levels below the root) small in the face of arbitrary item insertions and deletions.

These structures provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets.

The red–black tree, which is a type of self-balancing binary search tree, was called symmetric binary B-tree and was renamed but can still be confused with the generic concept of self-balancing binary search tree because of the initials.

Most operations on a binary search tree (BST) take time directly proportional to the height of the tree, so it is desirable to keep the height small. A binary tree with height "h" can contain at most 2+2+···+2 = 2−1 nodes. It follows that for any tree with "n" nodes and height "h":

formula_1

And that implies:

formula_2.

In other words, the minimum height of a binary tree with "n" nodes is log("n"), rounded down; that is, formula_3.

However, the simplest algorithms for BST item insertion may yield a tree with nodes "n" in rather common situations. For example, when the items are inserted in sorted key order, the tree degenerates into a linked list with "n" nodes. The difference in performance between the two situations may be enormous: for example, when "n" = 1,000,000, the minimum height is formula_4.

If the data items are known ahead of time, the height can be kept small, in the average sense, by adding values in a random order, resulting in a random binary search tree. However, there are many situations (such as online algorithms) where this randomization is not viable.

Self-balancing binary trees solve this problem by performing transformations on the tree (such as tree rotations) at key insertion times, in order to keep the height proportional to log("n"). Although a certain overhead is involved, it may be justified in the long run by ensuring fast execution of later operations.

While it is possible to maintain a BST with minimum height with expected formula_5 time operations (lookup/insertion/removal), the additional space requirements required to maintain such a structure tend to outweigh the decrease in search time. For comparison, an AVL tree is guaranteed to be within a factor of 1.44 of the optimal height while requiring only two additional bits of storage in a naive implementation. Therefore, most self-balanced BST algorithms keep the height within a constant factor of this lower bound.

In the asymptotic ("Big-O") sense, a self-balancing BST structure containing "n" items allows the lookup, insertion, and removal of an item in O(log "n") worst-case time, and ordered enumeration of all items in O("n") time. For some implementations these are per-operation time bounds, while for others they are amortized bounds over a sequence of operations. These times are asymptotically optimal among all data structures that manipulate the key only through comparisons.

Data structures implementing this type of tree include:


Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues. They can also be used for associative arrays; key-value pairs are simply inserted with an ordering based on the key alone. In this capacity, self-balancing BSTs have a number of advantages and disadvantages over their main competitor, hash tables. One advantage of self-balancing BSTs is that they allow fast (indeed, asymptotically optimal) enumeration of the items "in key order", which hash tables do not provide. One disadvantage is that their lookup algorithms get more complicated when there may be multiple items with the same key. Self-balancing BSTs have better worst-case lookup performance than hash tables (O(log n) compared to O(n)), but have worse average-case performance (O(log n) compared to O(1)).

Self-balancing BSTs can be used to implement any algorithm that requires mutable ordered lists, to achieve optimal worst-case asymptotic performance. For example, if binary tree sort is implemented with a self-balanced BST, we have a very simple-to-describe yet asymptotically optimal O("n" log "n") sorting algorithm. Similarly, many algorithms in computational geometry exploit variations on self-balancing BSTs to solve problems such as the line segment intersection problem and the point location problem efficiently. (For average-case performance, however, self-balanced BSTs may be less efficient than other solutions. Binary tree sort, in particular, is likely to be slower than merge sort, quicksort, or heapsort, because of the tree-balancing overhead as well as cache access patterns.)

Self-balancing BSTs are flexible data structures, in that it's easy to extend them to efficiently record additional information or perform new operations. For example, one can record the number of nodes in each subtree having a certain property, allowing one to count the number of nodes in a certain key range with that property in O(log "n") time. These extensions can be used, for example, to optimize database queries or other list-processing algorithms.




</doc>
<doc id="30816" url="https://en.wikipedia.org/wiki?curid=30816" title="Tree rotation">
Tree rotation

In discrete mathematics, tree rotation is an operation on a binary tree that changes the structure without interfering with the order of the elements. A tree rotation moves one node up in the tree and one node down. It is used to change the shape of the tree, and in particular to decrease its height by moving smaller subtrees down and larger subtrees up, resulting in improved performance of many tree operations.

There exists an inconsistency in different descriptions as to the definition of the direction of rotations. Some say that the direction of rotation reflects the direction that a node is moving upon rotation (a left child rotating into its parent's location is a right rotation) while others say that the direction of rotation reflects which subtree is rotating (a left subtree rotating into its parent's location is a left rotation, the opposite of the former). This article takes the approach of the directional movement of the rotating node.

The right rotation operation as shown in the adjacent image is performed with "Q" as the root and hence is a right rotation on, or rooted at, "Q". This operation results in a rotation of the tree in the clockwise direction. The inverse operation is the left rotation, which results in a movement in a counter-clockwise direction (the left rotation shown above is rooted at "P"). The key to understanding how a rotation functions is to understand its constraints. In particular the order of the leaves of the tree (when read left to right for example) cannot change (another way to think of it is that the order that the leaves would be visited in an in-order traversal must be the same after the operation as before). Another constraint is the main property of a binary search tree, namely that the right child is greater than the parent and the left child is less than the parent. Notice that the right child of a left child of the root of a sub-tree (for example node B in the diagram for the tree rooted at Q) can become the left child of the root, that itself becomes the right child of the "new" root in the rotated sub-tree, without violating either of those constraints. As you can see in the diagram, the order of the leaves doesn't change. The opposite operation also preserves the order and is the second kind of rotation.

Assuming this is a binary search tree, as stated above, the elements must be interpreted as variables that can be compared to each other. The alphabetic characters to the left are used as placeholders for these variables. In the animation to the right, capital alphabetic characters are used as variable placeholders while lowercase Greek letters are placeholders for an entire set of variables. The circles represent individual nodes and the triangles represent subtrees. Each subtree could be empty, consist of a single node, or consist of any number of nodes.

When a subtree is rotated, the subtree side upon which it is rotated increases its height by one node while the other subtree decreases its height. This makes tree rotations useful for rebalancing a tree.

Using the terminology of Root for the parent node of the subtrees to rotate, Pivot for the node which will become the new parent node, RS for rotation side upon to rotate and OS for opposite side of rotation. In the above diagram for the root Q, the RS is C and the OS is P. The pseudo code for the rotation is:

This is a constant time operation.

The programmer must also make sure that the root's parent points to the pivot after the rotation. Also, the programmer should note that this operation may result in a new root for the entire tree and take care to update pointers accordingly.

The tree rotation renders the inorder traversal of the binary tree invariant. This implies the order of the elements are not affected when a rotation is performed in any part of the tree. Here are the inorder traversals of the trees shown above:

Computing one from the other is very simple. The following is example Python code that performs that computation:

Another way of looking at it is:

Right rotation of node Q:

Left rotation of node P:

All other connections are left as-is.

There are also "double rotations", which are combinations of left and right rotations. A "double left" rotation at X can be defined to be a right rotation at the right child of X followed by a left rotation at X; similarly, a "double right" rotation at X can be defined to be a left rotation at the left child of X followed by a right rotation at X.

Tree rotations are used in a number of tree data structures such as AVL trees, red-black trees, splay trees, and treaps. They require only constant time because they are "local" transformations: they only operate on 5 nodes, and need not examine the rest of the tree.

A tree can be rebalanced using rotations. After a rotation, the side of the rotation increases its height by 1 whilst the side opposite the rotation decreases its height similarly. Therefore, one can strategically apply rotations to nodes whose left child and right child differ in height by more than 1. Self-balancing binary search trees apply this operation automatically. A type of tree which uses this rebalancing technique is the AVL tree.

The rotation distance between any two binary trees with the same number of nodes is the minimum number of rotations needed to transform one into the other. With this distance, the set of "n"-node binary trees becomes a metric space: the distance is symmetric, positive when given two different trees, and satisfies the triangle inequality.

It is an open problem whether there exists a polynomial time algorithm for calculating rotation distance.

Daniel Sleator, Robert Tarjan and William Thurston showed that the rotation distance between any two "n"-node trees (for "n" ≥ 11) is at most 2"n" − 6, and that some pairs of trees are this far apart as soon as "n" is sufficiently large. Lionel Pournin showed that, in fact, such pairs exist whenever "n" ≥ 11.




</doc>
<doc id="4849574" url="https://en.wikipedia.org/wiki?curid=4849574" title="Weight-balanced tree">
Weight-balanced tree

In computer science, weight-balanced binary trees (WBTs) are a type of self-balancing binary search trees that can be used to implement dynamic sets, dictionaries (maps) and sequences. These trees were introduced by Nievergelt and Reingold in the 1970s as trees of bounded balance, or BB[α] trees. Their more common name is due to Knuth.

Like other self-balancing trees, WBTs store bookkeeping information pertaining to balance in their nodes and perform rotations to restore balance when it is disturbed by insertion or deletion operations. Specifically, each node stores the size of the subtree rooted at the node, and the sizes of left and right subtrees are kept within some factor of each other. Unlike the balance information in AVL trees (which store the height of subtrees) and red-black trees (which store a fictional "color" bit), the bookkeeping information in a WBT is an actually useful property for applications: the number of elements in a tree is equal to the size of its root, and the size information is exactly the information needed to implement the operations of an order statistic tree, viz., getting the 'th largest element in a set or determining an element's index in sorted order.

Weight-balanced trees are popular in the functional programming community and are used to implement sets and maps in MIT Scheme, SLIB and implementations of Haskell.

A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes. That is, a node has fields


By definition, the size of a leaf (typically represented by a pointer) is zero. The size of an internal node is the sum of sizes of its two children, plus one (). Based on the size, one defines the weight as .
Operations that modify the tree must make sure that the weight of the left and right subtrees of every node remain within some factor of each other, using the same rebalancing operations used in AVL trees: rotations and double rotations. Formally, node balance is defined as follows:

Here, is a numerical parameter to be determined when implementing weight balanced trees. Larger values of produce "more balanced" trees, but not all values of are appropriate; Nievergelt and Reingold proved that

is a necessary condition for the balancing algorithm to work. Later work showed a lower bound of for , although it can be made arbitrarily small if a custom (and more complicated) rebalancing algorithm is used.

Applying balancing correctly guarantees a tree of elements will have height

The number of balancing operations required in a sequence of insertions and deletions is linear in , i.e., balancing takes a constant amount of overhead in an amortized sense.

While maintaining tree with the minimum search cost requires four kinds of double rotations (LL, LR, RL, RR as in AVL tree) in insert/delete operations, if we desire only logarithmic performance, LR and RL are the only rotations required in a single top-down pass.

Several set operations have been defined on weight-balanced trees: union, intersection and set difference. Then fast "bulk" operations on insertions or deletions can be implemented based on these set functions. These set operations rely on two helper operations, "Split" and "Join". With the new operations, the implementation of weight-balanced trees can be more efficient and highly-parallelizable.


The join algorithm is as follows:

Here balanceformula_5 means two weights formula_6 and formula_7 are balanced. expose(v)=(l,k,r) means to extract a tree node formula_8's left child formula_9, the key of the node formula_10 and the right child formula_11. Node(l,k,r) means to create a node of left child formula_9, key formula_10 and right child formula_11.

The split algorithm is as follows:

The union of two weight-balanced trees and representing sets and , is a weight-balanced tree that represents . The following recursive function computes this union:

Here, "Split" is presumed to return two trees: one holding the keys less its input key, one holding the greater keys. (The algorithm is non-destructive, but an in-place destructive version exists as well.)

The algorithm for intersection or difference is similar, but requires the "Join2" helper routine that is the same as "Join" but without the middle key. Based on the new functions for union, intersection or difference, either one key or multiple keys can be inserted to or deleted from the weight-balanced tree. Since "Split" and "Union" call "Join" but do not deal with the balancing criteria of weight-balanced trees directly, such an implementation is usually called the join-based algorithms.

The complexity of each of union, intersection and difference is formula_15 for two weight-balanced trees of sizes formula_16 and formula_17. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth formula_18. When formula_19, the join-based implementation has the same computational directed acyclic graph (DAG) as single-element insertion and deletion if the root of the larger tree is used to split the smaller tree.


</doc>
<doc id="4262609" url="https://en.wikipedia.org/wiki?curid=4262609" title="Threaded binary tree">
Threaded binary tree

In computing, a threaded binary tree is a binary tree variant that facilitates traversal in a particular order (often the same order already defined for the tree).

An entire binary sort tree can be easily traversed in order of the main key, but given only a pointer to a node, finding the node which comes next may be slow or impossible. For example, leaf nodes by definition have no descendants, so no other node can be reached given only a pointer to a leaf node -- of course that includes the desired "next" node. A threaded tree adds extra information in some or all nodes, so the "next" node can be found quickly. It can also be traversed without recursion and the extra storage (proportional to the tree's depth) that requires.

A threaded binary tree defined as follows:
"A binary tree is "threaded" by making all right child pointers that would normally be null point to the in-order successor of the node (if it exists), and all left child pointers that would normally be null point to the in-order predecessor of the node."
This definition assumes the traversal order is the same as in-order traversal of the tree. However, pointers can instead (or in addition) be added to tree nodes, rather than replacing linked lists thus defined are also commonly called "threads", and can be used to enable traversal in any order(s) desired. For example, a tree whose nodes represent information about people might be sorted by name, but have extra threads allowing quick traversal in order of birth date, weight, or any other known characteristic.

Trees, including (but not limited to) binary search trees, can be used to store items in a particular order, such as the value of some property stored in each node, often called a key. One useful operation on such a tree is "traversal": visiting all the items in order of the key.

A simple recursive traversal algorithm that visits each node of a binary search tree is the following. Assume is a pointer to a node, or . "Visiting" can mean performing any action on the node or its contents.

One problem with this algorithm is that, because of its recursion, it uses stack space proportional to the height of a tree. If the tree is fairly balanced, this amounts to space for a tree containing elements. In the worst case, when the tree takes the form of a chain, the height of the tree is so the algorithm takes space. A second problem is that all traversals must begin at the root when nodes have pointers only to their children. It is common to have a pointer to a particular node, but that is not sufficient to get back to the rest of the tree unless extra information is added, such as thread pointers.

In this approach, it may not be possible to tell whether the left and/or right pointers in a given node actually point to children, or are a consequence of threading. If the distinction is necessary, adding a single bit to each node is enough to record it.

In 1968, Donald Knuth asked whether a non-recursive algorithm for in-order traversal exists, that uses no stack and leaves the tree unmodified. One of the solutions to this problem is tree threading, presented by J. H. Morris in 1979.

Another way to achieve similar goals is to include a pointer in every node, to that node's parent node. Given that, the "next" node can always be reached. "right" pointers are still null whenever there are no right children. To find the "next" node from a node whose right pointer is null, walk up through "parent" pointers until reaching a node whose right pointer is not null, and is not the child you just came up from. That node is the "next" node, and after it come its descendants on the right.

It is also possible to discover the parent of a node from a threaded binary tree, without explicit use of parent pointers or a stack, although it is slower. To see this, consider a node "k" with right child "r". Then the left pointer of "r" must be either a child or a thread back to "k". In the case that "r" has a left child, that left child must in turn have either a left child of its own or a thread back to "k", and so on for all successive left children. So by following the chain of left pointers from "r", we will eventually find a thread pointing back to "k". The situation is symmetrically similar when "q" is the left child of "p"—we can follow "q"'s right children to a thread pointing ahead to "p".


In Python:
Threads are reference to the predecessors and successors of the node according to an inorder traversal. 

In-order traversal of the threaded tree is codice_1, the predecessor of codice_2 is codice_3, the successor of codice_2 is codice_5.

Let's make the Threaded Binary tree out of a normal binary tree:

The in-order traversal for the above tree is — D B A E C. So, the respective Threaded Binary tree will be --

In an m-way threaded binary tree with "n" nodes, there are n*m - (n-1) void links.

As this is a non-recursive method for traversal, it has to be an iterative procedure; meaning, all the steps for the traversal of a node have to be under a loop so that the same can be applied to all the nodes in the tree. 

We will consider the IN-ORDER traversal again. Here, for every node, we'll visit the left sub-tree (if it exists) first (if and only if we haven't visited it earlier); then we visit (i.e. print its value, in our case) the node itself and then the right sub-tree (if it exists). If the right sub-tree is not there, we check for the threaded link and make the threaded node the current node in consideration. Please, follow the example given below.




</doc>
<doc id="2118" url="https://en.wikipedia.org/wiki?curid=2118" title="AVL tree">
AVL tree

In computer science, an AVL tree (named after inventors Adelson-Velsky and Landis) is a self-balancing binary search tree. It was the first such data structure to be invented. In an AVL tree, the heights of the two child subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. Lookup, insertion, and deletion all take time in both the average and worst cases, where formula_1 is the number of nodes in the tree prior to the operation. Insertions and deletions may require the tree to be rebalanced by one or more tree rotations.

The AVL tree is named after its two Soviet inventors, Georgy Adelson-Velsky and Evgenii Landis, who published it in their 1962 paper "An algorithm for the organization of information".

AVL trees are often compared with red–black trees because both support the same set of operations and take formula_2 time for the basic operations. For lookup-intensive applications, AVL trees are faster than red–black trees because they are more strictly balanced. Similar to red–black trees, AVL trees are height-balanced. Both are, in general, neither weight-balanced nor formula_3-balanced for any formula_4;





</doc>
<doc id="1665969" url="https://en.wikipedia.org/wiki?curid=1665969" title="AA tree">
AA tree

An AA tree in computer science is a form of balanced tree used for storing and retrieving ordered data efficiently. AA trees are named for Arne Andersson, their inventor.

AA trees are a variation of the red-black tree, a form of binary search tree which supports efficient addition and deletion of entries. Unlike red-black trees, red nodes on an AA tree can only be added as a right subchild. In other words, no red node can be a left sub-child. This results in the simulation of a 2-3 tree instead of a 2-3-4 tree, which greatly simplifies the maintenance operations. The maintenance algorithms for a red-black tree need to consider seven different shapes to properly balance the tree:

An AA tree on the other hand only needs to consider two shapes due to the strict requirement that only right links can be red:

Whereas red-black trees require one bit of balancing metadata per node (the color), AA trees require O(log(log(N))) bits of metadata per node, in the form of an integer "level". The following invariants hold for AA trees:


A link where the child's level is equal to that of its parent is called a "horizontal" link, and is analogous to a red link in the red-black tree. Individual right horizontal links are allowed, but consecutive ones are forbidden; all left horizontal links are forbidden. These are more restrictive constraints than the analogous ones on red-black trees, with the result that re-balancing an AA tree is procedurally much simpler than re-balancing a red-black tree.

Insertions and deletions may transiently cause an AA tree to become unbalanced (that is, to violate the AA tree invariants). Only two distinct operations are needed for restoring balance: "skew" and "split". Skew is a right rotation to replace a subtree containing a left horizontal link with one containing a right horizontal link instead. Split is a left rotation and level increase to replace a subtree containing two or more consecutive right horizontal links with one containing two fewer consecutive right horizontal links. Implementation of balance-preserving insertion and deletion is simplified by relying on the skew and split operations to modify the tree only if needed, instead of making their callers decide whether to skew or split.

Skew: 

Split: 

Insertion begins with the normal binary tree search and insertion procedure. Then, as the call stack unwinds (assuming a recursive implementation of the search), it's easy to check the validity of the tree and perform any rotations as necessary. If a horizontal left link arises, a skew will be performed, and if two horizontal right links arise, a split will be performed, possibly incrementing the level of the new root node of the current subtree. Note, in the code as given above, the increment of level(T). This makes it necessary to continue checking the validity of the tree as the modifications bubble up from the leaves.

As in most balanced binary trees, the deletion of an internal node can be turned into the deletion of a leaf node by swapping the internal node with either its closest predecessor or successor, depending on which are in the tree or on the implementor's whims. Retrieving a predecessor is simply a matter of following one left link and then all of the remaining right links. Similarly, the successor can be found by going right once and left until a null pointer is found. Because of the AA property of all nodes of level greater than one having two children, the successor or predecessor node will be in level 1, making their removal trivial.

To re-balance a tree, there are a few approaches. The one described by Andersson in his original paper is the simplest, and it is described here, although actual implementations may opt for a more optimized approach. After a removal, the first step to maintaining tree validity is to lower the level of any nodes whose children are two levels below them, or who are missing children. Then, the entire level must be skewed and split. This approach was favored, because when laid down conceptually, it has three easily understood separate steps:


However, we have to skew and split the entire level this time instead of just a node, complicating our code.

A good example of deletion by this algorithm is present in the Andersson paper.

The performance of an AA tree is equivalent to the performance of a red-black tree. While an AA tree makes more rotations than a red-black tree, the simpler algorithms tend to be faster, and all of this balances out to result in similar performance. A red-black tree is more consistent in its performance than an AA tree, but an AA tree tends to be flatter, which results in slightly faster search times. 




</doc>
<doc id="1377178" url="https://en.wikipedia.org/wiki?curid=1377178" title="Scapegoat tree">
Scapegoat tree

In computer science, a scapegoat tree is a self-balancing binary search tree, invented by Arne Andersson and again by Igal Galperin and Ronald L. Rivest. It provides worst-case "O"(log "n") lookup time, and "O"(log "n") amortized insertion and deletion time.

Unlike most other self-balancing binary search trees which provide worst case "O"(log "n") lookup time, scapegoat trees have no additional per-node memory overhead compared to a regular binary search tree: a node stores only a key and two pointers to the child nodes. This makes scapegoat trees easier to implement and, due to data structure alignment, can reduce node overhead by up to one-third.

Instead of the small incremental rebalancing operations used by most balanced tree algorithms, scapegoat trees rarely but expensively choose a "scapegoat" and completely rebuild the subtree rooted at the scapegoat into a complete binary tree. Thus, scapegoat trees have "O"("n") worst-case update performance.

A binary search tree is said to be weight-balanced if half the nodes are on the left of the root, and half on the right.
An α-weight-balanced node is defined as meeting a relaxed weight balance criterion:
Where size can be defined recursively as:

Even a degenerate tree (linked list) satisfies this condition if α=1, whereas an α=0.5 would only match almost complete binary trees.

A binary search tree that is α-weight-balanced must also be α-height-balanced, that is 

By contraposition, a tree that is not α-height-balanced is not α-weight-balanced.

Scapegoat trees are not guaranteed to keep α-weight-balance at all times, but are always loosely α-height-balanced in that
Violations of this height balance condition can be detected at insertion time, and imply that a violation of the weight balance condition must exist. 

This makes scapegoat trees similar to red-black trees in that they both have restrictions on their height. They differ greatly though in their implementations of determining where the rotations (or in the case of scapegoat trees, rebalances) take place. Whereas red-black trees store additional 'color' information in each node to determine the location, scapegoat trees find a scapegoat which isn't α-weight-balanced to perform the rebalance operation on. This is loosely similar to AVL trees, in that the actual rotations depend on 'balances' of nodes, but the means of determining the balance differs greatly. Since AVL trees check the balance value on every insertion/deletion, it is typically stored in each node; scapegoat trees are able to calculate it only as needed, which is only when a scapegoat needs to be found.

Unlike most other self-balancing search trees, scapegoat trees are entirely flexible as to their balancing. They support any α such that 0.5 < α < 1. A high α value results in fewer balances, making insertion quicker but lookups and deletions slower, and vice versa for a low α. Therefore in practical applications, an α can be chosen depending on how frequently these actions should be performed.

Lookup is not modified from a standard binary search tree, and has a worst-case time of O(log "n"). This is in contrast to splay trees which have a worst-case time of O("n"). The reduced node memory overhead compared to other self-balancing binary search trees can further improve locality of reference and caching.

Insertion is implemented with the same basic ideas as an unbalanced binary search tree, however with a few significant changes.

When finding the insertion point, the depth of the new node must also be recorded. This is implemented via a simple counter that gets incremented during each iteration of the lookup, effectively counting the number of edges between the root and the inserted node. If this node violates the α-height-balance property (defined above), a rebalance is required.

To rebalance, an entire subtree rooted at a scapegoat undergoes a balancing operation. The scapegoat is defined as being an ancestor of the inserted node which isn't α-weight-balanced. There will always be at least one such ancestor. Rebalancing any of them will restore the α-height-balanced property.

One way of finding a scapegoat, is to climb from the new node back up to the root and select the first node that isn't α-weight-balanced.

Climbing back up to the root requires O(log "n") storage space, usually allocated on the stack, or parent pointers. This can actually be avoided by pointing each child at its parent as you go down, and repairing on the walk back up.

To determine whether a potential node is a viable scapegoat, we need to check its α-weight-balanced property. To do this we can go back to the definition:
However a large optimisation can be made by realising that we already know two of the three sizes, leaving only the third to be calculated.

Consider the following example to demonstrate this. Assuming that we're climbing back up to the root:
But as:
The case is trivialized down to:
Where x = this node, x + 1 = parent and size(sibling) is the only function call actually required.

Once the scapegoat is found, the subtree rooted at the scapegoat is completely rebuilt to be perfectly balanced. This can be done in O("n") time by traversing the nodes of the subtree to find their values in sorted order and recursively choosing the median as the root of the subtree.

As rebalance operations take O("n") time (dependent on the number of nodes of the subtree), insertion has a worst-case performance of O("n") time. However, because these worst-case scenarios are spread out, insertion takes O(log "n") amortized time.

Define the Imbalance of a node "v" to be the absolute value of the difference in size between its left node and right node minus 1, or 0, whichever is greater. In other words:

formula_1

Immediately after rebuilding a subtree rooted at "v", I("v") = 0.

Lemma: Immediately before rebuilding the subtree rooted at "v", 
formula_2

Proof of lemma:

Let formula_4 be the root of a subtree immediately after rebuilding. formula_5. If there are formula_6 degenerate insertions (that is, where each inserted node increases the height by 1), then 
formula_7,
formula_8 and
formula_9.

Since formula_10 before rebuilding, there were formula_11 insertions into the subtree rooted at formula_12 that did not result in rebuilding. Each of these insertions can be performed in formula_13 time. The final insertion that causes rebuilding costs formula_14. Using aggregate analysis it becomes clear that the amortized cost of an insertion is formula_13:

formula_16

Scapegoat trees are unusual in that deletion is easier than insertion. To enable deletion, scapegoat trees need to store an additional value with the tree data structure. This property, which we will call MaxNodeCount simply represents the highest achieved NodeCount. It is set to NodeCount whenever the entire tree is rebalanced, and after insertion is set to max(MaxNodeCount, NodeCount).

To perform a deletion, we simply remove the node as you would in a simple binary search tree, but if
then we rebalance the entire tree about the root, remembering to set MaxNodeCount to NodeCount.

This gives deletion its worst-case performance of O("n") time; however, it is amortized to O(log "n") average time.

Suppose the scapegoat tree has formula_17 elements and has just been rebuilt (in other words, it is a complete binary tree). At most formula_18 deletions can be performed before the tree must be rebuilt. Each of these deletions take formula_19 time (the amount of time to search for the element and flag it as deleted). The formula_20 deletion causes the tree to be rebuilt and takes formula_21 (or just formula_22) time. Using aggregate analysis it becomes clear that the amortized cost of a deletion is formula_19:

formula_24

The name Scapegoat tree "[...] is based on the common wisdom that, when something goes wrong, the first thing people tend to do is find someone to blame (the scapegoat)." In the Bible, a scapegoat is an animal that is ritually burdened with the sins of others, and then driven away. 




</doc>
<doc id="28382" url="https://en.wikipedia.org/wiki?curid=28382" title="Splay tree">
Splay tree

A splay tree is a self-balancing binary search tree with the additional property that recently accessed elements are quick to access again. It performs basic operations such as insertion, look-up and removal in O(log n) amortized time. For many sequences of non-random operations, splay trees perform better than other search trees, even when the specific pattern of the sequence is unknown. The splay tree was invented by Daniel Sleator and Robert Tarjan in 1985.

All normal operations on a binary search tree are combined with one basic operation, called "splaying". Splaying the tree for a certain element rearranges the tree so that the element is placed at the root of the tree. One way to do this with the basic search operation is to first perform a standard binary tree search for the element in question, and then use tree rotations in a specific fashion to bring the element to the top. Alternatively, a top-down algorithm can combine the search and the tree reorganization into a single phase.

Good performance for a splay tree depends on the fact that it is self-optimizing, in that frequently accessed nodes will move nearer to the root where they can be accessed more quickly. The worst-case height—though unlikely—is O(n), with the average being O(log "n").
Having frequently-used nodes near the root is an advantage for many practical applications (also see Locality of reference), and is particularly useful for implementing caches and garbage collection algorithms.

Advantages include:

The most significant disadvantage of splay trees is that the height of a splay tree can be linear. For example, this will be the case after accessing all "n" elements in non-decreasing order. Since the height of a tree corresponds to the worst-case access time, this means that the actual cost of an operation can be high. However the amortized access cost of this worst case is logarithmic, O(log "n"). Also, the expected access cost can be reduced to O(log "n") by using a randomized variant.

The representation of splay trees can change even when they are accessed in a 'read-only' manner (i.e. by "find" operations). This complicates the use of such splay trees in a multi-threaded environment. Specifically, extra management is needed if multiple threads are allowed to perform "find" operations concurrently. This also makes them unsuitable for general use in purely functional programming, although even there they can be used in limited ways to implement priority queues.

When a node "x" is accessed, a splay operation is performed on "x" to move it to the root. To perform a splay operation we carry out a sequence of "splay steps", each of which moves "x" closer to the root. By performing a splay operation on the node of interest after every access, the recently accessed nodes are kept near the root and the tree remains roughly balanced, so that we achieve the desired amortized time bounds.

Each particular step depends on three factors:

It is important to remember to set "gg" (the "great-grandparent" of x) to now point to x after any splay operation. If "gg" is null, then x obviously is now the root and must be updated as such.

There are three types of splay steps, each of which has two symmetric variants: left- and right-handed. For the sake of brevity, only one of these two is shown for each type. These three types are:

Zig step: this step is done when "p" is the root. The tree is rotated on the edge between "x" and "p". Zig steps exist to deal with the parity issue and will be done only as the last step in a splay operation and only when "x" has odd depth at the beginning of the operation.

Zig-zig step: this step is done when "p" is not the root and "x" and "p" are either both right children or are both left children. The picture below shows the case where "x" and "p" are both left children. The tree is rotated on the edge joining "p" with "its" parent "g", then rotated on the edge joining "x" with "p". Note that zig-zig steps are the only thing that differentiate splay trees from the "rotate to root" method introduced by Allen and Munro prior to the introduction of splay trees.

Zig-zag step: this step is done when "p" is not the root and "x" is a right child and "p" is a left child or vice versa. The tree is rotated on the edge between "p" and x, and then rotated on the resulting edge between "x" and g.

Given two trees S and T such that all elements of S are smaller than the elements of T, the following steps can be used to join them to a single tree:

Given a tree and an element "x", return two new trees: one containing all elements less than or equal to "x" and the other containing all elements greater than "x". This can be done in the following way:

To insert a value "x" into a splay tree:

Alternatively:

To delete a node "x", use the same method as with a binary search tree:


In this way, deletion is reduced to the problem of removing a node with 0 or 1 children. Unlike a binary search tree, in a splay tree after deletion, we splay the parent of the removed node to the top of the tree.

Alternatively:


Splaying, as mentioned above, is performed during a second, bottom-up pass over the access path of a node. It is possible to record the access path during the first pass for use during the second, but that requires extra space during the access operation. Another alternative is to keep a parent pointer in every node, which avoids the need for extra space during access operations but may reduce overall time efficiency because of the need to update those pointers.

Another method which can be used is based on the argument that we can restructure the tree on our way down the access path instead of making a second pass. This top-down splaying routine uses three sets of nodes - left tree, right tree and middle tree. The first two contain all items of original tree known to be less than or greater than current item respectively. The middle tree consists of the sub-tree rooted at the current node. These three sets are updated down the access path while keeping the splay operations in check. Another method, semisplaying, modifies the zig-zig case to reduce the amount of restructuring done in all operations.

Below there is an implementation of splay trees in C++, which uses pointers to represent each node on the tree. This implementation is based on bottom-up splaying version and uses the second method of deletion on a splay tree. Also, unlike the above definition, this C++ version does "not" splay the tree on finds - it only splays on insertions and deletions, and the find operation, therefore, has linear time complexity.

A simple amortized analysis of static splay trees can be carried out using the potential method. Define:

Φ will tend to be high for poorly balanced trees and low for well-balanced trees.

To apply the potential method, we first calculate ΔΦ: the change in the potential caused by a splay operation. We check each case separately. Denote by rank′ the rank function after the operation. x, p and g are the nodes affected by the rotation operation (see figures above).

The amortized cost of any operation is ΔΦ plus the actual cost. The actual cost of any zig-zig or zig-zag operation is 2 since there are two rotations to make. Hence:

When summed over the entire splay operation, this telescopes to 3(rank(root)−rank("x")) which is O(log "n"). The Zig operation adds an amortized cost of 1, but there's at most one such operation.

So now we know that the total "amortized" time for a sequence of "m" operations is:

To go from the amortized time to the actual time, we must add the decrease in potential from the initial state before any operation is done (Φ) to the final state after all operations are completed (Φ).

where the last inequality comes from the fact that for every node "x", the minimum rank is 0 and the maximum rank is log("n").

Now we can finally bound the actual time:

The above analysis can be generalized in the following way.

The same analysis applies and the amortized cost of a splaying operation is again:
where "W" is the sum of all weights.

The decrease from the initial to the final potential is bounded by:
since the maximum size of any single node is "W" and the minimum is "w(x)".

Hence the actual time is bounded by:

There are several theorems and conjectures regarding the worst-case runtime for performing a sequence "S" of "m" accesses in a splay tree containing "n" elements.

In addition to the proven performance guarantees for splay trees there is an unproven conjecture of great interest from the original Sleator and Tarjan paper. This conjecture is known as the "dynamic optimality conjecture" and it basically claims that splay trees perform as well as any other binary search tree algorithm up to a constant factor.

There are several corollaries of the dynamic optimality conjecture that remain unproven:

In order to reduce the number of restructuring operations, it is possible to replace the splaying with "semi-splaying", in which an element is splayed only halfway towards the root.

Another way to reduce restructuring is to do full splaying, but only in some of the access operations - only when the access path is longer than a threshold, or only in the first "m" access operations.




</doc>
<doc id="1333086" url="https://en.wikipedia.org/wiki?curid=1333086" title="T-tree">
T-tree

In computer science a T-tree is a type of binary tree
data structure that is used by main-memory databases, such as
Datablitz, EXtremeDB, MySQL Cluster, Oracle TimesTen and MobileLite.

A T-tree is a balanced index tree data structure optimized for cases
where both the index and the actual data are fully kept in memory,
just as a B-tree is an index structure optimized for storage on block
oriented secondary storage devices like hard disks. T-trees seek to gain the performance benefits
of in-memory tree structures such as AVL trees while avoiding the large storage space overhead which
is common to them.

T-trees do not keep copies of the indexed data fields within the index tree nodes themselves. Instead, they take advantage of the fact that the actual data is always in main memory together with the index so that they just contain pointers to the actual data fields.

The 'T' in T-tree refers to the shape of the node data structures
in the original paper which first described this type of index.

Although T-trees seem to be widely used for main-memory databases, recent research indicates that they actually do not perform better than B-trees on modern hardware.

The main reason seems to be that the traditional assumption of memory references having uniform cost is no longer valid given the current speed gap between cache access and main memory access.

A T-tree node usually consists of pointers to the parent node, the left and right child node,
an ordered array of data pointers and some extra control data. Nodes with two subtrees
are called "internal nodes", nodes without subtrees are called "leaf nodes"
and nodes with only one subtree are named "half-leaf" nodes.
A node is called the "bounding node" for a value if the value is between the node's current minimum and maximum value, inclusively.

For each internal node, leaf or half leaf nodes exist that contain the predecessor of its smallest
data value (called the "greatest lower bound") and one that contains the successor of its largest
data value (called the "least upper bound"). Leaf and half-leaf nodes can contain any number of
data elements from one to the maximum size of the data array. Internal nodes keep their occupancy
between predefined minimum and maximum numbers of elements



If a new node was added then the tree might need to be rebalanced, as described below.


Now we have to distinguish by node type:

If the node's data array now has less than the minimum number of elements then move the greatest lower bound value of this node to its data value. Proceed with one of the following two steps for the half leaf or leaf node the value was removed from.
If this was the only element in the data array then delete the node. Rebalance the tree if needed.
If the node's data array can be merged with its leaf's data array without overflow then do so and remove the leaf node. Rebalance the tree if needed.

A T-tree is implemented on top of an underlying self-balancing binary search tree.
Specifically, Lehman and Carey's article describes a T-tree balanced like an AVL tree: it becomes out of balance when a node's child trees differ in height by at least two levels.
This can happen after an insertion or deletion of a node.
After an insertion or deletion, the tree is scanned from the leaf to the root.
If an imbalance is found, one tree rotation or pair of rotations is performed, which is guaranteed to balance the whole tree.

When the rotation results in an internal node having fewer than the minimum number of items, items from the node's new child(ren) are moved into the internal node.





</doc>
<doc id="334990" url="https://en.wikipedia.org/wiki?curid=334990" title="Rope">
Rope

A rope is a group of yarns, plies, fibers or strands that are twisted or braided together into a larger and stronger form. Ropes have tensile strength and so can be used for dragging and lifting. Rope is thicker and stronger than similarly constructed cord, string, and twine. 

Rope may be constructed of any long, stringy, fibrous material, but generally is constructed of certain natural or synthetic fibres. Synthetic fibre ropes are significantly stronger than their natural fibre counterparts, they have a higher tensile strength, they are more resistant to rotting than ropes created from natural fibers, and can be made to float on water. But synthetic rope also possess certain disadvantages, including slipperiness, and some can be damaged more easily by UV light.

Common natural fibres for rope are manila hemp, hemp, linen, cotton, coir, jute, straw, and sisal. Synthetic fibres in use for rope-making include polypropylene, nylon, polyesters (e.g. PET, LCP, Vectran), polyethylene (e.g. Dyneema and Spectra), Aramids (e.g. Twaron, Technora and Kevlar) and acrylics (e.g. Dralon). Some ropes are constructed of mixtures of several fibres or use co-polymer fibres. Wire rope is made of steel or other metal alloys. Ropes have been constructed of other fibrous materials such as silk, wool, and hair, but such ropes are not generally available. Rayon is a regenerated fibre used to make decorative rope.

The twist of the strands in a twisted or braided rope serves not only to keep a rope together, but enables the rope to more evenly distribute tension among the individual strands. Without any twist in the rope, the shortest strand(s) would always be supporting a much higher proportion of the total load.

The long history of rope means that many systems have been used to state the size of a rope. In systems that use the "inch" (British Imperial and United States Customary Measure), large ropes over diameter such as are used on ships are measured by their circumference in inches; smaller ropes have a nominal diameter based on the circumference divided by three (rounded-down value for pi). In metric systems of measurement, nominal diameter is given in millimetres. The current preferred international standard for rope sizes is to give the mass per unit length, in kilograms per metre. However, even sources otherwise using metric units may still give a "rope number" for large ropes, which is the circumference in inches.

Rope is of paramount importance in fields as diverse as construction, seafaring, exploration, sports, theatre, and communications, and has been used since prehistoric times. To fasten rope, many types of knots have been invented for countless uses. Pulleys redirect the pulling force to another direction, and can create mechanical advantage so that multiple strands of rope share a load and multiply the force applied to the end. Winches and capstans are machines designed to pull ropes.

The modern sport of rock climbing uses so-called "dynamic" rope, which stretches under load in an elastic manner to absorb the energy required to arrest a person in free fall without generating forces high enough to injure them. Such ropes normally use a kernmantle construction, as described below. "Static" ropes, used for example in caving, rappelling, and rescue applications, are designed for minimal stretch; they are not designed to arrest free falls. The UIAA, in concert with the CEN, sets climbing-rope standards and oversees testing. Any rope bearing a GUIANA or CE certification tag is suitable for climbing. Despite the hundreds of thousands of falls climbers suffer every year, there are few recorded instances of a climbing rope breaking in a fall; the cases that do are often attributable to previous damage to, or contamination of, the rope. Climbing ropes, however, do cut easily when under load. Keeping them away from sharp rock edges is imperative.

Rock climbing ropes come with either a designation for single, double or twin use. A single rope is the most common and it is intended to be used by itself, as a single strand. Single ropes range in thickness from roughly 9 mm to 11 mm. Smaller ropes are lighter, but wear out faster. Double ropes are thinner ropes, usually 9 mm and under, and are intended for use as a pair. These ropes offer a greater margin or security against cutting, since it is unlikely that both ropes will be cut, but they complicate belaying and leading. Double ropes are usually reserved for ice and mixed climbing, where there is need for two ropes to rappel or abseil. They are also popular among traditional climbers, and particularly in the UK, due to the ability to clip each rope into alternating pieces of protection; allowing the ropes to stay straighter and hence reduce rope drag.
Twin ropes are not to be confused with doubles. When using twin ropes, both ropes are clipped into the same piece of protection, treating the two as a single strand. This would be favourable in a situation where there was a high chance of a rope being cut. However new lighter-weight ropes with greater safety have virtually replaced this type of rope.

The butterfly coil is a method of carrying a rope used by climbers where the rope remains attached to the climber and ready to be uncoiled at short notice. Another method of carrying a rope is the alpine coil.

Rope is also an aerial acrobatics circus skill, where a performer makes artistic figures on a vertical suspended rope. Tricks performed on the rope are, for example, drops, rolls and hangs. They must also be strong. "See also Corde lisse".

The use of ropes for hunting, pulling, fastening, attaching, carrying, lifting, and climbing dates back to prehistoric times. It is likely that the earliest "ropes" were naturally occurring lengths of plant fibre, such as vines, followed soon by the first attempts at twisting and braiding these strands together to form the first proper ropes in the modern sense of the word. Impressions of cordage found on fired clay provide evidence of string and rope-making technology in Europe dating back 28,000 years. Fossilized fragments of "probably two-ply laid rope of about 7 mm diameter" were found in one of the caves at Lascaux, dating to approximately 15,000 BC.

The ancient Egyptians were probably the first civilization to develop special tools to make rope. Egyptian rope dates back to 4000 to 3500 BC and was generally made of water reed fibres. Other rope in antiquity was made from the fibres of date palms, flax, grass, papyrus, leather, or animal hair. The use of such ropes pulled by thousands of workers allowed the Egyptians to move the heavy stones required to build their monuments. Starting from approximately 2800 BC, rope made of hemp fibres was in use in China. Rope and the craft of rope making spread throughout Asia, India, and Europe over the next several thousand years.

From the Middle Ages until the 18th century, in Europe ropes were constructed in ropewalks, very long buildings where strands the full length of the rope were spread out and then "laid up" or twisted together to form the rope. The cable length was thus set by the length of the available rope walk. This is related to the unit of length termed "cable length". This allowed for long ropes of up to 300 yards long or longer to be made. These long ropes were necessary in shipping as short ropes would require splicing to make them long enough to use for sheets and halyards. The strongest form of splicing is the short splice, which doubles the cross-sectional area of the rope at the area of the splice, which would cause problems in running the line through pulleys. Any splices narrow enough to maintain smooth running would be less able to support the required weight.

Leonardo da Vinci drew sketches of a concept for a ropemaking machine, but it was never built. Nevertheless, remarkable feats of construction were accomplished without advanced technology: In 1586, Domenico Fontana erected the 327 ton obelisk on Rome's Saint Peter's Square with a concerted effort of 900 men, 75 horses, and countless pulleys and meters of rope. By the late 18th century several working machines had been built and patented.

Some rope is still made from natural fibres, such as coir and sisal, despite the dominance of synthetic fibres such as nylon and polypropylene, which have become increasingly popular since the 1950s.

Laid rope, also called twisted rope, is historically the prevalent form of rope, at least in modern Western history. Common twisted rope generally consists of three strands and is normally right-laid, or given a final right-handed twist. The ISO 2 standard uses the uppercase letters S and Z to indicate the two possible directions of twist, as suggested by the direction of slant of the central portions of these two letters. The handedness of the twist is the direction of the twists as they progress away from an observer. Thus Z-twist rope is said to be right-handed, and S-twist to be left-handed.

Twisted ropes are built up in three steps. First, fibres are gathered and spun into yarns. A number of these yarns are then formed into strands by twisting. The strands are then twisted together to lay the rope. The twist of the yarn is opposite to that of the strand, and that in turn is opposite to that of the rope. It is this counter-twist, introduced with each successive operation, which holds the final rope together as a stable, unified object.

Traditionally, a three strand laid rope is called a "plain-" or "hawser-laid", a four strand rope is called "shroud-laid", and a larger rope formed by counter-twisting three or more multi-strand ropes together is called "cable-laid". Cable-laid rope is sometimes clamped to maintain a tight counter-twist rendering the resulting cable virtually waterproof. Without this feature, deep water sailing (before the advent of steel chains and other lines) was largely impossible, as any appreciable length of rope for anchoring or ship to ship transfers, would become too waterlogged – and therefore too heavy – to lift, even with the aid of a capstan or windlass.

One property of laid rope is partial untwisting when used. This can cause spinning of suspended loads, or stretching, kinking, or hockling of the rope itself. An additional drawback of twisted construction is that every fibre is exposed to abrasion numerous times along the length of the rope. This means that the rope can degrade to numerous inch-long fibre fragments, which is not easily detected visually.

Twisted ropes have a preferred direction for coiling. Normal right-laid rope should be coiled clockwise, to prevent kinking. Coiling this way imparts a twist to the rope. Rope of this type must be bound at its ends by some means to prevent untwisting.

While rope may be made from three or more strands, modern braided rope consists of a braided (tubular) jacket over strands of fiber (these may also be braided). Some forms of braided rope with untwisted cores have a particular advantage; they do not impart an additional twisting force when they are stressed. The lack of added twisting forces is an advantage when a load is freely suspended, as when a rope is used for rappelling or to suspend an arborist. Other specialized cores reduce the shock from arresting a fall when used as a part of a personal or group safety system.

Braided ropes are generally made from nylon, polyester, polypropylene or high performance fibers such as high modulus polyethylene (HMPE) and aramid. Nylon is chosen for its strength and elastic stretch properties. However, nylon absorbs water and is 10–15% weaker when wet. Polyester is about 90% as strong as nylon but stretches less under load and is not affected by water. It has somewhat better UV resistance, and is more abrasion resistant. Polypropylene is preferred for low cost and light weight (it floats on water) but it has limited resistance to ultraviolet light, is susceptible to friction and has a poor heat resistance.

Braided ropes (and objects like garden hoses, fibre optic or coaxial cables, etc.) that have no "lay" (or inherent twist) uncoil better if each alternate loop is twisted in the opposite direction, such as in figure-eight coils, where the twist reverses regularly and essentially cancels out.

Single braid consists of an even number of strands, eight or twelve being typical, braided into a circular pattern with half of the strands going clockwise and the other half going anticlockwise. The strands can interlock with either twill or plain weave. The central void may be large or small; in the former case the term hollow braid is sometimes preferred.

Double braid, also called braid on braid, consists of an inner braid filling the central void in an outer braid, that may be of the same or different material. Often the inner braid fibre is chosen for strength while the outer braid fibre is chosen for abrasion resistance.

In solid braid, the strands all travel the same direction, clockwise or anticlockwise, and alternate between forming the outside of the rope and the interior of the rope. This construction is popular for general purpose utility rope but rare in specialized high performance line.
Kernmantle rope has a core (kern) of long twisted fibres in the center, with a braided outer sheath or mantle of woven fibres. The kern provides most of the strength (about 70%), while the mantle protects the kern and determines the handling properties of the rope (how easy it is to hold, to tie knots in, and so on). In dynamic climbing line, core fibres are usually twisted, and chopped into shorter lengths, which makes the rope more elastic. Static kernmantle ropes are made with untwisted core fibres and tighter braid, which causes them to be stiffer in addition to limiting the stretch.

Plaited rope is made by braiding twisted strands, and is also called "square braid". It is not as round as twisted rope and coarser to the touch. It is less prone to kinking than twisted rope and, depending on the material, very flexible and therefore easy to handle and knot. This construction exposes all fibres as well, with the same drawbacks as described above. Brait rope is a combination of braided and plaited, a non-rotating alternative to laid three-strand ropes. Due to its excellent energy-absorption characteristics, it is often used by arborists. It is also a popular rope for anchoring and can be used as mooring warps. This type of construction was pioneered by Yale Cordage.

Endless winding rope is made by winding single strands of high-performance yarns around two end terminations until the desired break strength or stiffness has been reached. This type of rope (often specified as cable to make the difference between a braided or twined construction) has the advantage of having no construction stretch as is the case with above constructions. Endless winding is pioneered by SmartRigging and FibreMax.

Rope made from hemp, cotton or nylon is generally stored in a cool dry place for proper storage. To prevent kinking it is usually coiled. To prevent fraying or unravelling, the ends of a rope are bound with twine (whipping), tape, or heat shrink tubing. The ends of plastic fibre ropes are often melted and fused solid; however, the rope and knotting expert Geoffrey Budworth warns against this practice thus:
"Sealing rope ends this way is lazy and dangerous." A tugboat operator once sliced the palm of his hand open down to the sinews after the hardened (and obviously "sharp") end of a rope that had been heat-sealed pulled through his grasp. There is no substitute for a properly made whipping.
If a load-bearing rope gets a sharp or sudden jolt or the rope shows signs of deteriorating, it is recommended that the rope be replaced immediately and should be discarded or only used for non-load-bearing tasks.

The average rope life-span is 5 years. Serious inspection should be given to line after that point. However, the use to which a rope is put affects frequency of inspection. Rope used in mission-critical applications, such as mooring lines or running rigging, should be regularly inspected on a much shorter timescale than this, and rope used in life-critical applications such as mountain climbing should be inspected on a far more frequent basis, up to and including before each use.

When preparing for a climb, it is important to stack the rope on the ground or a tarp and check for any "dead-spots".

Avoid stepping on rope, as this might force tiny pieces of rock through the sheath, which can eventually deteriorate the core of the rope.
Ropes may be flemished into coils on deck for safety and presentation/tidiness as shown in the picture. 

Many kinds of filaments in ropes are weakened by acids or other corrosive liquids or solvents, and high temperatures. Such damage is treacherous because it often is hard to tell by eye. Rope damaged in such ways is dangerous to use. Ropes therefore should be kept away from all kinds of solvents and from corrosive acids, alkalis, and oxidising agents.

In addition, ropes should avoid sudden load, as a shock load can destroy a rope easily. Any operation of ropes should obey the principle of safe working load, which is usually much less than its ultimate strength. The rope should be replaced immediately if any evidences of shock load have been found.

A rope under tension – particularly if it has a great deal of elasticity – can be very hazardous if it should part, snapping backward and potentially causing grave or lethal injury to people, or damage to objects, in its path. There are occasions when it is proper to cut a taut rope under load, but this should be done "only" when necessary and "only" with great forethought and preparation for the potential consequences.

"Rope" refers to the manufactured material, and is one type of cordage as distinguished from cable or small stuff, in nautical usage. Once rope is purposely sized, cut, spliced, or simply assigned a function, the result is often referred to as a "line", especially in nautical usage. The most common sail control lines, which controls the clew of a sail, are referred to as sheets (e.g. jib sheet). A halyard, for example, is a line used to raise and lower a sail, and is typically made of a length of rope with a shackle attached at one end. Other examples include clothesline, chalk line, anchor line ("rode"), stern line, fishing line, marline and so on. But cf. man rope, bolt rope, bell rope.







</doc>
<doc id="9871765" url="https://en.wikipedia.org/wiki?curid=9871765" title="Tango tree">
Tango tree

A tango tree is a type of binary search tree proposed by Erik D. Demaine, Dion Harmon, John Iacono, and Mihai Pătrașcu in 2004. It is named after Buenos Aires, of which the tango is emblematic.

It is an online binary search tree that achieves an formula_1 competitive ratio relative to the offline optimal binary search tree, while only using formula_1 additional bits of memory per node. This improved upon the previous best known competitive ratio, which was formula_3.

Tango trees work by partitioning a binary search tree into a set of "preferred paths", which are themselves stored in auxiliary trees (so the tango tree is represented as a tree of trees).

To construct a tango tree, we simulate a complete binary search tree called the "reference tree", which is simply a traditional binary search tree containing all the elements. This tree never shows up in the actual implementation, but is the conceptual basis behind the following pieces of a tango tree.

First, we define for each node its "preferred child", which informally is the most-recently touched child by a traditional binary search tree lookup. More formally, consider a subtree "T", rooted at "p", with children "l" (left) and "r" (right). We set "r" as the preferred child of "p" if the most recently accessed node in "T" is in the subtree rooted at "r", and "l" as the preferred child otherwise. Note that if the most recently accessed node of "T" is "p" itself, then "l" is the preferred child by definition.

A preferred path is defined by starting at the root and following the preferred children until reaching a leaf node. Removing the nodes on this path partitions the remainder of the tree into a number of subtrees, and we recurse on each subtree (forming a preferred path from its root, which partitions the subtree into more subtrees).

To represent a preferred path, we store its nodes in a balanced binary search tree, specifically a red-black tree. For each non-leaf node "n" in a preferred path "P", it has a non-preferred child "c", which is the root of a new auxiliary tree. We attach this other auxiliary tree's root ("c") to "n" in "P", thus linking the auxiliary trees together. We also augment the auxiliary tree by storing at each node the minimum and maximum depth (depth in the reference tree, that is) of nodes in the subtree under that node.

To search for an element in the tango tree, we simply simulate searching the reference tree. We start by searching the preferred path connected to the root, which is simulated by searching the auxiliary tree corresponding to that preferred path. If the auxiliary tree doesn't contain the desired element, the search terminates on the parent of the root of the subtree containing the desired element (the beginning of another preferred path), so we simply proceed by searching the auxiliary tree for that preferred path, and so forth.

In order to maintain the structure of the tango tree (auxiliary trees correspond to preferred paths), we must do some updating work whenever preferred children change as a result of searches. When a preferred child changes, the top part of a preferred path becomes detached from the bottom part (which becomes its own preferred path) and reattached to another preferred path (which becomes the new bottom part). In order to do this efficiently, we'll define "cut" and "join" operations on our auxiliary trees.

Our "join" operation will combine two auxiliary trees as long as they have the property that the top node of one (in the reference tree) is a child of the bottom node of the other (essentially, that the corresponding preferred paths can be concatenated). This will work based on the "concatenate" operation of red-black trees, which combines two trees as long as they have the property that all elements of one are less than all elements of the other, and "split", which does the reverse. In the reference tree, note that there exist two nodes in the top path such that a node is in the bottom path if and only if its key-value is between them. Now, to join the bottom path to the top path, we simply "split" the top path between those two nodes, then "concatenate" the two resulting auxiliary trees on either side of the bottom path's auxiliary tree, and we have our final, joined auxiliary tree.

Our "cut" operation will break a preferred path into two parts at a given node, a top part and a bottom part. More formally, it'll partition an auxiliary tree into two auxiliary trees, such that one contains all nodes at or above a certain depth in the reference tree, and the other contains all nodes below that depth. As in "join", note that the top part has two nodes that bracket the bottom part. Thus, we can simply "split" on each of these two nodes to divide the path into three parts, then "concatenate" the two outer ones so we end up with two parts, the top and bottom, as desired.

In order to bound the competitive ratio for tango trees, we must find a lower bound on the performance of the optimal offline tree that we use as a benchmark. Once we find an upper bound on the performance of the tango tree, we can divide them to bound the competitive ratio.

To find a lower bound on the work done by the optimal offline binary search tree, we again use the notion of preferred children. When considering an access sequence (a sequence of searches), we keep track of how many times a reference tree node's preferred child switches. The total number of switches (summed over all nodes) gives an asymptotic lower bound on the work done by any binary search tree algorithm on the given access sequence. This is called the "interleave lower bound".

In order to connect this to tango trees, we will find an upper bound on the work done by the tango tree for a given access sequence. Our upper bound will be formula_4, where "k" is the number of interleaves.

The total cost is divided into two parts, searching for the element, and updating the structure of the tango tree to maintain the proper invariants (switching preferred children and re-arranging preferred paths).

To see that the searching (not updating) fits in this bound, simply note that every time an auxiliary tree search is unsuccessful and we have to move to the next auxiliary tree, that results in a preferred child switch (since the parent preferred path now switches directions to join the child preferred path). Since all auxiliary tree searches are unsuccessful except the last one (we stop once a search is successful, naturally), we search formula_5 auxiliary trees. Each search takes formula_1, because an auxiliary tree's size is bounded by formula_7, the height of the reference tree.

The update cost fits within this bound as well, because we only have to perform one "cut" and one "join" for every visited auxiliary tree. A single "cut" or "join" operation takes only a constant number of searches, "splits", and "concatenates", each of which takes logarithmic time in the size of the auxiliary tree, so our update cost is formula_4.

Tango trees are formula_1-competitive, because the work done by the optimal offline binary search tree is at least linear in "k" (the total number of preferred child switches), and the work done by the tango tree is at most formula_4.



</doc>
<doc id="1189425" url="https://en.wikipedia.org/wiki?curid=1189425" title="Van Emde Boas tree">
Van Emde Boas tree

A van Emde Boas tree (), also known as a vEB tree or van Emde Boas priority queue, is a tree data structure which implements an associative array with -bit integer keys. It performs all operations in time, or equivalently in time, where is the maximum number of elements that can be stored in the tree. The is not to be confused with the actual number of elements stored in the tree, by which the performance of other tree data-structures is often measured. The vEB tree has good space efficiency when it contains many elements, as discussed below. It was invented by a team led by Dutch computer scientist Peter van Emde Boas in 1975.

A vEB supports the operations of an "ordered associative array", which includes the usual associative array operations along with two more "order" operations, "FindNext" and "FindPrevious":

A vEB tree also supports the operations "Minimum" and "Maximum", which return the minimum and maximum element stored in the tree respectively. These both run in time, since the minimum and maximum element are stored as attributes in each tree.

 For the sake of simplicity, let for some integer "k". Define . A vEB tree over the universe } has a root node that stores an array of length . is a pointer to a vEB tree that is responsible for the values }. Additionally, "T" stores two values and as well as an auxiliary vEB tree .

Data is stored in a vEB tree as follows: The smallest value currently in the tree is stored in and largest value is stored in . Note that is not stored anywhere else in the vEB tree, while is. If "T" is empty then we use the convention that and . Any other value "x" is stored in the subtree where . The auxiliary tree keeps track of which children are non-empty, so contains the value "j" if and only if is non-empty.

The operation that searches for the successor of an element "x" in a vEB tree proceeds as follows: If then the search is complete, and the answer is . If then the next element does not exist, return M. Otherwise, let . If then the value being searched for is contained in so the search proceeds recursively in . Otherwise, we search for the value "i" in . This gives us the index "j" of the first subtree that contains an element larger than "x". The algorithm then returns . The element found on the children level needs to be composed with the high bits to form a complete next element.

Note that, in any case, the algorithm performs work and then possibly recurses on a subtree over a universe of size (an bit universe). This gives a recurrence for the running time of , which resolves to .

The call that inserts a value into a vEB tree operates as follows:


In code:

The key to the efficiency of this procedure is that inserting an element into an empty vEB tree takes time. So, even though the algorithm sometimes makes two recursive calls, this only occurs when the first recursive call was into an empty subtree. This gives the same running time recurrence of as before.

Deletion from vEB trees is the trickiest of the operations. The call that deletes a value "x" from a vEB tree T operates as follows:


In code:

Again, the efficiency of this procedure hinges on the fact that deleting from a vEB tree that contains only one element takes only constant time. In particular, the last line of code only executes if "x" was the only element in prior to the deletion.

The assumption that is an integer is unnecessary. The operations and can be replaced by taking only higher-order and the lower-order bits of , respectively. On any existing machine, this is more efficient than division or remainder computations.

The implementation described above uses pointers and occupies a total space of . This can be seen as follows. The recurrence is formula_1.
Resolving that would lead to formula_2.
One can, fortunately, also show that by induction.

In practical implementations, especially on machines with "shift-by-k" and "find first zero" instructions, performance can further be improved by switching to a bit array once equal to the word size (or a small multiple thereof) is reached. Since all operations on a single word are constant time, this does not affect the asymptotic performance, but it does avoid the majority of the pointer storage and several pointer dereferences, achieving a significant practical savings in time and space with this trick.

An obvious optimization of vEB trees is to discard empty subtrees. This makes vEB trees quite compact when they contain many elements, because no subtrees are created until something needs to be added to them. Initially, each element added creates about new trees containing about pointers all together. As the tree grows, more and more subtrees are reused, especially the larger ones. In a full tree of elements, only space is used. Moreover, unlike a binary search tree, most of this space is being used to store data: even for billions of elements, the pointers in a full vEB tree number in the thousands.

However, for small trees the overhead associated with vEB trees is enormous: on the order of . This is one reason why they are not popular in practice. One way of addressing this limitation is to use only a fixed number of bits per level, which results in a trie. Alternatively, each table may be replaced by a hash table, reducing the space to (where is the number of elements stored in the data structure) at the expense of making the data structure randomized. Other structures, including y-fast tries and x-fast tries have been proposed that have comparable update and query times and also use randomized hash tables to reduce the space to or .




</doc>
<doc id="15843635" url="https://en.wikipedia.org/wiki?curid=15843635" title="Cartesian tree">
Cartesian tree

In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers; it can be uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence. Introduced by in the context of geometric range searching data structures, Cartesian trees have also been used in the definition of the treap and randomized binary search tree data structures for binary search problems. The Cartesian tree for a sequence may be constructed in linear time using a stack-based algorithm for finding all nearest smaller values in a sequence.

The Cartesian tree for a sequence of distinct numbers can be uniquely defined by the following properties:
Based on the heap property, the root of the tree must be the smallest number in the sequence. From this, the tree itself may also be defined recursively: the root is the minimum value of the sequence, and the left and right subtrees are the Cartesian trees for the subsequences to the left and right of the root value. Therefore, the three properties above uniquely define the Cartesian tree.

If a sequence of numbers contains repetitions, the Cartesian tree may be defined by determining a consistent tie-breaking rule (for instance, determining that the first of two equal elements is treated as the smaller of the two) before applying the above rules.

An example of a Cartesian tree is shown in the figure above.

Cartesian trees may be used as part of an efficient data structure for range minimum queries, a range searching problem involving queries that ask for the minimum value in a contiguous subsequence of the original sequence. In a Cartesian tree, this minimum value may be found at the lowest common ancestor of the leftmost and rightmost values in the subsequence. For instance, in the subsequence (12,10,20,15) of the sequence shown in the first illustration, the minimum value of the subsequence (10) forms the lowest common ancestor of the leftmost and rightmost values (12 and 15). Because lowest common ancestors may be found in constant time per query, using a data structure that takes linear space to store and that may be constructed in linear time, the same bounds hold for the range minimization problem.

The same range minimization problem may also be given an alternative interpretation in terms of two dimensional range searching. A collection of finitely many points in the Cartesian plane may be used to form a Cartesian tree, by sorting the points by their "x"-coordinates and using the "y"-coordinates in this order as the sequence of values from which this tree is formed. If "S" is the subset of the input points within some vertical slab defined by the inequalities "L" ≤ "x" ≤ "R", "p" is the leftmost point in "S" (the one with minimum "x"-coordinate), and "q" is the rightmost point in "S" (the one with maximum "x"-coordinate) then the lowest common ancestor of "p" and "q" in the Cartesian tree is the bottommost point in the slab. A three-sided range query, in which the task is to list all points within a region bounded by the three inequalities "L" ≤ "x" ≤ "R" and "y" ≤ "T", may be answered by finding this bottommost point "b", comparing its "y"-coordinate to "T", and (if the point lies within the three-sided region) continuing recursively in the two slabs bounded between "p" and "b" and between "b" and "q". In this way, after the leftmost and rightmost points in the slab are identified, all points within the three-sided region may be listed in constant time per point.

The same construction, of lowest common ancestors in a Cartesian tree, makes it possible to construct a data structure with linear space that allows the distances between pairs of points in any ultrametric space to be queried in constant time per query. The distance within an ultrametric is the same as the minimax path weight in the minimum spanning tree of the metric. From the minimum spanning tree, one can construct a Cartesian tree, the root node of which represents the heaviest edge of the minimum spanning tree. Removing this edge partitions the minimum spanning tree into two subtrees, and Cartesian trees recursively constructed for these two subtrees form the children of the root node of the Cartesian tree. The leaves of the Cartesian tree represent points of the metric space, and the lowest common ancestor of two leaves in the Cartesian tree is the heaviest edge between those two points in the minimum spanning tree, which has weight equal to the distance between the two points. Once the minimum spanning tree has been found and its edge weights sorted, the Cartesian tree may be constructed in linear time.

Because a Cartesian tree is a binary tree, it is natural to use it as a binary search tree for an ordered sequence of values. However, defining a Cartesian tree based on the same values that form the search keys of a binary search tree does not work well: the Cartesian tree of a sorted sequence is just a path, rooted at its leftmost endpoint, and binary searching in this tree degenerates to sequential search in the path. However, it is possible to generate more-balanced search trees by generating "priority" values for each search key that are different than the key itself, sorting the inputs by their key values, and using the corresponding sequence of priorities to generate a Cartesian tree. This construction may equivalently be viewed in the geometric framework described above, in which the "x"-coordinates of a set of points are the search keys and the "y"-coordinates are the priorities.

This idea was applied by , who suggested the use of random numbers as priorities. The data structure resulting from this random choice is called a treap, due to its combination of binary search tree and binary heap features. An insertion into a treap may be performed by inserting the new key as a leaf of an existing tree, choosing a priority for it, and then performing tree rotation operations along a path from the node to the root of the tree to repair any violations of the heap property caused by this insertion; a deletion may similarly be performed by a constant amount of change to the tree followed by a sequence of rotations along a single path in the tree.

If the priorities of each key are chosen randomly and independently once whenever the key is inserted into the tree, the resulting Cartesian tree will have the same properties as a random binary search tree, a tree computed by inserting the keys in a randomly chosen permutation starting from an empty tree, with each insertion leaving the previous tree structure unchanged and inserting the new node as a leaf of the tree. Random binary search trees had been studied for much longer, and are known to behave well as search trees (they have logarithmic depth with high probability); the same good behavior carries over to treaps. It is also possible, as suggested by Aragon and Seidel, to reprioritize frequently-accessed nodes, causing them to move towards the root of the treap and speeding up future accesses for the same keys.

A Cartesian tree may be constructed in linear time from its input sequence.
One method is to simply process the sequence values in left-to-right order, maintaining the Cartesian tree of the nodes processed so far, in a structure that allows both upwards and downwards traversal of the tree. To process each new value "x", start at the node representing the value prior to "x" in the sequence and follow the path from this node to the root of the tree until finding a value "y" smaller than "x". This node "y" is the parent of "x", and the previous right child of "y" becomes the new left child of "x". The total time for this procedure is linear, because the time spent searching for the parent "y" of each new node "x" can be charged against the number of nodes that are removed from the rightmost path in the tree.

An alternative linear-time construction algorithm is based on the all nearest smaller values problem. In the input sequence, one may define the "left neighbor" of a value "x" to be the value that occurs prior to "x", is smaller than "x", and is closer in position to "x" than any other smaller value. The "right neighbor" is defined symmetrically. The sequence of left neighbors may be found by an algorithm that maintains a stack containing a subsequence of the input. For each new sequence value "x", the stack is popped until it is empty or its top element is smaller than "x", and then "x" is pushed onto the stack. The left neighbor of "x" is the top element at the time "x" is pushed. The right neighbors may be found by applying the same stack algorithm to the reverse of the sequence. The parent of "x" in the Cartesian tree is either the left neighbor of "x" or the right neighbor of "x", whichever exists and has a larger value. The left and right neighbors may also be constructed efficiently by parallel algorithms, so this formulation may be used to develop efficient parallel algorithms for Cartesian tree construction.

Another linear-time algorithm for Cartesian tree construction is based on divide-and-conquer. In particular, the algorithm recursively constructs the tree on each half of the input, and then merging the two trees by taking the right spine of the left tree and left spine of the right tree and performing a standard merging operation. The algorithm is also parallelizable since on each level of recursion, each of the two sub-problems can be computed in parallel, and the merging operation can be efficiently parallelized as well.

 describe a sorting algorithm based on Cartesian trees. They describe the algorithm as based on a tree with the maximum at the root, but it may be modified straightforwardly to support a Cartesian tree with the convention that the minimum value is at the root. For consistency, it is this modified version of the algorithm that is described below.

The Levcopoulos–Petersson algorithm can be viewed as a version of selection sort or heap sort that maintains a priority queue of candidate minima, and that at each step finds and removes the minimum value in this queue, moving this value to the end of an output sequence. In their algorithm, the priority queue consists only of elements whose parent in the Cartesian tree has already been found and removed. Thus, the algorithm consists of the following steps:

As Levcopoulos and Petersson show, for input sequences that are already nearly sorted, the size of the priority queue will remain small, allowing this method to take advantage of the nearly-sorted input and run more quickly. Specifically, the worst-case running time of this algorithm is O("n" log "k"), where "k" is the average, over all values "x" in the sequence, of the number of consecutive pairs of sequence values that bracket "x". They also prove a lower bound stating that, for any "n" and "k" = ω(1), any comparison-based sorting algorithm must use Ω("n" log "k") comparisons for some inputs.

Cartesian trees were introduced and named by . The name is derived from the Cartesian coordinate system for the plane: in Vuillemin's version of this structure, as in the two-dimensional range searching application discussed above, a Cartesian tree for a point set has the sorted order of the points by their "x"-coordinates as its symmetric traversal order, and it has the heap property according to the "y"-coordinates of the points.



</doc>
<doc id="249855" url="https://en.wikipedia.org/wiki?curid=249855" title="Treap">
Treap

In computer science, the treap and the randomized binary search tree are two closely related forms of binary search tree data structures that maintain a dynamic set of ordered keys and allow binary searches among the keys. After any sequence of insertions and deletions of keys, the shape of the tree is a random variable with the same probability distribution as a random binary tree; in particular, with high probability its height is proportional to the logarithm of the number of keys, so that each search, insertion, or deletion operation takes logarithmic time to perform.

The treap was first described by Raimund Seidel and Cecilia R. Aragon in 1989; its name is a portmanteau of tree and heap.
It is a Cartesian tree in which each key is given a (randomly chosen) numeric priority. As with any binary search tree, the inorder traversal order of the nodes is the same as the sorted order of the keys. The structure of the tree is determined by the requirement that it be heap-ordered: that is, the priority number for any non-leaf node must be greater than or equal to the priority of its children. Thus, as with Cartesian trees more generally, the root node is the maximum-priority node, and its left and right subtrees are formed in the same manner from the subsequences of the sorted order to the left and right of that node.

An equivalent way of describing the treap is that it could be formed by inserting the nodes highest-priority-first into a binary search tree without doing any rebalancing. Therefore, if the priorities are independent random numbers (from a distribution over a large enough space of possible priorities to ensure that two nodes are very unlikely to have the same priority) then the shape of a treap has the same probability distribution as the shape of a random binary search tree, a search tree formed by inserting the nodes without rebalancing in a randomly chosen insertion order. Because random binary search trees are known to have logarithmic height with high probability, the same is true for treaps.

Aragon and Seidel also suggest assigning higher priorities to frequently accessed nodes, for instance by a process that, on each access, chooses a random number and replaces the priority of the node with that number if it is higher than the previous priority. This modification would cause the tree to lose its random shape; instead, frequently accessed nodes would be more likely to be near the root of the tree, causing searches for them to be faster.

Naor and Nissim describe an application in maintaining authorization certificates in public-key cryptosystems.

Treaps support the following basic operations:


In addition to the single-element insert, delete and lookup operations, several fast "bulk" operations have been defined on treaps: union, intersection and set difference. These rely on two helper operations, "split" and "join".


The join algorithm is as follows:

The split algorithm is as follows:

The union of two treaps and , representing sets and is a treap that represents . The following recursive algorithm computes the union:

Here, "split" is presumed to return two trees: one holding the keys less than its input key, one holding the greater keys. (The algorithm is non-destructive, but an in-place destructive version exists as well.)

The algorithm for intersection is similar, but requires the "join" helper routine. The complexity of each of union, intersection and difference is for treaps of sizes and , with . Moreover, since the recursive calls to union are independent of each other, they can be executed in parallel. 

Split and Union call Join but do not deal with the balancing criteria of treaps directly, such an implementation is usually called the "join-based" implementation.

Note that if hash values of keys are used as priorities and structurally equal nodes are merged already at construction, then each merged node will be a unique representation of a set of keys. Provided that there can only be one simultaneous root node representing a given set of keys, two sets can be tested for equality by pointer comparison, which is constant in time.

This technique can be used to enhance the merge algorithms to perform fast also when the difference between two sets is small. If input sets are equal, the union and intersection functions could break immediately returning one of the input sets as result, while the difference function should return the empty set.

Let be the size of the symmetric difference. The modified merge algorithms will then also be bounded by .

The randomized binary search tree, introduced by Martínez and Roura subsequently to the work of Aragon and Seidel on treaps, stores the same nodes with the same random distribution of tree shape, but maintains different information within the nodes of the tree in order to maintain its randomized structure.

Rather than storing random priorities on each node, the randomized binary search tree stores a small integer at each node, the number of its descendants (counting itself as one); these numbers may be maintained during tree rotation operations at only a constant additional amount of time per rotation. When a key "x" is to be inserted into a tree that already has "n" nodes, the insertion algorithm chooses with probability 1/("n" + 1) to place "x" as the new root of the tree, and otherwise it calls the insertion procedure recursively to insert "x" within the left or right subtree (depending on whether its key is less than or greater than the root). The numbers of descendants are used by the algorithm to calculate the necessary probabilities for the random choices at each step. Placing "x" at the root of a subtree may be performed either as in the treap by inserting it at a leaf and then rotating it upwards, or by an alternative algorithm described by Martínez and Roura that splits the subtree into two pieces to be used as the left and right children of the new node.

The deletion procedure for a randomized binary search tree uses the same information per node as the insertion procedure, but unlike the insertion procedure it only needs on average O(1) random decisions to join the two subtrees descending from the left and right children of the deleted node into a single tree. That is because the subtrees to be joined are on average at depth Θ(log n); joining two trees of size n and m needs Θ(log(n+m)) random choices on average. If the left or right subtree of the node to be deleted is empty, the join operation is trivial; otherwise, the left or right child of the deleted node is selected as the new subtree root with probability proportional to its number of descendants, and the join proceeds recursively.

The information stored per node in the randomized binary tree is simpler than in a treap (a small integer rather than a high-precision random number), but it makes a greater number of calls to the random number generator (O(log "n") calls per insertion or deletion rather than one call per insertion) and the insertion procedure is slightly more complicated due to the need to update the numbers of descendants per node. A minor technical difference is that, in a treap, there is a small probability of a collision (two keys getting the same priority), and in both cases there will be statistical differences between a true random number generator and the pseudo-random number generator typically used on digital computers. However, in any case the differences between the theoretical model of perfect random choices used to design the algorithm and the capabilities of actual random number generators are vanishingly small.

Although the treap and the randomized binary search tree both have the same random distribution of tree shapes after each update, the history of modifications to the trees performed by these two data structures over a sequence of insertion and deletion operations may be different. For instance, in a treap, if the three numbers 1, 2, and 3 are inserted in the order 1, 3, 2, and then the number 2 is deleted, the remaining two nodes will have the same parent-child relationship that they did prior to the insertion of the middle number. In a randomized binary search tree, the tree after the deletion is equally likely to be either of the two possible trees on its two nodes, independently of what the tree looked like prior to the insertion of the middle number.




</doc>
<doc id="4674" url="https://en.wikipedia.org/wiki?curid=4674" title="B-tree">
B-tree

In computer science, a B-tree is a self-balancing tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children. Unlike other self-balancing binary search trees, the B-tree is well suited for storage systems that read and write relatively large blocks of data, such as discs. It is commonly used in databases and file systems.

What, if anything, the "B" stands for has never been established.

B-trees were invented by Rudolf Bayer and Edward M.McCreight in 1970 for the purpose of efficiently managing index pages for large random access files. The basic assumption was that indexes would be so voluminous that only small chunks of the tree could fit in main memory. The first article on this invention was written in July and published in November 1970.

In B-trees, internal (non-leaf) nodes can have a variable number of child nodes within some pre-defined range. When data is inserted or removed from a node, its number of child nodes changes. In order to maintain the pre-defined range, internal nodes may be joined or split. Because a range of child nodes is permitted, B-trees do not need re-balancing as frequently as other self-balancing search trees, but may waste some space, since nodes are not entirely full. The lower and upper bounds on the number of child nodes are typically fixed for a particular implementation. For example, in a 2-3 B-tree (often simply referred to as a 2-3 tree), each internal node may have only 2 or 3 child nodes.

Each internal node of a B-tree contains a number of keys. The keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: "a" and "a". All values in the leftmost subtree will be less than "a", all values in the middle subtree will be between "a" and "a", and all values in the rightmost subtree will be greater than "a".

Usually, the number of keys is chosen to vary between formula_1 and formula_2, where formula_1 is the minimum number of keys, and formula_4 is the minimum degree or branching factor of the tree. In practice, the keys take up the most space in a node. The factor of 2 will guarantee that nodes can be split or combined. If an internal node has formula_2 keys, then adding a key to that node can be accomplished by splitting the hypothetical formula_6 key node into two formula_1 key nodes and moving the key that would have been in the middle to the parent node. Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have formula_1 keys, then a key may be deleted from the internal node by combining it with its neighbor. Deleting the key would make the internal node have formula_9 keys; joining the neighbor would add formula_1 keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of formula_2 keys.

The number of branches (or child nodes) from a node will be one more than the number of keys stored in the node. In a 2-3 B-tree, the internal nodes will store either one key (with two child nodes) or two keys (with three child nodes). A B-tree is sometimes described with the parameters formula_12 — formula_13 or simply with the highest branching order, formula_13.

A B-tree is kept balanced after insertion by splitting a would-be overfilled node, of formula_6 keys, into two formula_1-key siblings and inserting the mid-value key into the parent. Depth only increases when the root is split, maintaining balance. Similarly, a B-tree is kept balanced after deletion by merging or redistributing keys among siblings to maintain the formula_1-key minimum for non-root nodes. A merger reduces the number of keys in the parent potentially forcing it to merge or redistribute keys with its siblings, and so on. The only change in depth occurs when the root has two children, of formula_1 and (transitionally) formula_9 keys, in which case the two siblings and parent are merged, reducing the depth by one.

This depth will increase slowly as elements are added to the tree, but an increase in the overall depth is infrequent, and results in all leaf nodes being one more node farther away from the root.

B-trees have substantial advantages over alternative implementations when the time to access the data of a node greatly exceeds the time spent processing that data, because then the cost of accessing the node may be amortized over multiple operations within the node. This usually occurs when the node data are in secondary storage such as disk drives. By maximizing the number of keys within each internal node, the height of the tree decreases and the number of expensive node accesses is reduced. In addition, rebalancing of the tree occurs less often. The maximum number of child nodes depends on the information that must be stored for each child node and the size of a full disk block or an analogous size in secondary storage. While 2-3 B-trees are easier to explain, practical B-trees using secondary storage need a large number of child nodes to improve performance.

The term B-tree may refer to a specific design or it may refer to a general class of designs. In the narrow sense, a B-tree stores keys in its internal nodes but need not store those keys in the records at the leaves. The general class includes variations such as the B+ tree and the B tree.

Usually, sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using order notation. A binary search of a sorted table with records, for example, can be done in roughly comparisons. If the table had 1,000,000 records, then a specific record could be located with at most 20 comparisons: .

Large databases have historically been kept on disk drives. The time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available. The time to read a record from a disk drive involves a seek time and a rotational delay. The seek time may be 0 to 20 or more milliseconds, and the rotational delay averages about half the rotation period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds. For simplicity, assume reading from disk takes about 10 milliseconds.

Naively, then, the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read, which is 0.2 seconds.

The time won't be that bad because individual records are grouped together in a disk block. A disk block might be 16 kilobytes. If each record is 160 bytes, then 100 records could be stored in each block. The disk read time above was actually for an entire block. Once the disk head is in position, one or more disk blocks can be read with little delay. With 100 records per block, the last 6 or so comparisons don't need to do any disk reads—the comparisons are all within the last disk block read.

To speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.

A significant improvement can be made with an index. In the example above, initial disk reads narrowed the search range by a factor of two. That can be improved substantially by creating an auxiliary index that contains the first record in each disk block (sometimes called a sparse index). This auxiliary index would be 1% of the size of the original database, but it can be searched more quickly. Finding an entry in the auxiliary index would tell us which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main database—at a cost of one more disk read. The index would hold 10,000 entries, so it would take at most 14 comparisons. Like the main database, the last six or so comparisons in the auxiliary index would be on the same disk block. The index could be searched in about eight disk reads, and the desired record could be accessed in 9 disk reads.

The trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index. That would make an aux-aux index that would need only 100 entries and would fit in one disk block.

Instead of reading 14 disk blocks to find the desired record, we only need to read 3 blocks. This blocking is the core idea behind the creation of the B-tree, where the disk blocks fill-out a hierarchy of levels to make up the index. Reading and searching the first (and only) block of the aux-aux index which is the root of the tree identifies the relevant block in aux-index in the level below. Reading and searching that aux-index block identifies the relevant block to read, until the final level, known as the leaf level, identifies a record in the main database. Instead of 150 milliseconds, we need only 30 milliseconds to get the record.

The auxiliary indices have turned the search problem from a binary search requiring roughly disk reads to one requiring only disk reads where is the blocking factor (the number of entries per block: entries per block in our example; reads).

In practice, if the main database is being frequently searched, the aux-aux index and much of the aux index may reside in a disk cache, so they would not incur a disk read.

If the database does not change, then compiling the index is simple to do, and the index need never be changed. If there are changes, then managing the database and its index becomes more complicated.

Deleting records from a database is relatively easy. The index can stay the same, and the record can just be marked as deleted. The database remains in sorted order. If there are a large number of deletions, then searching and storage become less efficient.

Insertions can be very slow in a sorted sequential file because room for the inserted record must be made. Inserting a record before the first record requires shifting all of the records down one. Such an operation is just too expensive to be practical. One solution is to leave some spaces. Instead of densely packing all the records in a block, the block can have some free space to allow for subsequent insertions. Those spaces would be marked as if they were "deleted" records.

Both insertions and deletions are fast as long as space is available on a block. If an insertion won't fit on the block, then some free space on some nearby block must be found and the auxiliary indices adjusted. The hope is that enough space is available nearby, such that a lot of blocks do not need to be reorganized. Alternatively, some out-of-sequence disk blocks may be used.

The B-tree uses all of the ideas described above. In particular, a B-tree:

In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.

The literature on B-trees is not uniform in its terminology .

, , and others define the order of B-tree as the minimum number of keys in a non-root node. points out that terminology is ambiguous because the maximum number of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a maximum of 7 keys. avoids the problem by defining the order to be maximum number of children (which is one more than the maximum number of keys).

The term leaf is also inconsistent. considered the leaf level to be the lowest level of keys, but Knuth considered the leaf level to be one level below the lowest keys . There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the data record. Those choices are not fundamental to the idea of a B-tree.

For simplicity, most authors assume there are a fixed number of keys that fit in a node. The basic assumption is the key size is fixed and the node size is fixed. In practice, variable length keys may be employed .

According to Knuth's definition, a B-tree of order "m" is a tree which satisfies the following properties:


Each internal node’s keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: "a" and "a". All values in the leftmost subtree will be less than "a", all values in the middle subtree will be between "a" and "a", and all values in the rightmost subtree will be greater than "a".




A B-tree of depth "n"+1 can hold about "U" times as many items as a B-tree of depth "n", but the cost of search, insert, and delete operations grows with the depth of the tree. As with any balanced tree, the cost grows much more slowly than the number of elements.

Some balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf nodes and internal nodes. B-trees keep values in every node in the tree except leaf nodes.

Let be the height of the classic B-tree (see Tree (data structure) § Terminology for the tree height definition). Let be the number of entries in the tree. Let "m" be the maximum number of children a node can have. Each node can have at most keys.

It can be shown (by induction for example) that a B-tree of height "h" with all its nodes completely filled has entries. Hence, the best case height (i.e. the minimum height) of a B-tree is:

Let formula_1 be the minimum number of children an internal (non-root) node can have. For an ordinary B-tree, formula_22

Searching is similar to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search reduces its field of view to the child pointer (subtree) whose range includes the search value. A subtree's range is defined by the values, or keys, contained in its parent node. These limiting values are also known as separation values.

Binary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.

All insertions start at a leaf node. To insert a new element, search the tree to find the leaf node where the new element should be added. Insert the new element into that node with the following steps:


If the splitting goes all the way up to the root, it creates a new root with a single separator value and two children, which is why the lower bound on the size of internal nodes does not apply to the root. The maximum number of elements per node is "U"−1. When a node is split, one element moves to the parent, but one element is added. So, it must be possible to divide the maximum number "U"−1 of elements into two legal nodes. If this number is odd, then "U"=2"L" and one of the new nodes contains ("U"−2)/2 = "L"−1 elements, and hence is a legal node, and the other contains one more element, and hence it is legal too. If "U"−1 is even, then "U"=2"L"−1, so there are 2"L"−2 elements in the node. Half of this number is "L"−1, which is the minimum number of elements allowed per node.

An alternative algorithm supports a single pass down the tree from the root to the node where the insertion will take place, splitting any full nodes encountered on the way preemptively. This prevents the need to recall the parent nodes into memory, which may be expensive if the nodes are on secondary storage. However, to use this algorithm, we must be able to send one element to the parent and split the remaining "U"−2 elements into two legal nodes, without adding a new element. This requires "U" = 2"L" rather than "U" = 2"L"−1, which accounts for why some textbooks impose this requirement in defining B-trees.

There are two popular strategies for deletion from a B-tree.


The algorithm below uses the former strategy.

There are two special cases to consider when deleting an element:


The procedures for these cases are in order below.


Each element in an internal node acts as a separation value for two subtrees, therefore we need to find a replacement for separation. Note that the largest element in the left subtree is still less than the separator. Likewise, the smallest element in the right subtree is still greater than the separator. Both of those elements are in leaf nodes, and either one can be the new separator for the two subtrees. Algorithmically described below:


Rebalancing starts from a leaf and proceeds toward the root until the tree is balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes. That redistribution operation is called a rotation. If no sibling can spare an element, then the deficient node must be merged with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows:


While freshly loaded databases tend to have good sequential behavior, this behavior becomes increasingly difficult to maintain as a database grows, resulting in more random I/O and performance challenges.

A common special case is adding a large amount of "pre-sorted" data into an initially empty B-tree. While it is quite possible to simply perform a series of successive inserts, inserting sorted data results in a tree composed almost entirely of half-full nodes. Instead, a special "bulk loading" algorithm can be used to produce a more efficient tree with a higher branching factor.

When the input is sorted, all insertions are at the rightmost edge of the tree, and in particular any time a node is split, we are guaranteed that the no more insertions will take place in the left half. When bulk loading, we take advantage of this, and instead of splitting overfull nodes evenly, split them as "unevenly" as possible: leave the left node completely full and create a right node with zero keys and one child (in violation of the usual B-tree rules).

At the end of bulk loading, the tree is composed almost entirely of completely full nodes; only the rightmost node on each level may be less than full. Because those nodes may also be less than "half" full, to re-establish the normal B-tree rules, combine such nodes with their (guaranteed full) left siblings and divide the keys to produce two nodes at least half full. The only node which lacks a full left sibling is the root, which is permitted to be less than half full.

In addition to its use in databases, the B-tree (or ) is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block formula_24 address into a disk block (or perhaps to a cylinder-head-sector) address.

Some operating systems require the user to allocate the maximum size of the file when the file is created. The file can then be allocated as contiguous disk blocks. In that case, to convert the file block address formula_24 into a disk block address, the operating system simply adds the file block address formula_24 to the address of the first disk block constituting the file. The scheme is simple, but the file cannot exceed its created size.

Other operating systems allow a file to grow. The resulting disk blocks may not be contiguous, so mapping logical blocks to physical blocks is more involved.

MS-DOS, for example, used a simple File Allocation Table (FAT). The FAT has an entry for each disk block, and that entry identifies whether its block is used by a file and if so, which block (if any) is the next disk block of the same file. So, the allocation of each file is represented as a linked list in the table. In order to find the disk address of file block formula_24, the operating system (or disk utility) must sequentially follow the file's linked list in the FAT. Worse, to find a free disk block, it must sequentially scan the FAT. For MS-DOS, that was not a huge penalty because the disks and files were small and the FAT had few entries and relatively short file chains. In the FAT12 filesystem (used on floppy disks and early hard disks), there were no more than 4,080 entries, and the FAT would usually be resident in memory. As disks got bigger, the FAT architecture began to confront penalties. On a large disk using FAT, it may be necessary to perform disk reads to learn the disk location of a file block to be read or written.

TOPS-20 (and possibly TENEX) used a 0 to 2 level tree that has similarities to a B-tree. A disk block was 512 36-bit words. If the file fit in a 512 (2) word block, then the file directory would point to that physical disk block. If the file fit in 2 words, then the directory would point to an aux index; the 512 words of that index would either be NULL (the block isn't allocated) or point to the physical address of the block. If the file fit in 2 words, then the directory would point to a block holding an aux-aux index; each entry would either be NULL or point to an aux index. Consequently, the physical disk block for a 2 word file could be located in two disk reads and read on the third.

Apple's filesystem HFS+, Microsoft's NTFS, AIX (jfs2) and some Linux filesystems, such as btrfs and Ext4, use B-trees.

B-trees are used in the HFS and Reiser4 file systems.

DragonFly BSD's HAMMER file system uses a modified B+-tree.

Lehman and Yao showed that all the read locks could be avoided (and thus concurrent access greatly improved) by linking the tree blocks at each level together with a "next" pointer. This results in a tree structure where both insertion and search operations descend from the root to the leaf. Write locks are only required as a tree block is modified. This maximizes access concurrency by multiple users, an important consideration for databases and/or other B-tree-based ISAM storage methods. The cost associated with this improvement is that empty pages cannot be removed from the btree during normal operations. (However, see for various strategies to implement node merging, and source code at.)

United States Patent 5283894, granted in 1994, appears to show a way to use a 'Meta Access Method' to allow concurrent B+ tree access and modification without locks. The technique accesses the tree 'upwards' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache. No reorganization for deletes is needed and there are no 'next' pointers in each block as in Lehman and Yao.

Rudolf Bayer and Ed McCreight invented the B-tree while working at Boeing Research Labs in 1971 , but did not explain what, if anything, the "B" stands for: "Boeing", "balanced", "broad", "bushy", and "Bayer" have been suggested. McCreight has said that "the more you think about what the B in B-trees means, the better you understand B-trees."







</doc>
<doc id="1326443" url="https://en.wikipedia.org/wiki?curid=1326443" title="B+ tree">
B+ tree

A B+ tree is an N-ary tree with a variable but often large number of children per node. A B+ tree consists of a root, internal nodes and leaves. The root may be either a leaf or a node with two or more children.

A B+ tree can be viewed as a B-tree in which each node contains only keys (not key–value pairs), and to which an additional level is added at the bottom with linked leaves.

The primary value of a B+ tree is in storing data for efficient retrieval in a block-oriented storage context — in particular, filesystems. This is primarily because unlike binary search trees, B+ trees have very high fanout (number of pointers to child nodes in a node, typically on the order of 100 or more), which reduces the number of I/O operations required to find an element in the tree.
The ReiserFS, NSS, XFS, JFS, ReFS, and BFS filesystems all use this type of tree for metadata indexing; BFS also uses B+ trees for storing directories. NTFS uses B+ trees for directory and security-related metadata indexing. EXT4 uses extent trees (a modified B+ tree data structure) for file extent indexing. Relational database management systems such as IBM DB2, Informix, Microsoft SQL Server, Oracle 8, Sybase ASE, and SQLite support this type of tree for table indices. Key–value database management systems such as CouchDB and Tokyo Cabinet support this type of tree for data access.

The order, or branching factor, of a B+ tree measures the capacity of nodes (i.e., the number of children nodes) for internal nodes in the tree. The actual number of children for a node, referred to here as , is constrained for internal nodes so that formula_1. The root is an exception: it is allowed to have as few as two children. For example, if the order of a B+ tree is 7, each internal node (except for the root) may have between 4 and 7 children; the root may have between 2 and 7. Leaf nodes have no children, but are constrained so that the number of keys must be at least formula_2 and at most formula_3. In the situation where a B+ tree is nearly empty, it only contains one node, which is a leaf node. (The root is also the single leaf, in this case.) This node is permitted to have as little as one key if necessary and at most formula_4.

The root of a B+ Tree represents the whole range of values in the tree, where every internal node is a subinterval.

We are looking for a value codice_1 in the B+ Tree. Starting from the root, we are looking for the leaf which may contain the value codice_1 . At each node, we figure out which internal pointer we should follow. An internal B+ Tree node has at most formula_5 ≤ formula_3 children, where every one of them represents a different sub-interval. We select the corresponding node by searching on the key values of the node.

This pseudocode assumes that no duplicates are allowed.


B-trees grow at the root and not at the leaves.

Given a collection of data records, we want to create a B+ tree index on some key field. One approach is to insert each record into an empty tree. However, it is quite expensive, because each entry requires us to start from the root and go down to the appropriate leaf page. An efficient alternative is to use bulk-loading.

Note :

For a -order B+ tree with levels of index: 

The leaves (the bottom-most index blocks) of the B+ tree are often linked to one another in a linked list; this makes range queries or an (ordered) iteration through the blocks simpler and more efficient (though the aforementioned upper bound can be achieved even without this addition). This does not substantially increase space consumption or maintenance on the tree. This illustrates one of the significant advantages of a B+tree over a B-tree; in a B-tree, since not all keys are present in the leaves, such an ordered linked list cannot be constructed. A B+tree is thus particularly useful as a database system index, where the data typically resides on disk, as it allows the B+tree to actually provide an efficient structure for housing the data itself (this is described in as index structure "Alternative 1").

If a storage system has a block size of B bytes, and the keys to be stored have a size of k, arguably the most efficient B+ tree is one where formula_17. Although theoretically the one-off is unnecessary, in practice there is often a little extra space taken up by the index blocks (for example, the linked list references in the leaf blocks). Having an index block which is slightly larger than the storage system's actual block represents a significant performance decrease; therefore erring on the side of caution is preferable.

If nodes of the B+ tree are organized as arrays of elements, then it may take a considerable time to insert or delete an element as half of the array will need to be shifted on average. To overcome this problem, elements inside a node can be organized in a binary tree or a B+ tree instead of an array.

B+ trees can also be used for data stored in RAM. In this case a reasonable choice for block size would be the size of processor's cache line.

Space efficiency of B+ trees can be improved by using some compression techniques. One possibility is to use delta encoding to compress keys stored into each block. For internal blocks, space saving can be achieved by either compressing keys or pointers. For string keys, space can be saved by using the following technique: Normally the "i"-th entry of an internal block contains the first key of block . Instead of storing the full key, we could store the shortest prefix of the first key of block that is strictly greater (in lexicographic order) than last key of block "i". There is also a simple way to compress pointers: if we suppose that some consecutive blocks are stored contiguously, then it will suffice to store only a pointer to the first block and the count of consecutive blocks.

All the above compression techniques have some drawbacks. First, a full block must be decompressed to extract a single element. One technique to overcome this problem is to divide each block into sub-blocks and compress them separately. In this case searching or inserting an element will only need to decompress or compress a sub-block instead of a full block. Another drawback of compression techniques is that the number of stored elements may vary considerably from a block to another depending on how well the elements are compressed inside each block.

The B tree was first described in the paper "Organization and Maintenance of Large Ordered Indices. Acta Informatica 1": 173–189 (1972) by Rudolf Bayer and Edward M. McCreight. There is no single paper introducing the B+ tree concept. Instead, the notion of maintaining all data in leaf nodes is repeatedly brought up as an interesting variant. An early survey of B trees also covering B+ trees is Douglas Comer. Comer notes that the B+ tree was used in IBM's VSAM data access software and he refers to an IBM published article from 1973.





</doc>
<doc id="1605712" url="https://en.wikipedia.org/wiki?curid=1605712" title="Dancing tree">
Dancing tree

In computer science, a dancing tree is a tree data structure similar to B+ trees. It was invented by Hans Reiser, for use by the Reiser4 file system. As opposed to self-balancing binary search trees that attempt to keep their nodes balanced at all times, dancing trees only balance their nodes when flushing data to a disk (either because of memory constraints or because a transaction has completed).

The idea behind this is to speed up file system operations by delaying optimization of the tree and only writing to disk when necessary, as writing to disk is thousands of times slower than writing to memory. Also, because this optimization is done less often than with other tree data structures, the optimization can be more extensive.

In some sense, this can be considered to be a self-balancing binary search tree that is optimized for storage on a slow medium, in that the on-disc form will always be balanced but will get no mid-transaction writes; doing so eases the difficulty (at the time) of adding and removing nodes, and instead performs these (slow) rebalancing operations at the same time as the (much slower) write to the storage medium.

However, a (negative) side effect of this behavior is witnessed in cases of unexpected shutdown, incomplete data writes, and other occurrences that may prevent the final (balanced) transaction from completing. In general, dancing trees will pose a greater difficulty for data recovery from incomplete transactions than a normal tree; though this can be addressed by either adding extra transaction logs or developing an algorithm to locate data on disk not previously present, then going through with the optimizations once more before continuing with any other pending operations/transactions.



</doc>
<doc id="1051942" url="https://en.wikipedia.org/wiki?curid=1051942" title="Fusion tree">
Fusion tree

In computer science, a fusion tree is a type of tree data structure that implements an associative array on -bit integers. When operating on a collection of key–value pairs, it uses space and performs searches in time, which is asymptotically faster than a traditional self-balancing binary search tree, and also better than the van Emde Boas tree for large values of . It achieves this speed by exploiting certain constant-time operations that can be done on a machine word. Fusion trees were invented in 1990 by Michael Fredman and Dan Willard.

Several advances have been made since Fredman and Willard's original 1990 paper. In 1999 it was shown how to implement fusion trees under a model of computation in which all of the underlying operations of the algorithm belong to AC, a model of circuit complexity that allows addition and bitwise Boolean operations but disallows the multiplication operations used in the original fusion tree algorithm. A dynamic version of fusion trees using hash tables was proposed in 1996 which matched the original structure's runtime in expectation. Another dynamic version using exponential tree was proposed in 2007 which yields worst-case runtimes of per operation. It remains open whether dynamic fusion trees can achieve per operation with high probability.

A fusion tree is essentially a B-tree with branching factor of (any small exponent is also possible), which gives it a height of . To achieve the desired runtimes for updates and queries, the fusion tree must be able to search a node containing up to keys in constant time. This is done by compressing ("sketching") the keys so that all can fit into one machine word, which in turn allows comparisons to be done in parallel.

Sketching is the method by which each -bit key at a node containing keys is compressed into only bits. Each key may be thought of as a path in the full binary tree of height starting at the root and ending at the leaf corresponding to . To distinguish two paths, it suffices to look at their branching point (the first bit where the two keys differ). All paths together have branching points, so at most bits are needed to distinguish any two of the keys.

An important property of the sketch function is that it preserves the order of the keys. That is, for any two keys .

If the locations of the sketch bits are "b" < "b" < ··· < "b", then the sketch of the key "x"···"x""x" is the "r"-bit integer formula_1.

With only standard word operations, such as those of the C programming language, it is difficult to directly compute the sketch of a key in constant time. Instead, the sketch bits can be packed into a range of size at most "r", using bitwise AND and multiplication. The bitwise AND operation serves to clear all non-sketch bits from the key, while the multiplication shifts the sketch bits into a small range. Like the "perfect" sketch, the approximate sketch preserves the order of the keys.

Some preprocessing is needed to determine the correct multiplication constant. Each sketch bit in location "b" will get shifted to "b" + "m" via a multiplication by "m" = formula_2 2. For the approximate sketch to work, the following three properties must hold:


An inductive argument shows how the "m" can be constructed. Let "m" = "w" − "b". Suppose that 1 < "t" ≤ "r" and that "m", "m"... "m" have already been chosen. Then pick the smallest integer "m" such that both properties (1) and (2) are satisfied. Property (1) requires that "m" ≠ "b" − "b" + "m" for all 1 ≤ "i", "j" ≤ "r" and 1 ≤ "l" ≤ "t"-1. Thus, there are less than "tr" ≤ "r" values that "m" must avoid. Since "m" is chosen to be minimal, ("b" + "m") ≤ ("b" + "m") + "r". This implies Property (3).

The approximate sketch is thus computed as follows:

The purpose of the compression achieved by sketching is to allow all of the keys to be stored in one "w"-bit word. Let the "node sketch" of a node be the bit string

We can assume that the sketch function uses exactly "b" ≤ "r" bits. Then each block uses 1 + "b" ≤ "w" bits, and since "k" ≤ "w", the total number of bits in the node sketch is at most "w".

A brief notational aside: for a bit string "s" and nonnegative integer "m", let "s" denote the concatenation of "s" to itself "m" times. If "t" is also a bit string "st" denotes the concatenation of "t" to "s".

The node sketch makes it possible to search the keys for any "b"-bit integer "y". Let "z" = (0"y"), which can be computed in constant time (multiply "y" by the constant (01)). Note that 1codice_1("x") - 0"y" is always positive, but preserves its leading 1 iff codice_1("x") ≥ "y". We can thus compute the smallest index "i" such that codice_1("x") ≥ "y" as follows:


For an arbitrary query "q", parallel comparison computes the index "i" such that
Unfortunately, the sketch function is not in general order-preserving outside the set of keys, so it is not necessarily the case that "x" ≤ "q" ≤ "x". What is true is that, among all of the keys, either "x" or "x" has the longest common prefix with "q". This is because any key "y" with a longer common prefix with "q" would also have more sketch bits in common with "q", and thus codice_1("y") would be closer to codice_1("q") than any codice_1("x").

The length longest common prefix between two "w"-bit integers "a" and "b" can be computed in constant time by finding the most significant bit of the bitwise XOR between "a" and "b". This can then be used to mask out all but the longest common prefix.

Note that "p" identifies exactly where "q" branches off from the set of keys. If the next bit of "q" is 0, then the successor of "q" is contained in the "p"1 subtree, and if the next bit of "q" is 1, then the predecessor of "q" is contained in the "p"0 subtree. This suggests the following algorithm:


An application of fusion trees to hash tables was given by Willard, who describes a data structure for hashing in which an outer-level hash table with hash chaining is combined with a fusion tree representing each hash chain.
In hash chaining, in a hash table with a constant load factor, the average size of a chain is constant, but additionally with high probability all chains have size , where is the number of hashed items.
This chain size is small enough that a fusion tree can handle searches and updates within it in constant time per operation. Therefore, the time for all operations in the data structure is constant with high probability.
More precisely, with this data structure, for every inverse-quasipolynomial probability , there is a constant such that the probability that there exists an operation that exceeds time is at most .



</doc>
<doc id="19926373" url="https://en.wikipedia.org/wiki?curid=19926373" title="Bx-tree">
Bx-tree

In computer science, the B tree is basically a query that is used to update efficient B+ tree-based index structures for moving objects.

The base structure of the B-tree is a B+ tree in which the internal nodes serve as a directory, each containing a pointer to its right sibling. In the earlier version of the B-tree, the leaf nodes contained the moving-object locations being indexed and corresponding index time. In the optimized version, each leaf node entry contains the id, velocity, single-dimensional mapping value and the latest update time of the object. The fanout is increased by not storing the locations of moving objects, as these can be derived from the mapping values.

As for many other moving objects indexes, a two-dimensional moving object is modeled as a linear function as O = ((x, y), (vx, vy), t ), where (x, y) and (vx, vy) are location and velocity of the object at a given time instance "t", i.e., the time of last update. The B+ tree is a structure for indexing single-dimensional data. In order to adopt the B+ tree as a moving object index, the B-tree uses a linearization technique which helps to integrate objects' location at time "t" into single dimensional value. Specifically, objects are first partitioned according to their update time. For objects within the same partition, the B-tree stores their locations at a given time which are estimated by linear interpolation. By doing so, the B-tree keeps a consistent view of all objects within the same partition without storing the update time of an objects.

Secondly, the space is partitioned by a grid and the location of an object is linearized within the partitions according to a space-filling curve, e.g., the Peano or Hilbert curves.

Finally, with the combination of the partition number (time information) and the linear order (location information), an object is indexed in B-tree with a one-dimensional index key Bvalue:

Here index-partition is an index partition determined by the update time and xrep is the space-filling curve value of the object position at the indexed time, formula_2 denotes the binary value of x, and “+” means concatenation.

Given an object O ((7, 2), (-0.1,0.05), 10), tmu = 120, the Bvalue for O can be computed as follows.


Given a new object, its index key is computed and then the object is inserted into the B-tree as in the B+ tree. An update consists of a deletion followed by an insertion. An auxiliary structure is employed to keep the latest key of each index so that an object can be deleted by searching for the key. The indexing key is computed before affecting the tree. In this way, the B-tree directly inherits the good properties of the B+ tree, and achieves efficient update performance.

A range query retrieves all objects whose location falls within the rectangular range formula_3 at time formula_4 not prior to the current time.

The B-tree uses query-window enlargement technique to answer queries. Since the B-tree stores an object's location as of sometime after its update time, the enlargement involves two cases: a location must either be brought back to an earlier time or forward to a later time. The main idea is to enlarge the query window so that it encloses all objects whose positions are not within query window at its label timestamp but will enter the query window at the query timestamp.

After the enlargement, the partitions of the B-tree need to be traversed to find objects falling in the enlarged query window. In each partition, the use of a space-filling curve means that a range query in the native, two-dimensional space becomes a set of range queries in the transformed, one-dimensional space.

To avoid excessively large query region after expansion in skewed datasets, an optimization of the query algorithm exists, which improves the query efficiency by avoiding unnecessary query enlargement.

K nearest neighbor query is computed by iteratively performing range queries with an incrementally enlarged search region until k answers are obtained. Another possibility is to employ similar querying ideas in The iDistance Technique.

The range query and K Nearest Neighbor query algorithms can be easily extended to support interval queries, continuous queries, etc.

Since the B-tree is an index built on top of a B+ tree index, all operations in the B-tree, including the insertion, deletion and search, are the same as those in the B+ tree. There is no need to change the implementations of these operations. The only difference is to implement the procedure of deriving the indexing key as a stored procedure in an existing DBMS. Therefore, the B-tree can be easily integrated into existing DBMS without touching the kernel.

SpADE is moving object management system built on top of a popular relational database system MySQL, which uses the B-tree for indexing the objects. In the implementation, moving object data is transformed and stored directly on MySQL, and queries are transformed into standard SQL statements which are efficiently processed in the relational engine. Most importantly, all these are achieved neatly and independently without infiltrating into the MySQL core.

The B tree uses a grid for space partitioning while mapping two-dimensional location into one-dimensional key. This may introduce performance degradation to both query and update operations while dealing with skewed data. If grid cell is oversize, many objects are contained in a cell. Since objects in a cell are indistinguishable to the index, there will be some overflow nodes in the underlying B+ tree. The existing of overflow pages not only destroys the balancing of the tree but also increases the update cost. As for the queries, for the given query region, large cell incurs more false positives and increases the processing time. On the other hand, if the space is partitioned with finer grid, i.e. smaller cells, each cell contains few objects. There is hardly overflow pages so that the update cost is minimized. Fewer false positives are retrieved in a query. However, more cells are needed to be searched. The increase in the number of cells searched also increases the workload of a query.

The STB-tree introduces a self-tuning framework for tuning the performance of the B-tree while dealing with data skew in space and data change with time. In order to deal with data skew in space, the STB-tree splits the entire space into regions of different object density using a set of reference points. Each region uses an individual grid whose cell size is determined by the object density inside of it.

The B-tree have multiple partitions regarding different time intervals. As time elapsed, each partition grows and shrinks alternately. The STB-tree utilizes this feature to tune the index online in order to adjust the space partitioning to make itself accommodate to the data changes with time. In particular, as a partition shrinks to empty and starts growing, it chooses a new set of reference points and new grid for each reference point according to the latest data density. The tuning is based on the latest statistics collected during a given period of time, so that the way of space partitioning is supposed to fit the latest data distribution best. By this means, the STB-tree is expected to minimize the effect caused by data skew in space and data changes with time.



</doc>
<doc id="4095441" url="https://en.wikipedia.org/wiki?curid=4095441" title="Heap">
Heap

Heap or HEAP may refer to:





</doc>
<doc id="69890" url="https://en.wikipedia.org/wiki?curid=69890" title="Binary heap">
Binary heap

A binary heap is a heap data structure that takes the form of a binary tree. Binary heaps are a common way of implementing priority queues. The binary heap was introduced by J. W. J. Williams in 1964, as a data structure for heapsort.

A binary heap is defined as a binary tree with two additional constraints:


Heaps where the parent key is greater than or equal to (≥) the child keys are called "max-heaps"; those where it is less than or equal to (≤) are called "min-heaps". Efficient (logarithmic time) algorithms are known for the two operations needed to implement a priority queue on a binary heap: inserting an element, and removing the smallest or largest element from a min-heap or max-heap, respectively. Binary heaps are also commonly employed in the heapsort sorting algorithm, which is an in-place algorithm because binary heaps can be implemented as an implicit data structure, storing keys in an array and using their relative positions within that array to represent child-parent relationships.

Both the insert and remove operations modify the heap to conform to the shape property first, by adding or removing from the end of the heap. Then the heap property is restored by traversing up or down the heap. Both operations take O(log "n") time.

To add an element to a heap we must perform an "up-heap" operation (also known as "bubble-up", "percolate-up", "sift-up", "trickle-up", "swim-up", "heapify-up", or "cascade-up"), by following this algorithm:


The number of operations required depends only on the number of levels the new element must rise to satisfy the heap property, thus the insertion operation has a worst-case time complexity of O(log "n") but an average-case complexity of O(1).

As an example of binary heap insertion, say we have a max-heap

and we want to add the number 15 to the heap. We first place the 15 in the position marked by the X. However, the heap property is violated since , so we need to swap the 15 and the 8. So, we have the heap looking as follows after the first swap:

However the heap property is still violated since , so we need to swap again:

which is a valid max-heap. There is no need to check the left child after this final step: at the start, the max-heap was valid, meaning ; if , and , then , because of the transitive relation.

The procedure for deleting the root from the heap (effectively extracting the maximum element in a max-heap or the minimum element in a min-heap) and restoring the properties is called "down-heap" (also known as "bubble-down", "percolate-down", "sift-down", "sink-down", "trickle down", "heapify-down", "cascade-down", and "extract-min/max").


So, if we have the same max-heap as before

We remove the 11 and replace it with the 4.

Now the heap property is violated since 8 is greater than 4. In this case, swapping the two elements, 4 and 8, is enough to restore the heap property and we need not swap elements further:

The downward-moving node is swapped with the "larger" of its children in a max-heap (in a min-heap it would be swapped with its smaller child), until it satisfies the heap property in its new position. This functionality is achieved by the Max-Heapify function as defined below in pseudocode for an array-backed heap "A" of length "heap_length"["A"]. Note that "A" is indexed starting at 1.

For the above algorithm to correctly re-heapify the array, the node at index "i" and its two direct children must violate the heap property. If they do not, the algorithm will fall through with no change to the array. The down-heap operation (without the preceding swap) can also be used to modify the value of the root, even when an element is not being deleted. In the pseudocode above, what starts with "//" is a comment. Note that "A" is an array (or list) that starts being indexed from "1" up to "length(A)", according to the pseudocode.

In the worst case, the new root has to be swapped with its child on each level until it reaches the bottom level of the heap, meaning that the delete operation has a time complexity relative to the height of the tree, or O(log "n").

Building a heap from an array of input elements can be done by starting with an empty heap, then successively inserting each element. This approach, called Williams’ method after the inventor of binary heaps, is easily seen to run in time: it performs insertions at cost each.

However, Williams’ method is suboptimal. A faster method (due to Floyd) starts by arbitrarily putting the elements on a binary tree, respecting the shape property (the tree could be represented by an array, see below). Then starting from the lowest level and moving upwards, sift the root of each subtree downward as in the deletion algorithm until the heap property is restored. More specifically if all the subtrees starting at some height formula_1 have already been “heapified” (the bottommost level corresponding to formula_2), the trees at height formula_3 can be heapified by sending their root down along the path of maximum valued children when building a max-heap, or minimum valued children when building a min-heap. This process takes formula_4 operations (swaps) per node. In this method most of the heapification takes place in the lower levels. Since the height of the heap is formula_5, the number of nodes at height formula_1 is formula_7. Therefore, the cost of heapifying all subtrees is:

This uses the fact that the given infinite series formula_9 converges.

The exact value of the above (the worst-case number of comparisons during the heap construction) is known to be equal to:

where is the sum of all digits of the binary representation of and is the exponent of in the prime factorization of .

The average case is more complex to analyze, but it can be shown to asymptotically approach comparisons.

The Build-Max-Heap function that follows, converts an array "A" which stores a complete
binary tree with "n" nodes to a max-heap by repeatedly using Max-Heapify in a bottom up manner.
It is based on the observation that the array elements indexed by
, , ..., "n"
are all leaves for the tree (assuming that indices start at 1), thus each is a one-element heap. Build-Max-Heap runs
Max-Heapify on each of the remaining tree nodes.

Heaps are commonly implemented with an array. Any binary tree can be stored in an array, but because a binary heap is always a complete binary tree, it can be stored compactly. No space is required for pointers; instead, the parent and children of each node can be found by arithmetic on array indices. These properties make this heap implementation a simple example of an implicit data structure or Ahnentafel list. Details depend on the root position, which in turn may depend on constraints of a programming language used for implementation, or programmer preference. Specifically, sometimes the root is placed at index 1, in order to simplify arithmetic.

Let "n" be the number of elements in the heap and "i" be an arbitrary valid index of the array storing the heap. If the tree root is at index 0, with valid indices 0 through "n − "1, then each element "a" at index "i" has
Alternatively, if the tree root is at index 1, with valid indices 1 through "n", then each element "a" at index "i" has

This implementation is used in the heapsort algorithm, where it allows the space in the input array to be reused to store the heap (i.e. the algorithm is done in-place). The implementation is also useful for use as a Priority queue where use of a dynamic array allows insertion of an unbounded number of items.

The upheap/downheap operations can then be stated in terms of an array as follows: suppose that the heap property holds for the indices "b", "b"+1, ..., "e". The sift-down function extends the heap property to "b"−1, "b", "b"+1, ..., "e".
Only index "i" = "b"−1 can violate the heap property.
Let "j" be the index of the largest child of "a"["i"] (for a max-heap, or the smallest child for a min-heap) within the range "b", ..., "e".
By swapping the values "a"["i"] and "a"["j"] the heap property for position "i" is established.
At this point, the only problem is that the heap property might not hold for index "j".
The sift-down function is applied tail-recursively to index "j" until the heap property is established for all elements.

The sift-down function is fast. In each step it only needs two comparisons and one swap. The index value where it is working doubles in each iteration, so that at most log "e" steps are required.

For big heaps and using virtual memory, storing elements in an array according to the above scheme is inefficient: (almost) every level is in a different page. B-heaps are binary heaps that keep subtrees in a single page, reducing the number of pages accessed by up to a factor of ten.

The operation of merging two binary heaps takes Θ("n") for equal-sized heaps. The best you can do is (in case of array implementation) simply concatenating the two heap arrays and build a heap of the result. A heap on "n" elements can be merged with a heap on "k" elements using O(log "n" log "k") key comparisons, or, in case of a pointer-based implementation, in O(log "n" log "k") time. An algorithm for splitting a heap on "n" elements into two heaps on "k" and "n-k" elements, respectively, based on a new view
of heaps as an ordered collections of subheaps was presented in. The algorithm requires O(log "n" * log "n") comparisons. The view also presents a new and conceptually simple algorithm for merging heaps. When merging is a common task, a different heap implementation is recommended, such as binomial heaps, which can be merged in O(log "n").

Additionally, a binary heap can be implemented with a traditional binary tree data structure, but there is an issue with finding the adjacent element on the last level on the binary heap when adding an element. This element can be determined algorithmically or by adding extra data to the nodes, called "threading" the tree—instead of merely storing references to the children, we store the inorder successor of the node as well.

It is possible to modify the heap structure to allow extraction of both the smallest and largest element in formula_11formula_12 time. To do this, the rows alternate between min heap and max heap. The algorithms are roughly the same, but, in each step, one must consider the alternating rows with alternating comparisons. The performance is roughly the same as a normal single direction heap. This idea can be generalised to a min-max-median heap.

In an array-based heap, the children and parent of a node can be located via simple arithmetic on the node's index. This section derives the relevant equations for heaps with their root at index 0, with additional notes on heaps with their root at index 1.

To avoid confusion, we'll define the level of a node as its distance from the root, such that the root itself occupies level 0.

For a general node located at index formula_13 (beginning from 0), we will first derive the index of its right child, formula_14.

Let node formula_13 be located in level formula_16, and note that any level formula_17 contains exactly formula_18 nodes. Furthermore, there are exactly formula_19 nodes contained in the layers up to and including layer formula_17 (think of binary arithmetic; 0111...111 = 1000...000 - 1). Because the root is stored at 0, the formula_21th node will be stored at index formula_22. Putting these observations together yields the following expression for the index of the last node in layer l.

Let there be formula_24 nodes after node formula_13 in layer L, such that

Each of these formula_24 nodes must have exactly 2 children, so there must be formula_28 nodes separating formula_13's right child from the end of its layer (formula_30).

As required.

Noting that the left child of any node is always 1 place before its right child, we get formula_32.

If the root is located at index 1 instead of 0, the last node in each level is instead at index formula_19. Using this throughout yields formula_34 and formula_35 for heaps with their root at 1.

Every node is either the left or right child of its parent, so we know that either of the following is true.


Hence,

Now consider the expression formula_39.

If node formula_13 is a left child, this gives the result immediately, however, it also gives the correct result if node formula_13 is a right child. In this case, formula_42 must be even, and hence formula_43 must be odd.

Therefore, irrespective of whether a node is a left or right child, its parent can be found by the expression:

Since the ordering of siblings in a heap is not specified by the heap property, a single node's two children can be freely interchanged unless doing so violates the shape property (compare with treap). Note, however, that in the common array-based heap, simply swapping the children might also necessitate moving the children's sub-tree nodes to retain the heap property.

The binary heap is a special case of the d-ary heap in which d = 2.




</doc>
<doc id="254138" url="https://en.wikipedia.org/wiki?curid=254138" title="Binomial heap">
Binomial heap

In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged together.
It is important as an implementation of the mergeable heap abstract data type (also called meldable heap), which is a priority queue supporting merge operation. It is implemented as a heap similar to a binary heap but using a special tree structure that is different from the complete binary trees used by binary heaps. Binomial heaps were invented in 1978 by Jean Vuillemin.

A binomial heap is implemented as a set of binomial trees (compare with a binary heap, which has a shape of a single binary tree), which are defined recursively as follows:

A binomial tree of order formula_1 has formula_5 nodes, and height formula_1. 
The name comes from the shape: a binomial tree of order formula_1 has formula_8 nodes at depth formula_9, a binomial coefficient.
Because of its structure, a binomial tree of order formula_1 can be constructed from two trees of order formula_2 by attaching one of them as the leftmost child of the root of the other tree. This feature is central to the "merge" operation of a binomial heap, which is its major advantage over other conventional heaps.

A binomial heap is implemented as a set of binomial trees that satisfy the "binomial heap properties":
The first property ensures that the root of each binomial tree contains the smallest key in the tree. It follows that the smallest key in the entire heap is one of the roots.

The second property implies that a binomial heap with formula_12 nodes consists of at most formula_13 binomial trees, where formula_14 is the binary logarithm. The number and orders of these trees are uniquely determined by the number of nodes formula_12: there is one binomial tree for each nonzero bit in the binary representation of the number formula_12. For example, the decimal number 13 is 1101 in binary, formula_17, and thus a binomial heap with 13 nodes will consist of three binomial trees of orders 3, 2, and 0 (see figure below).

<br>"Example of a binomial heap containing 13 nodes with distinct keys.The heap consists of three binomial trees with orders 0, 2, and 3."

The number of different ways that formula_12 items with distinct keys can be arranged into a binomial heap equals the largest odd divisor of formula_19. For formula_20 these numbers are
If the formula_12 items are inserted into a binomial heap in a uniformly random order, each of these arrangements is equally likely.

Because no operation requires random access to the root nodes of the binomial trees, the roots of the binomial trees can be stored in a linked list, ordered by increasing order of the tree. Because the number of children for each node is variable, it does not work well for each node to have separate links to each of its children, as would be common in a binary tree; instead, it is possible to implement this tree using links from each node to its highest-order child in the tree, and to its sibling of the next smaller order than it. These sibling pointers can be interpreted as the next pointers in a linked list of the children of each node, but with the opposite order from the linked list of roots: from largest to smallest order, rather than vice versa. This representation allows two trees of the same order to be linked together, making a tree of the next larger order, in constant time.

The operation of merging two heaps is used as a subroutine in most other operations.
A basic subroutine within this procedure merges pairs of binomial trees of the same order. This may be done by comparing the keys at the roots of the two trees (the smallest keys in both trees). The root node with the larger key is made into a child of the root node with the smaller key, increasing its order by one:

To merge two heaps more generally, the lists of roots of both heaps are traversed simultaneously in a manner similar to that of the merge algorithm,
in a sequence from smaller orders of trees to larger orders. When only one of the two heaps being merged contains a tree of order formula_22, this tree is moved to the output heap. When both of the two heaps contain a tree of order formula_22, the two trees are merged to one tree of order formula_24 so that the minimum-heap property is satisfied. It may later become necessary to merge this tree with some other tree of order formula_24 in one of the two input heaps. In the course of the algorithm, it will examine at most three trees of any order, two from the two heaps we merge and one composed of two smaller trees.

Because each binomial tree in a binomial heap corresponds to a bit in the binary representation of its size, there is an analogy between the merging of two heaps and the binary addition of the "sizes" of the two heaps, from right-to-left. Whenever a carry occurs during addition, this corresponds to a merging of two binomial trees during the merge.

Each tree has order at most formula_26 and therefore the running time is formula_27.

Inserting a new element to a heap can be done by simply creating a new heap containing only this element and then merging it with the original heap. Because of the merge, a single insertion takes time formula_27. However, this can be sped up using a merge procedure that shortcuts the merge after it reaches a point where only one of the merged heaps has trees of larger order. With this speedup, across a series of formula_1 consecutive insertions, the total time for the insertions is formula_30. Another way of stating this is that (after logarithmic overhead for the first insertion in a sequence) each successive insert has an "amortized" time of formula_31 (i.e. constant) per insertion.

A variant of the binomial heap, the skew binomial heap, achieves constant worst case insertion time by using forests whose tree sizes are based on the skew binary number system rather than on the binary number system.

To find the minimum element of the heap, find the minimum among the roots of the binomial trees. This can be done in formula_27 time, as there are just formula_27 tree roots to examine.

By using a pointer to the binomial tree that contains the minimum element, the time for this operation can be reduced to formula_31. The pointer must be updated when performing any operation other than finding the minimum. This can be done in formula_27 time per update, without raising the overall asymptotic running time of any operation.

To delete the minimum element from the heap, first find this element, remove it from the root of its binomial tree, and obtain a list of its child subtrees (which are each themselves binomial trees, of distinct orders). Transform this list of subtrees into a separate binomial heap by reordering them from smallest to largest order. Then merge this heap with the original heap. Since each root has at most formula_26 children, creating this new heap takes time formula_27. Merging heaps takes time formula_27, so the entire delete minimum operation takes time formula_27.

After decreasing the key of an element, it may become smaller than the key of its parent, violating the minimum-heap property. If this is the case, exchange the element with its parent, and possibly also with its grandparent, and so on, until the minimum-heap property is no longer violated. Each binomial tree has height at most formula_26, so this takes formula_27 time. However, this operation requires that the representation of the tree include pointers from each node to its parent in the tree, somewhat complicating the implementation of other operations.

To delete an element from the heap, decrease its key to negative infinity (or equivalently, to some value lower than any element in the heap) and then delete the minimum in the heap.





</doc>
<doc id="254142" url="https://en.wikipedia.org/wiki?curid=254142" title="Fibonacci heap">
Fibonacci heap

In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection of heap-ordered trees. It has a better amortized running time than many other priority queue data structures including the binary heap and binomial heap. Michael L. Fredman and Robert E. Tarjan developed Fibonacci heaps in 1984 and published them in a scientific journal in 1987. Fibonacci heaps are named after the Fibonacci numbers, which are used in their running time analysis.

For the Fibonacci heap, the find-minimum operation takes constant ("O"(1)) amortized time. The insert and decrease key operations also work in constant amortized time. Deleting an element (most often used in the special case of deleting the minimum element) works in "O"(log "n") amortized time, where "n" is the size of the heap. This means that starting from an empty data structure, any sequence of "a" insert and decrease key operations and "b" delete operations would take "O"("a" + "b" log "n") worst case time, where "n" is the maximum heap size. In a binary or binomial heap such a sequence of operations would take "O"(("a" + "b") log "n") time. A Fibonacci heap is thus better than a binary or binomial heap when "b" is smaller than "a" by a non-constant factor. It is also possible to merge two Fibonacci heaps in constant amortized time, improving on the logarithmic merge time of a binomial heap, and improving on binary heaps which cannot handle merges efficiently.

Using Fibonacci heaps for priority queues improves the asymptotic running time of important algorithms, such as Dijkstra's algorithm for computing the shortest path between two nodes in a graph, compared to the same algorithm using other slower priority queue data structures.

A Fibonacci heap is a collection of trees satisfying the minimum-heap property, that is, the key of a child is always greater than or equal to the key of the parent. This implies that the minimum key is always at the root of one of the trees. Compared with binomial heaps, the structure of a Fibonacci heap is more flexible. The trees do not have a prescribed shape and in the extreme case the heap can have every element in a separate tree. This flexibility allows some operations to be executed in a lazy manner, postponing the work for later operations. For example, merging heaps is done simply by concatenating the two lists of trees, and operation "decrease key" sometimes cuts a node from its parent and forms a new tree.

However, at some point order needs to be introduced to the heap to achieve the desired running time. In particular, degrees of nodes (here degree means the number of children) are kept quite low: every node has degree at most "O"(log "n") and the size of a subtree rooted in a node of degree "k" is at least "F", where "F" is the "k"th Fibonacci number. This is achieved by the rule that we can cut at most one child of each non-root node. When a second child is cut, the node itself needs to be cut from its parent and becomes the root of a new tree (see Proof of degree bounds, below). The number of trees is decreased in the operation "delete minimum", where trees are linked together.

As a result of a relaxed structure, some operations can take a long time while others are done very quickly. For the amortized running time analysis we use the potential method, in that we pretend that very fast operations take a little bit longer than they actually do. This additional time is then later combined and subtracted from the actual running time of slow operations. The amount of time saved for later use is measured at any given moment by a potential function. The potential of a Fibonacci heap is given by

where "t" is the number of trees in the Fibonacci heap, and "m" is the number of marked nodes. A node is marked if at least one of its children was cut since this node was made a child of another node (all roots are unmarked).
The amortized time for an operation is given by the sum of the actual time and "c" times the difference in potential, where "c" is a constant (chosen to match the constant factors in the "O" notation for the actual time).

Thus, the root of each tree in a heap has one unit of time stored. This unit of time can be used later to link this tree with another tree at amortized time 0. Also, each marked node has two units of time stored. One can be used to cut the node from its parent. If this happens, the node becomes a root and the second unit of time will remain stored in it as in any other root.

To allow fast deletion and concatenation, the roots of all trees are linked using a circular doubly linked list. The children of each node are also linked using such a list. For each node, we maintain its number of children and whether the node is marked. Moreover, we maintain a pointer to the root containing the minimum key.

Operation find minimum is now trivial because we keep the pointer to the node containing it. It does not change the potential of the heap, therefore both actual and amortized cost are constant.

As mentioned above, merge is implemented simply by concatenating the lists of tree roots of the two heaps. This can be done in constant time and the potential does not change, leading again to constant amortized time.

Operation insert works by creating a new heap with one element and doing merge. This takes constant time, and the potential increases by one, because the number of trees increases. The amortized cost is thus still constant.

Operation extract minimum (same as "delete minimum") operates in three phases. First we take the root containing the minimum element and remove it. Its children will become roots of new trees. If the number of children was "d", it takes time "O"("d") to process all new roots and the potential increases by "d"−1. Therefore, the amortized running time of this phase is "O"("d") = "O"(log "n").

However to complete the extract minimum operation, we need to update the pointer to the root with minimum key. Unfortunately there may be up to "n" roots we need to check. In the second phase we therefore decrease the number of roots by successively linking together roots of the same degree. When two roots "u" and "v" have the same degree, we make one of them a child of the other so that the one with the smaller key remains the root. Its degree will increase by one. This is repeated until every root has a different degree. To find trees of the same degree efficiently we use an array of length "O"(log "n") in which we keep a pointer to one root of each degree. When a second root is found of the same degree, the two are linked and the array is updated. The actual running time is "O"(log "n" + "m") where "m" is the number of roots at the beginning of the second phase. At the end we will have at most "O"(log "n") roots (because each has a different degree). Therefore, the difference in the potential function from before this phase to after it is: "O"(log "n") − "m", and the amortized running time is then at most "O"(log "n" + "m") + "c"("O"(log "n") − "m"). With a sufficiently large choice of "c", this simplifies to "O"(log "n").

In the third phase we check each of the remaining roots and find the minimum. This takes "O"(log "n") time and the potential does not change. The overall amortized running time of extract minimum is therefore "O"(log "n").

Operation decrease key will take the node, decrease the key and if the heap property becomes violated (the new key is smaller than the key of the parent), the node is cut from its parent. If the parent is not a root, it is marked. If it has been marked already, it is cut as well and its parent is marked. We continue upwards until we reach either the root or an unmarked node. Now we set the minimum pointer to the decreased value if it is the new minimum. In the process we create some number, say "k", of new trees. Each of these new trees except possibly the first one was marked originally but as a root it will become unmarked. One node can become marked. Therefore, the number of marked nodes changes by −("k" − 1) + 1 = − "k" + 2. Combining these 2 changes, the potential changes by 2(−"k" + 2) + "k" = −"k" + 4. The actual time to perform the cutting was "O"("k"), therefore (again with a sufficiently large choice of "c") the amortized running time is constant.

Finally, operation delete can be implemented simply by decreasing the key of the element to be deleted to minus infinity, thus turning it into the minimum of the whole heap. Then we call extract minimum to remove it. The amortized running time of this operation is "O"(log "n").

The amortized performance of a Fibonacci heap depends on the degree (number of children) of any tree root being "O"(log "n"), where "n" is the size of the heap. Here we show that the size of the (sub)tree rooted at any node "x" of degree "d" in the heap must have size at least "F", where "F" is the "k"th Fibonacci number. The degree bound follows from this and the fact (easily proved by induction) that formula_1 for all integers formula_2, where formula_3. (We then have formula_4, and taking the log to base formula_5 of both sides gives formula_6 as required.)

Consider any node "x" somewhere in the heap ("x" need not be the root of one of the main trees). Define size("x") to be the size of the tree rooted at "x" (the number of descendants of "x", including "x" itself). We prove by induction on the height of "x" (the length of a longest simple path from "x" to a descendant leaf), that size("x") ≥ "F", where "d" is the degree of "x".

Base case: If "x" has height 0, then "d" = 0, and size("x") = 1 = "F".

Inductive case: Suppose "x" has positive height and degree "d">0. Let "y", "y", ..., "y" be the children of "x", indexed in order of the times they were most recently made children of "x" ("y" being the earliest and "y" the latest), and let "c", "c", ..., "c" be their respective degrees. We claim that "c" ≥ "i"-2 for each "i" with 2≤"i"≤"d": Just before "y" was made a child of "x", "y"...,"y" were already children of "x", and so "x" had degree at least "i"−1 at that time. Since trees are combined only when the degrees of their roots are equal, it must have been that "y" also had degree at least "i"-1 at the time it became a child of "x". From that time to the present, "y" can only have lost at most one child (as guaranteed by the marking process), and so its current degree "c" is at least "i"−2. This proves the claim.

Since the heights of all the "y" are strictly less than that of "x", we can apply the inductive hypothesis to them to get size("y") ≥ "F" ≥ "F" = "F". The nodes "x" and "y" each contribute at least 1 to size("x"), and so we have

formula_7

A routine induction proves that formula_8 for any formula_2, which gives the desired lower bound on size("x").

Although Fibonacci heaps look very efficient, they have the following two drawbacks (as mentioned in the paper "The Pairing Heap: A new form of Self Adjusting Heap"): "They are complicated when it comes to coding them. Also they are not as efficient in practice when compared with the theoretically less efficient forms of heaps, since in their simplest version they require storage and manipulation of four pointers per node, compared to the two or three pointers per node needed for other structures ". These other structures are referred to Binary heap, Binomial heap, Pairing Heap, Brodal Heap and Rank Pairing Heap.

Although the total running time of a sequence of operations starting with an empty structure is bounded by the bounds given above, some (very few) operations in the sequence can take very long to complete (in particular delete and delete minimum have linear running time in the worst case). For this reason Fibonacci heaps and other amortized data structures may not be appropriate for real-time systems. It is possible to create a data structure which has the same worst-case performance as the Fibonacci heap has amortized performance. One such structure, the Brodal queue, is, in the words of the creator, "quite complicated" and "[not] applicable in practice." Created in 2012, the strict Fibonacci heap is a simpler (compared to Brodal's) structure with the same worst-case bounds. Despite having simpler structure, experiments show that in practice the strict Fibonacci heap performs slower than more complicated Brodal queue and also slower than basic Fibonacci heap. The run-relaxed heaps of Driscoll et al. give good worst-case performance for all Fibonacci heap operations except merge.

Fibonacci heaps have a reputation for being slow in practice due to large memory consumption per node and high constant factors on all operations. Recent experimental results suggest that Fibonacci heaps are more efficient in practice than most of its later derivatives, including quake heaps, violation heaps, strict Fibonacci heaps, rank pairing heaps, but less efficient than either pairing heaps or array-based heaps.



</doc>
<doc id="3402053" url="https://en.wikipedia.org/wiki?curid=3402053" title="Pairing heap">
Pairing heap

A pairing heap is a type of heap data structure with relatively simple implementation and excellent practical amortized performance, introduced by Michael Fredman, Robert Sedgewick, Daniel Sleator, and Robert Tarjan in 1986.
Pairing heaps are heap-ordered multiway tree structures, and can be considered simplified Fibonacci heaps. They are considered a "robust choice" for implementing such algorithms as Prim's MST algorithm, and support the following operations (assuming a min-heap):


The analysis of pairing heaps' time complexity was initially inspired by that of splay trees.
The amortized time per "delete-min" is , and the operations "find-min", "meld", and "insert" run in amortized time.

When a "decrease-key" operation is added as well, determining the precise asymptotic running time of pairing heaps has turned out to be difficult. Initially, the time complexity of this operation was conjectured on empirical grounds to be , but Fredman proved that the amortized time per "decrease-key" is at least formula_1 for some sequences of operations.
Using a different amortization argument, Pettie then proved that "insert", "meld", and "decrease-key" all run in formula_2 amortized time, which is formula_3.
Elmasry later introduced elaborations of pairing heaps for which "decrease-key" runs in formula_4 amortized time and other operations have optimal amortized bounds, but no tight formula_5 bound is known for the original data structure.

Although this is worse than other priority queue algorithms such as Fibonacci heaps, which perform "decrease-key" in formula_6 amortized time, the performance in practice is excellent. Stasko and Vitter,
Moret and Shapiro,
and Larkin, Sen, and Tarjan
conducted experiments on pairing heaps and other heap data structures. They concluded that pairing heaps are often faster in practice than array-based binary heaps and d-ary heaps, and almost always faster in practice than other pointer-based heaps, including data structures like Fibonacci heaps that are theoretically more efficient.

A pairing heap is either an empty heap, or a pairing tree consisting of a root element and a possibly empty list of pairing trees. The heap ordering property requires that parent of any node is no greater than the node itself. The following description assumes a purely functional heap that does not support the "decrease-key" operation.

A pointer-based implementation for RAM machines, supporting "decrease-key", can be achieved using three pointers per node, by representing the children of a node by a singly-linked list: a pointer to the node's first child, one to its next sibling, and one to its previous sibling (or, for the leftmost sibling, to its parent). Alternatively, the previous-pointer can be omitted by letting the last child point back to the parent, if a single boolean flag is added to indicate "end of list". This achieves a more compact structure at the expense of a constant overhead factor per operation.

The function "find-min" simply returns the root element of the heap:

Melding with an empty heap returns the other heap, otherwise a new heap is returned that has the minimum of the two root elements as its root element and just adds the heap with the larger root to the list of subheaps:

The easiest way to insert an element into a heap is to meld the heap with a new heap containing just this element and an empty list of subheaps:

The only non-trivial fundamental operation is the deletion of the minimum element from the heap. This requires performing repeated melds of its children until only one tree remains. The standard strategy first melds the subheaps in pairs (this is the step that gave this data structure its name) from left to right and then melds the resulting list of heaps from right to left:

This uses the auxiliary function "merge-pairs":

That this does indeed implement the described two-pass left-to-right then right-to-left merging strategy can be seen from this reduction:



</doc>
<doc id="3335635" url="https://en.wikipedia.org/wiki?curid=3335635" title="Beap">
Beap

A beap, or bi-parental heap, is a data structure where a node usually has two parents (unless it is the first or last on a level) and two children (unless it is on the last level). Unlike a heap, a beap allows sublinear search. The beap was introduced by Ian Munro and Hendra Suwanda. A related data structure is the Young tableau.

The height of the structure is approximately formula_1. Also, assuming the last level is full, the number of elements on that level is also formula_1. In fact, because of these properties all basic operations (insert, remove, find) run in formula_3 time on average. Find operations in the heap can be formula_4 in the worst case. Removal and insertion of new elements involves propagation of elements up or down (much like in a heap) in order to restore the beap invariant. An additional perk is that beap provides constant time access to the smallest element and formula_3 time for the maximum element.

Actually, a formula_3 find operation can be implemented if parent pointers at each node are maintained. You would start at the absolute bottom-most element of the top node (similar to the left-most child in a heap) and move either up or right to find the element of interest.



</doc>
<doc id="2754256" url="https://en.wikipedia.org/wiki?curid=2754256" title="Leftist tree">
Leftist tree

In computer science, a leftist tree or leftist heap is a priority queue implemented with a variant of a binary heap. Every node x has an "s-value" which is the distance to the nearest leaf in subtree rooted at x. In contrast to a "binary heap", a leftist tree attempts to be very unbalanced. In addition to the heap property, leftist trees are maintained so the right descendant of each node has the lower s-value.

The height-biased leftist tree was invented by Clark Allan Crane. The name comes from the fact that the left subtree is usually taller than the right subtree.

A leftist tree is a mergeable heap. When inserting a new node into a tree, a new one-node tree is created and merged into the existing tree. To delete an item, it is replaced by the merge of its left and right sub-trees. Both these operations take O(log "n") time. For insertions, this is slower than Fibonacci heaps, which support insertion in O(1) (constant) amortized time, and O(log "n") worst-case.

Leftist trees are advantageous because of their ability to merge quickly, compared to binary heaps which take Θ("n"). In almost all cases, the merging of skew heaps has better performance. However merging leftist heaps has worst-case O(log "n") complexity while merging skew heaps has only amortized O(log "n") complexity.

The usual leftist tree is a "height-biased" leftist tree. However, other biases can exist, such as in the "weight-biased" leftist tree.

The s-value (or rank) of a node is the distance from that node to the nearest leaf in the subtree rooted at that node. Put another way, the s-value of a codice_1 child is implicitly zero. Other nodes have an s-value equal to one more the minimum of their children's s-values. Thus, in the example at right, all nodes with at least one missing child have an s-value of 1, while node 4 has an s-value of 2, since its right child (8) has an s-value of 1. (In some descriptions, the s-value of null children is assumed to be −1.)

Knowing the shortest path to the nearest missing leaf in the subtree rooted at "x" is exactly of "s"("x"), every node at depth "s"("x")−1 or less has exactly 2 children since "s"("x") would have been less if not. Meaning that the size of the tree rooted at "x" is at least formula_1. Thus, "s"("x") is at most formula_2, "m" being the number of nodes of the subtree rooted at "x".

Most operations on a Height Biased Leftist Tree are done using the merge operation.

The merge operation takes two Min HBLTs as input and returns a Min HBLT containing all the nodes in the original Min HBLTs put together.

If either of A or B is empty, the merge returns the other one.

In case of Min HBLTs, assume we have two trees rooted at A and B where A.key formula_3 B.key. Otherwise we can swap A and B so that the condition above holds.

The merge is done recursively by merging B with A's right subtree. This might change the S-value of A's right subtree. To maintain the leftist tree property, after each merge is done, we check if the S-value of right subtree became bigger than the S-value of left subtree during the recursive merge calls. If so, we swap the right and left subtrees (If one child is missing, it should be the right one).

Since we assumed that A's root is greater than B's, the heap property is also maintained.

 MERGE(A, B)

An example of how the merge operation in a leftist tree works is depicted. The boxes represent each merge call.When the recursion unwinds, we swap left and right children if x.right.s_value > x.left.s_value for every node x. In this case we swapped the subtrees rooted at nodes with keys 7 and 10.

Insertion is done using the merge operation. An insertion of a node into an already existing

Min HBLT, creates a HBLT tree of size one with that node and merges it with the existing tree.

The Min element in a Min HBLT is the root. Thus, in order to delete the Min, the root is deleted and its subtrees are merged to form the new Min HBLT.

Initializing a height biased leftist tree is primarily done in one of two ways. The first is to merge each node one at a time into one HBLT. This process is inefficient and takes O("nlogn") time. The other approach is to use a queue to store each node and resulting tree. The first two items in the queue are removed, merged, and placed back into the queue. This can initialize a HBLT in O("n") time. This approach is detailed in the three diagrams supplied. A min height biased leftist tree is shown.

To initialize a min HBLT, place each element to be added to the tree into a queue. In the example (see Part 1 to the left), the set of numbers [4, 8, 10, 9, 1, 3, 5, 6, 11] are initialized. Each line of the diagram represents another cycle of the algorithm, depicting the contents of the queue. The first five steps are easy to follow. Notice that the freshly created HBLT is added to the end of the queue. In the fifth step, the first occurrence of an s-value greater than 1 occurs. The sixth step shows two trees merged with each other, with predictable results.

In part 2 a slightly more complex merge happens. The tree with the lower value (tree x) has a right child, so merge must be called again on the subtree rooted by tree x's right child and the other tree. After the merge with the subtree, the resulting tree is put back into tree x. The s-value of the right child (s=2) is now greater than the s-value of the left child (s=1), so they must be swapped. The s-value of the root node 4 is also now 2.
Part 3 is the most complex. Here, we recursively call merge twice (each time with the right child 's subtree that is not grayed out). This uses the same process described for part 2.

If we have a pointer to a node x in a Min HBLT, we can delete it as follows: Replace the node x with the result of merging its two subtrees and update the s-values of the nodes on the path from x to the root, swapping the right and left subtrees if necessary to maintain the leftist tree property.

The upward traversal is continued until either we hit the root or the s-values does not change. Since we are deleting an element, the S-values on the path traversed cannot be increased. Every node that is already the right child of its parent and causes its parent's s-value to be decreased, will remain on the right. Every node that is its parent's left child and causes the parent's s-value to be decreased will be swapped with its right sibling since the decrease in the s-value means that the child causing the change has less s-value than the current right child which used to have the lowest s-value.

Each node needs to have a pointer to its parent, so that we can traverse the path to the root updating the s-values.

When the traversal ends at some node y, the nodes traversed all lie on the rightmost path rooted at node y. An example is shown below. It follows that the number of nodes traversed is at most log(m), m being the size of the subtree rooted at y. Thus, this operation also takes O(lg m) to perform.

Leftist trees can also be weight biased. In this case, instead of storing s-values in node x, we store an attribute w(x) denoting the number of nodes in the subtree rooted at x:

w(x) = w(x.right) + w(x.left) + 1

WBLTs ensure w(x.left) ≥ w(x.right) for all internal nodes x. WBLT operations ensure this invariant by swapping the children of a node when the right subtree outgrows the left one, just as in HBLT operations.

The merge operation in WBLTs can be done using a single top to bottom traversal since the number of nodes in the subtrees are known prior to recursive call to merge. Thus, we can swap left and right subtrees if the total number of nodes in the right subtree and the tree to be merged is bigger than the number of nodes in the left subtree. This allows the operations be completed in a single path and so improves the time complexity of the operations by a constant factor.

The merge operation is depicted in the graph below.

Insertions and deletion of the min element can be done in the same as for HBLTs using the merge operation.

Although WBLTs outperform HBLTs in merge, insertion and deletion of the Min key by a constant factor, the O(logn) bound is not guaranteed when deleting an arbitrary element from WBLTs, since θ(n) nodes have to be traversed.

If this was an HBLT, then deleting the leaf node with key 60 would take O(1) time and updating the s-values is not needed since the length of rightmost path for all the nodes does not change.

But in an WBLT tree, we have to update the weight of each node back to the root, which takes O(n) worst case.

Several variations on the basic leftist tree exist, which make only minor changes to the basic algorithm:




</doc>
<doc id="3707999" url="https://en.wikipedia.org/wiki?curid=3707999" title="Skew heap">
Skew heap

A skew heap (or self-adjusting heap) is a heap data structure implemented as a binary tree. Skew heaps are advantageous because of their ability to merge more quickly than binary heaps. In contrast with binary heaps, there are no structural constraints, so there is no guarantee that the height of the tree is logarithmic. Only two conditions must be satisfied:

A skew heap is a self-adjusting form of a leftist heap which attempts to maintain balance by unconditionally swapping all nodes in the merge path when merging two heaps. (The merge operation is also used when adding and removing values.) 

With no structural constraints, it may seem that a skew heap would be horribly inefficient. However, amortized complexity analysis can be used to demonstrate that all operations on a skew heap can be done in O(log n).
In fact, with φ=(1+√5)/2 denoting the golden ratio, the exact amortized complexity is known to be log n (approximately 1.44 log n).

Skew heaps may be described with the following recursive definition:


When two skew heaps are to be merged, we can use a similar process as the merge of two leftist heaps:


Before:

after
Alternatively, there is a non-recursive approach which is more wordy, and does require some sorting at the outset.


Adding a value to a skew heap is like merging a tree with one node together with the original tree.

Removing the first value in a heap can be accomplished by removing the root and merging its child subtrees.

In many functional languages, skew heaps become extremely simple to implement. Here is a complete sample implementation in Haskell.



</doc>
<doc id="546678" url="https://en.wikipedia.org/wiki?curid=546678" title="Soft heap">
Soft heap

In computer science, a soft heap is a variant on the simple heap data structure that has constant amortized time for 5 types of operations. This is achieved by carefully "corrupting" (increasing) the keys of at most a certain number of values in the heap. The constant time operations are:

Other heaps such as Fibonacci heaps achieve most of these bounds without any corruption, but cannot provide a constant-time bound on the critical "delete" operation. The amount of corruption can be controlled by the choice of a parameter ε, but the lower this is set, the more time insertions require (O(log 1/ε) for an error rate of ε).

More precisely, the guarantee offered by the soft heap is the following: for a fixed value "ε" between 0 and 1/2, at any point in time there will be at most "ε*n" corrupted keys in the heap, where "n" is the number of elements inserted so far. Note that this does not guarantee that only a fixed percentage of the keys "currently" in the heap are corrupted: in an unlucky sequence of insertions and deletions, it can happen that all elements in the heap will have corrupted keys. Similarly, we have no guarantee that in a sequence of elements extracted from the heap with "findmin" and "delete", only a fixed percentage will have corrupted keys: in an unlucky scenario only corrupted elements are extracted from the heap.

The soft heap was designed by Bernard Chazelle in 2000. The term "corruption" in the structure is the result of what Chazelle called "carpooling" in a soft heap. Each node in the soft heap contains a linked-list of keys and one common key. The common key is an upper bound on the values of the keys in the linked-list. Once a key is added to the linked-list, it is considered corrupted because its value is never again relevant in any of the soft heap operations: only the common keys are compared. This is what makes soft heaps "soft"; you can't be sure whether or not any particular value you put into it will be corrupted. The purpose of these corruptions is effectively to lower the information entropy of the data, enabling the data structure to break through information-theoretic barriers regarding heaps.

Despite their limitations and unpredictable nature, soft heaps are useful in the design of deterministic algorithms. They were used to achieve the best complexity to date for finding a minimum spanning tree. They can also be used to easily build an optimal selection algorithm, as well as "near-sorting" algorithms, which are algorithms that place every element near its final position, a situation in which insertion sort is fast.

One of the simplest examples is the selection algorithm. Say we want to find the "k"th largest of a group of "n" numbers. First, we choose an error rate of 1/3; that is, at most about 33% of the keys we insert will be corrupted. Now, we insert all "n" elements into the heap — we call the original values the "correct" keys, and the values stored in the heap the "stored" keys. At this point, at most "n"/3 keys are corrupted, that is, for at most "n"/3 keys is the "stored" key larger than the "correct" key, for all the others the stored key equals the correct key.

Next, we delete the minimum element from the heap "n"/3 times (this is done according to the "stored" key). As the total number of insertions we have made so far is still n, there are still at most "n"/3 corrupted keys in the heap. Accordingly, at least 2"n"/3 − "n"/3 = "n"/3 of the keys remaining in the heap are not corrupted. 

Let "L" be the element with the largest correct key among the elements we removed. The stored key of "L" is possibly larger than its correct key (if "L" was corrupted), and even this larger value is smaller than all the stored keys of the remaining elements in the heap (as we were removing minimums). Therefore, the correct key of "L" is smaller than the remaining "n"/3 uncorrupted elements in the soft heap. Thus, "L" divides the elements somewhere between 33%/66% and 66%/33%. We then partition the set about "L" using the "partition" algorithm from quicksort and apply the same algorithm again to either the set of numbers less than "L" or the set of numbers greater than "L", neither of which can exceed 2"n"/3 elements. Since each insertion and deletion requires O(1) amortized time, the total deterministic time is T("n") = T(2"n"/3) + O("n"). Using case 3 of the master theorem for divide-and-conquer recurrences (with ε=1 and c=2/3), we know that T("n") = Θ("n").

The final algorithm looks like this:



</doc>
<doc id="11960848" url="https://en.wikipedia.org/wiki?curid=11960848" title="D-ary heap">
D-ary heap

The -ary heap or -heap is a priority queue data structure, a generalization of the binary heap in which the nodes have children instead of 2. Thus, a binary heap is a 2-heap, and a ternary heap is a 3-heap. According to Tarjan and Jensen et al., -ary heaps were invented by Donald B. Johnson in 1975.

This data structure allows decrease priority operations to be performed more quickly than binary heaps, at the expense of slower delete minimum operations. This tradeoff leads to better running times for algorithms such as Dijkstra's algorithm in which decrease priority operations are more common than delete min operations. Additionally, -ary heaps have better memory cache behavior than binary heaps, allowing them to run more quickly in practice despite having a theoretically larger worst-case running time. Like binary heaps, -ary heaps are an in-place data structure that uses no additional storage beyond that needed to store the array of items in the heap.

The -ary heap consists of an array of items, each of which has a priority associated with it. These items may be viewed as the nodes in a complete -ary tree, listed in breadth first traversal order: the item at position 0 of the array (using zero-based numbering) forms the root of the tree, the items at positions 1 through are its children, the next items are its grandchildren, etc. Thus, the parent of the item at position (for any ) is the item at position and its children are the items at positions through . According to the heap property, in a min-heap, each item has a priority that is at least as large as its parent; in a max-heap, each item has a priority that is no larger than its parent.

The minimum priority item in a min-heap (or the maximum priority item in a max-heap) may always be found at position 0 of the array. To remove this item from the priority queue, the last item "x" in the array is moved into its place, and the length of the array is decreased by one. Then, while item "x" and its children do not satisfy the heap property, item "x" is swapped with one of its children (the one with the smallest priority in a min-heap, or the one with the largest priority in a max-heap), moving it downward in the tree and later in the array, until eventually the heap property is satisfied. The same downward swapping procedure may be used to increase the priority of an item in a min-heap, or to decrease the priority of an item in a max-heap.

To insert a new item into the heap, the item is appended to the end of the array, and then while the heap property is violated it is swapped with its parent, moving it upward in the tree and earlier in the array, until eventually the heap property is satisfied. The same upward-swapping procedure may be used to decrease the priority of an item in a min-heap, or to increase the priority of an item in a max-heap.

To create a new heap from an array of items, one may loop over the items in reverse order, starting from the item at position and ending at the item at position 0, applying the downward-swapping procedure for each item.

In a -ary heap with items in it, both the upward-swapping procedure and the downward-swapping procedure may perform as many as swaps. In the upward-swapping procedure, each swap involves a single comparison of an item with its parent, and takes constant time. Therefore, the time to insert a new item into the heap, to decrease the priority of an item in a min-heap, or to increase the priority of an item in a max-heap, is . In the downward-swapping procedure, each swap involves comparisons and takes time: it takes comparisons to determine the minimum or maximum of the children and then one more comparison against the parent to determine whether a swap is needed. Therefore, the time to delete the root item, to increase the priority of an item in a min-heap, or to decrease the priority of an item in a max-heap, is .

When creating a -ary heap from a set of "n" items, most of the items are in positions that will eventually hold leaves of the -ary tree, and no downward swapping is performed for those items. At most items are non-leaves, and may be swapped downwards at least once, at a cost of time to find the child to swap them with. At most nodes may be swapped downward two times, incurring an additional cost for the second swap beyond the cost already counted in the first term, etc. Therefore, the total amount of time to create a heap in this way is

The exact value of the above (the worst-case number of comparisons during the construction of d-ary heap) is known to be equal to:

where s(n) is the sum of all digits of the standard base-d representation of n and e(n) is the exponent of d in the factorization of n.
This reduces to
for d = 2, and to

for d = 3.

The space usage of the heap, with insert and delete-min operations, is linear, as it uses no extra storage other than an array containing a list of the items in the heap. If changes to the priorities of existing items need to be supported, then one must also maintain pointers from the items to their positions in the heap, which again uses only linear storage.

When operating on a graph with edges and vertices, both Dijkstra's algorithm for shortest paths and Prim's algorithm for minimum spanning trees use a min-heap in which there are delete-min operations and as many as decrease-priority operations. By using a -ary heap with , the total times for these two types of operations may be balanced against each other, leading to a total time of for the algorithm, an improvement over the running time of binary heap versions of these algorithms whenever the number of edges is significantly larger than the number of vertices. An alternative priority queue data structure, the Fibonacci heap, gives an even better theoretical running time of , but in practice -ary heaps are generally at least as fast, and often faster, than Fibonacci heaps for this application.

4-heaps may perform better than binary heaps in practice, even for delete-min operations. Additionally,
a -ary heap typically runs much faster than a binary heap for heap sizes that exceed the size of the computer's cache memory:
A binary heap typically requires more cache misses and virtual memory page faults than a -ary heap, each one taking far more time than the extra work incurred by the additional comparisons a -ary heap makes compared to a binary heap.



</doc>
<doc id="1481659" url="https://en.wikipedia.org/wiki?curid=1481659" title="Radix tree">
Radix tree

In computer science, a radix tree (also radix trie or compact prefix tree) is a data structure that represents a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent. The result is that the number of children of every internal node is at most the radix of the radix tree, where is a positive integer and a power of 2, having ≥ 1. Unlike regular trees, edges can be labeled with sequences of elements as well as single elements. This makes radix trees much more efficient for small sets (especially if the strings are long) and for sets of strings that share long prefixes.

Unlike regular trees (where whole keys are compared "en masse" from their beginning up to the point of inequality), the key at each node is compared chunk-of-bits by chunk-of-bits, where the quantity of bits in that chunk at that node is the radix of the radix trie. When the is 2, the radix trie is binary (i.e., compare that node's 1-bit portion of the key), which minimizes sparseness at the expense of maximizing trie depth—i.e., maximizing up to conflation of nondiverging bit-strings in the key. When is an integer power of 2 having ≥ 4, then the radix trie is an -ary trie, which lessens the depth of the radix trie at the expense of potential sparseness.

As an optimization, edge labels can be stored in constant size by using two pointers to a string (for the first and last elements).

Note that although the examples in this article show strings as sequences of characters, the type of the string elements can be chosen arbitrarily; for example, as a bit or byte of the string representation when using multibyte character encodings or Unicode.

Radix trees are useful for constructing associative arrays with keys that can be expressed as strings. They find particular application in the area of IP routing, where the ability to contain large ranges of values with a few exceptions is particularly suited to the hierarchical organization of IP addresses. They are also used for inverted indexes of text documents in information retrieval.

Radix trees support insertion, deletion, and searching operations. Insertion adds a new string to the trie while trying to minimize the amount of data stored. Deletion removes a string from the trie. Searching operations include (but are not necessarily limited to) exact lookup, find predecessor, find successor, and find all strings with a prefix. All of these operations are O("k") where k is the maximum length of all strings in the set, where length is measured in the quantity of bits equal to the radix of the radix trie.

 The lookup operation determines if a string exists in a trie. Most operations modify this approach in some way to handle their specific tasks. For instance, the node where a string terminates may be of importance. This operation is similar to tries except that some edges consume multiple elements.

The following pseudo code assumes that these classes exist.

Edge

Node

To insert a string, we search the tree until we can make no further progress. At this point we either add a new outgoing edge labeled with all remaining elements in the input string, or if there is already an outgoing edge sharing a prefix with the remaining input string, we split it into two edges (the first labeled with the common prefix) and proceed. This splitting step ensures that no node has more children than there are possible string elements.

Several cases of insertion are shown below, though more may exist. Note that r simply represents the root. It is assumed that edges can be labelled with empty strings to terminate strings where necessary and that the root has no incoming edge. (The lookup algorithm described above will not work when using empty-string edges.)

To delete a string x from a tree, we first locate the leaf representing x. Then, assuming x exists, we remove the corresponding leaf node. If the parent of our leaf node has only one other child, then that child's incoming label is appended to the parent's incoming label and the child is removed.


Donald R. Morrison first described what Donald Knuth, pages 498-500 in Volume III of The Art of Computer Programming, calls "Patricia's trees" in 1968. Gernot Gwehenberger independently invented and described the data structure at about the same time. PATRICIA trees are radix trees with radix equals 2, which means that each bit of the key is compared individually and each node is a two-way (i.e., left versus right) branch.

Unlike balanced trees, radix trees permit lookup, insertion, and deletion in O("k") time rather than O(log "n"). This does not seem like an advantage, since normally "k" ≥ log "n", but in a balanced tree every comparison is a string comparison requiring O("k") worst-case time, many of which are slow in practice due to long common prefixes (in the case where comparisons begin at the start of the string). In a trie, all comparisons require constant time, but it takes "m" comparisons to look up a string of length "m". Radix trees can perform these operations with fewer comparisons, and require many fewer nodes.

Radix trees also share the disadvantages of tries, however: as they can only be applied to strings of elements or elements with an efficiently reversible mapping to strings, they lack the full generality of balanced search trees, which apply to any data type with a total ordering. A reversible mapping to strings can be used to produce the required total ordering for balanced search trees, but not the other way around. This can also be problematic if a data type only provides a comparison operation, but not a (de)serialization operation.

Hash tables are commonly said to have expected O(1) insertion and deletion times, but this is only true when considering computation of the hash of the key to be a constant-time operation. When hashing the key is taken into account, hash tables have expected O("k") insertion and deletion times, but may take longer in the worst case depending on how collisions are handled. Radix trees have worst-case O("k") insertion and deletion. The successor/predecessor operations of radix trees are also not implemented by hash tables.

A common extension of radix trees uses two colors of nodes, 'black' and 'white'. To check if a given string is stored in the tree, the search starts from the top and follows the edges of the input string until no further progress can be made. If the search string is consumed and the final node is a black node, the search has failed; if it is white, the search has succeeded. This enables us to add a large range of strings with a common prefix to the tree, using white nodes, then remove a small set of "exceptions" in a space-efficient manner by "inserting" them using black nodes.

The HAT-trie is a cache-conscious data structure based on radix trees that offers efficient string storage and retrieval, and ordered iterations. Performance, with respect to both time and space, is 
comparable to the cache-conscious hashtable. See HAT trie implementation notes at 

A PATRICIA trie is a special variant of the radix 2 (binary) trie, in which rather than explicitly store every bit of every key, the nodes store only the position of the first bit which differentiates two sub-trees. During traversal the algorithm examines the indexed bit of the search key and chooses the left or right sub-tree as appropriate. Notable features of the PATRICIA trie include that the trie only requires one node to be inserted for every unique key stored, making PATRICIA much more compact than a standard binary trie. Also, since the actual keys are no longer explicitly stored it is necessary to perform one full key comparison on the indexed record in order to confirm a match. In this respect PATRICIA bears a certain resemblance to indexing using a hash table. .

The adaptive radix tree is a radix tree variant that integrates adaptive node sizes to the radix tree. One major drawback of the usual radix trees is the use of space, because it uses a constant node size in every level. The major difference between the radix tree and the adaptive radix tree is its variable size for each node based on the number of child elements, which grows while adding new entries. Hence, the adaptive radix tree leads to a better use of space without reducing its speed.

A common practice is to relax the criteria of disallowing parents with only one child in situations where the parent represents a valid key in the data set. This variant of radix tree achieves a higher space efficiency than the one which only allows internal nodes with at least two children.




</doc>
<doc id="794679" url="https://en.wikipedia.org/wiki?curid=794679" title="Suffix tree">
Suffix tree

In computer science, a suffix tree (also called PAT tree or, in an earlier form, position tree) is a compressed trie containing all the suffixes of the given text as their keys and positions in the text as their values. Suffix trees allow particularly fast implementations of many important string operations.

The construction of such a tree for the string formula_1 takes time and space linear in the length of formula_1. Once constructed, several operations can be performed quickly, for instance locating a substring in formula_1, locating a substring if a certain number of mistakes are allowed, locating matches for a regular expression pattern etc. Suffix trees also provide one of the first linear-time solutions for the longest common substring problem. These speedups come at a cost: storing a string's suffix tree typically requires significantly more space than storing the string itself.

The concept was first introduced by , which Donald Knuth subsequently characterized as "Algorithm of the Year 1973". The construction was greatly simplified by 
, and also by . Ukkonen provided the first online-construction of suffix trees, now known as Ukkonen's algorithm, with running time that matched the then fastest algorithms.
These algorithms are all linear-time for a constant-size alphabet, and have worst-case running time of formula_4 in general.

for strings drawn from an alphabet of integers in a polynomial range. Farach's algorithm has become the basis for new algorithms for constructing both suffix trees and suffix arrays, for example, in external memory, compressed, succinct, etc.

The suffix tree for the string formula_1 of length formula_6 is defined as a tree such that:

Since such a tree does not exist for all strings, formula_1 is padded with a terminal symbol not seen in the string (usually denoted codice_1). This ensures that no suffix is a prefix of another, and that there will be formula_6 leaf nodes, one for each of the formula_6 suffixes of formula_1. Since all internal non-root nodes are branching, there can be at most "n" −  1 such nodes, and "n" + ("n" − 1) + 1 = 2"n" nodes in total ("n" leaves, "n" − 1 internal non-root nodes, 1 root).

Suffix links are a key feature for older linear-time construction algorithms, although most newer algorithms, which are based on Farach's algorithm, dispense with suffix links. In a complete suffix tree, all internal non-root nodes have a suffix link to another internal node. If the path from the root to a node spells the string formula_11, where formula_12 is a single character and formula_13 is a string (possibly empty), it has a suffix link to the internal node representing formula_13. See for example the suffix link from the node for codice_2 to the node for codice_3 in the figure above. Suffix links are also used in some algorithms running on the tree.

A generalized suffix tree is a suffix tree made for a set of words instead of a single word. It represents all suffixes from this set of words. Each word must be terminated by a different termination symbol or word.

A suffix tree for a string formula_1 of length formula_6 can be built in formula_17 time, if the letters come from an alphabet of integers in a polynomial range (in particular, this is true for constant-sized alphabets).
For larger alphabets, the running time is dominated by first sorting the letters to bring them into a range of size formula_18; in general, this takes formula_4 time.
The costs below are given under the assumption that the alphabet is constant.

Assume that a suffix tree has been built for the string formula_1 of length formula_6, or that a generalised suffix tree has been built for the set of strings formula_22 of total length formula_23.
You can:



The suffix tree can be prepared for constant time lowest common ancestor retrieval between nodes in formula_17 time. One can then also:

Suffix trees can be used to solve a large number of string problems that occur in text-editing, free-text search, computational biology and other application areas. Primary applications include:

Suffix trees are often used in bioinformatics applications, searching for patterns in DNA or protein sequences (which can be viewed as long strings of characters). The ability to search efficiently with mismatches might be considered their greatest strength. Suffix trees are also used in data compression; they can be used to find repeated data, and can be used for the sorting stage of the Burrows–Wheeler transform. Variants of the LZW compression schemes use suffix trees (LZSS). A suffix tree is also used in suffix tree clustering, a data clustering algorithm used in some search engines.

If each node and edge can be represented in formula_59 space, the entire tree can be represented in formula_17 space. The total length of all the strings on all of the edges in the tree is formula_76, but each edge can be stored as the position and length of a substring of , giving a total space usage of formula_17 computer words. The worst-case space usage of a suffix tree is seen with a fibonacci word, giving the full formula_78 nodes.

An important choice when making a suffix tree implementation is the parent-child relationships between nodes. The most common is using linked lists called sibling lists. Each node has a pointer to its first child, and to the next node in the child list it is a part of. Other implementations with efficient running time properties use hash maps, sorted or unsorted arrays (with array doubling), or balanced search trees. We are interested in:

Let be the size of the alphabet. Then you have the following costs:

The insertion cost is amortised, and that the costs for hashing are given for perfect hashing.

The large amount of information in each edge and node makes the suffix tree very expensive, consuming about 10 to 20 times the memory size of the source text in good implementations. The suffix array reduces this requirement to a factor of 8 (for array including LCP values built within 32-bit address space and 8-bit characters.) This factor depends on the properties and may reach 2 with usage of 4-byte wide characters (needed to contain any symbol in some UNIX-like systems, see wchar_t) on 32-bit systems. Researchers have continued to find smaller indexing structures.

Various parallel algorithms to speed up suffix tree construction have been proposed.
Recently, a practical parallel algorithm for suffix tree construction with formula_18 work (sequential time) and formula_81 span has been developed. The algorithm achieves good parallel scalability on shared-memory multicore machines and can index the 3GB human genome in under 3 minutes using a 40-core machine.

Though linear, the memory usage of a suffix tree is significantly higher
than the actual size of the sequence collection. For a large text,
construction may require external memory approaches.

There are theoretical results for constructing suffix trees in external
memory.
The algorithm by 
is theoretically optimal, with an I/O complexity equal to that of sorting.
However the overall intricacy of this algorithm has prevented, so far, its
practical implementation.

On the other hand, there have been practical works for constructing
disk-based suffix trees
which scale to (few) GB/hours.
The state of the art methods are TDD,
TRELLIS,
DiGeST,
and
BST.

TDD and TRELLIS scale up to the entire human genome – approximately 3GB – resulting in a disk-based suffix tree of a size in the tens of gigabytes. However, these methods cannot handle efficiently collections of sequences exceeding 3GB. DiGeST performs significantly better and is able to handle collections of sequences in the order of 6GB in about 6 hours.
All these methods can efficiently build suffix trees for the case when the
tree does not fit in main memory,
but the input does.
The most recent method, BST, scales to handle
inputs that do not fit in main memory. ERA is a recent parallel suffix tree construction method that is significantly faster. ERA can index the entire human genome in 19 minutes on an 8-core desktop computer with 16GB RAM. On a simple Linux cluster with 16 nodes (4GB RAM per node), ERA can index the entire human genome in less than 9 minutes.





</doc>
<doc id="1303494" url="https://en.wikipedia.org/wiki?curid=1303494" title="Suffix array">
Suffix array

In computer science, a suffix array is a sorted array of all suffixes of a string. It is a data structure used, among others, in full text indices, data compression algorithms and within the field of bibliometrics.

Suffix arrays were introduced by as a simple, space efficient alternative to suffix trees. They had independently been discovered by Gaston Gonnet in 1987 under the name "PAT array" .

Enhanced suffix arrays (ESAs) are suffix arrays with additional tables that reproduce the full functionality of suffix trees preserving the same time and memory complexity.
The suffix array for a subset of all suffixes of a string is called sparse suffix array. Multiple probabilistic algorithms have been developed to minimize the additional memory usage including an optimal time and memory algorithm.

Let formula_3 be a string and let formula_4 denote the substring of formula_5 ranging from formula_6 to formula_7.

The suffix array formula_8 of formula_5 is now defined to be an array of integers providing the starting positions of suffixes of formula_5 in lexicographical order. This means, an entry formula_11 contains the starting position of the formula_6-th smallest suffix in formula_5 and thus for all formula_14: formula_15.

Each suffix of formula_5 shows up in formula_8 exactly once. Note suffixes are simple strings. These strings are sorted (as in a paper dictionary), before their starting positions (integer indices) are saved in formula_8.

Consider the text formula_5=codice_1 to be indexed:
The text ends with the special sentinel letter codice_2 that is unique and lexicographically smaller than any other character. The text has the following suffixes:

These suffixes can be sorted in ascending order:

The suffix array formula_8 contains the starting positions of these sorted suffixes:

The suffix array with the suffixes written out vertically underneath for clarity:

So for example, formula_21 contains the value 4, and therefore refers to the suffix starting at position 4 within formula_5, which is the suffix codice_3.

Suffix arrays are closely related to suffix trees:


It has been shown that every suffix tree algorithm can be systematically replaced with an algorithm that uses a suffix array enhanced with additional information (such as the LCP array) and solves the same problem in the same time complexity.
Advantages of suffix arrays over suffix trees include improved space requirements, simpler linear time construction algorithms (e.g., compared to Ukkonen's algorithm) and improved cache locality.

Suffix arrays were introduced by in order to improve over the space requirements of suffix trees: Suffix arrays store formula_23 integers. Assuming an integer requires formula_24 bytes, a suffix array requires formula_25 bytes in total. This is significantly less than the formula_26 bytes which are required by a careful suffix tree implementation.

However, in certain applications, the space requirements of suffix arrays may still be prohibitive. Analyzed in bits, a suffix array requires formula_27 space, whereas the original text over an alphabet of size formula_28 only requires formula_29 bits.
For a human genome with formula_30 and formula_31 the suffix array would therefore occupy about 16 times more memory than the genome itself.

Such discrepancies motivated a trend towards compressed suffix arrays and BWT-based compressed full-text indices such as the FM-index. These data structures require only space within the size of the text or even less.

A suffix tree can be built in formula_1 and can be converted into a suffix array by traversing the tree depth-first also in formula_1, so there exist algorithms that can build a suffix array in formula_1.

A naive approach to construct a suffix array is to use a comparison-based sorting algorithm. These algorithms require formula_27 suffix comparisons, but a suffix comparison runs in formula_1 time, so the overall runtime of this approach is formula_37.

More advanced algorithms take advantage of the fact that the suffixes to be sorted are not arbitrary strings but related to each other. These algorithms strive to achieve the following goals:

One of the first algorithms to achieve all goals is the SA-IS algorithm of . The algorithm is also rather simple (< 100 LOC) and can be enhanced to simultaneously construct the LCP array. The SA-IS algorithm is one of the fastest known suffix array construction algorithms. A careful implementation by Yuta Mori outperforms most other linear or super-linear construction approaches.

Beside time and space requirements, suffix array construction algorithms are also differentiated by their supported alphabet: "constant alphabets" where the alphabet size is bound by a constant, "integer alphabets" where characters are integers in a range depending on formula_23 and "general alphabets" where only character comparisons are allowed.

Most suffix array construction algorithms are based on one of the following approaches:

A well-known recursive algorithm for integer alphabets is the "DC3 / skew" algorithm of . It runs in linear time and has successfully been used as the basis for parallel and external memory suffix array construction algorithms.

Recent work by proposes an algorithm for updating the suffix array of a text that has been edited instead of rebuilding a new suffix array from scratch. Even if the theoretical worst-case time complexity is formula_27, it appears to perform well in practice: experimental results from the authors showed that their implementation of dynamic suffix arrays is generally more efficient than rebuilding when considering the insertion of a reasonable number of letters in the original text.

The suffix array of a string can be used as an index to quickly locate every occurrence of a substring pattern formula_41 within the string formula_5. Finding every occurrence of the pattern is equivalent to finding every suffix that begins with the substring. Thanks to the lexicographical ordering, these suffixes will be grouped together in the suffix array and can be found efficiently with two binary searches. The first search locates the starting position of the interval, and the second one determines the end position:
Finding the substring pattern formula_41 of length formula_44 in the string formula_5 of length formula_23 takes formula_47 time, given that a single suffix comparison needs to compare formula_44 characters. describe how this bound can be improved to formula_49 time using LCP information. The idea is that a pattern comparison does not need to re-compare certain characters, when it is already known that these are part of the longest common prefix of the pattern and the current search interval. improve the bound even further and achieve a search time of formula_50 as known from suffix trees.

Suffix sorting algorithms can be used to compute the Burrows–Wheeler transform (BWT). The BWT requires sorting of all cyclic permutations of a string. If this string ends in a special end-of-string character that is lexicographically smaller than all other character (i.e., $), then the order of the sorted rotated BWT matrix corresponds to the order of suffixes in a suffix array. The BWT can therefore be computed in linear time by first constructing a suffix array of the text and then deducing the BWT string: formula_51.

Suffix arrays can also be used to look up substrings in Example-Based Machine Translation, demanding much less storage than a full phrase table as used in Statistical machine translation.

Many additional applications of the suffix array require the LCP array. Some of these are detailed in the application section of the latter.




</doc>
<doc id="25296445" url="https://en.wikipedia.org/wiki?curid=25296445" title="Compressed suffix array">
Compressed suffix array

In computer science, a compressed suffix array is a compressed data structure for pattern matching. Compressed suffix arrays are a general class of data structure that improve on the suffix array. These data structures enable quick search for an arbitrary string with a comparatively small index.

Given a text "T" of "n" characters from an alphabet Σ, a compressed suffix array supports searching for arbitrary patterns in "T". For an input pattern "P" of "m" characters, the search time is typically O("m") or O("m" + log("n")). The space used is typically formula_1, where formula_2 is the k-th order empirical entropy of the text "T". The time and space to construct a compressed suffix array are normally .

The original instantiation of the compressed suffix array solved a long-standing open problem by showing that fast pattern matching was possible using only a linear-space data structure, namely, one proportional to the size of the text "T", which takes formula_3 bits. The conventional suffix array and suffix tree use formula_4 bits, which is substantially larger. The basis for the data structure is a recursive decomposition using the "neighbor function," which allows a suffix array to be represented by one of half its length. The construction is repeated multiple times until the resulting suffix array uses a linear number of bits. Following work showed that the actual storage space was related to the zeroth-order entropy and that the index supports self-indexing. The space bound was further improved achieving the ultimate goal of higher-order entropy; the compression is obtained by partitioning the neighbor function by high-order contexts, and compressing each partition with a wavelet tree. The space usage is extremely competitive in practice with other state-of-the-art compressors, and it also supports fast pattern matching.

The memory accesses made by compressed suffix arrays and other compressed data structures for pattern matching are typically not localized, and thus these data structures have been notoriously hard to design efficiently for use in external memory. Recent progress using geometric duality takes advantage of the block access provided by disks to speed up the I/O time significantly In addition, potentially practical search performance for a compressed suffix array in external-memory has been demonstrated.

There are several open source implementations of compressed suffix arrays available (see External Links below). Bowtie and Bowtie2 are open-source compressed suffix array implementations of read alignment for use in bioinformatics. The Succinct Data Structure Library (SDSL) is a library containing a variety of compressed data structures including compressed suffix arrays. FEMTO is an implementation of compressed suffix arrays for external memory. In addition, a variety of implementations, including the original FM-index implementations, are available from the Pizza & Chili Website (see external links).


Implementations:


</doc>
<doc id="22143928" url="https://en.wikipedia.org/wiki?curid=22143928" title="FM-index">
FM-index

In computer science, an FM-index is a compressed full-text substring index based on the Burrows-Wheeler transform, with some similarities to the suffix array. It was created by Paolo Ferragina and Giovanni Manzini, who describe it as an opportunistic data structure as it allows compression of the input text while still permitting fast substring queries. The name stands for Full-text index in Minute space.

It can be used to efficiently find the number of occurrences of a pattern within the compressed text, as well as locate the position of each occurrence. The query time, as well as the required storage space, has a sublinear complexity with respect to the size of the input data.

The original authors have devised improvements to their original approach and dubbed it "FM-Index version 2". A further improvement, the alphabet-friendly FM-index, combines the use of compression boosting and wavelet trees to significantly reduce the space usage for large alphabets.

The FM-index has found use in, among other places, bioinformatics.

Using an index is a common strategy to efficiently search a large body of text. When the text is larger than what reasonably fits within a computer's main memory, there is a need to compress not only the text but also the index. When the FM-index was introduced, there were several suggested solutions that were based on traditional compression methods and tried to solve the compressed matching problem. In contrast, the FM-index is a compressed self-index, which means that it compresses the data and indexes it at the same time.

An FM-index is created by first taking the Burrows-Wheeler transform (BWT) of the input text. For example, the BWT of the string "abracadabra$" is "ard$rcaaaabb", and here it is represented by the matrix where each row is a rotation of the text, and the rows have been sorted lexicographically. The transform corresponds to the last column labeled .

The BWT in itself allows for some compression with, for instance, move to front and Huffman encoding, but the transform has even more uses. The rows in the matrix are essentially the sorted suffixes of the text and the first column F of the matrix shares similarities with suffix arrays. How the suffix array relates to the BWT lies at the heart of the FM-index.

The operation "count" takes a pattern and returns the number of occurrences of that pattern in the original text . Since the rows of matrix are sorted, and it contains every suffix of , the occurrences of pattern will be next to each other in a single continuous range. The operation iterates backwards over the pattern. For every character in the pattern, the range that has the character as a suffix is found. For example, the count of the pattern "bra" in "abracadabra" follows these steps:

If the range becomes empty or the range boundaries cross each other before the whole pattern has been looked up, the pattern does not occur in . Because can be performed in constant time, count can complete in linear time in the length of the pattern: time.

The operation "locate" takes as input an index of a character in and returns its position in . For instance . To locate every occurrence of a pattern, first the range of character is found whose suffix is the pattern in the same way the "count" operation found the range. Then the position of every character in the range can be located.

To map an index in to one in , a subset of the indices in are associated with a position in . If has a position associated with it, is trivial. If it's not associated, the string is followed with until an associated index is found. By associating a suitable number of indices, an upper bound can be found. "Locate" can be implemented to find "occ" occurrences of a pattern in a text in time with formula_1 bits per input symbol for any .

FM index with backtracking has been successfully (>2000 citations) applied to approximate string matching/sequence alignment, See Bowtie http://bowtie-bio.sourceforge.net/index.shtml



</doc>
<doc id="3142847" url="https://en.wikipedia.org/wiki?curid=3142847" title="Generalized suffix tree">
Generalized suffix tree

In computer science, a generalized suffix tree is a suffix tree for a set of strings. Given the set of strings formula_1 of total length formula_2, it is a Patricia tree containing all formula_2 suffixes of the strings. It is mostly used in bioinformatics.

It can be built in formula_4 time and space, and can be used to find all formula_5 occurrences of a string formula_6 of length formula_7 in formula_8 time, which is asymptotically optimal (assuming the size of the alphabet is constant).

When constructing such a tree, each string should be padded with a unique out-of-alphabet marker symbol (or string) to ensure no suffix is a substring of another, guaranteeing each suffix is represented by a unique leaf node.

Algorithms for constructing a GST include Ukkonen's algorithm (1995) and McCreight's algorithm (1976).

A suffix tree for the strings codice_1 and codice_2 is shown in a figure above. They are padded with the unique terminator strings codice_3 and codice_4. The numbers in the leaf nodes are string number and starting position. Notice how a left to right traversal of the leaf nodes corresponds to the sorted order of the suffixes. The terminators might be strings or unique single symbols. Edges on codice_5 from the root are left out in this example.

An alternative to building a generalized suffix tree is to concatenate the strings, and build a regular suffix tree or suffix array for the resulting string. When hits are evaluated after a search, global positions are mapped into documents and local positions with some algorithm and/or data structure, such as a binary search in the starting/ending positions of the documents.



</doc>
<doc id="484569" url="https://en.wikipedia.org/wiki?curid=484569" title="Judy array">
Judy array

In computer science, a Judy array is a data structure implementing a type of associative array with high performance and low memory usage. Unlike most other key-value stores, Judy arrays use no hashing, leverage compression on their keys (which may be integers or strings), and can efficiently represent sparse data, that is, they may have large ranges of unassigned indices without greatly increasing memory usage or processing time. They are designed to remain efficient even on structures with sizes in the peta-element range, with performance scaling on the order of O(log "n"). Roughly speaking, Judy arrays are highly optimized 256-ary radix trees.

Judy trees are usually faster than AVL trees, B-trees, hash tables and skip lists because they are highly optimized to maximize usage of the CPU cache. In addition, they require no tree balancing and no hashing algorithm is used.

The Judy array was invented by Douglas Baskins and named after his sister.

Judy arrays are dynamic and can grow or shrink as elements are added to, or removed from, the array. The memory used by Judy arrays is nearly proportional to the number of elements in the Judy array.

Judy arrays are designed to minimize the number of expensive cache-line fills from RAM, and so the algorithm contains much complex logic to avoid cache misses as often as possible. Due to these cache optimizations, Judy arrays are fast, especially for very large datasets. On data sets that are sequential or nearly sequential, Judy arrays can even outperform hash tables, since, unlike hash tables, the internal tree structure of Judy arrays maintains the ordering of the keys.

Judy arrays are extremely complicated. The smallest implementations are thousands of lines of code. In addition, Judy arrays are optimized for machines with 64 byte cache lines, making them essentially unportable without a significant rewrite. In most applications the possible performance advantage is too small to justify the high complexity of the data structure implementation.



</doc>
<doc id="33676538" url="https://en.wikipedia.org/wiki?curid=33676538" title="Ctrie">
Ctrie

A concurrent hash-trie or Ctrie is a concurrent thread-safe lock-free implementation of a hash array mapped trie. It is used to implement the concurrent map abstraction. It has particularly scalable concurrent insert and remove operations and is memory-efficient. It is the first known concurrent data-structure that supports O(1), atomic, lock-free snapshots.

The Ctrie data structure is a non-blocking concurrent hash array mapped trie based on single-word compare-and-swap instructions in a shared-memory system. It supports concurrent lookup, insert and remove operations. Just like the hash array mapped trie, it uses the entire 32-bit space for hash values thus having low risk of hashcode collisions. Each node may branch to up to 32 sub tries. To conserve memory, each node contains a 32 bits bitmap where each bit indicates the presence of a branch followed by an array of length equal to the Hamming weight of the bitmap.

Keys are inserted by doing an atomic compare-and-swap operation on the node which needs to be modified. To ensure that updates are done independently and in a proper order, a special indirection node (an I-node) is inserted between each regular node and its subtries.

The figure above illustrates the Ctrie insert operation. Trie A is empty - an atomic CAS instruction is used to swap the old node C1 with the new version of C1 which has the new key "k1". If the CAS is not successful, the operation is restarted. If the CAS is successful, we obtain the trie B. This procedure is repeated when a new key "k2" is added (trie C). If two hashcodes of the keys in the Ctrie collide as is the case with "k2" and "k3", the Ctrie must be extended with at least one more level - trie D has a new indirection node I2 with a new node C2 which holds both colliding keys. Further CAS instructions are done on the contents of the indirection nodes I1 and I2 - such CAS instructions can be done independently of each other, thus enabling concurrent updates with less contention.

The Ctrie is defined by the pointer to the root indirection node (or a root I-node). The following types of nodes are defined for the Ctrie:

A C-node is a branching node. It typically contains up to 32 branches, so "W" above is 5. Each branch may either be a key-value pair (represented with an S-node) or another I-node. To avoid wasting 32 entries in the branching array when some branches may be empty, an integer bitmap is used to denote which bits are full and which are empty. The helper method "flagpos" is used to inspect the relevant hashcode bits for a given level and extract the value of the bit in the bitmap to see if its set or not - denoting whether there is a branch at that position or not. If there is a bit, it also computes its position in the branch array. The formula used to do this is:

Note that the operations treat only the I-nodes as mutable nodes - all other nodes are never changed after being created and added to the Ctrie.

Below is an illustration of the pseudocode of the insert operation:

The "inserted" and "updated" methods on nodes return new versions of the C-node with a value inserted or updated at the specified position, respectively. Note that the insert operation above is tail-recursive, so it can be rewritten as a while loop. Other operations are described in more detail in the original paper on Ctries.

The data-structure has been proven to be correct - Ctrie operations have been shown to have the atomicity, linearizability and lock-freedom properties. The lookup operation can be modified to guarantee wait-freedom.

Ctries have been shown to be comparable in performance with concurrent skip lists, concurrent hash tables and similar data structures in terms of the lookup operation, being slightly slower than hash tables and faster than skip lists due to the lower level of indirections. However, they are far more scalable than most concurrent hash tables where the insertions are concerned. Most concurrent hash tables are bad at conserving memory - when the keys are removed from the hash table, the underlying array is not shrunk. Ctries have the property that the allocated memory is always a function of only the current number of keys in the data-structure.

Ctries have logarithmic complexity bounds of the basic operations, albeit with a low constant factor due to the high branching level (usually 32).

Ctries support a lock-free, linearizable, constant-time snapshot operation, based on the insight obtained from persistent data structures. This is a breakthrough in concurrent data-structure design, since existing concurrent data-structures do not support snapshots. The snapshot operation allows implementing lock-free, linearizable iterator, size and clear operations - existing concurrent data-structures have implementations which either use global locks or are correct only given that there are no concurrent modifications to the data-structure. In particular, Ctries have an O(1) iterator creation operation, O(1) clear operation, O(1) duplicate operation and an amortized O(logn) size retrieval operation.

Most concurrent data structures require dynamic memory allocation, and lock-free concurrent data structures rely on garbage collection on most platforms. The current implementation of the Ctrie is written for the JVM, where garbage collection is provided by the platform itself. While it's possible to keep a concurrent memory pool for the nodes shared by all instances of Ctries in an application or use reference counting to properly deallocate nodes, the only implementation so far to deal with manual memory management of nodes used in Ctries is the common-lisp implementation cl-ctrie, which implements several stop-and-copy and mark-and-sweep garbage collection techniques for persistent, memory-mapped storage. Hazard pointers are another possible solution for a correct manual management of removed nodes. Such a technique may be viable for managed environments as well, since it could lower the pressure on the GC. A Ctrie implementation in Rust makes use of hazard pointers for this purpose.

A Ctrie implementation for Scala 2.9.x is available on GitHub. It is a mutable thread-safe implementation which ensures progress and supports lock-free, linearizable, O(1) snapshots.


Ctries were first described in 2011 by Aleksandar Prokopec. To quote the author:

"Ctrie is a non-blocking concurrent shared-memory hash trie based on single-word compare-and-swap instructions. Insert, lookup and remove operations modifying different parts of the hash trie can be run independent of each other and do not contend. Remove operations ensure that the unneeded memory is freed and that the trie is kept compact."

In 2012, a revised version of the Ctrie data structure was published, simplifying the data structure and introducing an optional constant-time, lock-free, atomic snapshot operation.


</doc>
<doc id="38022094" url="https://en.wikipedia.org/wiki?curid=38022094" title="Directed acyclic word graph">
Directed acyclic word graph

Directed acyclic word graph (DAWG) may refer to two related, but distinct, automata constructions in computer science:



</doc>
<doc id="3025819" url="https://en.wikipedia.org/wiki?curid=3025819" title="Ternary search tree">
Ternary search tree

In computer science, a ternary search tree is a type of trie (sometimes called a "prefix tree") where nodes are arranged in a manner similar to a binary search tree, but with up to three children rather than the binary tree's limit of two. Like other prefix trees, a ternary search tree can be used as an associative map structure with the ability for incremental string search. However, ternary search trees are more space efficient compared to standard prefix trees, at the cost of speed. Common applications for ternary search trees include spell-checking and auto-completion.

Each node of a ternary search tree stores a single character, an object (or a pointer to an object depending on implementation), and pointers to its three children conventionally named "equal kid", "lo kid" and "hi kid", which can also be referred respectively as "middle (child)", "lower (child)" and "higher (child)". A node may also have a pointer to its parent node as well as an indicator as to whether or not the node marks the end of a word. The "lo kid" pointer must point to a node whose character value is "less than the current node". The "hi kid" pointer must point to a node whose character is "greater than the current node". The "equal kid" points to the next character in the word.
The figure below shows a ternary search tree with the strings "cute","cup","at","as","he","us" and "i":

As with other trie data structures, each node in a ternary search tree represents a prefix of the stored strings. All strings in the middle subtree of a node start with that prefix.

Inserting a value into a ternary search can be defined recursively much as lookups are defined. This recursive method is continually called on nodes of the tree given a key which gets progressively shorter by pruning characters off the front of the key. If this method reaches a node that has not been created, it creates the node and assigns it the character value of the first character in the key. Whether a new node is created or not, the method checks to see if the first character in the string is greater than or less than the character value in the node and makes a recursive call on the appropriate node as in the lookup operation. If, however, the key's first character is equal to the node's value then the insertion procedure is called on the equal kid and the key's first character is pruned away. Like binary search trees and other data structures, ternary search trees can become degenerate depending on the order of the keys. Inserting keys in alphabetical order is one way to attain the worst possible degenerate tree. Inserting the keys in random order often produces a well-balanced tree.

To look up a particular node or the data associated with a node, a string key is needed. A lookup procedure begins by checking the root node of the tree and determining which of the following conditions has occurred. If the first character of the string is less than the character in the root node, a recursive lookup can be called on the tree whose root is the lo kid of the current root. Similarly, if the first character is greater than the current node in the tree, then a recursive call can be made to the tree whose root is the hi kid of the current node.
As a final case, if the first character of the string is equal to the character of the current node then the function returns the node if there are no more characters in the key. If there are more characters in the key then the first character of the key must be removed and a recursive call is made given the equal kid node and the modified key.
This can also be written in a non-recursive way by using a pointer to the current node and a pointer to the current character of the key.

 function search("string" query) {

The running time of ternary search trees varies significantly with the input. Ternary search trees run best when given several "similar strings", especially when those strings "share a common prefix". Alternatively, ternary search trees are effective when storing a large number of relatively "short strings" (such as words in a dictionary).
Running times for ternary search trees are similar to binary search trees, in that they typically run in logarithmic time, but can run in linear time in the degenerate (worst) case.

Time complexities for ternary search tree operations:

While being slower than other prefix trees, ternary search trees can be better suited for larger data sets due to their space-efficiency.

Hashtables can also be used in place of ternary search trees for mapping strings to values. However, hash maps also frequently use more memory than ternary search trees (but not as much as tries). Additionally, hash maps are typically slower at reporting a string that is not in the same data structure, because it must compare the entire string rather than just the first few characters. There is some evidence that shows ternary search trees running faster than hash maps. Additionally, hash maps do not allow for many of the uses of ternary search trees, such as "near-neighbor lookups".

If storing dictionary words is all that is required (i.e., storage of information auxiliary to each word is not required), a minimal deterministic acyclic finite state automaton (DAFSA) would use less space than a trie or a ternary search tree. This is because a DAFSA can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored.

Ternary search trees can be used to solve many problems in which a large number of strings must be stored and retrieved in an arbitrary order. Some of the most common or most useful of these are below:





</doc>
<doc id="12379384" url="https://en.wikipedia.org/wiki?curid=12379384" title="And–or tree">
And–or tree

An and–or tree is a graphical representation of the reduction of problems (or goals) to conjunctions and disjunctions of subproblems (or subgoals).

The and-or tree:

represents the search space for solving the problem P, using the goal-reduction methods:

Given an initial problem P and set of problem solving methods of the form:

the associated and-or tree is a set of labelled nodes such that: 


A node N, labelled by a problem P, is a success node if there is a method of the form P if nothing (i.e., P is a "fact"). The node is a failure node if there is no method for solving P.

If all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node.

An and-or tree specifies only the search space for solving a problem. Different search strategies for searching the space are possible. These include searching the tree depth-first, breadth-first, or best-first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel.

The methods used for generating and-or trees are propositional logic programs (without variables). In the case of logic programs containing variables, the solutions of conjoint sub-problems must be compatible. Subject to this complication, sequential and parallel search strategies for and-or trees provide a computational model for executing logic programs.

And–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves. 
For each of these children nodes, there exists a set of non-conjoint children nodes, corresponding to all of the player's defending moves.

For solving game trees with proof-number search family of algorithms, game trees are to be mapped to and-or trees. MAX-nodes (i.e. maximizing player to move) are represented as OR nodes, MIN-nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is "player to move wins the game".



</doc>
<doc id="4436119" url="https://en.wikipedia.org/wiki?curid=4436119" title="(a,b)-tree">
(a,b)-tree

In computer science, an (a,b) tree is a kind of balanced search tree.

An (a,b)-tree has all of its leaves at the same depth, and all internal nodes except for the root have between and children, where and are integers such that . The root has, if it is not a leaf, between 2 and children.

Let , be positive integers such that . Then a rooted tree is an (a,b)-tree when:

Every internal node of a (a,b)-tree has the following representation:



</doc>
<doc id="9328337" url="https://en.wikipedia.org/wiki?curid=9328337" title="Link/cut tree">
Link/cut tree

A link/cut tree is a data structure for representing a forest, a set of rooted trees, and offers the following operations:


The represented forest may consist of very deep trees, so if we represent the forest as a plain collection of parent pointer trees, it might take us a long time to find the root of a given node. However, if we represent each tree in the forest as a link/cut tree, we can find which tree an element belongs to in O(log(n)) amortized time. Moreover, we can quickly adjust the collection of link/cut trees to changes in the represented forest. In particular, we can adjust it to merge (link) and split (cut) in O(log(n)) amortized time.

Link/cut trees divide each tree in the represented forest into vertex-disjoint paths, where each path is represented by an auxiliary data structure (often splay trees, though the original paper predates splay trees and thus uses biased binary search trees). The nodes in the auxiliary data structure are ordered by their depth in the corresponding represented tree. In one variation, "Naive Partitioning", the paths are determined by the most recently accessed paths and nodes, similar to Tango Trees. In "Partitioning by Size" paths are determined by the heaviest child (child with the most children) of the given node. This gives a more complicated structure, but reduces the cost of the operations from amortized O(log n) to worst case O(log n). It has uses in solving a variety of network flow problems and to jive data sets.

In the original publication, Sleator and Tarjan referred to link/cut trees as “dynamic trees”, or "dynamic dyno trees".

We take a tree where each node has an arbitrary degree of unordered nodes and split it into paths. We call this the "represented tree". These paths are represented internally by auxiliary trees (here we will use splay trees), where the nodes from left to right represent the path from root to the last node on the path. Nodes that are connected in the represented tree that are not on the same preferred path (and therefore not in the same auxiliary tree) are connected via a "path-parent pointer". This pointer is stored in the root of the auxiliary tree representing the path.

When an access to a node "v" is made on the "represented tree", the path that is taken becomes the preferred path. The preferred child of a node is the last child that was on the access path, or null if the last access was to "v" or if no accesses were made to this particular branch of the tree. A preferred edge is the edge that connects the preferred child to "v".

In an alternate version, preferred paths are determined by the heaviest child.

The operations we are interested in are FindRoot(Node v), Cut(Node v), Link(Node v, Node w), and Path(Node v). 
Every operation is implemented using the Access(Node v) subroutine. When we "access" a vertex "v", the preferred path of the represented tree is changed to a path from the root "R" of the represented tree to the node "v". If a node on
the access path previously had a preferred child "u", and the path now goes to child "w", the old "preferred edge"
is deleted (changed to a "path-parent pointer"), and the new path now goes through "w".

After performing an access to node "v", it will no longer have any preferred children, and will be at the end of the path. Since nodes in the auxiliary tree are keyed by depth, this means that any nodes to the right of "v" in the auxiliary tree must be disconnected. In a splay tree this is a relatively simple procedure; we splay at "v", which brings "v" to the root of the auxiliary tree. We then disconnect the right subtree of "v", which is every node that came below it on the previous preferred path. The root of the disconnected tree will have a path-parent pointer, which we point to "v".

We now walk up the represented tree to the root "R", breaking and resetting the preferred path where necessary. To do this we follow the path-parent pointer from "v" (since "v" is now the root, we have direct access to the path-parent pointer). If the path that "v" is on already contains the root "R" (since the nodes are keyed by depth, it would be the left most node in the auxiliary tree), the path-parent pointer will be null, and we are done the access. Otherwise we follow the pointer to some node on another path "w". We want to break the old preferred path of "w" and reconnect it to the path "v" is on. To do this we splay at "w", and disconnect its right subtree, setting its path-parent pointer to "w". Since all nodes are keyed by depth, and every node in the path of "v" is deeper than every node in the path of "w" (since they are children of "w" in the represented tree), we simply connect the tree of "v" as the right child of "w". We splay at "v" again, which, since "v" is a child of the root "w", simply rotates "v" to root. We repeat this entire process until the path-parent pointer of "v" is null, at which point it is on the same preferred path as the root of the represented tree "R".

FindRoot refers to finding the root of the represented tree that contains the node "v". Since the "access" subroutine puts "v" on the preferred path, we first execute an access. Now the node "v" is on the same preferred path, and thus the same auxiliary tree as the root "R". Since the auxiliary trees are keyed by depth, the root "R" will be the leftmost node of the auxiliary tree. So we simply choose the left child of "v" recursively until we can go no further, and this node is the
root "R". The root may be linearly deep (which is worst case for a splay tree), we therefore splay it so that the next access will be quick.

Here we would like to cut the represented tree at node "v". First we access "v". This puts all the elements lower than "v" in the represented tree as the right child of "v" in the auxiliary tree. All the elements now in the left subtree of "v" are the nodes higher than "v" in the represented tree. We therefore disconnect the left child of "v" (which still maintains an attachment to the original represented tree through its path-parent pointer). Now "v" is the root of a represented tree. Accessing "v" breaks the preferred path below "v" as well, but that subtree maintains its connection to "v" through its path-parent pointer.

If "v" is a tree root and "w" is a vertex in another tree, link the trees
containing "v" and "w" by adding the edge(v, w), making "w" the parent of "v".
To do this we access both "v" and "w" in their respective trees, and make "w" the left
child of "v". Since "v" is the root, and nodes are keyed by depth in the auxiliary tree, accessing "v" means
that "v" will have no left child in the auxiliary tree (since as root it is the minimum depth). Adding "w" as a left
child effectively makes it the parent of "v" in the represented tree.

For this operation we wish to do some aggregate function over all the nodes (or edges) on the path from root "R" to node "v" (such as "sum" or "min" or "max" or "increase", etc.). To do this we access "v", which gives us an auxiliary tree with all the nodes on the path from root "R" to node "v". The data structure can be augmented with data we wish to retrieve, such as min or max values, or the sum of the costs in the subtree, which can then be returned from a given path in constant time.

Cut and link have O(1) cost, plus that of the access. FindRoot has an O(log n) amortized upper bound, plus the cost of the access. The data structure can be augmented with additional information (such as the min or max valued node in its subtrees, or the sum), depending on the implementation. Thus Path can return this information in constant time plus the access bound.

So it remains to bound the "access" to find our running time.

Access makes use of splaying, which we know has an O(log n) amortized upper bound. So the remaining analysis deals with the number of times we need to splay. This is equal to the number of preferred child changes (the number of edges changed in the preferred path) as we traverse up the tree.

We bound "access" by using a technique called Heavy-Light Decomposition.

This technique calls an edge heavy or light depending on the number of nodes in the subtree. represents the number of nodes in the subtree of "v" in the represented tree. An edge is called "heavy" if size(v) > size(parent(v)). Thus we can see that each node can have at most 1 "heavy" edge. An edge that is not a "heavy" edge is referred to as a "light" edge.

The "light-depth" refers to the number of light edges on a given path from root to vertex "v". "Light-depth" ≤ lg "n" because each time we traverse a light-edge we decrease the number of nodes by at least a factor of 2 (since it can have at most half the nodes of the parent).

So a given edge in the represented tree can be any of four possibilities: "heavy-preferred", "heavy-unpreferred", "light-preferred" or "light-unpreferred".

First we prove an formula_1 upper bound.

The splay operation of the access gives us log "n", so we need to bound the number of accesses to log "n" to prove the "O"(log "n") upper bound.
Every change of preferred edge results in a new preferred edge being formed. So we count the number of preferred edges formed. Since there are at most log "n" edges that are light on any given path, there are at most log "n" light edges changing to preferred.

The number of heavy edges becoming preferred can be for any given operation, but it is amortized. Over a series of executions we can have "n"-1 heavy edges become preferred (as there are at most "n"-1 heavy edges total in the represented tree), but from then on the number of heavy edges that become preferred is equal to the number of heavy edges that became unpreferred on a previous step. For every heavy edge that becomes unpreferred a light edge must become preferred. We have seen already that the number of light edges that can become preferred is at most log "n". So the number of heavy edges that become preferred for "m" operations is . Over enough operations () this averages to .

We have bound the number of preferred child changes at , so if we can show that each preferred child change has cost O(1) amortized we can bound the "access" operation at . This is done using the potential method.

Let s(v) be the number of nodes under "v" in the tree of auxiliary trees. Then the potential function formula_2. We know that the amortized cost of splaying is bounded by:

We know that after splaying, "v" is the child of its path-parent node "w". So we know that:

We use this inequality and the amortized cost of access to achieve a telescoping sum that is bounded by:

where "R" is the root of the represented tree, and we know the number of preferred child changes is . "s"("R") = "n", so we have amortized.

Link/cut trees can be used to solve the dynamic connectivity problem for acyclic graphs. Given two nodes x and y, they are connected if and only if FindRoot(x) = FindRoot(y). Another data structure that can be used for the same purpose is Euler tour tree.

In solving the maximum flow problem, link/cut trees can be used to improve the running time of Dinic's algorithm from formula_6 to formula_7.




</doc>
<doc id="11220797" url="https://en.wikipedia.org/wiki?curid=11220797" title="SPQR tree">
SPQR tree

In graph theory, a branch of mathematics, the triconnected components of a biconnected graph are a system of smaller graphs that describe all of the 2-vertex cuts in the graph. An SPQR tree is a tree data structure used in computer science, and more specifically graph algorithms, to represent the triconnected components of a graph. The SPQR tree of a graph may be constructed in linear time and has several applications in dynamic graph algorithms and graph drawing.

The basic structures underlying the SPQR tree, the triconnected components of a graph, and the connection between this decomposition and the planar embeddings of a planar graph, were first investigated by ; these structures were used in efficient algorithms by several other researchers prior to their formalization as the SPQR tree by .

An SPQR tree takes the form of an unrooted tree in which for each node "x" there is associated an undirected graph or multigraph "G". The node, and the graph associated with it, may have one of four types, given the initials SPQR:
Each edge "xy" between two nodes of the SPQR tree is associated with two directed "virtual edges", one of which is an edge in "G" and the other of which is an edge in "G". Each edge in a graph "G" may be a virtual edge for at most one SPQR tree edge.

An SPQR tree "T" represents a 2-connected graph "G", formed as follows. Whenever SPQR tree edge "xy" associates the virtual edge "ab" of "G" with the virtual edge "cd" of "G", form a single larger graph by merging "a" and "c" into a single supervertex, merging "b" and "d" into another single supervertex, and deleting the two virtual edges. That is, the larger graph is the 2-clique-sum of "G" and "G". Performing this gluing step on each edge of the SPQR tree produces the graph "G"; the order of performing the gluing steps does not affect the result. Each vertex in one of the graphs "G" may be associated in this way with a unique vertex in "G", the supervertex into which it was merged.

Typically, it is not allowed within an SPQR tree for two S nodes to be adjacent, nor for two P nodes to be adjacent, because if such an adjacency occurred the two nodes could be merged into a single larger node. With this assumption, the SPQR tree is uniquely determined from its graph. When a graph "G" is represented by an SPQR tree with no adjacent P nodes and no adjacent S nodes, then the graphs "G" associated with the nodes of the SPQR tree are known as the triconnected components of "G".

The SPQR tree of a given 2-vertex-connected graph can be constructed in linear time.

The problem of constructing the triconnected components of a graph was first solved in linear time by . Based on this algorithm, suggested that the full SPQR tree structure, and not just the list of components, should be constructible in linear time. After an implementation of a slower algorithm for SPQR trees was provided as part of the GDToolkit library, provided the first linear-time implementation. As part of this process of implementing this algorithm, they also corrected some errors in the earlier work of .

The algorithm of includes the following overall steps.

To find the split components, use depth-first search to find a structure that they call a palm tree; this is a depth-first search tree with its edges oriented away from the root of the tree, for the edges belonging to the tree, and towards the root for all other edges. They then find a special preorder numbering of the nodes in the tree, and use certain patterns in this numbering to identify pairs of vertices that can separate the graph into smaller components. When a component is found in this way, a stack data structure is used to identify the edges that should be part of the new component.

With the SPQR tree of a graph "G" (without Q nodes) it is straightforward to find every pair of vertices "u" and "v" in "G" such that removing "u" and "v" from "G" leaves a disconnected graph, and the connected components of the remaining graphs:

If a planar graph is 3-connected, it has a unique planar embedding up to the choice of which face is the outer face and of orientation of the embedding: the faces of the embedding are exactly the nonseparating cycles of the graph. However, for a planar graph (with labeled vertices and edges) that is 2-connected but not 3-connected, there may be greater freedom in finding a planar embedding. Specifically, whenever two nodes in the SPQR tree of the graph are connected by a pair of virtual edges, it is possible to flip the orientation of one of the nodes (replacing it by its mirror image) relative to the other one. Additionally, in a P node of the SPQR tree, the different parts of the graph connected to virtual edges of the P node may be arbitrarily permuted. All planar representations may be described in this way.





</doc>
<doc id="1037551" url="https://en.wikipedia.org/wiki?curid=1037551" title="Disjoint-set data structure">
Disjoint-set data structure

In computer science, a disjoint-set data structure (also called a union–find data structure or merge–find set) is a data structure that tracks a set of elements partitioned into a number of disjoint (non-overlapping) subsets. It provides near-constant-time operations (bounded by the inverse Ackermann function) to add new sets, to merge existing sets, and to determine whether elements are in the same set. In addition to many other uses (see the Applications section), disjoint-sets play a key role in Kruskal's algorithm for finding the minimum spanning tree of a graph.

Disjoint-set forests were first described by Bernard A. Galler and Michael J. Fischer in 1964. In 1973, their time complexity was bounded to formula_1, the iterated logarithm of formula_2, by Hopcroft and Ullman. (A proof is available here.) In 1975, Robert Tarjan was the first to prove the formula_3 (inverse Ackermann function) upper bound on the algorithm's time complexity, and, in 1979, showed that this was the lower bound for a restricted case. In 1989, Fredman and Saks showed that formula_4 (amortized) words must be accessed by "any" disjoint-set data structure per operation, thereby proving the optimality of the data structure.

In 1991, Galil and Italiano published a survey of data structures for disjoint-sets.

In 1994, Richard J. Anderson and Heather Woll described a parallelized version of Union–Find that never needs to block.

In 2007, Sylvain Conchon and Jean-Christophe Filliâtre developed a persistent version of the disjoint-set forest data structure, allowing previous versions of the structure to be efficiently retained, and formalized its correctness using the proof assistant Coq. However, the implementation is only asymptotic if used ephemerally or if the same version of the structure is repeatedly used with limited backtracking.

A disjoint-set forest consists of a number of elements each of which stores an id, a parent pointer, and, in efficient algorithms, either a size or a "rank" value.

The parent pointers of elements are arranged to form one or more trees, each representing a set. If an element's parent pointer points to no other element, then the element is the root of a tree and is the representative member of its set. A set may consist of only a single element. However, if the element has a parent, the element is part of whatever set is identified by following the chain of parents upwards until a representative element (one without a parent) is reached at the root of the tree.

Forests can be represented compactly in memory as arrays in which parents are indicated by their array index.

The "MakeSet" operation makes a new set by creating a new element with a unique id, a rank of 0, and a parent pointer to itself. The parent pointer to itself indicates that the element is the representative member of its own set.

The "MakeSet" operation has time complexity, so initializing n sets has time complexity.

Pseudocode:

"Find(x)" follows the chain of parent pointers from up the tree until it reaches a root element, whose parent is itself. This root element is the representative member of the set to which "x" belongs, and may be "x" itself.

"Path compression" flattens the structure of the tree by making every node point to the root whenever "Find" is used on it. This is valid, since each element visited on the way to a root is part of the same set. The resulting flatter tree speeds up future operations not only on these elements, but also on those referencing them.

Tarjan and Van Leeuwen also developed one-pass "Find" algorithms that are more efficient in practice while retaining the same worst-case complexity: path splitting and path halving.

"Path halving" makes every other node on the path point to its grandparent.

"Path splitting" makes every node on the path point to its grandparent.

Path compression can be implemented using iteration by first finding the root then updating the parents:

Path splitting can be represented without multiple assignment (where the right hand side is evaluated first):

or

"Union(x,y)" uses "Find" to determine the roots of the trees "x" and "y" belong to. If the roots are distinct, the trees are combined by attaching the root of one to the root of the other. If this is done naively, such as by always making "x" a child of "y", the height of the trees can grow as formula_5. To prevent this "union by rank" or "union by size" is used.

"Union by rank" always attaches the shorter tree to the root of the taller tree. Thus, the resulting tree is no taller than the originals unless they were of equal height, in which case the resulting tree is taller by one node. 

To implement "union by rank", each element is associated with a rank. Initially a set has one element and a rank of zero. If two sets are unioned and have the same rank, the resulting set's rank is one larger; otherwise, if two sets are unioned and have different ranks, the resulting set's rank is the larger of the two. Ranks are used instead of height or depth because path compression will change the trees' heights over time.

"Union by size" always attaches the tree with fewer elements to the root of the tree having more elements.

Without "path compression" (or a variant), "union by rank", or "union by size", the height of trees can grow unchecked as formula_5, implying that "Find" and "Union" operations will take formula_5 time.

Using "path compression" alone gives a worst-case running time of formula_8, for a sequence of "MakeSet" operations (and hence at most formula_9 "Union" operations) and "Find" operations.

Using "union by rank" alone gives a running-time of formula_10 (tight bound) for operations of any sort of which are "MakeSet" operations.

Using both "path compression, splitting, or halving and "union by rank or size ensures that the amortized time per operation is only formula_3, which is optimal, where formula_12 is the inverse Ackermann function. This function has a value formula_13 for any value of that can be written in this physical universe, so the disjoint-set operations take place in essentially constant time.

Disjoint-set data structures model the partitioning of a set, for example to keep track of the connected components of an undirected graph. This model can then be used to determine whether two vertices belong to the same component, or whether adding an edge between them would result in a cycle. The Union–Find algorithm is used in high-performance implementations of unification.

This data structure is used by the Boost Graph Library to implement its Incremental Connected Components functionality. It is also a key component in implementing Kruskal's algorithm to find the minimum spanning tree of a graph.

Note that the implementation as disjoint-set forests doesn't allow the deletion of edges, even without path compression or the rank heuristic.

Sharir and Agarwal report connections between the worst-case behavior of disjoint-sets and the length of Davenport–Schinzel sequences, a combinatorial structure from computational geometry.




</doc>
<doc id="1676608" url="https://en.wikipedia.org/wiki?curid=1676608" title="Space partitioning">
Space partitioning

In geometry, space partitioning is the process of dividing a space (usually a Euclidean space) into two or more disjoint subsets (see also partition of a set). In other words, space partitioning divides a space into non-overlapping regions. Any point in the space can then be identified to lie in exactly one of the regions.

Space-partitioning systems are often hierarchical, meaning that a space (or a region of space) is divided into several regions, and then the same space-partitioning system is recursively applied to each of the regions thus created. The regions can be organized into a tree, called a space-partitioning tree.

Most space-partitioning systems use planes (or, in higher dimensions, hyperplanes) to divide space: points on one side of the plane form one region, and points on the other side form another. Points exactly on the plane are usually arbitrarily assigned to one or the other side. Recursively partitioning space using planes in this way produces a BSP tree, one of the most common forms of space partitioning.

Space partitioning is particularly important in computer graphics, especially heavily used in ray tracing, where it is frequently used to organize the objects in a virtual scene. A typical scene may contain millions of polygons. Performing a ray/polygon intersection test with each would be a very computationally expensive task. 

Storing objects in a space-partitioning data structure ("k"-d tree or BSP tree for example) makes it easy and fast to perform certain kinds of geometry queries—for example in determining whether a ray intersects an object, space partitioning can reduce the number of intersection test to just a few per primary ray, yielding a logarithmic time complexity with respect to the number of polygons.

Space partitioning is also often used in scanline algorithms to eliminate the polygons out of the camera's viewing frustum, limiting the number of polygons processed by the pipeline. There is also a usage in collision detection: determining whether two objects are close to each other can be much faster using space partitioning.

In integrated circuit design, an important step is design rule check. This step ensures that the completed design is manufacturable. The check involves rules that specify widths and spacings and other geometry patterns. A modern design can have billions of polygons that represent wires and transistors. Efficient checking relies heavily on geometry query. For example, a rule may specify that any polygon must be at least "n" nanometers from any other polygon. This is converted into a geometry query by enlarging a polygon by "n/2" at all sides and query to find all intersecting polygons.

The number of components in a space partition plays a central role in some results in probability theory. See Growth function for more details.

Common space-partitioning systems include:

Suppose the n-dimensional Euclidean space is partitioned by formula_1 hyperplanes that are formula_2-dimensional. What is the number of components in the partition? The largest number of components is attained when the hyperplanes are in general position, i.e, no two are parallel and no three have the same intersection. Denote this maximum number of components by formula_3. Then, the following recurrence relation holds:
And its solution is:
which is upper-bounded as:



</doc>
<doc id="73613" url="https://en.wikipedia.org/wiki?curid=73613" title="Binary space partitioning">
Binary space partitioning

In computer science, binary space partitioning (BSP) is a method for recursively subdividing a space into two convex sets by using hyperplanes as partitions. This process of subdividing gives rise to a representation of objects within the space in the form of a tree data structure known as a BSP tree.

Binary space partitioning was developed in the context of 3D computer graphics in 1969, where the structure of a BSP tree allows for spatial information about the objects in a scene that is useful in rendering, such as objects being ordered from front-to-back with respect to a viewer at a given location, to be accessed rapidly. Other applications of BSP include: performing geometrical operations with shapes (constructive solid geometry) in CAD, collision detection in robotics and 3D video games, ray tracing, and other applications that involve the handling of complex spatial scenes.

In 1993, "Doom" was the first video game to make use of BSP after John Carmack utilized the most efficient 1991 algorithms describing front-to-back rendering with the use of a specialized data structure to record parts of the screen that had been drawn already. Prior to that, "Wolfenstein 3D" had made use of ray casting. "Quake" utilized developments in 1992 regarding a pre-processing step that generated "potentially" visible sets.

Binary space partitioning is a generic process of recursively dividing a scene into two until the partitioning satisfies one or more requirements. It can be seen as a generalisation of other spatial tree structures such as "k"-d trees and quadtrees, one where hyperplanes that partition the space may have any orientation, rather than being aligned with the coordinate axes as they are in "k"-d trees or quadtrees. When used in computer graphics to render scenes composed of planar polygons, the partitioning planes are frequently chosen to coincide with the planes defined by polygons in the scene.

The specific choice of partitioning plane and criterion for terminating the partitioning process varies depending on the purpose of the BSP tree. For example, in computer graphics rendering, the scene is divided until each node of the BSP tree contains only polygons that can render in arbitrary order. When back-face culling is used, each node therefore contains a convex set of polygons, whereas when rendering double-sided polygons, each node of the BSP tree contains only polygons in a single plane. In collision detection or ray tracing, a scene may be divided up into primitives on which collision or ray intersection tests are straightforward.

Binary space partitioning arose from the computer graphics need to rapidly draw three-dimensional scenes composed of polygons. A simple way to draw such scenes is the painter's algorithm, which produces polygons in order of distance from the viewer, back to front, painting over the background and previous polygons with each closer object. This approach has two disadvantages: time required to sort polygons in back to front order, and the possibility of errors in overlapping polygons. Fuchs and co-authors showed that constructing a BSP tree solved both of these problems by providing a rapid method of sorting polygons with respect to a given viewpoint (linear in the number of polygons in the scene) and by subdividing overlapping polygons to avoid errors that can occur with the painter's algorithm. A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming. Typically, it is therefore performed once on static geometry, as a pre-calculation step, prior to rendering or other realtime operations on a scene. The expense of constructing a BSP tree makes it difficult and inefficient to directly implement moving objects into a tree.

BSP trees are often used by 3D video games, particularly first-person shooters and those with indoor environments. Game engines using BSP trees include the Doom (id Tech 1), Quake (id Tech 2 variant), GoldSrc and Source engines. In them, BSP trees containing the static geometry of a scene are often used together with a Z-buffer, to correctly merge movable objects such as doors and characters onto the background scene. While binary space partitioning provides a convenient way to store and retrieve spatial information about polygons in a scene, it does not solve the problem of visible surface determination.

The canonical use of a BSP tree is for rendering polygons (that are double-sided, that is, without back-face culling) with the painter's algorithm. Each polygon is designated with a front side and a back side which could be chosen arbitrarily and only affects the structure of the tree but not the required result. Such a tree is constructed from an unsorted list of all the polygons in a scene. The recursive algorithm for construction of a BSP tree from that list of polygons is:

The following diagram illustrates the use of this algorithm in converting a list of lines or polygons into a BSP tree. At each of the eight steps (i.-viii.), the algorithm above is applied to a list of lines, and one new node is added to the tree.

The final number of polygons or lines in a tree is often larger (sometimes much larger) than the original list, since lines or polygons that cross the partitioning plane must be split into two. It is desirable to minimize this increase, but also to maintain reasonable balance in the final tree. The choice of which polygon or line is used as a partitioning plane (in step 1 of the algorithm) is therefore important in creating an efficient BSP tree.

A BSP tree is traversed in a linear time, in an order determined by the particular function of the tree. Again using the example of rendering double-sided polygons using the painter's algorithm, to draw a polygon "P" correctly requires that all polygons behind the plane "P" lies in must be drawn first, then polygon "P", then finally the polygons in front of "P". If this drawing order is satisfied for all polygons in a scene, then the entire scene renders in the correct order. This procedure can be implemented by recursively traversing a BSP tree using the following algorithm. From a given viewing location "V", to render a BSP tree,
Applying this algorithm recursively to the BSP tree generated above results in the following steps:

The tree is traversed in linear time and renders the polygons in a far-to-near ordering ("D1", "B1", "C1", "A", "D2", "B2", "C2", "D3") suitable for the painter's algorithm.







</doc>
<doc id="13682464" url="https://en.wikipedia.org/wiki?curid=13682464" title="Segment tree">
Segment tree

In computer science, a segment tree also known as a statistic tree is a tree data structure used for storing information about intervals, or segments. It allows querying which of the stored segments contain a given point. It is, in principle, a static structure; that is, it's a structure that cannot be modified once it's built. A similar data structure is the interval tree.

A segment tree for a set of "n" intervals uses "O"("n" log "n") storage and can be built in "O"("n" log "n") time. Segment trees support searching for all the intervals that contain a query point in "O"(log "n" + "k"), "k" being the number of retrieved intervals or segments.

Applications of the segment tree are in the areas of computational geometry, and geographic information systems.

The segment tree can be generalized to higher dimension spaces.

"This section describes the structure of a segment tree in a one-dimensional space."

Let "S" be a set of intervals, or segments. Let "p", "p", ..., "p" be the list of distinct interval endpoints, sorted from left to right. Consider the partitioning of the real line induced by those points. The regions of this partitioning are called "elementary intervals". Thus, the elementary intervals are, from left to right:

That is, the list of elementary intervals consists of open intervals between two consecutive endpoints "p" and "p", alternated with closed intervals consisting of a single endpoint. Single points are treated themselves as intervals because the answer to a query is not necessarily the same at the interior of an elementary interval and its endpoints.

Given a set of intervals, or segments, a segment tree "T" for is structured as follows:

"This section analyzes the storage cost of a segment tree in a one-dimensional space."

A segment tree "T" on a set of "n" intervals uses "O"("n" log "n") storage.

"This section describes the construction of a segment tree in a one-dimensional space."

A segment tree from the set of segments , can be built as follows. First, the endpoints of the intervals in are sorted. The elementary intervals are obtained from that. Then, a balanced binary tree is built on the elementary intervals, and for each node "v" it is determined the interval Int("v") it represents. It remains to compute the canonical subsets for the nodes. To achieve this, the intervals in are inserted one by one into the segment tree. An interval "X" = ["x", "x′"] can be inserted in a subtree rooted at "T", using the following procedure:
The complete construction operation takes "O"("n" log "n") time, "n" being the number of segments in .
"This section describes the query operation of a segment tree in a one-dimensional space."

A query for a segment tree, receives a point "q"(should be one of the leaves of tree), and retrieves a list of all the segments stored which contain the point "q".

Formally stated; given a node (subtree) "v" and a query point "q", the query can be done using the following algorithm:

In a segment tree that contains "n" intervals, those containing a given query point can be reported in "O"(log "n" + "k") time, where "k" is the number of reported intervals.
The segment tree can be generalized to higher dimension spaces, in the form of multi-level segment trees. In higher dimensional versions, the segment tree stores a collection of axis-parallel (hyper-)rectangles, and can retrieve the rectangles that contain a given query point. The structure uses "O"("n" log "n") storage, and answers queries in "O"(log "n").

The use of fractional cascading lowers the query time bound by a logarithmic factor. The use of the interval tree on the deepest level of associated structures lowers the storage bound by a logarithmic factor.

A query that asks for all the intervals containing a given point is often referred as a "stabbing query".

The segment tree is less efficient than the interval tree for range queries in one dimension, due to its higher storage requirement: "O"("n" log "n") against the O("n") of the interval tree. The importance of the segment tree is that the segments within each node’s canonical subset can be stored in any arbitrary manner.

For "n" intervals whose endpoints are in a small integer range (e.g., in the range [1,…,"O"("n")]), optimal data structures exist with a linear preprocessing time and query time "O"(1 + "k") for reporting all "k" intervals containing a given query point.

Another advantage of the segment tree is that it can easily be adapted to counting queries; that is, to report the number of segments containing a given point, instead of reporting the segments themselves. Instead of storing the intervals in the canonical subsets, it can simply store the number of them. Such a segment tree uses linear storage, and requires an "O"(log "n") query time, so it is optimal.

Higher dimensional versions of the interval tree and the priority search tree do not exist; that is, there is no clear extension of these structures that solves the analogous problem in higher dimensions. But the structures can be used as associated structure of segment trees.

The segment tree was invented by Jon Louis Bentley in 1977; in "Solutions to Klee’s rectangle problems".



</doc>
<doc id="1533767" url="https://en.wikipedia.org/wiki?curid=1533767" title="Interval tree">
Interval tree

In computer science, an interval tree is a tree data structure to hold intervals. Specifically, it allows one to efficiently find all intervals that overlap with any given interval or point. It is often used for windowing queries, for instance, to find all roads on a computerized map inside a rectangular viewport, or to find all visible elements inside a three-dimensional scene. A similar data structure is the segment tree.

The trivial solution is to visit each interval and test whether it intersects the given point or interval, which requires O("n") time, where "n" is the number of intervals in the collection. Since a query may return all intervals, for example if the query is a large interval intersecting all intervals in the collection, this is asymptotically optimal; however, we can do better by considering output-sensitive algorithms, where the runtime is expressed in terms of "m", the number of intervals produced by the query. Interval trees have a query time of O(log "n" + "m") and an initial creation time of O("n" log "n"), while limiting memory consumption to O("n"). After creation, interval trees may be dynamic, allowing efficient insertion and deletion of an interval in O(log "n") time. If the endpoints of intervals are within a small integer range (e.g., in the range [1...,O("n")]), faster and in fact optimal data structures exist with preprocessing time O("n") and query time O(1+"m") for reporting "m" intervals containing a given query point (see for a very simple one).

In a simple case, the intervals do not overlap and they can be inserted into a simple binary search tree and queried in O(log "n") time. However, with arbitrarily overlapping intervals, there is no way to compare two intervals for insertion into the tree since orderings sorted by the beginning points or the ending points may be different. A naive approach might be to build two parallel trees, one ordered by the beginning point, and one ordered by the ending point of each interval. This allows discarding half of each tree in O(log "n") time, but the results must be merged, requiring O("n") time. This gives us queries in O("n" + log "n") = O("n"), which is no better than brute-force.

Interval trees solve this problem. This article describes two alternative designs for an interval tree, dubbed the "centered interval tree" and the "augmented tree".

Queries require O(log "n" + "m") time, with "n" being the total number of intervals and "m" being the number of reported results. Construction requires O("n" log "n") time, and storage requires O("n") space.

Given a set of "n" intervals on the number line, we want to construct a data structure so that we can efficiently retrieve all intervals overlapping another interval or point.

We start by taking the entire range of all the intervals and dividing it in half at "x_center" (in practice, "x_center" should be picked to keep the tree relatively balanced). This gives three sets of intervals, those completely to the left of "x_center" which we'll call "S_left", those completely to the right of "x_center" which we'll call "S_right", and those overlapping "x_center" which we'll call "S_center".

The intervals in "S_left" and "S_right" are recursively divided in the same manner until there are no intervals left.

The intervals in S_center that overlap the center point are stored in a separate data structure linked to the node in the interval tree. This data structure consists of two lists, one containing all the intervals sorted by their beginning points, and another containing all the intervals sorted by their ending points.

The result is a binary tree with each node storing:

Given the data structure constructed above, we receive queries consisting of ranges or points, and return all the ranges in the original set overlapping this input.

The task is to find all intervals in the tree that overlap a given point "x". The tree is walked with a similar recursive algorithm as would be used to traverse a traditional binary tree, but with extra logic to support searching the intervals overlapping the "center" point at each node.

For each tree node, "x" is compared to "x_center", the midpoint used in node construction above. If "x" is less than "x_center", the leftmost set of intervals, "S_left", is considered. If "x" is greater than "x_center", the rightmost set of intervals, "S_right", is considered.
As each node is processed as we traverse the tree from the root to a leaf, the ranges in its "S_center" are processed. If "x" is less than "x_center", we know that all intervals in "S_center" end after "x", or they could not also overlap "x_center". Therefore, we need only find those intervals in "S_center" that begin before "x". We can consult the lists of "S_center" that have already been constructed. Since we only care about the interval beginnings in this scenario, we can consult the list sorted by beginnings. Suppose we find the closest number no greater than "x" in this list. All ranges from the beginning of the list to that found point overlap "x" because they begin before "x" and end after "x" (as we know because they overlap "x_center" which is larger than "x"). Thus, we can simply start enumerating intervals in the list until the startpoint value exceeds "x".

Likewise, if "x" is greater than "x_center", we know that all intervals in "S_center" must begin before "x", so we find those intervals that end after "x" using the list sorted by interval endings.

If "x" exactly matches "x_center", all intervals in "S_center" can be added to the results without further processing and tree traversal can be stopped.

For a result interval "r" to intersect our query interval "q" one of the following must hold:


We first find all intervals with start and/or end points inside "q" using a separately-constructed tree. 
In the one-dimensional case, we can use a search tree containing all the start and end points in the interval set, each with a pointer to its corresponding interval.
A binary search in O(log "n") time for the start and end of "q" reveals the minimum and maximum points to consider. 
Each point within this range references an interval that overlaps "q" and is added to the result list. 
Care must be taken to avoid duplicates, since an interval might both begin and end within "q". 
This can be done using a binary flag on each interval to mark whether or not it has been added to the result set.

Finally, we must find intervals that enclose "q". 
To find these, we pick any point inside "q" and use the algorithm above to find all intervals intersecting that point (again, being careful to remove duplicates).

The interval tree data structure can be generalized to a higher dimension "N" with identical query and construction time and O("n" log "n") space.

First, a range tree in "N" dimensions is constructed that allows efficient retrieval of all intervals with beginning and end points inside the query region "R". Once the corresponding ranges are found, the only thing that is left are those ranges that enclose the region in some dimension. To find these overlaps, "N" interval trees are created, and one axis intersecting "R" is queried for each. For example, in two dimensions, the bottom of the square "R" (or any other horizontal line intersecting "R") would be queried against the interval tree constructed for the horizontal axis. Likewise, the left (or any other vertical line intersecting "R") would be queried against the interval tree constructed on the vertical axis.

Each interval tree also needs an addition for higher dimensions. At each node we traverse in the tree, "x" is compared with "S_center" to find overlaps. Instead of two sorted lists of points as was used in the one-dimensional case, a range tree is constructed. This allows efficient retrieval of all points in "S_center" that overlap region "R".

If after deleting an interval from the tree, the node containing that interval contains no more intervals, that node may be deleted from the tree. This is more complex than a normal binary tree deletion operation.

An interval may overlap the center point of several nodes in the tree. Since each node stores the intervals that overlap it, with all intervals completely to the left of its center point in the left subtree, similarly for the right subtree, it follows that each interval is stored in the node closest to the root from the set of nodes whose center point it overlaps.

Normal deletion operations in a binary tree (for the case where the node being deleted has two children) involve promoting a node further from the leaf to the position of the node being deleted (usually the leftmost child of the right subtree, or the rightmost child of the left subtree).

As a result of this promotion, some nodes that were above the promoted node will become its descendants; it is necessary to search these nodes for intervals that also overlap the promoted node, and move those intervals into the promoted node. As a consequence, this may result in new empty nodes, which must be deleted, following the same algorithm again.

The same issues that affect deletion also affect rotation operations; rotation must preserve the invariant that nodes are stored as close to the root as possible.

Another way to represent intervals is described in .

Both insertion and deletion require O(log "n") time, with "n" being the total number of intervals in the tree prior to the insertion or deletion operation.

An augmented tree can be built from a simple ordered tree, for example a binary search tree or self-balancing binary search tree, ordered by the 'low' values of the intervals. An extra annotation is then added to every node, recording the maximum upper value among all the intervals from this node down. Maintaining this attribute involves updating all ancestors of the node from the bottom up whenever a node is added or deleted. This takes only O("h") steps per node addition or removal, where "h" is the height of the node added or removed in the tree. If there are any tree rotations during insertion and deletion, the affected nodes may need updating as well.

Now, it is known that two intervals "A" and "B" overlap only when both "A".low ≤ "B".high and "A".high ≥ "B".low. When searching the trees for nodes overlapping with a given interval, you can immediately skip:

Some performance may be gained if the tree avoids unnecessary traversals. These can occur when adding intervals that already exist or removing intervals that don't exist.

A total order can be defined on the intervals by ordering them first by their lower bounds and then by their upper bounds. Then, a membership check can be performed in O(log "n") time, versus the O("k" + log "n") time required to find duplicates if "k" intervals overlap the interval to be inserted or removed. This solution has the advantage of not requiring any additional structures. The change is strictly algorithmic. The disadvantage is that membership queries take O(log "n") time.

Alternately, at the rate of O("n") memory, membership queries in expected constant time can be implemented with a hash table, updated in lockstep with the interval tree. This may not necessarily double the total memory requirement, if the intervals are stored by reference rather than by value.

The key of each node is the interval itself, hence nodes are ordered first by low value and finally by high value, and the value of each node is the end point of the interval:

To search for an interval, one walks the tree, using the key (codice_1) and high value (codice_2) to omit any branches that cannot overlap the query. The simplest case is a point query:
where

The code to search for an interval is similar, except for the check in the middle:
overlapsWith() is defined as:

Augmented trees can be extended to higher dimensions by cycling through the dimensions at each level of the tree. For example, for two dimensions, the odd levels of the tree might contain ranges for the "x"-coordinate, while the even levels contain ranges for the "y"-coordinate. This approach effectively converts the data structure from an augmented binary tree to an augmented kd-tree, thus significantly complicating the balancing algorithms for insertions and deletions.

A simpler solution is to use nested interval trees. First, create a tree using the ranges for the "y"-coordinate. Now, for each node in the tree, add another interval tree on the "x"-ranges, for all elements whose "y"-range is the same as that node's "y"-range.

The advantage of this solution is that it can be extended to an arbitrary number of dimensions using the same code base.

At first, the additional cost of the nested trees might seem prohibitive, but this is usually not so. As with the non-nested solution earlier, one node is needed per "x"-coordinate, yielding the same number of nodes for both solutions. The only additional overhead is that of the nested tree structures, one per vertical interval. This structure is usually of negligible size, consisting only of a pointer to the root node, and possibly the number of nodes and the depth of the tree.

A medial- or length-oriented tree is similar to an augmented tree, but symmetrical, with the binary search tree ordered by the medial points of the intervals. There is a maximum-oriented binary heap in every node, ordered by the length of the interval (or half of the length). Also we store the minimum and maximum possible value of the subtree in each node (thus the symmetry).

Using only start and end values of two intervals formula_1, for formula_2, the overlap test can be performed as follows:

formula_3 and formula_4

This can be simplified using the sum and difference:

formula_5

formula_6

Which reduces the overlap test to:

formula_7

Adding new intervals to the tree is the same as for a binary search tree using the medial value as the key. We push formula_8 onto the binary heap associated with the node, and update the minimum and maximum possible values associated with all higher nodes.

Let's use formula_9 for the query interval, and formula_10 for the key of a node (compared to formula_11 of intervals)

Starting with root node, in each node, first we check if it is possible that our query interval overlaps with the node subtree using minimum and maximum values of node (if it is not, we don't continue for this node).

Then we calculate formula_12 for intervals inside this node (not its children) to overlap with query interval (knowing formula_13):

formula_14

and perform a query on its binary heap for the formula_8's bigger than formula_12

Then we pass through both left and right children of the node, doing the same thing.
In the worst-case, we have to scan all nodes of the binary search tree, but since binary heap query is optimum, this is acceptable (a 2- dimensional problem can not be optimum in both dimensions)

This algorithm is expected to be faster than a traditional interval tree (augmented tree) for search operations. Adding elements is a little slower in practice, though the order of growth is the same.



</doc>
<doc id="14514547" url="https://en.wikipedia.org/wiki?curid=14514547" title="Range tree">
Range tree

In computer science, a range tree is an ordered tree data structure to hold a list of points. It allows all points within a given range to be reported efficiently, and is typically used in two or higher dimensions. Range trees were introduced by Jon Louis Bentley in 1979. Similar data structures were discovered independently by Lueker, Lee and Wong, and Willard.
The range tree is an alternative to the "k"-d tree. Compared to "k"-d trees, range trees offer faster query times of (in Big O notation) formula_1 but worse storage of formula_2, where "n" is the number of points stored in the tree, "d" is the dimension of each point and "k" is the number of points reported by a given query.

Bernard Chazelle improved this to query time formula_3 and space complexity formula_4.

A range tree on a set of 1-dimensional points is a balanced binary search tree on those points. The points stored in the tree are stored in the leaves of the tree; each internal node stores the largest value contained in its left subtree.
A range tree on a set of points in "d"-dimensions is a recursively defined multi-level binary search tree. Each level of the data structure is a binary search tree on one of the "d"-dimensions.
The first level is a binary search tree on the first of the "d"-coordinates. Each vertex "v" of this tree contains an associated structure that is a ("d"−1)-dimensional range tree on the last ("d"−1)-coordinates of the points stored in the subtree of "v".

A 1-dimensional range tree on a set of "n" points is a binary search tree, which can be constructed in formula_5 time. Range trees in higher dimensions are constructed recursively by constructing a balanced binary search tree on the first coordinate of the points, and then, for each vertex "v" in this tree, constructing a ("d"−1)-dimensional range tree on the points contained in the subtree of "v". Constructing a range tree this way would require formula_6 time.

This construction time can be improved for 2-dimensional range trees to formula_5.
Let "S" be a set of "n" 2-dimensional points. If "S" contains only one point, return a leaf containing that point. Otherwise, construct the associated structure of "S", a 1-dimensional range tree on the "y"-coordinates of the points in "S". Let "x" be the median "x"-coordinate of the points. Let "S" be the set of points with "x"-coordinate less than or equal to "x" and let "S" be the set of points with "x"-coordinate greater than "x". Recursively construct "v", a 2-dimensional range tree on "S", and "v", a 2-dimensional range tree on "S". Create a vertex "v" with left-child "v" and right-child "v".
If we sort the points by their "y"-coordinates at the start of the algorithm, and maintain this ordering when splitting the points by their "x"-coordinate, we can construct the associated structures of each subtree in linear time.
This reduces the time to construct a 2-dimensional range tree to formula_5, and also reduces the time to construct a "d"-dimensional range tree to formula_9.

A range query on a range tree reports the set of points that lie inside a given interval. To report the points that lie in the interval ["x", "x"], we start by searching for "x" and "x". At some vertex in the tree, the search paths to "x" and "x" will diverge. Let "v" be the last vertex that these two search paths have in common. For every vertex "v" in the search path from "v" to "x", if the value stored at "v" is greater than "x", report every point in the right-subtree of "v". If "v" is a leaf, report the value stored at "v" if it is inside the query interval. Similarly, reporting all of the points stored in the left-subtrees of the vertices with values less than "x" along the search path from "v" to "x", and report the leaf of this path if it lies within the query interval.

Since the range tree is a balanced binary tree, the search paths to "x" and "x" have length formula_10. Reporting all of the points stored in the subtree of a vertex can be done in linear time using any tree traversal algorithm. It follows that the time to perform a range query is formula_11, where "k" is the number of points in the query interval.

Range queries in "d"-dimensions are similar. Instead of reporting all of the points stored in the subtrees of the search paths, perform a ("d"−1)-dimensional range query on the associated structure of each subtree. Eventually, a 1-dimensional range query will be performed and the correct points will be reported. Since a "d"-dimensional query consists of formula_10 ("d"−1)-dimensional range queries, it follows that the time required to perform a "d"-dimensional range query is formula_13, where "k" is the number of points in the query interval. This can be reduced to formula_14 using a variant of fractional cascading.




</doc>
<doc id="1676725" url="https://en.wikipedia.org/wiki?curid=1676725" title="K-d tree">
K-d tree

In computer science, a "k"-d tree (short for "k-dimensional tree") is a space-partitioning data structure for organizing points in a "k"-dimensional space. "k"-d trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g. range searches and nearest neighbor searches). "k"-d trees are a special case of binary space partitioning trees.

The "k"-d tree is a binary tree in which "every" leaf node is a "k"-dimensional point. Every non-leaf node can be thought of as implicitly generating a splitting hyperplane that divides the space into two parts, known as half-spaces. Points to the left of this hyperplane are represented by the left subtree of that node and points to the right of the hyperplane are represented by the right subtree. The hyperplane direction is chosen in the following way: every node in the tree is associated with one of the "k" dimensions, with the hyperplane perpendicular to that dimension's axis. So, for example, if for a particular split the "x" axis is chosen, all points in the subtree with a smaller "x" value than the node will appear in the left subtree and all points with larger "x" value will be in the right subtree. In such a case, the hyperplane would be set by the x-value of the point, and its normal would be the unit x-axis.

Since there are many possible ways to choose axis-aligned splitting planes, there are many different ways to construct "k"-d trees. The canonical method of "k"-d tree construction has the following constraints:

This method leads to a balanced "k"-d tree, in which each leaf node is approximately the same distance from the root. However, balanced trees are not necessarily optimal for all applications.

Note that it is not "required" to select the median point. In the case where median points are not selected, there is no guarantee that the tree will be balanced. To avoid coding a complex median-finding algorithm or using an sort such as heapsort or mergesort to sort all "n" points, a popular practice is to sort a "fixed" number of "randomly" "selected" points, and use the median of those points to serve as the splitting plane. In practice, this technique often results in nicely balanced trees.

Given a list of "n" points, the following algorithm uses a median-finding sort to construct a balanced "k"-d tree containing those points.

It is common that points "after" the median include only the ones that are strictly greater than the median. For points that lie on the median, it is possible to define a "superkey" function that compares the points in all dimensions. In some cases, it is acceptable to let points equal to the median lie on one side of the median, for example, by splitting the points into a "lesser than" subset and a "greater than or equal to" subset.

This algorithm creates the invariant that for any node, all the nodes in the left subtree are on one side of a splitting plane, and all the nodes in the right subtree are on the other side. Points that lie on the splitting plane may appear on either side. The splitting plane of a node goes through the point associated with that node (referred to in the code as "node.location").

Alternative algorithms for building a balanced presort the data prior to building the tree. Then, they maintain the order of the presort during tree construction and hence eliminate the costly step of finding the median at each level of subdivision. Two such algorithms build a balanced to sort triangles in order to improve the execution time of ray tracing for three-dimensional computer graphics. These algorithms presort "n" triangles prior to building the , then build the tree in time in the best case. An algorithm that builds a balanced to sort points has a worst-case complexity of . This algorithm presorts "n" points in each of "k" dimensions using an sort such as Heapsort or Mergesort prior to building the tree. It then maintains the order of these "k" presorts during tree construction and thereby avoids finding the median at each level of subdivision.

One adds a new point to a "k"-d tree in the same way as one adds an element to any other search tree. First, traverse the tree, starting from the root and moving to either the left or the right child depending on whether the point to be inserted is on the "left" or "right" side of the splitting plane. Once you get to the node under which the child should be located, add the new point as either the left or right child of the leaf node, again depending on which side of the node's splitting plane contains the new node.

Adding points in this manner can cause the tree to become unbalanced, leading to decreased tree performance. The rate of tree performance degradation is dependent upon the spatial distribution of tree points being added, and the number of points added in relation to the tree size. If a tree becomes too unbalanced, it may need to be re-balanced to restore the performance of queries that rely on the tree balancing, such as nearest neighbour searching.

To remove a point from an existing "k"-d tree, without breaking the invariant, the easiest way is to form the set of all nodes and leaves from the children of the target node, and recreate that part of the tree.

Another approach is to find a replacement for the point removed. First, find the node R that contains the point to be removed. For the base case where R is a leaf node, no replacement is required. For the general case, find a replacement point, say p, from the subtree rooted at R. Replace the point stored at R with p. Then, recursively remove p.

For finding a replacement point, if R discriminates on x (say) and R has a right child, find the point with the minimum x value from the subtree rooted at the right child. Otherwise, find the point with the maximum x value from the subtree rooted at the left child.

Balancing a "k"-d tree requires care because "k"-d trees are sorted in multiple dimensions so the tree rotation technique cannot be used to balance them as this may break the invariant.

Several variants of balanced "k"-d trees exist. They include divided "k"-d tree, pseudo "k"-d tree, "k"-d B-tree, hB-tree and Bkd-tree. Many of these variants are adaptive k-d trees.

The nearest neighbour search (NN) algorithm aims to find the point in the tree that is nearest to a given input point. This search can be done efficiently by using the tree properties to quickly eliminate large portions of the search space.

Searching for a nearest neighbour in a "k"-d tree proceeds as follows:

Generally the algorithm uses squared distances for comparison to avoid computing square roots. Additionally, it can save computation by holding the squared current best distance in a variable for comparison.

Finding the nearest point is an operation on average, in the case of randomly distributed points, although analysis in general is tricky.

In high-dimensional spaces, the curse of dimensionality causes the algorithm to need to visit many more branches than in lower-dimensional spaces. In particular, when the number of points is only slightly higher than the number of dimensions, the algorithm is only slightly better than a linear search of all of the points.

The algorithm can be extended in several ways by simple modifications. It can provide the "k" nearest neighbours to a point by maintaining "k" current bests instead of just one. A branch is only eliminated when "k" points have been found and the branch cannot have points closer than any of the "k" current bests.

It can also be converted to an approximation algorithm to run faster. For example, approximate nearest neighbour searching can be achieved by simply setting an upper bound on the number points to examine in the tree, or by interrupting the search process based upon a real time clock (which may be more appropriate in hardware implementations). Nearest neighbour for points that are in the tree already can be achieved by not updating the refinement for nodes that give zero distance as the result, this has the downside of discarding points that are not unique, but are co-located with the original search point.

Approximate nearest neighbour is useful in real-time applications such as robotics due to the significant speed increase gained by not searching for the best point exhaustively. One of its implementations is best-bin-first search.

A range search searches for ranges of parameters. For example, if a tree is storing values corresponding to income and age, then a range search might be something like looking for all members of the tree which have an age between 20 and 50 years and an income between 50,000 and 80,000. Since k-d trees divide the range of a domain in half at each level of the tree, they are useful for performing range searches.

Analyses of binary search trees has found that the worst case time for range search in a k-dimensional KD tree containing N nodes is given by the following equation.

Due to the curse of dimensionality which leads most searches in high dimensional spaces to end up being needlessly fancy brute searches, formula_2-d trees are not suitable for efficiently finding the nearest neighbour in high-dimensional spaces. As a general rule, if the dimensionality is formula_2, the number of points in the data, formula_4, should be formula_5. Otherwise, when formula_2-d trees are used with high-dimensional data, most of the points in the tree will be evaluated and the efficiency is no better than exhaustive search, and, if a good-enough fast answer is required, approximate nearest-neighbour methods should be used instead.


Instead of points, a "k"-d tree can also contain rectangles or hyperrectangles. Thus range search becomes the problem of returning all rectangles intersecting the search rectangle. The tree is constructed the usual way with all the rectangles at the leaves. In an orthogonal range search, the "opposite" coordinate is used when comparing against the median. For example, if the current level is split along x, we check the x coordinate of the search rectangle. If the median is less than the x coordinate of the search rectangle, then no rectangle in the left branch can ever intersect with the search rectangle and so can be pruned. Otherwise both branches should be traversed. See also interval tree, which is a 1-dimensional special case.

It is also possible to define a "k"-d tree with points stored solely in leaves. This form of "k"-d tree allows a variety of split mechanics other than the standard median split. The midpoint splitting rule selects on the middle of the longest axis of the space being searched, regardless of the distribution of points. This guarantees that the aspect ratio will be at most 2:1, but the depth is dependent on the distribution of points. A variation, called sliding-midpoint, only splits on the middle if there are points on both sides of the split. Otherwise, it splits on point nearest to the middle. Maneewongvatana and Mount show that this offers "good enough" performance on common data sets.

Using sliding-midpoint, an approximate nearest neighbour query can be answered in formula_7.
Approximate range counting can be answered in formula_8 with this method.

Close variations:

Related variations:

Problems that can be addressed with "k"-d trees:



</doc>
<doc id="12480975" url="https://en.wikipedia.org/wiki?curid=12480975" title="Implicit k-d tree">
Implicit k-d tree

An implicit "k"-d tree is a "k"-d tree defined implicitly above a rectilinear grid. Its split planes' positions and orientations are not given explicitly but implicitly by some recursive splitting-function defined on the hyperrectangles belonging to the tree's nodes. Each inner node's split plane is positioned on a grid plane of the underlying grid, partitioning the node's grid into two subgrids.

The terms "min/max "k"-d tree" and "implicit "k"-d tree" are sometimes mixed up. This is because the first publication using the term "implicit "k"-d tree" did actually use explicit min/max "k"-d trees but referred to them as "implicit "k"-d trees" to indicate that they may be used to ray trace implicitly given iso surfaces. Nevertheless, this publication used also slim "k"-d trees which are a subset of the implicit "k"-d trees with the restriction that they can only be built over integer hyperrectangles with sidelengths that are powers of two. Implicit "k"-d trees as defined here have recently been introduced, with applications in computer graphics. As it is possible to assign attributes to implicit "k"-d tree nodes, one may refer to an implicit "k"-d tree which has min/max values assigned to its nodes as an "implicit min/max "k"-d tree".

Implicit "k"-d trees are in general not constructed explicitly. When accessing a node, its split plane orientation and position are evaluated using the specific splitting-function defining the tree. Different splitting-functions may result in different trees for the same underlying grid.

Splitting-functions may be adapted to special purposes. Underneath two specifications of special splitting-function classes.


A complete splitting function is for example the grid median splitting-function. It creates fairly balanced implicit "k"-d trees by using "k"-dimensional integer hyperrectangles "hyprec[2][k]" belonging to each node of the implicit "k"-d tree. The hyperrectangles define which gridcells of the rectilinear grid belong to their corresponding node. If the volume of this hyperrectangle equals one, the corresponding node is a single grid cell and is therefore not further subdivided and marked as leaf node. Otherwise the hyperrectangle's longest extend is chosen as orientation "o". The corresponding split plane "p" is positioned onto the grid plane that is closest to the hyperrectangle's grid median along that orientation.

Split plane orientation "o":
Split plane position "p":

An obvious advantage of implicit "k"-d trees is that their split plane's orientations and positions need not to be stored explicitly.

But some applications require besides the split plane's orientations and positions further attributes at the inner tree nodes. These attributes may be for example single bits or single scalar values, defining if the subgrids belonging to the nodes are of interest or not. For complete implicit "k"-d trees it is possible to pre-allocate a correctly sized array of attributes and to assign each inner node of the tree to a unique element in that allocated array.

The amount of gridcells in the grid is equal the volume of the integer hyperrectangle belonging to the grid. As a complete implicit "k"-d tree has one inner node less than grid cells, it is known in advance how many attributes need to be stored. The relation "Volume of integer hyperrectangle to inner nodes" defines together with the complete splitting-function a recursive formula assigning to each split plane a unique element in the allocated array. The corresponding algorithm is given in C-pseudo code underneath.

It is worth mentioning that this algorithm works for all rectilinear grids. The corresponding integer hyperrectangle does not necessarily have to have sidelengths that are powers of two.

Implicit max-"k"-d trees are used for ray casting isosurfaces/MIP (maximum intensity projection). The attribute assigned to each inner node is the maximal scalar value given in the subgrid belonging to the node. Nodes are not traversed if their scalar values are smaller than the searched iso-value/current maximum intensity along the ray. The low storage requirements of the implicit max "k"d-tree and the favorable visualization complexity of ray casting allow to ray cast (and even change the isosurface for) very large scalar fields at interactive framerates on commodity PCs. Similarly an implicit min/max kd-tree may be used to efficiently evaluate queries such as terrain line of sight.

Given an implicit "k"-d tree spanned over an "k"-dimensional grid with "n" gridcells.



</doc>
<doc id="12481023" url="https://en.wikipedia.org/wiki?curid=12481023" title="Min/max kd-tree">
Min/max kd-tree

A min/max "k"d-tree is a "k"-d tree with two scalar values - a minimum and a maximum - assigned to its nodes. The minimum/maximum of an inner node is equal to the minimum/maximum of its children's minima/maxima.

Min/max "k"d-trees may be constructed recursively. Starting with the root node, the splitting plane orientation and position is evaluated. Then the children's splitting planes and min/max values are evaluated recursively. The min/max value of the current node is simply the minimum/maximum of its children's minima/maxima.

The min/max "k"dtree has - besides the properties of an "k"d-tree - the special property that an inner node's min/max values coincide each with a min/max value of either one child. This allows to discard the storage of min/max values at the leaf nodes by storing two bits at inner nodes, assigning min/max values to the children: Each inner node's min/max values will be known in advance, where the root node's min/max values are stored separately. Each inner node has besides two min/max values also two bits given, defining to which child those min/max values are assigned (0: to the left child 1: to the right child). The non-assigned min/max values of the children are the from the current node already known min/max values. The two bits may also be stored in the least significant bits of the min/max values which have therefore to be approximated by fractioning them down/up.

The resulting memory reduction is not minor, as the leaf nodes of full binary "k"d-trees are one half of the tree's nodes.

Min/max "k"d-trees are used for ray casting isosurfaces/MIP (maximum intensity projection). Isosurface ray casting only traverses nodes for which the chosen isovalue lies between the min/max values of the current node. Nodes that do not fulfill this requirement do not contain an isosurface to the given isovalue and are therefore skipped (empty space skipping). For MIP, nodes are not traversed if their maximum is smaller than the current maximum intensity along the ray. The favorable visualization complexity of ray casting allows to ray cast (and even change the isosurface for) very large scalar fields at interactive framerates on commodity PCs. Especially implicit max "k"d-trees are an optimal choice for visualizing scalar fields defined on rectilinear grids (see ). Similarly an implicit min/max kd-tree may be used to efficiently evaluate queries such as terrain line of sight.



</doc>
<doc id="4436211" url="https://en.wikipedia.org/wiki?curid=4436211" title="Adaptive k-d tree">
Adaptive k-d tree

An adaptive k-d tree is a tree for multidimensional points where successive levels may be split along different dimensions.


</doc>
<doc id="577097" url="https://en.wikipedia.org/wiki?curid=577097" title="Quadtree">
Quadtree

A quadtree is a tree data structure in which each internal node has exactly four children. Quadtrees are the two-dimensional analog of octrees and are most often used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions. The data associated with a leaf cell varies by application, but the leaf cell represents a "unit of interesting spatial information".

The subdivided regions may be square or rectangular, or may have arbitrary shapes. This data structure was named a quadtree by Raphael Finkel and J.L. Bentley in 1974. A similar partitioning is also known as a "Q-tree". All forms of quadtrees share some common features:

A tree-pyramid (T-pyramid) is a "complete" tree; every node of the T-pyramid has four child nodes except leaf nodes; all leaves are on the same level, the level that corresponds to individual pixels in the image. The data in a tree-pyramid can be stored compactly in an array as an implicit data structure similar to the way a complete binary tree can be stored compactly in an array.

Quadtrees may be classified according to the type of data they represent, including areas, points, lines and curves. Quadtrees may also be classified by whether the shape of the tree is independent of the order in which data is processed. The following are common types of quadtrees.

The region quadtree represents a partition of space in two dimensions by decomposing the region into four equal quadrants, subquadrants, and so on with each leaf node containing data corresponding to a specific subregion. Each node in the tree either has exactly four children, or has no children (a leaf node). The height of quadtrees that follow this decomposition strategy (i.e. subdividing subquadrants as long as there is interesting data in the subquadrant for which more refinement is desired) is sensitive to and dependent on the spatial distribution of interesting areas in the space being decomposed. The region quadtree is a type of trie.

A region quadtree with a depth of n may be used to represent an image consisting of 2 × 2 pixels, where each pixel value is 0 or 1. The root node represents the entire image region. If the pixels in any region are not entirely 0s or 1s, it is subdivided. In this application, each leaf node represents a block of pixels that are all 0s or all 1s. Note the potential savings in terms of space when these trees are used for storing images; images often have many regions of considerable size that have the same colour value throughout. Rather than store a big 2-D array of every pixel in the image, a quadtree can capture the same information potentially many divisive levels higher than the pixel-resolution sized cells that we would otherwise require. The tree resolution and overall size is bounded by the pixel and image sizes.

A region quadtree may also be used as a variable resolution representation of a data field. For example, the temperatures in an area may be stored as a quadtree, with each leaf node storing the average temperature over the subregion it represents.

If a region quadtree is used to represent a set of point data (such as the latitude and longitude of a set of cities), regions are subdivided until each leaf contains at most a single point.

The point quadtree is an adaptation of a binary tree used to represent two-dimensional point data. It shares the features of all quadtrees but is a true tree as the center of a subdivision is always on a point. It is often very efficient in comparing two-dimensional, ordered data points, usually operating in O(log n) time. Point quadtrees are worth mentioning for completeness, but they have been surpassed by "k"-d trees as tools for generalized binary search.

Point quadtrees are constructed as follows. Given the next point to insert, we find the cell in which it lies and add it to the tree. The new point is added such that the cell that contains it is divided into quadrants by the vertical and horizontal lines that run through the point. Consequently, cells are rectangular but not necessarily square. In these trees, each node contains one of the input points.

Since the division of the plane is decided by the order of point-insertion, the tree's height is sensitive to and dependent on insertion order. Inserting in a "bad" order can lead to a tree of height linear in the number of input points (at which point it becomes a linked-list). If the point-set is static, pre-processing can be done to create a tree of balanced height.

A node of a point quadtree is similar to a node of a binary tree, with the major difference being that it has four pointers (one for each quadrant) instead of two ("left" and "right") as in an ordinary binary tree. Also a key is usually decomposed into two parts, referring to x and y coordinates. Therefore, a node contains the following information:

Point-region (PR) quadtrees are very similar to region quadtrees. The difference is the type of information stored about the cells. In a region quadtree, a uniform value is stored that applies to the entire area of the cell of a leaf. The cells of a PR quadtree, however, store a list of points that exist within the cell of a leaf. As mentioned previously, for trees following this decomposition strategy the height depends on the spatial distribution of the points. Like the point quadtree, the PR quadtree may also have a linear height when given a "bad" set.

Edge quadtrees (much like PM quadtrees) are used to store lines rather than points. Curves are approximated by subdividing cells to a very fine resolution, specifically until there is a single line segment per cell. Near corners/vertices, edge quadtrees will continue dividing until they reach their maximum level of decomposition. This can result in extremely unbalanced trees which may defeat the purpose of indexing.

The polygonal map quadtree (or PM Quadtree) is a variation of quadtree which is used to store collections of polygons that may be degenerate (meaning that they have isolated vertices or edges).

There are three main classes of PM Quadtrees, which vary depending on what information they store within each black node. PM3 quadtrees can store any amount of non-intersecting edges and at most one point. PM2 quadtrees are the same as PM3 quadtrees except that all edges must share the same end point. Finally PM1 quadtrees are similar to PM2, but black nodes can contain a point and its edges or just a set of edges that share a point, but you cannot have a point and a set of edges that do not contain the point.

This section summarizes a subsection from a book by Sariel Har-Peled.

If we were to store every node corresponding to a subdivided cell, we may end up storing a lot of empty nodes. We can cut down on the size of such sparse trees by only storing subtrees whose leaves have interesting data (i.e. "important subtrees"). We can actually cut down on the size even further. When we only keep important subtrees, the pruning process may leave long paths in the tree where the intermediate nodes have degree two (a link to one parent and one child). It turns out that we only need to store the node formula_1 at the beginning of this path (and associate some meta-data with it to represent the removed nodes) and attach the subtree rooted at its end to formula_1. It is still possible for these compressed trees to have a linear height when given "bad" input points.

Although we trim a lot of the tree when we perform this compression, it is still possible to achieve logarithmic-time search, insertion, and deletion by taking advantage of "Z"-order curves. The "Z"-order curve maps each cell of the full quadtree (and hence even the compressed quadtree) in formula_3 time to a one-dimensional line (and maps it back in formula_3 time too), creating a total order on the elements. Therefore, we can store the quadtree in a data structure for ordered sets (in which we store the nodes of the tree). We must state a reasonable assumption before we continue: we assume that given two real numbers formula_5 expressed as binary, we can compute in formula_3 time the index of the first bit in which they differ. We also assume that we can compute in formula_3 time the lowest common ancestor of two points/cells in the quadtree and establish their relative "Z"-ordering, and we can compute the floor function in formula_3 time. With these assumptions, point location of a given point formula_9 (i.e. determining the cell that would contain formula_9), insertion, and deletion operations can all be performed in formula_11 time (i.e. the time it takes to do a search in the underlying ordered set data structure).

To perform a point location for formula_9 (i.e. find its cell in the compressed tree):
Without going into specific details, to perform insertions and deletions we first do a point location for the thing we want to insert/delete, and then insert/delete it. Care must be taken to reshape the tree as appropriate, creating and removing nodes as needed.


Quadtrees, particularly the region quadtree, have lent themselves well to image processing applications. We will limit our discussion to binary image data, though region quadtrees and the image processing operations performed on them are just as suitable for colour images.

One of the advantages of using quadtrees for image manipulation is that the set operations of union and intersection can be done simply and quickly.

Given two binary images, the image union (also called "overlay") produces an image wherein a pixel is black if either of the input images has a black pixel in the same location. That is, a pixel in the output image is white only when the corresponding pixel in "both" input images is white, otherwise the output pixel is black. Rather than do the operation pixel by pixel, we can compute the union more efficiently by leveraging the quadtree's ability to represent multiple pixels with a single node. For the purposes of discussion below, if a subtree contains both black and white pixels we will say that the root of that subtree is coloured grey.

The algorithm works by traversing the two input quadtrees (formula_21 and formula_22) while building the output quadtree formula_23. Informally, the algorithm is as follows. Consider the nodes formula_24 and formula_25 corresponding to the same region in the images.

While this algorithm works, it does not by itself guarantee a minimally sized quadtree. For example, consider the result if we were to union a checkerboard (where every tile is a pixel) of size formula_38 with its complement. The result is a giant black square which should be represented by a quadtree with just the root node (coloured black), but instead the algorithm produces a full 4-ary tree of depth formula_39. To fix this, we perform a bottom-up traversal of the resulting quadtree where we check if the four children nodes have the same colour, in which case we replace their parent with a leaf of the same colour.

The intersection of two images is almost the same algorithm. One way to think about the intersection of the two images is that we are doing a union with respect to the "white" pixels. As such, to perform the intersection we swap the mentions of black and white in the union algorithm.

Consider two neighbouring black pixels in a binary image. They are "adjacent" if they share a bounding horizontal or vertical edge. In general, two black pixels are "connected" if one can be reached from the other by moving only to adjacent pixels (i.e. there is a path of black pixels between them where each consecutive pair is adjacent). Each maximal set of connected black pixels is a "connected component". Using the quadtree representation of images, Samet showed how we can find and label these connected components in time proportional to the size of the quadtree. This algorithm can also be used for polygon colouring.

The algorithm works in three steps: 
To simplify the discussion, let us assume the children of a node in the quadtree follow the "Z"-order (SW, NW, SE, NE). Since we can count on this structure, for any cell we know how to navigate the quadtree to find the adjacent cells in the different levels of the hierarchy.

Step one is accomplished with a post-order traversal of the quadtree. For each black leaf formula_14 we look at the node or nodes representing cells that are Northern neighbours and Eastern neighbours (i.e. the Northern and Eastern cells that share edges with the cell of formula_14). Since the tree is organized in "Z"-order, we have the invariant that the Southern and Western neighbours have already been taken care of and accounted for. Let the Northern or Eastern neighbour currently under consideration be formula_1. If formula_1 represents black pixels:
Step two can be accomplished using the union-find data structure. We start with each unique label as a separate set. For every equivalence relation noted in the first step, we union the corresponding sets. Afterwards, each distinct remaining set will be associated with a distinct connected component in the image.

Step three performs another post-order traversal. This time, for each black node formula_14 we use the union-find's "find" operation (with the old label of formula_14) to find and assign formula_14 its new label (associated with the connected component of which formula_14 is part).

This section summarizes a chapter from a book by Har-Peled and de Berg et al.

Mesh generation is essentially the triangulation of a point set for which further processing may be performed. As such, it is desirable for the resulting triangulation to have certain properties (like non-uniformity, triangles that are not "too skinny", large triangles in sparse areas and small triangles in dense ones, etc.) to make further processing quicker and less error-prone. Quadtrees built on the point set can be used to create meshes with these desired properties.

Consider a leaf of the quadtree and its corresponding cell formula_14. We say formula_14 is "balanced" (for mesh generation) if the cell's sides are intersected by the corner points of neighbouring cells at most once on each side. This means that the quadtree levels of leaves adjacent to formula_14 differ by at most one from the level of formula_14. When this is true for all leaves, we say the whole quadtree is balanced (for mesh generation).

Consider the cell formula_14 and the formula_57 neighbourhood of same-sized cells centred at formula_14. We call this neighbourhood the "extended cluster". We say the quadtree is "well-balanced" if it is balanced, and for every leaf formula_1 that contains a point of the point set, its extended cluster is also in the quadtree and the extended cluster contains no other point of the point set.

Creating the mesh is done as follows:
We consider the corner points of the tree cells as vertices in our triangulation. Before the transformation step we have a bunch of boxes with points in some of them. The transformation step is done in the following manner: for each point, warp the closest corner of its cell to meet it and triangulate the resulting four quadrangles to make "nice" triangles (the interested reader is referred to chapter 12 of Har-Peled for more details on what makes "nice" triangles).

The remaining squares are triangulated according to some simple rules. For each regular square (no points within and no corner points in its sides), introduce the diagonal. Note that due to the way in which we separated points with the well-balancing property, no square with a corner intersecting a side is one that was warped. As such, we can triangulate squares with intersecting corners as follows. If there is one intersected side, the square becomes three triangles by adding the long diagonals connecting the intersection with opposite corners. If there are four intersected sides, we split the square in half by adding an edge between two of the four intersections, and then connect these two endpoints to the remaining two intersection points. For the other squares, we introduce a point in the middle and connect it to all four corners of the square as well as each intersection point.

At the end of it all, we have a nice triangulated mesh of our point set built from a quadtree.

The following pseudo code shows one means of implementing a quadtree which handles only points. There are other approaches available.

It is assumed these structures are used.

This class represents both one quad tree and the node where it is rooted.

The following method inserts a point into the appropriate quad of a quadtree, splitting if necessary.

The following method finds all points contained within a range.


Surveys by Aluru and Samet give a nice overview of quadtrees.



</doc>
<doc id="675699" url="https://en.wikipedia.org/wiki?curid=675699" title="Octree">
Octree

An octree is a tree data structure in which each internal node has exactly eight children. Octrees are most often used to partition a three-dimensional space by recursively subdividing it into eight octants. Octrees are the three-dimensional analog of quadtrees. The name is formed from "oct" + "tree", but note that it is normally written "octree" with only one "t". Octrees are often used in 3D graphics and 3D game engines.

Each node in an octree subdivides the space it represents into eight octants. In a point region (PR) octree, the node stores an explicit three-dimensional point, which is the "center" of the subdivision for that node; the point defines one of the corners for each of the eight children. In a matrix based (MX) octree, the subdivision point is implicitly the center of the space the node represents. The root node of a PR octree can represent infinite space; the root node of an MX octree must represent a finite bounded space so that the implicit centers are well-defined. Note that Octrees are not the same as "k"-d trees: "k"-d trees split along a dimension and octrees split around a point. Also "k"-d trees are always binary, which is not the case for octrees.
By using a depth-first search the nodes are to be traversed and only required surfaces are to be viewed.

The use of octrees for 3D computer graphics was pioneered by Donald Meagher at Rensselaer Polytechnic Institute, described in a 1980 report "Octree Encoding: A New Technique for the Representation, Manipulation and Display of Arbitrary 3-D Objects by Computer", for which he holds a 1995 patent (with a 1984 priority date) "High-speed image generation of complex solid objects using octree encoding" 


The octree color quantization algorithm, invented by Gervautz and Purgathofer in 1988, encodes image color data as an octree up to nine levels deep. Octrees are used because formula_1 and there are three color components in the RGB system. The node index to branch out from at the top level is determined by a formula that uses the most significant bits of the red, green, and blue color components, e.g. 4r + 2g + b. The next lower level uses the next bit significance, and so on. Less significant bits are sometimes ignored to reduce the tree size.

The algorithm is highly memory efficient because the tree's size can be limited. The bottom level of the octree consists of leaf nodes that accrue color data not represented in the tree; these nodes initially contain single bits. If much more than the desired number of palette colors are entered into the octree, its size can be continually reduced by seeking out a bottom-level node and averaging its bit data up into a leaf node, pruning part of the tree. Once sampling is complete, exploring all routes in the tree down to the leaf nodes, taking note of the bits along the way, will yield approximately the required number of colors.

The example recursive algorithm outline below (MATLAB syntax) decomposes an array of 3-dimensional points into octree style bins. The implementation begins with a single bin surrounding all given points, which then recursively subdivides into its 8 octree regions. Recursion is stopped when a given exit condition is met. Examples of such exit conditions (shown in code below) are:

function [binDepths,binParents,binCorners,pointBins] = OcTree(points)
binDepths = [0] % Initialize an array of bin depths with this single base-level bin
binParents = [0] % This base level bin is not a child of other bins
binCorners = [min(points) max(points)] % It surrounds all points in XYZ space
pointBins(:) = 1 % Initially, all points are assigned to this first bin
divide(1) % Begin dividing this first bin
function divide(binNo)
% If this bin meets any exit conditions, do not divide it any further.
binPointCount = nnz(pointBins==binNo)
binEdgeLengths = binCorners(binNo,1:3) - binCorners(binNo,4:6)
binDepth = binDepths(binNo)
exitConditionsMet = binPointCount<value || min(binEdgeLengths)<value || binDepth>value
if exitConditionsMet
end
% Otherwise, split this bin into 8 new sub-bins with a new division point
newDiv = (binCorners(binNo,1:3) + binCorners(binNo,4:6)) / 2
for i = 1:8
end
Taking the full list of colors of a 24-bit RGB image as point input to the Octree point decomposition implementation outlined above, the following example show the results of octree color quantization. The first image is the original (532818 distinct colors), while the second is the quantized image (184 distinct colors) using octree decomposition, with each pixel assigned the color at the center of the octree bin in which it falls. Alternatively, final colors could be chosen at the centroid of all colors in each octree bin, however this added computation has very little effect on the visual result.

% Read the original RGB image
Img = imread('IMG_9980.CR2');
% Extract pixels as RGB point triplets
pts = reshape(Img,[],3);
% Create OcTree decomposition object using a target bin capacity
OT = OcTree(pts,'BinCapacity',ceil((size(pts,1) / 256) *7));
% Find which bins are "leaf nodes" on the octree object
leafs = find(~ismember(1:OT.BinCount, OT.BinParents) & ...
% Find the central RGB location of each leaf bin
binCents = mean(reshape(OT.BinBoundaries(leafs,:),[],3,2),3);
% Make a new "indexed" image with a color map
ImgIdx = zeros(size(Img,1), size(Img,2));
for i = 1:length(leafs)
end
ImgMap = binCents / 255; % Convert 8-bit color to MATLAB rgb values
% Display the original 532818-color image and resulting 184-color image 
figure
subplot(1,2,1), imshow(Img)
title(sprintf('Original %d color image', size(unique(pts,'rows'),1)))
subplot(1,2,2), imshow(ImgIdx, ImgMap)
title(sprintf('Octree-quantized %d color image', size(ImgMap,1)))



</doc>
<doc id="1699416" url="https://en.wikipedia.org/wiki?curid=1699416" title="Z-order curve">
Z-order curve

In mathematical analysis and computer science, functions which are Z-order, Lebesgue curve, Morton space filling curve, Morton order or Morton code map multidimensional data to one dimension while preserving locality of the data points. It is named after Guy Macdonald Morton, who first applied the order to file sequencing in 1966. The z-value of a point in multidimensions is simply calculated by interleaving the binary representations of its coordinate values. Once the data are sorted into this ordering, any one-dimensional data structure can be used such as binary search trees, B-trees, skip lists or (with low significant bits truncated) hash tables. The resulting ordering can equivalently be described as the order one would get from a depth-first traversal of a quadtree.

The figure below shows the Z-values for the two dimensional case with integer coordinates 0 ≤ "x" ≤ 7, 0 ≤ "y" ≤ 7 (shown both in decimal and binary). Interleaving the binary coordinate values yields binary "z"-values as shown. Connecting the "z"-values in their numerical order produces the recursively Z-shaped curve. Two-dimensional Z-values are also called as quadkey ones.
The Z-values of x's are described as binary numbers from the Moser–de Bruijn sequence, having nonzero bits only in their even positions:

The sum and difference of two x's are calculated by using bitwise operations:

This property can be used to offset a Z-value, for example in two dimensions the coordinates to the top (decreasing y), bottom (increasing y), left (decreasing x) and right (increasing x) from the current Z-value "z" are:
And in general to add two two-dimensional Z-values:

The Z-ordering can be used to efficiently build a quadtree for a set of points. The basic idea is to sort the input set according to Z-order. Once sorted, the points can either be stored in a binary search tree and used directly, which is called a linear quadtree, or they can be used to build a pointer based quadtree.

The input points are usually scaled in each dimension to be positive integers, either as a fixed point representation over the unit range [0, 1] or corresponding to the machine word size. Both representations are equivalent and allow for the highest order non-zero bit to be found in constant time. Each square in the quadtree has a side length which is a power of two, and corner coordinates which are multiples of the side length. Given any two points, the "derived square" for the two points is the smallest square covering both points. The interleaving of bits from the x and y components of each point is called the "shuffle" of x and y, and can be extended to higher dimensions.

Points can be sorted according to their shuffle without explicitly interleaving the bits. To do this, for each dimension, the most significant bit of the exclusive or of the coordinates of the two points for that dimension is examined. The dimension for which the most significant bit is largest is then used to compare the two points to determine their shuffle order.

The exclusive or operation masks off the higher order bits for which the two coordinates are identical. Since the shuffle interleaves bits from higher order to lower order, identifying the coordinate with the largest most significant bit, identifies the first bit in the shuffle order which differs, and that coordinate can be used to compare the two points. This is shown in the following Python code:

One way to determine whether the most significant bit is smaller is to compare the floor of the base-2 logarithm of each point. It turns out the following operation is equivalent, and only requires exclusive or operations:
It is also possible to compare floating point numbers using the same technique. The "less_msb" function is modified to first compare the exponents. Only when they are equal is the standard "less_msb" function used on the mantissas.

Once the points are in sorted order, two properties make it easy to build a quadtree: The first is that the points contained in a square of the quadtree form a contiguous interval in the sorted order. The second is that if more than one child of a square contains an input point, the square is the "derived square" for two adjacent points in the sorted order.

For each adjacent pair of points, the derived square is computed and its side length determined. For each derived square, the interval containing it is bounded by the first larger square to the right and to the left in sorted order. Each such interval corresponds to a square in the quadtree. The result of this is a compressed quadtree, where only nodes containing input points or two or more children are present. A non-compressed quadtree can be built by restoring the missing nodes, if desired.

Rather than building a pointer based quadtree, the points can be maintained in sorted order in a data structure such as a binary search tree. This allows points to be added and deleted in O(log n) time. Two quadtrees can be merged by merging the two sorted sets of points, and removing duplicates. Point location can be done by searching for the points preceding and following the query point in the sorted order. If the quadtree is compressed, the predecessor node found may be an arbitrary leaf inside the compressed node of interest. In this case, it is necessary to find the predecessor of the least common ancestor of the query point and the leaf found.

Although preserving locality well, for efficient range searches an algorithm is necessary for calculating, from a point encountered in the data structure, the next Z-value which is in the multidimensional search range:

In this example, the range being queried ("x" = 2, ..., 3, "y" = 2, ..., 6) is indicated by the dotted rectangle. Its highest Z-value (MAX) is 45. In this example, the value "F" = 19 is encountered when searching a data structure in increasing Z-value direction, so we would have to search in the interval between F and MAX (hatched area). To speed up the search, one would calculate the next Z-value which is in the search range, called BIGMIN (36 in the example) and only search in the interval between BIGMIN and MAX (bold values), thus skipping most of the hatched area. Searching in decreasing direction is analogous with LITMAX which is the highest Z-value in the query range lower than F. The BIGMIN problem has first been stated and its solution shown in Tropf and Herzog. This solution is also used in UB-trees ("GetNextZ-address"). As the approach does not depend on the one dimensional data structure chosen, there is still free choice of structuring the data, so well known methods such as balanced trees can be used to cope with dynamic data (in contrast for example to R-trees where special considerations are necessary). Similarly, this independence makes it easier to incorporate the method into existing databases.

Applying the method hierarchically (according to the data structure at hand), optionally in both increasing and decreasing direction, yields highly efficient multidimensional range search which is important in both commercial and technical applications, e.g. as a procedure underlying nearest neighbour searches. Z-order is one of the few multidimensional access methods that has found its way into commercial database systems (Oracle database 1995, Transbase 2000 ).

As long ago as 1966, G.M.Morton proposed Z-order for file sequencing of a static two dimensional geographical database. Areal data units are contained in one or a few quadratic frames represented by their sizes and lower right corner Z-values, the sizes complying with the Z-order hierarchy at the corner position. With high probability, changing to an adjacent frame is done with one or a few relatively small scanning steps.

As an alternative, the Hilbert curve has been suggested as it has a better order-preserving behaviour, and, in fact, was used in a optimized index, the S2-geometry. Before S2, it was avoided because the calculations are somewhat more complicated, leading to significant processor overhead. BIGMIN source code for both Z-curve and Hilbert-curve were described in a patent by H. Tropf.

For a recent overview on multidimensional data processing, including e.g. nearest neighbour searches, see Hanan Samet's textbook.

The Strassen algorithm for matrix multiplication is based on splitting the matrices in four blocks, and then recursively splitting each of these blocks in four smaller blocks, until the blocks are single elements (or more practically: until reaching matrices so small that the Moser–de Bruijn sequencetrivial algorithm is faster). Arranging the matrix elements in Z-order then improves locality, and has the additional advantage (compared to row- or column-major ordering) that the subroutine for multiplying two blocks does not need to know the total size of the matrix, but only the size of the blocks and their location in memory. Effective use of Strassen multiplication
with Z-order has been demonstrated, see Valsalam and Skjellum's 2002 paper.

Buluç "et al." present a sparse matrix data structure that Z-orders its non-zero elements to enable parallel matrix-vector multiplication.

Some GPUs store texture maps in Z-order to increase spatial locality of reference during texture mapped rasterization. This allows cache lines to represent square tiles, increasing the probability that nearby accesses are in the cache. This is important because 3d rendering involves arbitrary transformations (rotations, scaling, perspective, and distortion by animated surfaces). These are referred to as "swizzled textures" or "twidled textures". Other tiled formats may also be used.

An image or a video frame is a matrix, transform the matrix into a vector of pixels in the z-order, then compress and encode the pixel vector.




</doc>
<doc id="5786138" url="https://en.wikipedia.org/wiki?curid=5786138" title="UB-tree">
UB-tree

The UB-tree as proposed by Rudolf Bayer and Volker Markl is a balanced tree for storing and efficiently retrieving multidimensional data. It is basically a B+ tree (information only in the leaves) with records stored according to Z-order, also called Morton order. Z-order is simply calculated by bitwise interlacing the keys.

Insertion, deletion, and point query are done as with ordinary B+ trees. To perform range searches in multidimensional point data, however, an algorithm must be provided for calculating, from a point encountered in the data base, the next Z-value which is in the multidimensional search range.

The original algorithm to solve this key problem was exponential with the dimensionality and thus not feasible ("GetNextZ-address"). A solution to this "crucial part of the UB-tree range query" linear with the z-address bit length has been described later. This method has already been described in an older paper where using Z-order with search trees has first been proposed.


</doc>
<doc id="865249" url="https://en.wikipedia.org/wiki?curid=865249" title="R-tree">
R-tree

R-trees are tree data structures used for spatial access methods, i.e., for indexing multi-dimensional information such as geographical coordinates, rectangles or polygons. The R-tree was proposed by Antonin Guttman in 1984 and has found significant use in both theoretical and applied contexts. A common real-world usage for an R-tree might be to store spatial objects such as restaurant locations or the polygons that typical maps are made of: streets, buildings, outlines of lakes, coastlines, etc. and then find answers quickly to queries such as "Find all museums within 2 km of my current location", "retrieve all road segments within 2 km of my location" (to display them in a navigation system) or "find the nearest gas station" (although not taking roads into account). The R-tree can also accelerate nearest neighbor search for various distance metrics, including great-circle distance.

The key idea of the data structure is to group nearby objects and represent them with their minimum bounding rectangle in the next higher level of the tree; the "R" in R-tree is for rectangle. Since all objects lie within this bounding rectangle, a query that does not intersect the bounding rectangle also cannot intersect any of the contained objects. At the leaf level, each rectangle describes a single object; at higher levels the aggregation of an increasing number of objects. This can also be seen as an increasingly coarse approximation of the data set.

Similar to the B-tree, the R-tree is also a balanced search tree (so all leaf nodes are at the same depth), organizes the data in pages, and is designed for storage on disk (as used in databases). Each page can contain a maximum number of entries, often denoted as formula_1. It also guarantees a minimum fill (except for the root node), however best performance has been experienced with a minimum fill of 30%–40% of the maximum number of entries (B-trees guarantee 50% page fill, and B*-trees even 66%). The reason for this is the more complex balancing required for spatial data as opposed to linear data stored in B-trees.

As with most trees, the searching algorithms (e.g., intersection, containment, nearest neighbor search) are rather simple. The key idea is to use the bounding boxes to decide whether or not to search inside a subtree. In this way, most of the nodes in the tree are never read during a search. Like B-trees, R-trees are suitable for large data sets and databases, where nodes can be paged to memory when needed, and the whole tree cannot be kept in main memory. Even if data can be fit in memory (or cached), the R-trees in most practical applications will usually provide performance advantages over naive check of all objects when the number of objects is more than few hundred or so. However, for in-memory applications, there are similar alternatives that can provide slightly better performance or be simpler to implement in practice.

The key difficulty of R-tree is to build an efficient tree that on one hand is balanced (so the leaf nodes are at the same height) on the other hand the rectangles do not cover too much empty space and do not overlap too much (so that during search, fewer subtrees need to be processed). For example, the original idea for inserting elements to obtain an efficient tree is to always insert into the subtree that requires least enlargement of its bounding box. Once that page is full, the data is split into two sets that should cover the minimal area each. Most of the research and improvements for R-trees aims at improving the way the tree is built and can be grouped into two objectives: building an efficient tree from scratch (known as bulk-loading) and performing changes on an existing tree (insertion and deletion).

R-trees do not guarantee good worst-case performance, but generally perform well with real-world data. While more of theoretical interest, the (bulk-loaded) Priority R-tree variant of the R-tree is worst-case optimal, but due to the increased complexity, has not received much attention in practical applications so far.

When data is organized in an R-tree, the neighbors within a given distance r and the k nearest neighbors (for any L-Norm) of all points can efficiently be computed using a spatial join. This is beneficial for many algorithms based on such queries, for example the Local Outlier Factor. DeLi-Clu, Density-Link-Clustering is a cluster analysis algorithm that uses the R-tree structure for a similar kind of spatial join to efficiently compute an OPTICS clustering.


Data in R-trees is organized in pages, that can have a variable number of entries (up to some pre-defined maximum, and usually above a minimum fill). Each entry within a non-leaf node stores two pieces of data: a way of identifying a child node, and the bounding box of all entries within this child node. Leaf nodes store the data required for each child, often a point or bounding box representing the child and an external identifier for the child. For point data, the leaf entries can be just the points themselves. For polygon data (that often requires the storage of large polygons) the common setup is to store only the MBR (minimum bounding rectangle) of the polygon along with a unique identifier in the tree.

In range searching, the input is a search rectangle (Query box). Searching is quite similar to searching in a B+ tree. The search starts from the root node of the tree. Every internal node contains a set of rectangles and pointers to the corresponding child node and every leaf node contains the rectangles of spatial objects (the pointer to some spatial object can be there). For every rectangle in a node, it has to be decided if it overlaps the search rectangle or not. If yes, the corresponding child node has to be searched also. Searching is done like this in a recursive manner until all overlapping nodes have been traversed. When a leaf node is reached, the contained bounding boxes (rectangles) are tested against the search rectangle and their objects (if there are any) are put into the result set if they lie within the search rectangle.

For priority search such as nearest neighbor search, the query consists of a point or rectangle. The root node is inserted into the priority queue. Until the queue is empty or the desired number of results have been returned the search continues by processing the nearest entry in the queue. Tree nodes are expanded and their children reinserted. Leaf entries are returned when encountered in the queue. This approach can be used with various distance metrics, including great-circle distance for geographic data.

To insert an object, the tree is traversed recursively from the root node. At each step, all rectangles in the current directory node are examined, and a candidate is chosen using a heuristic such as choosing the rectangle which requires least enlargement. The search then descends into this page, until reaching a leaf node. If the leaf node is full, it must be split before the insertion is made. Again, since an exhaustive search is too expensive, a heuristic is employed to split the node into two. Adding the newly created node to the previous level, this level can again overflow, and these overflows can propagate up to the root node; when this node also overflows, a new root node is created and the tree has increased in height.

At each level, the algorithm needs to decide in which subtree to insert the new data object. When a data object is fully contained in a single rectangle, the choice is clear. When there are multiple options or rectangles in need of enlargement, the choice can have a significant impact on the performance of the tree.

In the classic R-tree, objects are inserted into the subtree that needs the least enlargement. In the more advanced R*-tree, a mixed heuristic is employed. At leaf level, it tries to minimize the overlap (in case of ties, prefer least enlargement and then least area); at the higher levels, it behaves similar to the R-tree, but on ties again preferring the subtree with smaller area. The decreased overlap of rectangles in the R*-tree is one of the key benefits over the traditional R-tree (this is also a consequence of the other heuristics used, not only the subtree choosing).

Since redistributing all objects of a node into two nodes has an exponential number of options, a heuristic needs to be employed to find the best split. In the classic R-tree, Guttman proposed two such heuristics, called QuadraticSplit and LinearSplit. In quadratic split, the algorithm searches for the pair of rectangles that is the worst combination to have in the same node, and puts them as initial objects into the two new groups. It then searches for the entry which has the strongest preference for one of the groups (in terms of area increase) and assigns the object to this group until all objects are assigned (satisfying the minimum fill).

There are other splitting strategies such as Greene's Split, the R*-tree splitting heuristic (which again tries to minimize overlap, but also prefers quadratic pages) or the linear split algorithm proposed by Ang and Tan (which however can produce very irregular rectangles, which are less performant for many real world range and window queries). In addition to having a more advanced splitting heuristic, the R*-tree also tries to avoid splitting a node by reinserting some of the node members, which is similar to the way a B-tree balances overflowing nodes. This was shown to also reduce overlap and thus increase tree performance.

Finally, the X-tree can be seen as a R*-tree variant that can also decide to not split a node, but construct a so-called super-node containing all the extra entries, when it doesn't find a good split (in particular for high-dimensional data).

Deleting an entry from a page may require updating the bounding rectangles of parent pages. However, when a page is underfull, it will not be balanced with its neighbors. Instead, the page will be dissolved and all its children (which may be subtrees, not only leaf objects) will be reinserted. If during this process the root node has a single element, the tree height can decrease.




</doc>
<doc id="1369821" url="https://en.wikipedia.org/wiki?curid=1369821" title="R+ tree">
R+ tree

An R+ tree is a method for looking up data using a location, often (x, y) coordinates, and often for locations on the surface of the earth. Searching on one number is a solved problem; searching on two or more, and asking for locations that are nearby in both x and y directions, requires craftier algorithms.

Fundamentally, an R+ tree is a tree data structure, a variant of the R tree, used for indexing spatial information.

R+ trees are a compromise between R-trees and kd-trees: they avoid overlapping of internal nodes by inserting an object into multiple leaves if necessary. Coverage is the entire area to cover all related rectangles. Overlap is the entire area which is contained in two or more nodes. Minimal coverage reduces the amount of "dead space" (empty area) which is covered by the nodes of the R-tree. Minimal overlap reduces the set of search paths to the leaves (even more critical for the access time than minimal coverage). Efficient search requires minimal coverage and overlap.

R+ trees differ from R trees in that: nodes are not guaranteed to be at least half filled, the entries of any internal node do not overlap, and an object ID may be stored in more than one leaf node.

Because nodes are not overlapped with each other, point query performance benefits since all spatial regions are covered by at most one node. A single path is followed and fewer nodes are visited than with the R-tree.

Since rectangles are duplicated, an R+ tree can be larger than an R tree built on same data set. Construction and maintenance of R+ trees is more complex than the construction and maintenance of R trees and other variants of the R tree.



</doc>
<doc id="1369832" url="https://en.wikipedia.org/wiki?curid=1369832" title="R* tree">
R* tree

In data processing R*-trees are a variant of R-trees used for indexing spatial information. R*-trees have slightly higher construction cost than standard R-trees, as the data may need to be reinserted; but the resulting tree will usually have a better query performance. Like the standard R-tree, it can store both point and spatial data.
It was proposed by Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger in 1990.

Minimization of both coverage and overlap is crucial to the performance of R-trees. Overlap means that, on data query or insertion, more than one branch of the tree needs to be expanded (due to the way data is being split in regions which may overlap). A minimized coverage improves pruning performance, allowing to exclude whole pages from search more often, in particular for negative range queries.
The R*-tree attempts to reduce both, using a combination of a revised node split algorithm and the concept of forced reinsertion
at node overflow. This is based on the observation that R-tree structures are highly susceptible 
to the order in which their entries are inserted, so an insertion-built (rather than bulk-loaded) structure 
is likely to be sub-optimal. Deletion and reinsertion of entries allows them to "find" a place in the tree 
that may be more appropriate than their original location.

When a node overflows, a portion of its entries are removed from the node and reinserted into the tree.
(In order to avoid an indefinite cascade of reinsertions caused by subsequent node overflow, the reinsertion 
routine may be called only once in each level of the tree when inserting any one new entry.) This has the 
effect of producing more well-clustered groups of entries in nodes, reducing node coverage. Furthermore, 
actual node splits are often postponed, causing average node occupancy to rise.
Re-insertion can be seen as a method of incremental tree optimization triggered on node overflow.



Worst case query and delete complexity are thus identical to the R-Tree. The insertion strategy to the R*-tree is with formula_1 more complex than the linear split strategy (formula_2) of the R-tree, but less complex than the quadratic split strategy (formula_3) for a page size of formula_4 objects and has little impact on the total complexity. The total insert complexity is still comparable to the R-tree: reinsertions affect at most one branch of the tree and thus formula_5 reinsertions, comparable to performing a split on a regular R-tree. So on overall, the complexity of the R*-tree is the same as that of a regular R-tree.

An implementation of the full algorithm must address many corner cases and tie situations not discussed here.


</doc>
<doc id="12039643" url="https://en.wikipedia.org/wiki?curid=12039643" title="Hilbert R-tree">
Hilbert R-tree

Hilbert R-tree, an R-tree variant, is an index for multidimensional objects such as lines, regions, 3-D objects, or high-dimensional feature-based parametric objects. It can be thought of as an extension to B+-tree for multidimensional objects.

The performance of R-trees depends on the quality of the algorithm that clusters the data rectangles on a node. Hilbert R-trees use space-filling curves, and specifically the Hilbert curve, to impose a linear ordering on the data rectangles.

There are two types of Hilbert R-trees: one for static databases, and one for dynamic databases. In both cases Hilbert space-filling curves are used to achieve better ordering of multidimensional objects in the node. This ordering has to be ‘good,’ in the sense that it should group ‘similar’ data rectangles together, to minimize the area and perimeter of the resulting minimum bounding rectangles (MBRs). Packed Hilbert R-trees are suitable for static databases in which updates are very rare or in which there are no updates at all.

The dynamic Hilbert R-tree is suitable for dynamic databases where insertions, deletions, or updates may occur in real time. Moreover, dynamic Hilbert R-trees employ flexible deferred splitting mechanism to increase the space utilization. Every node has a well defined set of sibling nodes. By adjusting the split policy the Hilbert R-tree can achieve a degree of space utilization as high as is desired. This is done by proposing an ordering on the R-tree nodes. The Hilbert R-tree sorts rectangles according to the Hilbert value of the center of the rectangles (i.e., MBR). (The Hilbert value of a point is the length of the Hilbert curve from the origin to the point.) Given the ordering, every node has a well-defined set of sibling nodes; thus, deferred splitting can be used. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, other R-tree variants have no control over the space utilization.

Although the following example is for a static environment, it explains the intuitive principles for good R-tree design. These principles are valid for both static and dynamic databases.

Roussopoulos and Leifker proposed a method for building a packed R-tree that achieves almost 100% space utilization. The idea is to sort the data on the x or y coordinate of one of the corners of the rectangles. Sorting on any of the four coordinates gives similar results. In this discussion points or rectangles are sorted on the x coordinate of the lower left corner of the rectangle, referred to as a "lowx packed R-tree." The sorted list of rectangles is scanned; successive rectangles are assigned to the same R-tree leaf node until that node is full; a new leaf node is then created, and the scanning of the sorted list continues. Thus, the nodes of the resulting R-tree will be fully packed, with the possible exception of the last node at each level. This leads to space utilization ≈100%. Higher levels of the tree are created in a similar way.

Figure 1 highlights the problem of the lowx packed R-tree. Figure 1 [Right] shows the leaf nodes of the R-tree that the lowx packing method will create for the points of Figure 1 [Left]. The fact that the resulting father nodes cover little area explains why the lowx packed R-tree achieves excellent performance for point queries. However, the fact that the fathers have large perimeters explains the degradation of performance for region queries. This is consistent with the analytical formulas for R-tree performance. Intuitively, the packing algorithm should ideally assign nearby points to the same leaf node. Ignorance of the y coordinate by the lowx packed R-tree tends to violate this empirical rule.

Figure 1: [Left] 200 points uniformly distributed; [Right] MBR of nodes generated by the "‘lowx packed R-tree’" algorithm

The section below describes two variants of the Hilbert R-trees. The first index is suitable for the static database in which updates are very rare or in which there are no updates at all. The nodes of the resulting R-tree will be fully packed, with the possible exception of the last node at each level. Thus, the space utilization is ≈100%; this structure is called a packed Hilbert R-tree. The second index, called a Dynamic Hilbert R-tree, supports insertions and deletions, and is suitable for a dynamic environment.

The following provides a brief introduction to the Hilbert curve. The basic Hilbert curve on a 2x2 grid, denoted by H is shown in Figure 2. To derive a curve of order i, each vertex of the basic curve is replaced by the curve of order i – 1, which may be appropriately rotated and/or reflected. Figure 2 also shows the Hilbert curves of order two and three. When the order of the curve tends to infinity, like other space filling curves, the resulting curve is a fractal, with a fractal dimension of two. The Hilbert curve can be generalized for higher dimensionalities. Algorithms for drawing the two-dimensional curve of a given order can be found in and. An algorithm for higher dimensionalities is given in.

The path of a space filling curve imposes a linear ordering on the grid points; this path may be calculated by starting at one end of the curve and following the path to the other end. The actual coordinate values of each point can be calculated. However, for the Hilbert curve this is much harder than for example the Z-order curve. Figure 2 shows one such ordering for a 4x4 grid (see curve H). For example, the point (0,0) on the H curve has a Hilbert value of 0, while the point (1,1) has a Hilbert value of 2. The Hilbert value of a rectangle is defined as the Hilbert value of its center.

Figure 2: Hilbert curves of order 1, 2, and 3

The Hilbert curve imposes a linear ordering on the data rectangles and then traverses the sorted list, assigning each set of C rectangles to a node in the R-tree. The final result is that the set of data rectangles on the same node will be close to each other in the linear ordering, and most likely in the native space; thus, the resulting R-tree nodes will have smaller areas. Figure 2 illustrates the intuitive reasons why our Hilbert-based methods will result in good performance. The data is composed of points (the same points as given in Figure 1). By grouping the points according to their Hilbert values, the MBRs of the resulting R-tree nodes tend to be small square-like rectangles. This indicates that the nodes will likely have small area and small perimeters. Small area values result in good performance for point queries; small area and small perimeter values lead to good performance for larger queries.

(packs rectangles into an R-tree)<br>
Step 1. Calculate the Hilbert value for each data rectangle<br>
Step 2. Sort data rectangles on ascending Hilbert values<br>
Step 3. /* Create leaf nodes (level l=0) */
Step 4. /* Create nodes at higher level (l + 1) */

The assumption here is that the data are static or the frequency of modification is low. This is a simple heuristic for constructing an R-tree with ~100% space utilization which at the same time will have a good response time.

The performance of R-trees depends on the quality of the algorithm that clusters the data rectangles on a node. Hilbert R-trees use space-filling curves, and specifically the Hilbert curve, to impose a linear ordering on the data rectangles. The Hilbert value of a rectangle is defined as the Hilbert value of its center.

The Hilbert R-tree has the following structure. A leaf node contains at most
C entries each of the form (R, obj _id) where C is the capacity of the leaf, R is the MBR of the real object (x, x, y, y) and obj-id is a pointer to the object description record. The main difference between the Hilbert R-tree and the R*-tree is that non-leaf nodes also contain information about the LHVs (Largest Hilbert Value). Thus, a non-leaf node in the Hilbert R-tree contains at most C entries of the form (R, ptr, LHV) where C is the capacity of a non-leaf node, R is the MBR that encloses all the children of that node, ptr is a pointer to the child node, and LHV is the largest Hilbert value among the data rectangles enclosed by R. Notice that since the non-leaf node picks one of the Hilbert values of the children to be the value of its own LHV, there is not extra cost for calculating the Hilbert values of the MBR of non-leaf nodes. Figure 3 illustrates some rectangles organized in a Hilbert R-tree. The Hilbert values of the centers are the numbers near the ‘x’ symbols (shown only for the parent node ‘II’). The LHV’s are in [brackets]. Figure 4 shows how the tree of Figure 3 is stored on the disk; the contents of the parent node ‘II’ are shown in more detail. Every data rectangle in node ‘I’ has a Hilbert value v ≤33; similarly every rectangle in node ‘II’ has a Hilbert value greater than 33 and ≤ 107, etc.

Figure 3: Data rectangles organized in a Hilbert R-tree (Hilbert values and largest Hilbert values (LHVs) are in Brackets)

A plain R-tree splits a node on overflow, creating two nodes from the original one. This policy is called a 1-to-2 splitting policy. It is possible also to defer the split, waiting until two nodes split into three. Note that this is similar to the B*-tree split policy. This method is referred to as the 2-to-3 splitting policy.

In general, this can be extended to s-to-(s+1) splitting policy; where s is the order of the splitting policy. To implement the order-s splitting policy, the overflowing node tries to push some of its entries to one of its s - 1 siblings; if all of them are full, then s-to-(s+1) split need to be done. The s -1 siblings are called the cooperating siblings.

Next, the algorithms for searching, insertion, and overflow handling are described in detail.

The searching algorithm is similar to the one used in other R-tree variants. Starting from the root, it descends the tree and examines all nodes that intersect the query rectangle. At the leaf level, it reports all entries that intersect the query window w as qualified data items.

Algorithm Search(node Root, rect w):<br>
S1. Search nonleaf nodes:<br>
S2. Search leaf nodes:<br>

Figure 4: The file structure for the Hilbert R-tree

To insert a new rectangle r in the Hilbert R-tree, the Hilbert value h of the center of the new rectangle is used as a key. At each level the node with the minimum LHV value greater than h of all its siblings is chosen. When a leaf node is reached, the rectangle r is inserted in its correct order according to h. After a new rectangle is inserted in a leaf node N, AdjustTree is called to fix the MBR and largest Hilbert values in the upper-level nodes.

Algorithm Insert(node Root, rect r):
/* Inserts a new rectangle r in the Hilbert R-tree. h is the Hilbert value of the rectangle*/<br>
I1. Find the appropriate leaf node:<br>
I2. Insert r in a leaf node L:<br>
I3. Propagate changes upward:
I4. Grow tree taller:

Algorithm ChooseLeaf(rect r, int h):<br>
/* Returns the leaf node in which to place a new rectangle r. */<br>
C1. Initialize:
C2. Leaf check:
C3. Choose subtree:
C4. Descend until a leaf is reached:

Algorithm AdjustTree(set S):<br>
/* S is a set of nodes that contains the node being updated, its cooperating siblings (if overflow has occurred) and the newly <br>created node NN (if split has occurred). The routine ascends from the leaf level towards the root, adjusting MBR and LHV of nodes that cover the nodes in S. It propagates splits (if any) */<br>
A1. If root level is reached, stop.<br>
A2. Propagate node split upward:<br>
A3. Adjust the MBR’s and LHV’s in the parent level:
A4. Move up to next level:

In the Hilbert R-tree, there is no need to re-insert orphaned nodes whenever a father node underflows. Instead, keys can be borrowed from the siblings or the underflowing node is merged with its siblings. This is possible because the nodes have a clear ordering (according to Largest Hilbert Value, LHV); in contrast, in R-trees there is no such concept concerning sibling nodes. Notice that deletion operations require s cooperating siblings, while insertion operations require s - 1 siblings.

Algorithm Delete(r):<br>
D1. Find the host leaf:<br>
D2. Delete r :<br>
D3. If L underflows<br>
D4. Adjust MBR and LHV in parent levels.<br>

The overflow handling algorithm in the Hilbert R-tree treats the overflowing nodes either by moving some of the entries to one of the s - 1 cooperating siblings or by splitting s nodes into s +1 nodes.

Algorithm HandleOverflow(node N, rect r):<br>
/* return the new node if a split occurred. */<br>
H1. Let ε be a set that contains all the entries from N 
H2. Add r to ε.<br>
H3. If at least one of the s - 1 cooperating siblings is not full,
H4. If all the s cooperating siblings are full,



</doc>
<doc id="2330838" url="https://en.wikipedia.org/wiki?curid=2330838" title="X-tree">
X-tree

In computer science, an X-tree (for "eXtended node tree") is an index tree structure based on the R-tree used for storing data in many dimensions. It appeared in 1996, and differs from R-trees (1984), R+-trees (1987) and R*-trees (1990) because it emphasizes prevention of overlap in the bounding boxes, which increasingly becomes a problem in high dimensions. In cases where nodes cannot be split without preventing overlap, the node split will be deferred, resulting in super-nodes. In extreme cases, the tree will linearize, which defends against worst-case behaviors observed in some other data structures. 

The X-tree consists of three different types of nodes-data nodes, normal directory nodes and supernodes. The data nodes of the X-tree contain rectilinear minimum bounding rectangles (MBRs) together with pointers to the actual data objects, and the directory nodes contain MBRs together with pointers to sub-MBRs. Supernodes are large directory nodes of variable size(a multiple of the usual block size). The basic goal of supernodes is to avoid splits in the directory that would result in an inefficient directory structure.


</doc>
<doc id="3417216" url="https://en.wikipedia.org/wiki?curid=3417216" title="Metric tree">
Metric tree

A metric tree is any tree data structure specialized to index data in metric spaces. Metric trees exploit properties of metric spaces such as the triangle inequality to make accesses to the data more efficient. Examples include the M-tree, vp-trees, cover trees, MVP Trees, and BK-trees.

Most algorithms and data structures for searching a dataset are based on the classical binary search algorithm, and generalizations such as the k-d tree or range tree work by interleaving the binary search algorithm over the separate coordinates and treating each spatial coordinate as an independent search constraint. These data structures are well-suited for range query problems asking for every point formula_1 that satisfies formula_2 and formula_3.

A limitation of these multidimensional search structures is that they are only defined for searching over objects that can be treated as vectors. They aren't applicable for the more general case in which the algorithm is given only a collection of objects and a function for measuring the distance or similarity between two objects. If, for example, someone were to create a function that returns a value indicating how similar one image is to another, a natural algorithmic problem would be to take a dataset of images and find the ones that are similar according to the function to a given query image.

If there is no structure to the similarity measure then a brute force search requiring the comparison of the query image to every image in the dataset is the best that can be done . If, however, the similarity function satisfies the triangle inequality then it is possible to use the result of each comparison to prune the set of candidates to be examined.

The first article on metric trees, as well as the first use of the term "metric tree", published in the open literature was by Jeffrey Uhlmann in 1991. Other researchers were working independently on similar data structures. In particular, Peter Yianilos claimed to have independently discovered the same method, which he called a vantage point tree (VP-tree).
The research on metric tree data structures blossomed in the late 1990s and included an examination by Google co-founder Sergey Brin of their use for very large databases. The first textbook on metric data structures was published in 2006.



</doc>
<doc id="3417338" url="https://en.wikipedia.org/wiki?curid=3417338" title="BK-tree">
BK-tree

A BK-tree is a metric tree suggested by Walter Austin Burkhard and Robert M. Keller specifically adapted to discrete metric spaces.
For simplicity, consider integer discrete metric formula_1. Then, BK-tree is defined in the following way. An arbitrary element "a" is selected as root node. The root node may have zero or more subtrees. The "k-th" subtree is recursively built of all elements "b" such that formula_2. BK-trees can be used for approximate string matching in a dictionary .





</doc>
<doc id="13833" url="https://en.wikipedia.org/wiki?curid=13833" title="Hash table">
Hash table

In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an "index", also called a "hash code", into an array of "buckets" or "slots", from which the desired value can be found.

Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash "collisions" where the hash function generates the same index for more than one key. Such collisions are always accommodated in some way.

In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized) constant average cost per operation.

In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.

The idea of hashing is to distribute the entries (key/value pairs) across an array of "buckets". Given a key, the algorithm computes an "index" that suggests where the entry can be found:

Often this is done in two steps:

In this method, the "hash" is independent of the array size, and it is then "reduced" to an index (a number between codice_1 and codice_2) using the modulo operator (codice_3).

In the case that the array size is a power of two, the remainder operation is reduced to masking, which improves speed, but can increase problems with a poor hash function.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number. The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid "clustering", the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash is claimed to have particularly poor clustering behavior.

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for " any" size is not necessary, a non-cryptographic hashing function might be preferable.

If all keys are known ahead of time, a perfect hash function can be used to create a perfect hash table that has no collisions. If minimal perfect hashing is used, every location in the hash table can be used as well.

Perfect hashing allows for constant time lookups in all cases. This is in contrast to most chaining and open addressing methods, where the time for lookup is low on average, but may be very large, O("n"), for instance when all the keys hash to a few values.

A critical statistic for a hash table is the "load factor", defined as
where

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a "fixed" number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.

Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot.

Therefore, almost all hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values.

In the method known as "separate chaining", each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation.

In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing function needs to be fixed.

Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.

The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform, the "average" cost of a lookup depends only on the average number of keys per bucket—that is, it is roughly proportional to the load factor.

For this reason, chained hash tables remain effective even when the number of table entries "n" is much higher than the number of slots. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list.

For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number "n" of entries in the table.

The bucket chains are often searched sequentially using the order the entries were added to the bucket. If the load factor is large and some keys are more likely to come up than others, then rearranging the chain with a move-to-front heuristic may be effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.

Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the codice_4 pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance, making the processor cache ineffective.

Some chaining implementations store the first record of each chain in the slot array itself.
The number of pointer traversals is decreased by one for most cases. The purpose is to increase cache efficiency of hash table access.

The disadvantage is that an empty bucket takes the same space as a bucket with one entry. To save space, such hash tables often have about as many slots as stored entries, meaning that many slots have two or more entries.

Instead of a list, one can use any other data structure that supports the required operations. For example, by using a self-balancing binary search tree, the theoretical worst-case time of common hash table operations (insertion, deletion, lookup) can be brought down to O(log "n") rather than O("n"). However, this introduces extra complexity into the implementation, and may cause even worse performance for smaller hash tables, where the time spent inserting into and balancing the tree is greater than the time needed to perform a linear search on all of the elements of a list. A real world example of a hash table that uses a self-balancing binary search tree for buckets is the codice_5 class in Java version 8.

The variant called array hash table uses a dynamic array to store all the entries that hash to the same slot. Each newly inserted entry gets appended to the end of the dynamic array that is assigned to the slot. The dynamic array is resized in an "exact-fit" manner, meaning it is grown only by as many bytes as needed. Alternative techniques such as growing the array by block sizes or "pages" were found to improve insertion performance, but at a cost in space. This variation makes more efficient use of CPU caching and the translation lookaside buffer (TLB), because slot entries are stored in sequential memory positions. It also dispenses with the codice_4 pointers that are required by linked lists, which saves space. Despite frequent array resizing, space overheads incurred by the operating system such as memory fragmentation were found to be small.

An elaboration on this approach is the so-called dynamic perfect hashing, where a bucket that contains "k" entries is organized as a perfect hash table with "k" slots. While it uses more memory ("n" slots for "n" entries, in the worst case and "n" × "k" slots in the average case), this variant has guaranteed constant worst-case lookup time, and low amortized time for insertion.
It is also possible to use a fusion tree for each bucket, achieving constant time for all operations with high probability.

In another strategy, called open addressing, all entry records are stored in the bucket array itself. When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some "probe sequence", until an unoccupied slot is found. When searching for an entry, the buckets are scanned in the same sequence, until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table. The name "open addressing" refers to the fact that the location ("address") of the item is not determined by its hash value. (This method is also called closed hashing; it should not be confused with "open hashing" or "closed addressing" that usually mean separate chaining.)

Well-known probe sequences include:

A drawback of all these open addressing schemes is that the number of stored entries cannot exceed the number of slots in the bucket array. In fact, even with good hash functions, their performance dramatically degrades when the load factor grows beyond 0.7 or so. For many applications, these restrictions mandate the use of dynamic resizing, with its attendant costs.

Open addressing schemes also put more stringent requirements on the hash function: besides distributing the keys more uniformly over the buckets, the function must also minimize the clustering of hash values that are consecutive in the probe order. Using separate chaining, the only concern is that too many objects map to the "same" hash value; whether they are adjacent or nearby is completely irrelevant.

Open addressing only saves memory if the entries are small (less than four times the size of a pointer) and the load factor is not too small. If the load factor is close to zero (that is, there are far more buckets than stored entries), open addressing is wasteful even if each entry is just two words.

Open addressing avoids the time overhead of allocating each new entry record, and can be implemented even in the absence of a memory allocator. It also avoids the extra indirection required to access the first entry of each bucket (that is, usually the only one). It also has better locality of reference, particularly with linear probing. With small record sizes, these factors can yield better performance than chaining, particularly for lookups. 
Hash tables with open addressing are also easier to serialize, because they do not use pointers.

On the other hand, normal open addressing is a poor choice for large elements, because these elements fill entire CPU cache lines (negating the cache advantage), and a large amount of space is wasted on large empty table slots. If the open addressing table only stores references to elements (external storage), it uses space comparable to chaining even for large records but loses its speed advantage.

Generally speaking, open addressing is better used for hash tables with small records that can be stored within the table (internal storage) and fit in a cache line. They are particularly suitable for elements of one word or less. If the table is expected to have a high load factor, the records are large, or the data is variable-sized, chained hash tables often perform as well or better.

A hybrid of chaining and open addressing, coalesced hashing links together chains of nodes within the table itself. Like open addressing, it achieves space usage and (somewhat diminished) cache advantages over chaining. Like chaining, it does not exhibit clustering effects; in fact, the table can be efficiently filled to a high density. Unlike chaining, it cannot have more elements than table slots.

Another alternative open-addressing solution is cuckoo hashing, which ensures constant lookup and deletion time in the worst case, and constant amortized time for insertions (with low probability that the worst-case will be encountered). It uses two or more hash functions, which means any key/value pair could be in two or more locations. For lookup, the first hash function is used; if the key/value is not found, then the second hash function is used, and so on. If a collision happens during insertion, then the key is re-hashed with the second hash function to map it to another bucket. If all hash functions are used and there is still a collision, then the key it collided with is removed to make space for the new key, and the old key is re-hashed with one of the other hash functions, which maps it to another bucket. If that location also results in a collision, then the process repeats until there is no collision or the process traverses all the buckets, at which point the table is resized. By combining multiple hash functions with multiple cells per bucket, very high space utilization can be achieved.

Another alternative open-addressing solution is hopscotch hashing, which combines the approaches of cuckoo hashing and linear probing, yet seems in general to avoid their limitations. In particular it works well even when the load factor grows beyond 0.9. The algorithm is well suited for implementing a resizable concurrent hash table.

The hopscotch hashing algorithm works by defining a neighborhood of buckets near the original hashed bucket, where a given entry is always found. Thus, search is limited to the number of entries in this neighborhood, which is logarithmic in the worst case, constant on average, and with proper alignment of the neighborhood typically requires one cache miss. When inserting an entry, one first attempts to add it to a bucket in the neighborhood. However, if all buckets in this neighborhood are occupied, the algorithm traverses buckets in sequence until an open slot (an unoccupied bucket) is found (as in linear probing). At that point, since the empty bucket is outside the neighborhood, items are repeatedly displaced in a sequence of hops. (This is similar to cuckoo hashing, but with the difference that in this case the empty slot is being moved into the neighborhood, instead of items being moved out with the hope of eventually finding an empty slot.) Each hop brings the open slot closer to the original neighborhood, without invalidating the neighborhood property of any of the buckets along the way. In the end, the open slot has been moved into the neighborhood, and the entry being inserted can be added to it.

One interesting variation on double-hashing collision resolution is Robin Hood hashing. The idea is that a new key may displace a key already inserted, if its probe count is larger than that of the key at the current position. The net effect of this is that it reduces worst case search times in the table. This is similar to ordered hash tables except that the criterion for bumping a key does not depend on a direct relationship between the keys. Since both the worst case and the variation in the number of probes is reduced dramatically, an interesting variation is to probe the table starting at the expected successful probe value and then expand from that position in both directions.
External Robin Hood hashing is an extension of this algorithm where the table is stored in an external file and each table position corresponds to a fixed-sized page or bucket with "B" records.

2-choice hashing employs two different hash functions, "h"("x") and "h"("x"), for the hash table. Both hash functions are used to compute two table locations. When an object is inserted in the table, it is placed in the table location that contains fewer objects (with the default being the "h"("x") table location if there is equality in bucket size). 2-choice hashing employs the principle of the power of two choices.

When an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be "rehashed". Rehashing includes increasing the size of the underlying data structure and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.

To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table—followed by a rehash—when items are deleted. From the point of space–time tradeoffs, this operation is similar to the deallocation in dynamic arrays.

A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold "r". Then a new larger table is allocated, each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Likewise, when the load factor falls below a second threshold "r", all entries are moved to a new smaller table.

For hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a "shrink-to-fit" operation can be provided that does this only on request.

If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries "n" and of the number "m" of operations performed.

For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If "m" elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most "m" − 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.

Some hash table implementations, notably in real-time systems, cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually.

Disk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high.

One alternative to enlarging the table all at once is to perform the rehashing gradually:
To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it
is necessary to increase the size of the table by a factor of at least ("r" + 1)/"r" during resizing.

If it is known that key values will always increase (or decrease) monotonically, then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function—and indeed hash table—both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(log("N")) ranges to check, and binary search time for the redirection would be O(log(log("N"))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown.

Linear hashing is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions.

Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent in disk-based and distributed hash tables, where rehashing is prohibitively costly.
The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem.
The four most popular approaches are rendezvous hashing, consistent hashing, the content addressable network algorithm, and Kademlia distance.

In the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size formula_2 with open addressing has no collisions and holds up to formula_2 elements with a single comparison for successful lookup, while a table of size formula_2 with chaining and formula_5 keys has the minimum collisions and comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with formula_6 amortized comparisons per insertion and up to formula_5 comparisons for a successful lookup.

Adding rehashing to this model is straightforward. As in a dynamic array, geometric resizing by a factor of formula_8 implies that only formula_9 keys are inserted formula_10 or more times, so that the total number of insertions is bounded above by , which is formula_6. By using rehashing to maintain , tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function.

In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform, the assumption is called "simple uniform hashing" and it can be shown that hashing with chaining requires comparisons on average for an unsuccessful lookup, and hashing with open addressing requires . Both these bounds are constant, if we maintain using table resizing, where formula_12 is a fixed constant less than 1.










Hash tables are commonly used to implement many types of in-memory tables. They are used to implement associative arrays (arrays whose indices are arbitrary strings or other complicated objects), especially in interpreted programming languages like Ruby, Python, and PHP.

When storing a new item into a multimap and a hash collision occurs, the multimap unconditionally stores both items.

When storing a new item into a typical associative array and a hash collision occurs, but the actual keys themselves are different, the associative array likewise stores both items. However, if the key of the new item exactly matches the key of an old item, the associative array typically erases the old item and overwrites it with the new item, so every item in the table has a unique key.

Hash tables may also be used as disk-based data structures and database indices (such as in dbm) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins.

Hash tables can be used to implement caches, auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries—usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.

Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not.

Those structures can therefore be used to implement a set data structure, which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.

Several dynamic languages, such as Perl, Python, JavaScript, Lua, and Ruby, use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.

Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single "string pool" implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing, and can be used with many other kinds of data (expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.).

Many programming languages provide hash table functionality, either as built-in associative arrays or as standard library modules. In C++11, for example, the codice_7 class provides hash tables for keys and values of arbitrary type.

The Java programming language (including the variant which is used on Android) includes the codice_8, codice_5, codice_10, and codice_11 generic collections.

In PHP 5 and 7, the Zend 2 engine and the Zend 3 engine (respectively) use one of the hash functions from Daniel J. Bernstein to generate the hash values used in managing the mappings of data pointers stored in a hash table. In the PHP source code, it is labelled as codice_12 (Daniel J. Bernstein, Times 33 with Addition).

Python's built-in hash table implementation, in the form of the codice_13 type, as well as Perl's hash type (%) are used internally to implement namespaces and therefore need to pay more attention to security, i.e., collision attacks. Python sets also use hashes internally, for fast lookup (though they store only keys, not values).

In the .NET Framework, support for hash tables is provided via the non-generic codice_14 and generic codice_15 classes, which store key-value pairs, and the generic codice_8 class, which stores only values.

In Ruby the hash table uses the open addressing model from Ruby 2.4 onwards.

In Rust's standard library, the generic codice_5 and codice_8 structs use linear probing with Robin Hood bucket stealing.

ANSI Smalltalk defines the classes codice_19 / codice_20 and codice_15 / codice_22. All Smalltalk implementations provide additional (not yet standardized) versions of codice_23, codice_24 and codice_25.

Tcl array variables are hash tables, and Tcl dictionaries are immutable values based on hashes. The functionality is also available as C library functions Tcl_InitHashTable et al. (for generic hash tables) and Tcl_NewDictObj et al. (for dictionary values). The performance has been independently benchmarked as extremely competitive.

The idea of hashing arose independently in different places. In January 1953, Hans Peter Luhn wrote an internal IBM memorandum that used hashing with chaining. Gene Amdahl, Elaine M. McGraw, Nathaniel Rochester, and Arthur Samuel implemented a program using hashing at about the same time. Open addressing with linear probing (relatively prime stepping) is credited to Amdahl, but Ershov (in Russia) had the same idea.


There are several data structures that use hash functions but cannot be considered special cases of hash tables:




</doc>
<doc id="13790" url="https://en.wikipedia.org/wiki?curid=13790" title="Hash function">
Hash function

A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called "hash values", "hash codes", "digests", or simply "hashes". The values are used to index a fixed-size table called a "hash table". Use of a hash function to index a hash table is called "hashing" or "scatter storage addressing".

Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval, and storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally and storage space efficient form of data access which avoids the non-linear access time of ordered and unordered lists and structured trees, and the often exponential storage requirements of direct access of state spaces of large or variable-length keys.

Use of hash functions relies on statistical properties of key and function interaction: worst case behavior is intolerably bad with a vanishingly small probability, and average case behavior can be nearly optimal (minimal collisions).

Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently.

A hash function takes as input a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length, like an integer, or variable length, like a name. In some cases, the key is the datum itself. The output is a hash code used to index a hash table holding the data or records, or pointers to them.

A hash function may be considered to perform three functions:
A good hash function satisfies two basic properties: 1) it should be very fast to compute; 2) it should minimize duplication of output values (collisions). Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant. High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. 
Hash functions can be designed to give best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot.

Hash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added to the table there. If the hash code indexes a full slot, some kind of collision resolution is required: the new item may be omitted (not added to the table), or replace the old item, or it can be added to the table in some other location by a specified procedure. That procedure depends on the structure of the hash table: In "chained hashing", each slot is the head of a linked list or chain, and items that collide at the slot are added to the chain. Chains may be kept in random order and searched linearly, or in serial order, or as a self-ordering list by frequency to speed up access. In "open address hashing", the table is probed starting from the occupied slot in a specified manner, usually by linear probing, quadratic probing, or double hashing until an open slot is located or the entire table is probed (overflow). Searching for the item follows the same procedure until the item is located, an open slot is found or the entire table has been searched (item not in table).

Hash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items.

Hash functions are an essential ingredient of the Bloom filter, a space-efficient probabilistic data structure that is used to test whether an element is a member of a set.

A special case of hashing is known as geometric hashing or "the grid method". In these applications, the set of all inputs is some sort of metric space, and the hashing function can be interpreted as a partition of that space into a grid of "cells". The table is often an array with two or more indices (called a "grid file", "grid index", "bucket grid", and similar names), and the hash function returns an index tuple. This principle is widely used in computer graphics, computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space, such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database, and so on.

Hash tables are also used to implement associative arrays and dynamic sets.

A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of "collisions"—pairs of inputs that are mapped to the same hash value—increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.

Note that this criterion only requires the value to be "uniformly distributed", not "random" in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.

Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.

In other words, if a typical set of "m" records is hashed to "n" table slots, the probability of a bucket receiving many more than "m"/"n" records should be vanishingly small. In particular, if "m" is less than "n", very few buckets should have more than one or two records. A small number of collisions is virtually inevitable, even if "n" is much larger than "m" – see the birthday problem.

In special cases when the keys are known in advance and the key set is static, a hash function can be found that achieves absolute or collisionless) uniformity. Such a hash function is said to be "perfect". There is no algorithmic way of constructing such a function - searching for one is a factorial function of the number of keys to be mapped versus the number of table slots they're mapped into. Finding a perfect hash function over more than a very small set of keys is usually computationally infeasible; the resulting function is likely to be more computationally complex than a standard hash function, and provides only a marginal advantage over a function with good statistical properties that yields a minimum number of collisions. See universal hash function.

When testing a hash function, the uniformity of the distribution of hash values can be evaluated by the chi-squared test. This test is a goodness-of-fit measure: it's the actual distribution of items in buckets versus the expected (or uniform) distribution of items. The formula is:
formula_1
where: formula_2 is the number of keys, formula_3 is the number of buckets, formula_4 is the number of items in bucket formula_5

A ratio within one confidence interval (0.95 - 1.05) is indicative that the hash function evaluated has an expected uniform distribution.

Hash functions can have some technical properties that make it more likely that they'll have a uniform distribution when applied. One is the strict avalanche criterion: whenever a single input bit is complemented, each of the output bits changes with a 50% probability. The reason for this property is that selected subsets of the key space may have low variability. In order for the output to be uniformly distributed, a low amount of variability, even one bit, should translate into a high amount of variability (i.e. distribution over the table space) in the output. Each bit should change with probability 50% because if some bits are reluctant to change, the keys become clustered around those values. If the bits want to change too readily, the mapping is approaching a fixed XOR function of a single bit. Standard tests for this property have been described in the literature. The relevance of the criterion to a multiplicative hash function is assessed here.

In data storage and retrieval applications, use of a hash function is a trade off between search time and data storage space. If search time were unbounded, a very compact unordered linear list would be the best medium; if storage space were unbounded, a randomly accessible structure indexable by the key value would be very large, very sparse, but very fast. A hash function takes a finite amount of time to map a potentially large key space to a feasible amount of storage space searchable in a bounded amount of time regardless of the number of keys. In most applications, it is highly desirable that the hash function be computable with minimum latency and secondarily in a minimum number of instructions.

Computational complexity varies with the number of instructions required and latency of individual instructions, with the simplest being the bitwise methods (folding), followed by the multiplicative methods, and the most complex (slowest) are the division-based methods.

Because collisions should be infrequent, and cause a marginal delay but are otherwise harmless, it's usually preferable to choose a faster hash function over one that needs more computation but saves a few collisions.

Division-based implementations can be of particular concern, because division is microprogrammed on nearly all chip architectures. Divide (modulo) by a constant can be inverted to become a multiply by the word-size multiplicative-inverse of the constant. This can be done by the programmer, or by the compiler. Divide can also be reduced directly into a series of shift-subtracts and shift-adds, though minimizing the number of such operations required is a daunting problem; the number of assembly instructions resulting may be more than a dozen, and swamp the pipeline. If the architecture has a hardware multiply functional unit, the multiply-by-inverse is likely a better approach.

We can allow the table size "n" to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let "n" be significantly less than 2. Consider a pseudorandom number generator function "P"(key) that is uniform on the interval [0, 2 − 1]. A hash function uniform on the interval [0, n-1] is "n" "P"(key)/2. We can replace the division by a (possibly faster) right bit shift: "nP"(key) » "b".

If keys are being hashed repeatedly, and the hash function is costly, computing time can be saved by precomputing the hash codes and storing them with the keys. Matching hash codes almost certainly mean the keys are identical. This technique is used for the transposition table in game-playing programs, which stores a 64-bit hashed representation of the board position.

A "universal hashing" scheme is a randomized algorithm that selects a hashing function "h" among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/"m", where "m" is the number of distinct hash values desired—independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will, however, have more collisions than perfect hashing and may require more operations than a special-purpose hash function.

A hash function should be applicable to all situations in which a hash function might be used.
A hash function that allows only certain table sizes, strings only up to a certain length, or can't accept a seed (i.e. allow double hashing) isn't as useful as one that does.

A hash procedure must be deterministic—meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection), although sometimes rehashing of the item is possible.

The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ.

It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. 
Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value.

In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters—the input data "z", and the number "n" of allowed hash values.

A common solution is to compute a fixed hash function with a very large range (say, 0 to 2 − 1), divide the result by "n", and use the division's remainder. If "n" is itself a power of 2, this can be done by bit masking and bit shifting. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and "n" − 1, for any value of "n" that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of "n", e.g. odd or prime numbers.

When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.

A hash function that will relocate the minimum number of records when the table is resized is desirable.
What is needed is a hash function "H"("z","n") – where "z" is the key being hashed and "n" is the number of allowed hash values – such that "H"("z","n" + 1) = "H"("z","n") with probability close to "n"/("n" + 1).

Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property. Extendible hashing uses a dynamic hash function that requires space proportional to "n" to compute the hash function, and it becomes a function of the previous keys that have been inserted. Several algorithms that preserve the uniformity property but require time proportional to "n" to compute the value of "H"("z","n") have been invented.

A hash function with minimal movement is especially useful in distributed hash tables.

In some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data equivalence criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.

There are several common algorithms for hashing integers. The method giving the best distribution is data-dependent. One of the simplest and most common methods in practice is the modulo division method.

If the data to be hashed is small enough, one can use the data itself (reinterpreted as an integer) as the hashed value. The cost of computing this "identity" hash function is effectively zero. This hash function is perfect, as it maps each input to a distinct hash value.

The meaning of "small enough" depends on the size of the type that is used as the hashed value. For example, in Java, the hash code is a 32-bit integer. Thus the 32-bit integer codice_1 and 32-bit floating-point codice_2 objects can simply use the value directly; whereas the 64-bit integer codice_3 and 64-bit floating-point codice_4 cannot use this method.

Other types of data can also use this hashing scheme. For example, when mapping character strings between upper and lower case, one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character ("A" for "a", "8" for "8", etc.). If each character is stored in 8 bits (as in extended ASCII or ISO Latin 1), the table has only 2 = 256 entries; in the case of Unicode characters, the table would have 17×2 = entries.

The same technique can be used to map two-letter country codes like "us" or "za" to country names (26 = 676 table entries), 5-digit zip codes like 13083 to city names ( entries), etc. Invalid data values (such as the country code "xx" or the zip code 00000) may be left undefined in the table or mapped to some appropriate "null" value.

If the keys are uniformly or sufficiently uniformly distributed over the key space, so that the key values are essentially random, they may be considered to be already 'hashed'. In this case, any number of any bits in the key may be dialed out and collated as an index into the hash table. A simple such hash function would be to mask off the bottom "m" bits to use as an index into a table of size 2.

A folding hash code is produced by dividing the input into n sections of m bits, where 2^m is the table size, and using a parity-preserving bitwise operation like ADD or XOR, to combine the sections. The final operation is a mask or shift to trim off any excess bits at the high or low end.
For example, for a table size of 15 bits and key value of 0x0123456789ABCDEF, there are 5 sections 0x4DEF, 0x1357, 0x159E, 0x091A and 0x8. Adding, we obtain 0x7AA4, a 15-bit value.

A mid-squares hash code is produced by squaring the input and extracting an appropriate number of middle digits or bits. For example, if the input is 123,456,789 and the hash table size 10,000, squaring the key produces 1.524157875019e16, so the hash code is taken as the middle 4 digits of the 17-digit number (ignoring the high digit) 8750. The mid-squares method produces a reasonable hash code if there are not a lot of leading or trailing zeros in the key. This is a variant of multiplicative hashing, but not as good, because an arbitrary key is not a good multiplier.

A standard technique is to use a modulo function on the key, by selecting a divisor formula_6 which is a prime number close to the table size, so formula_7. The table size is usually a power of 2. This gives a distribution from formula_8. This gives good results over a large number of key sets. A significant drawback of division hashing is that division is microprogrammed on most modern architectures including x86, and can be 10 times slower than multiply. A second drawback is that it won't break up clustered keys. For example, the keys 123000, 456000, 789000, etc. modulo 1000 all map to the same address. This technique works well in practice because many key sets are sufficiently random already, and the probability that a key set will be cyclical by a large prime number is small.

Algebraic coding is a variant of the division method of hashing which uses division by a polynomial modulo 2 instead of an integer to map n bits to m bits. In this approach, formula_9 and we postulate an formula_3th degree polynomial formula_11. A key formula_12 can be regarded as the polynomial formula_13. The remainder using polynomial arithmetic modulo 2 is formula_14. Then formula_15. If formula_16 is constructed to have t or fewer non-zero coefficients, then keys differing by t or fewer bits are guaranteed to not collide.

Z a function of k, t and n, a divisor of 2-1, is constructed from the GF(2) field. Knuth gives an example: for n=15, m=10 and t=7, formula_17. The derivation is as follows: 

The usual outcome is that either n will get large, or t will get large, or both, in order for the scheme to be computationally feasible. Therefore, its more suited to hardware or microcode implementation.

See also unique permutation hashing, which has a guaranteed best worst-case insertion time.

Standard multiplicative hashing uses the formula formula_33 which produces a hash value in formula_34. The value formula_35 is an appropriately chosen value that should be relatively prime to formula_36; it should be large and its binary representation a random mix of 1's and 0's. An important practical special case occurs when formula_37 and formula_38 are powers of 2 and formula_39 is the machine word size. In this case this formula becomes formula_40. This is special because arithmetic modulo formula_41 is done by default in low-level programming languages and integer division by a power of 2 is simply a right-shift, so, in C, for example, this function becomes
and for fixed formula_3 and formula_39 this translates into a single integer multiplication and right-shift making it one of the fastest hash functions to compute.

Multiplicative hashing is susceptible to a "common mistake" that leads to poor diffusion—higher-value input bits do not affect lower-value output bits. A transmutation on the input which shifts the span of retained top bits down and XORs or ADDs them to the key before the multiplication step corrects for this. So the resulting function looks like:

Fibonacci hashing is a form of multiplicative hashing in which the multiplier is formula_44, where formula_45 is the machine word length and 
formula_46 (phi) is the golden ratio. formula_46 is an irrational number with approximate value 5/3, and decimal expansion of 1.618033... A property of this multiplier is that it uniformly distributes over the table space, blocks of consecutive keys with respect to any block of bits in the key. Consecutive keys within the high bits or low bits of the key (or some other field) are relatively common. The multipliers for various word lengths formula_45 are:

Tabulation hashing, more generally known as "Zobrist hashing" after Albert Zobrist, an American computer scientist, is a method for constructing universal families of hash functions by combining table lookup with XOR operations. This algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer-number keys).

Zobrist hashing was originally introduced as a means of compactly representing chess positions in computer game playing programs. A unique random number was assigned to represent each type of piece (six each for black and white) on each space of the board. Thus a table of 64x12 such numbers is initialized at the start of the program. The random numbers could be any length, but 64 bits was natural due to the 64 squares on the board. A position was transcribed by cycling through the pieces in a position, indexing the corresponding random numbers (vacant spaces were not included in the calculation), and XORing them together (the starting value could be 0, the identity value for XOR, or a random seed). The resulting value was reduced by modulo, folding or some other operation to produce a hash table index. The original Zobrist hash was stored in the table as the representation of the position.

Later, the method was extended to hashing integers by representing each byte in each of 4 possible positions in the word by a unique 32-bit random number. Thus, a table of 2x4 of such random numbers is constructed. A 32-bit hashed integer is transcribed by successively indexing the table with the value of each byte of the plain text integer and XORing the loaded values together (again, the starting value can be the identity value or a random seed). The natural extension to 64-bit integers is by use of a table of 2x8 64-bit random numbers.

This kind of function has some nice theoretical properties, one of which is called "3-tuple independence" meaning every 3-tuple of keys is equally likely to be mapped to any 3-tuple of hash values.

A hash function can be designed to exploit existing entropy in the keys. If the keys have leading or trailing zeros, or particular fields that are unused, always zero or some other constant, or generally vary little, then masking out only the volatile bits and hashing on those will provide a better and possibly faster hash function. Selected divisors or multipliers in the division and multiplicative schemes may make more uniform hash functions if the keys are cyclic or have other redundancies.

When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.

Simplistic hash functions may add the first and last "n" characters of a string along with the length, or form a word-size hash from the middle 4 characters of a string. This saves iterating over the (potentially long) string,
but hash functions which do not hash on all characters of a string can readily become linear due to redundancies, clustering or other pathologies in the key set. Such strategies may be effective as a custom hash function if the structure of the keys is such that either the middle, ends or other field(s) are zero or some other invariant constant that doesn't differentiate the keys; then the invariant parts of the keys can be ignored.

The paradigmatic example of folding by characters is to add up the integer values of all the characters in the string. A better idea is to multiply the hash total by a constant, typically a sizeable prime number, before adding in the next character, ignoring overflow. Using exclusive 'or' instead of add is also a plausible alternative. The final operation would be a modulo, mask, or other function to reduce the word value to an index the size of the table. The weakness of this procedure is that information may cluster in the upper or lower bits of the bytes, which clustering will remain in the hashed result and cause more collisions than a proper randomizing hash. ASCII byte codes, for example, have an upper bit of 0 and printable strings don't use the first 32 byte codes, so the information (95 byte codes) is clustered in the remaining bits in an unobvious manner.

The classic approach dubbed the PJW hash based on the work of Peter. J. Weinberger at ATT Bell Labs in the 1970s, was originally designed for hashing identifiers into compiler symbol tables as given in the "Dragon Book". This hash function offsets the bytes 4 bits before ADDing them together. When the quantity wraps, the high 4 bits are shifted out and if non-zero, XORed back into the low byte of the cumulative quantity. The result is a word size hash code to which a modulo or other reducing operation can be applied to produce the final hash index.

Today, especially with the advent of 64-bit word sizes, much more efficient variable length string hashing by word-chunks is available.

Modern microprocessors will allow for much faster processing, if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64 bit integers and hashing/accumulating these "wide word" integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The final word, which may have unoccupied byte positions, is filled with zeros or a specified "randomizing" value before being folded into the hash. The accumulated hash code is reduced by a final modulo or other operation to yield an index into the table.

Analogous to the way an ASCII or EBCDIC character string representing a decimal number is converted to a numeric quantity for computing, a variable length string can be converted as (xa+xa+...+xa+x). This is simply a polynomial in a non-zero "radix" "a"!=1 that takes the components (x,x...,x) as the characters of the input string of length k. It can be used directly as the hash code, or a hash function applied to it to map the potentially large value to the hash table size. The value of "a" is usually a prime number at least large enough to hold the number of different characters in the character set of potential keys. Radix conversion hashing of strings minimizes the number of collisions. Available data sizes may restrict the maximum length of string that can be hashed with this method. For example, a 128-bit double long word will hash only a 26 character alphabetic string (ignoring case) with a radix of 29; a printable ASCII string is limited to 9 characters using radix 97 and a 64-bit long word. However, alphabetic keys are usually of modest length, because keys must be stored in the hash table. Numeric character strings are usually not a problem; 64 bits can count up to 10, or 19 decimal digits with radix 10.

In some applications, such as substring search, one can compute a hash function "h" for every "k"-character substring of a given "n"-character string by advancing a window of width "k" characters along the string; where "k" is a fixed integer, and "n" is greater than "k". The straightforward solution, which is to extract such a substring at every character position in the text and compute "h" separately, requires a number of operations proportional to "k"·"n". However, with the proper choice of "h", one can use the technique of rolling hash to compute all those hashes with an effort proportional to "mk" + "n" where "m" is the number of occurrences of the substring..
The most familiar algorithm of this type is Rabin-Karp with best and average case performance "O(n+mk)" and worst case "O(n·k)" (in all fairness, the worst case here is gravely pathological: both the text string and substring are composed of a repeated single character, such as t="AAAAAAAAAAA", and s="AAA"). The hash function used for the algorithm is usually the Rabin fingerprint, designed to avoid collisions in 8-bit character strings, but other suitable hash functions are also used.

Worst case result for a hash function can be assessed two ways: theoretical and practical. Theoretical worst case is the probability that all keys map to a single slot. Practical worst case is expected longest probe sequence (hash function + collision resolution method). This analysis considers uniform hashing, that is, any key will map to any particular slot with probability 1/m, characteristic of universal hash functions.

While Knuth worries about adversarial attack on real time systems, Gonnet has shown that the probability of such a case is "ridiculously small". His representation was that the probability of k of n keys mapping to a single slot is formula_49 where formula_50 is the load factor, n/m.

The term "hash" offers a natural analogy with its non-technical meaning (to "chop" or "make a mess" out of something), given how hash functions scramble their input data to derive their output. In his research for the precise origin of the term, Donald Knuth notes that, while Hans Peter Luhn of IBM appears to have been the first to use the concept of a hash function in a memo dated January 1953, the term itself would only appear in published literature in the late 1960s, on Herbert Hellerman's "Digital Computer System Principles", even though it was already widespread jargon by then.



</doc>
<doc id="1583843" url="https://en.wikipedia.org/wiki?curid=1583843" title="Open addressing">
Open addressing

Open addressing, or closed hashing, is a method of collision resolution in hash tables. With this method a hash collision is resolved by probing, or searching through alternate locations in the array (the "probe sequence") until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table. Well-known probe sequences include:


The main tradeoffs between these methods are that linear probing has the best cache performance but is most sensitive to clustering, while double hashing has poor cache performance but exhibits virtually no clustering; quadratic probing falls in-between in both areas. Double hashing can also require more computation than other forms of probing.

Some open addressing methods, such as
Hopscotch hashing,
Robin Hood hashing,
last-come-first-served hashing and cuckoo hashing move existing keys around in the array to make room for the new key. This gives better maximum search times than the methods based on probing.
A critical influence on performance of an open addressing hash table is the "load factor"; that is, the proportion of the slots in the array that are used. As the load factor increases towards 100%, the number of probes that may be required to find or insert a given key rises dramatically. Once the table becomes full, probing algorithms may even fail to terminate. Even with good hash functions, load factors are normally limited to 80%. A poor hash function can exhibit poor performance even at very low load factors by generating significant clustering, especially with the simplest linear addressing method. Generally typical load factors with most open addressing methods are 50%, whilst separate chaining typically can use up to 100%.
What causes hash functions to cluster is not well understood , and it is easy to unintentionally write a hash function that causes severe clustering.

The following pseudocode is an implementation of an open addressing hash table with linear probing and single-slot stepping, a common approach that is effective if the hash function is good. Each of the lookup, set and remove functions use a common internal function find_slot to locate the array slot that either does or should contain a given key.



Another technique for removal is simply to mark the slot as deleted. However this eventually requires rebuilding the table simply to remove deleted records. The methods above provide O(1) updating and removal of existing records, with occasional rebuilding if the high-water mark of the table size grows.

The O(1) remove method above is only possible in linearly probed hash tables with single-slot stepping. In the case where many records are to be deleted in one operation, marking the slots for deletion and later rebuilding may be more efficient.



</doc>
<doc id="13999239" url="https://en.wikipedia.org/wiki?curid=13999239" title="Lazy deletion">
Lazy deletion

In computer science, lazy deletion refers to a method of deleting elements from a hash table that uses open addressing. In this method, deletions are done by marking an element as deleted, rather than erasing it entirely. Deleted locations are treated as empty when inserting and as occupied during a search.

The problem with this scheme is that as the number of delete/insert operations increases, the cost of a successful search increases. To improve this, when an element is searched and found in the table, the element is relocated to the first location marked for deletion that was probed during the search. Instead of finding an element to relocate when the deletion occurs, the relocation occurs lazily during the next search.


</doc>
<doc id="1852304" url="https://en.wikipedia.org/wiki?curid=1852304" title="Linear probing">
Linear probing

Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of key–value pairs and looking up the value associated with a given key. It was invented in 1954 by Gene Amdahl, Elaine M. McGraw, and Arthur Samuel and first analyzed in 1963 by Donald Knuth.

Along with quadratic probing and double hashing, linear probing is a form of open addressing. In these schemes, each cell of a hash table stores a single key–value pair. When the hash function causes a collision by mapping a new key to a cell of the hash table that is already occupied by another key, linear probing searches the table for the closest following free location and inserts the new key there. Lookups are performed in the same way, by searching the table sequentially starting at the position given by the hash function, until finding a cell with a matching key or an empty cell.

As write, "Hash tables are the most commonly used nontrivial data structures, and the most popular implementation on standard hardware uses linear probing, which is both fast and simple."
Linear probing can provide high performance because of its good locality of reference, but is more sensitive to the quality of its hash function than some other collision resolution schemes. It takes constant expected time per search, insertion, or deletion when implemented using a random hash function, a 5-independent hash function, or tabulation hashing. Good results can also be achieved in practice with other hash functions such as MurmurHash.

Linear probing is a component of open addressing schemes for using a hash table to solve the dictionary problem. In the dictionary problem, a data structure should maintain a collection of key–value pairs subject to operations that insert or delete pairs from the collection or that search for the value associated with a given key.
In open addressing solutions to this problem, the data structure is an array (the hash table) whose cells (when nonempty) each store a single key–value pair. A hash function is used to map each key into the cell of where that key should be stored, typically scrambling the keys so that keys with similar values are not placed near each other in the table. A hash collision occurs when the hash function maps a key into a cell that is already occupied by a different key. Linear probing is a strategy for resolving collisions, by placing the new key into the closest following empty cell.

To search for a given key , the cells of are examined, beginning with the cell at index (where is the hash function) and continuing to the adjacent cells , , ..., until finding either an empty cell or a cell whose stored key is .
If a cell containing the key is found, the search returns the value from that cell. Otherwise, if an empty cell is found, the key cannot be in the table, because it would have been placed in that cell in preference to any later cell that has not yet been searched. In this case, the search returns as its result that the key is not present in the dictionary.

To insert a key–value pair into the table (possibly replacing any existing pair with the same key), the insertion algorithm follows the same sequence of cells that would be followed for a search, until finding either an empty cell or a cell whose stored key is .
The new key–value pair is then placed into that cell.

If the insertion would cause the load factor of the table (its fraction of occupied cells) to grow above some preset threshold, the whole table may be replaced by a new table, larger by a constant factor, with a new hash function, as in a dynamic array. Setting this threshold close to zero and using a high growth rate for the table size leads to faster hash table operations but greater memory usage than threshold values close to one and low growth rates. A common choice would be to double the table size when the load factor would exceed 1/2, causing the load factor to stay between 1/4 and 1/2.

It is also possible to remove a key–value pair from the dictionary. However, it is not sufficient to do so by simply emptying its cell. This would affect searches for other keys that have a hash value earlier than the emptied cell, but that are stored in a position later than the emptied cell. The emptied cell would cause those searches to incorrectly report that the key is not present.

Instead, when a cell is emptied, it is necessary to search forward through the following cells of the table until finding either another empty cell or a key that can be moved to cell (that is, a key whose hash value is equal to or earlier than ). When an empty cell is found, then emptying cell is safe and the deletion process terminates. But, when the search finds a key that can be moved to cell , it performs this move. This has the effect of speeding up later searches for the moved key, but it also empties out another cell, later in the same block of occupied cells. The search for a movable key continues for the new emptied cell, in the same way, until it terminates by reaching a cell that was already empty. In this process of moving keys to earlier cells, each key is examined only once. Therefore, the time to complete the whole process is proportional to the length of the block of occupied cells containing the deleted key, matching the running time of the other hash table operations.

Alternatively, it is possible to use a lazy deletion strategy in which a key–value pair is removed by replacing the value by a special flag value indicating a deleted key. However, these flag values will contribute to the load factor of the hash table. With this strategy, it may become necessary to clean the flag values out of the array and rehash all the remaining key–value pairs once too large a fraction of the array becomes occupied by deleted keys.

Linear probing provides good locality of reference, which causes it to require few uncached memory accesses per operation. Because of this, for low to moderate load factors, it can provide very high performance. However, compared to some other open addressing strategies, its performance degrades more quickly at high load factors because of primary clustering, a tendency for one collision to cause more nearby collisions. Additionally, achieving good performance with this method requires a higher-quality hash function than for some other collision resolution schemes. When used with low-quality hash functions that fail to eliminate nonuniformities in the input distribution, linear probing can be slower than other open-addressing strategies such as double hashing, which probes a sequence of cells whose separation is determined by a second hash function, or quadratic probing, where the size of each step varies depending on its position within the probe sequence.

Using linear probing, dictionary operations can be implemented in constant expected time. In other words, insert, remove and search operations can be implemented in O(1), as long as the load factor of the hash table is a constant strictly less than one.

In more detail, the time for any particular operation (a search, insertion, or deletion) is proportional to the length of the contiguous block of occupied cells at which the operation starts. If all starting cells are equally likely, in a hash table with cells, then a maximal block of occupied cells will have probability of containing the starting location of a search, and will take time whenever it is the starting location. Therefore, the expected time for an operation can be calculated as the product of these two terms, , summed over all of the maximal blocks of contiguous cells in the table. A similar sum of squared block lengths gives the expected time bound for a random hash function (rather than for a random starting location into a specific state of the hash table), by summing over all the blocks that could exist (rather than the ones that actually exist in a given state of the table), and multiplying the term for each potential block by the probability that the block is actually occupied. That is,
defining to be the event that there is a maximal contiguous block of occupied cells of length beginning at index , the expected time per operation is
This formula can be simplified by replacing by a simpler necessary condition , the event that
at least elements have hash values that lie within a block of cells of length . After this replacement, the value within the sum no longer depends on , and the factor cancels the terms of the outer summation. These simplifications lead to the bound
But by the multiplicative form of the Chernoff bound, when the load factor is bounded away from one, the probability that a block of length contains at least hashed values is exponentially small as a function of ,
causing this sum to be bounded by a constant independent of . It is also possible to perform the same analysis using Stirling's approximation instead of the Chernoff bound to estimate the probability that a block contains exactly hashed values.

In terms of the load factor , the expected time for a successful search is , and the expected time for an unsuccessful search (or the insertion of a new key) is .
For constant load factors, with high probability, the longest probe sequence (among the probe sequences for all keys stored in the table) has logarithmic length.

Because linear probing is especially sensitive to unevenly distributed hash values, it is important to combine it with a high-quality hash function that does not produce such irregularities.

The analysis above assumes that each key's hash is a random number independent of the hashes of all the other keys. This assumption is unrealistic for most applications of hashing.
However, random or pseudorandom hash values may be used when hashing objects by their identity rather than by their value. For instance, this is done using linear probing by the IdentityHashMap class of the Java collections framework.
The hash value that this class associates with each object, its identityHashCode, is guaranteed to remain fixed for the lifetime of an object but is otherwise arbitrary. Because the identityHashCode is constructed only once per object, and is not required to be related to the object's address or value, its construction may involve slower computations such as the call to a random or pseudorandom number generator. For instance, Java 8 uses an Xorshift pseudorandom number generator to construct these values.

For most applications of hashing, it is necessary to compute the hash function for each value every time that it is hashed, rather than once when its object is created. In such applications, random or pseudorandom numbers cannot be used as hash values, because then different objects with the same value would have different hashes. And cryptographic hash functions (which are designed to be computationally indistinguishable from truly random functions) are usually too slow to be used in hash tables. Instead, other methods for constructing hash functions have been devised. These methods compute the hash function quickly, and can be proven to work well with linear probing. In particular, linear probing has been analyzed from the framework of -independent hashing, a class of hash functions that are initialized from a small random seed and that are equally likely to map any -tuple of distinct keys to any -tuple of indexes. The parameter can be thought of as a measure of hash function quality: the larger is, the more time it will take to compute the hash function but it will behave more similarly to completely random functions.
For linear probing, 5-independence is enough to guarantee constant expected time per operation,
while some 4-independent hash functions perform badly, taking up to logarithmic time per operation.
Another method of constructing hash functions with both high quality and practical speed is tabulation hashing. In this method, the hash value for a key is computed by using each byte of the key as an index into a table of random numbers (with a different table for each byte position). The numbers from those table cells are then combined by a bitwise exclusive or operation. Hash functions constructed this way are only 3-independent. Nevertheless, linear probing using these hash functions takes constant expected time per operation. Both tabulation hashing and standard methods for generating 5-independent hash functions are limited to keys that have a fixed number of bits. To handle strings or other types of variable-length keys, it is possible to compose a simpler universal hashing technique that maps the keys to intermediate values and a higher quality (5-independent or tabulation) hash function that maps the intermediate values to hash table indices.

In an experimental comparison, Richter et al. found that the Multiply-Shift family of hash functions (defined as formula_3) was "the fastest hash function when integrated with all hashing schemes, i.e., producing the highest throughputs and also of good quality" whereas tabulation hashing produced "the lowest throughput".
They point out that each table look-up require several cycles, being more expensive than simple arithmetic operations. They also found MurmurHash to be superior than tabulation hashing: "By studying the results provided by Mult and Murmur, we think that the trade-off for by tabulation (...) is less attractive in practice".

The idea of an associative array that allows data to be accessed by its value rather than by its address dates back to the mid-1940s in the work of Konrad Zuse and Vannevar Bush, but hash tables were not described until 1953, in an IBM memorandum by Hans Peter Luhn. Luhn used a different collision resolution method, chaining, rather than linear probing.

The first theoretical analysis of linear probing, showing that it takes constant expected time per operation with random hash functions, was given by Knuth. Sedgewick calls Knuth's work "a landmark in the analysis of algorithms". Significant later developments include a more detailed analysis of the probability distribution of the running time, and the proof that linear probing runs in constant time per operation with practically usable hash functions rather than with the idealized random functions assumed by earlier analysis.


</doc>
<doc id="1852324" url="https://en.wikipedia.org/wiki?curid=1852324" title="Quadratic probing">
Quadratic probing

Quadratic probing is an open addressing scheme in computer programming for resolving hash collisions in hash tables. Quadratic probing operates by taking the original hash index and adding successive values of an arbitrary quadratic polynomial until an open slot is found.

An example sequence using quadratic probing is:

formula_1

Quadratic probing can be a more efficient algorithm in a open addressing table, since it better avoids the clustering problem that can occur with linear probing, although it is not immune. It also provides good memory caching because it preserves some locality of reference; however, linear probing has greater locality and, thus, better cache performance.

Let h(k) be a hash function that maps an element k to an integer in [0,m-1], where m is the size of the table. Let the i probe position for a value k be given by the function 
where c ≠ 0. If c = 0, then h(k,i) degrades to a linear probe. For a given hash table, the values of c and c remain constant.

Examples:

In the case of quadratic probing, with the exception of the triangular number case for a power-of-two-sized hash table, there is no guarantee of finding an empty cell once the table gets more than half full, or even before the table gets half full if the table size is not prime. This is because at most half of the table can be used as alternative locations to resolve collisions.
If the hash table size is formula_6 (a prime greater than 3), it can be proven that the first formula_7 alternative locations including the initial location h(k) are all distinct and unique.
Suppose, we assume two of the alternative locations to be given by formula_8 and formula_9, where 0 ≤ x, y ≤ (b / 2).
If these two locations point to the same key space, but x ≠ y. Then the following would have to be true,
As b (table size) is a prime greater than 3, either (x - y) or (x + y) has to be equal to zero.
Since x and y are unique, (x - y) cannot be zero.
Also, since 0 ≤ x, y ≤ (b / 2), (x + y) cannot be zero.

Thus, by contradiction, it can be said that the first (b / 2) alternative locations after h(k) are unique.
So an empty key space can always be found as long as at most (b / 2) locations are filled, i.e., the hash table is not more than half full.

If the sign of the offset is alternated (e.g. +1, -4, +9, -16 etc.), and if the number of buckets is a prime number p congruent to 3 modulo 4 (i.e. one of 3, 7, 11, 19, 23, 31 and so on), then the first p offsets will be unique modulo p.

In other words, a permutation of 0 through p-1 is obtained, and, consequently, a free bucket will always be found as long as there exists at least one.



</doc>
<doc id="1583816" url="https://en.wikipedia.org/wiki?curid=1583816" title="Double hashing">
Double hashing

Double hashing is a computer programming technique used in conjunction with open-addressing in hash tables to resolve hash collisions, by using a secondary hash of the key as an offset when a collision occurs. Double hashing with open addressing is a classical data structure on a table formula_1.

It uses one hash value as an index into the table and then repeatedly steps forward an interval until the desired value is located, an empty location is reached, or the entire table has been searched; but this interval is set by a second, independent hash function. Unlike the alternative collision-resolution methods of linear probing and quadratic probing, the interval depends on the data, so that values mapping to the same location have different bucket sequences; this minimizes repeated collisions and the effects of clustering. 

Given two random, uniform, and independent hash functions formula_2 and formula_3, the formula_4th location in the bucket sequence for value formula_5 in a hash table of formula_6 buckets is: formula_7
Generally, formula_2 and formula_3 are selected from a set of universal hash functions; formula_2 is selected to have a range of formula_11 and formula_3 to have a range of formula_13. Double hashing approximates a random distribution; more precisely, pair-wise independent hash functions yield a probability of formula_14 that any pair of keys will follow the same bucket sequence.

The secondary hash function formula_15 should have several characteristics:

In practice, if division hashing is used for both functions, the divisors are chosens as primes.

Let formula_19 be the number of elements stored in formula_1, then formula_1's load factor is formula_22. That is, start by randomly, uniformly and independently selecting two universal hash functions formula_2 and formula_3 to build a double hashing table formula_1. All elements are put in formula_1 by double hashing using formula_2 and formula_3.
Given a key formula_5, determining the formula_30-st hash location is computed by:

formula_31

Let formula_1 have fixed load factor formula_33.

Bradford and Katehakis
showed the expected number of probes for an unsuccessful search in formula_1, still using these initially chosen hash functions, is formula_35 regardless of the distribution of the inputs. Pair-wise independence of the hash functions suffices.

Like all other forms of open addressing, double hashing becomes linear as the hash table approaches maximum capacity. The usual heuristic is to limit the table loading to 75% of capacity. Eventually, rehashing to a larger size will be necessary, as with all other open addressing schemes.




</doc>
<doc id="4015872" url="https://en.wikipedia.org/wiki?curid=4015872" title="Cuckoo hashing">
Cuckoo hashing

Cuckoo hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table, with worst-case constant lookup time. The name derives from the behavior of some species of cuckoo, where the cuckoo chick pushes the other eggs or young out of the nest when it hatches; analogously, inserting a new key into a cuckoo hashing table may push an older key to a different location in the table.

Cuckoo hashing was first described by Rasmus Pagh and Flemming Friche Rodler in 2001.

Cuckoo hashing is a form of open addressing in which each non-empty cell of a hash table contains a key or key–value pair. A hash function is used to determine the location for each key, and its presence in the table (or the value associated with it) can be found by examining that cell of the table. However, open addressing suffers from collisions, which happen when more than one key is mapped to the same cell.
The basic idea of cuckoo hashing is to resolve collisions by using two hash functions instead of only one. This provides two possible locations in the hash table for each key. In one of the commonly used variants of the algorithm, the hash table is split into two smaller tables of equal size, and each hash function provides an index into one of these two tables. It is also possible for both hash functions to provide indexes into a single table.

Lookup requires inspection of just two locations in the hash table, which takes constant time in the worst case. This is in contrast to many other hash table algorithms, which may not have a constant worst-case bound on the time to do a lookup. Deletions, also, may be performed by blanking the cell containing a key, in constant worst case time, more simply than some other schemes such as linear probing.

When a new key is inserted, and one of its two cells is empty, it may be placed in that cell. However, when both cells are already full, it will be necessary to move other keys to their second locations (or back to their first locations) to make room for the new key. A greedy algorithm is used: The new key is inserted in one of its two possible locations, "kicking out", that is, displacing, any key that might already reside in this location. This displaced key is then inserted in its alternative location, again kicking out any key that might reside there. The process continues in the same way until an empty position is found, completing the algorithm. However, it is possible for this insertion process to fail, by entering an infinite loop or by finding a very long chain (longer than a preset threshold that is logarithmic in the table size). In this case, the hash table is rebuilt in-place using new hash functions:

Insertions succeed in expected constant time, even considering the possibility of having to rebuild the table, as long as the number of keys is kept below half of the capacity of the hash table, i.e., the load factor is below 50%.

One method of proving this uses the theory of random graphs: one may form an undirected graph called the "cuckoo graph" that has a vertex for each hash table location, and an edge for each hashed value, with the endpoints of the edge being the two possible locations of the value. Then, the greedy insertion algorithm for adding a set of values to a cuckoo hash table succeeds if and only if the cuckoo graph for this set of values is a pseudoforest, a graph with at most one cycle in each of its connected components. Any vertex-induced subgraph with more edges than vertices corresponds to a set of keys for which there are an insufficient number of slots in the hash table. When the hash function is chosen randomly, the cuckoo graph is a random graph in the Erdős–Rényi model. With high probability, for a random graph in which the ratio of the number of edges to the number of vertices is bounded below 1/2, the graph is a pseudoforest and the cuckoo hashing algorithm succeeds in placing all keys. Moreover, the same theory also proves that the expected size of a connected component of the cuckoo graph is small, ensuring that each insertion takes constant expected time.

In practice Cuckoo hashing is about 20-30% slower than linear probing which is the fastest of the common approaches.
The reason is that cuckoo hashing often causes two cache misses per search, to check the two locations where a key might be stored, while linear probing usually causes only one cache miss per search.
However, because of its worst case guarantees on search time, cuckoo hashing can still be valuable when real-time response rates are required.

The following hash functions are given:

formula_1
formula_2

Columns in the following two tables show the state of the hash tables over time as the elements are inserted.

If you now wish to insert the element 6, then you get into a cycle. In the last row of the table we find the same initial situation as at the beginning again.

formula_3
formula_4

Several variations of cuckoo hashing have been studied, primarily with the aim of improving its space usage by increasing the load factor that it can tolerate to a number greater than the 50% threshold of the basic algorithm. Some of these methods can also be used to reduce the failure rate of cuckoo hashing, causing rebuilds of the data structure to be much less frequent.

Generalizations of cuckoo hashing that use more than two alternative hash functions can be expected to utilize a larger part of the capacity of the hash table efficiently while sacrificing some lookup and insertion speed. Using just three hash functions increases the load to 91%.
Another generalization of cuckoo hashing, called "blocked cuckoo hashing" consists in using more than one key per bucket. Using just 2 keys per bucket permits a load factor above 80%.

Another variation of cuckoo hashing that has been studied is "cuckoo hashing with a stash". The stash, in this data structure, is an array of a constant number of keys, used to store keys that cannot successfully be inserted into the main hash table of the structure. This modification reduces the failure rate of cuckoo hashing to an inverse-polynomial function with an exponent that can be made arbitrarily large by increasing the stash size. However, larger stashes also mean slower searches for keys that are not present or are in the stash. A stash can be used in combination with more than two hash functions or with blocked cuckoo hashing to achieve both high load factors and small failure rates. The analysis of cuckoo hashing with a stash extends to practical hash functions, not just to the random hash function model commonly used in theoretical analysis of hashing.

Some people recommend a simplified generalization of cuckoo hashing called skewed-associative cache in some CPU caches.

Another variation of a cuckoo hash table, called a cuckoo filter, replaces the stored keys of a cuckoo hash table with much shorter fingerprints, computed by applying another hash function to the keys. In order to allow these fingerprints to be moved around within the cuckoo filter, without knowing the keys that they came from, the two locations of each fingerprint may be computed from each other by a bitwise exclusive or operation with the fingerprint, or with a hash of the fingerprint. This data structure forms an approximate set membership data structure with much the same properties as a Bloom filter: it can store the members of a set of keys, and test whether a query key is a member, with some chance of false positives (queries that are incorrectly reported as being part of the set) but no false negatives. However, it improves on a Bloom filter in multiple respects: its memory usage is smaller by a constant factor, it has better locality of reference, and (unlike Bloom filters) it allows for fast deletion of set elements with no additional storage penalty.

A study by Zukowski et al. has shown that cuckoo hashing is much faster than chained hashing for small, cache-resident hash tables on modern processors. Kenneth Ross has shown bucketized versions of cuckoo hashing (variants that use buckets that contain more than one key) to be faster than conventional methods also for large hash tables, when space utilization is high. The performance of the bucketized cuckoo hash table was investigated further by Askitis,
with its performance compared against alternative hashing schemes.

A survey by Mitzenmacher presents open problems related to cuckoo hashing as of 2009.





</doc>
<doc id="2381605" url="https://en.wikipedia.org/wiki?curid=2381605" title="Coalesced hashing">
Coalesced hashing

Coalesced hashing, also called coalesced chaining, is a strategy of collision resolution in a hash table that forms a hybrid of separate chaining and open addressing.

In a separate chaining hash table, items that hash to the same address are placed on a list (or "chain") at that address. This technique can result in a great deal of wasted memory because the table itself must be large enough to maintain a load factor that performs well (typically twice the expected number of items), and extra memory must be used for all but the first item in a chain (unless list headers are used, in which case extra memory must be used for all items in a chain).

Given a sequence "qrj," "aty," "qur," "dim," "ofu," "gcl," "rhv," "clq," "ecd," "qsu" of randomly generated three character long strings, the following table would be generated (using Bob Jenkins' One-at-a-Time hash algorithm) with a table of size 10:

This strategy is effective, efficient, and very easy to implement. However, sometimes the extra memory use might be prohibitive, and the most common alternative, open addressing, has uncomfortable disadvantages that decrease performance. The primary disadvantage of open addressing is primary and secondary clustering, in which searches may access long sequences of used buckets that contain items with different hash addresses; items with one hash address can thus lengthen searches for items with other hash addresses. 

One solution to these issues is coalesced hashing. Coalesced hashing uses a similar technique as separate chaining, but instead of allocating new nodes for the linked list, buckets in the actual table are used. The first empty bucket in the table at the time of a collision is considered the collision bucket. When a collision occurs anywhere in the table, the item is placed in the collision bucket and a link is made between the chain and the collision bucket. It is possible for a newly inserted item to collide with items with a different hash address, such as the case in the example in the image when item "clq" is inserted. The chain for "clq" is said to "coalesce" with the chain of "qrj," hence the name of the algorithm. However, the extent of coalescing is minor compared with the clustering exhibited by open addressing. For example, when coalescing occurs, the length of the chain grows by only 1, whereas in open addressing, search sequences of arbitrary length may combine.

An important optimization, to reduce the effect of coalescing, is to restrict the address space of the hash function to only a subset of the table. For example, if the table has size "M" with buckets numbered from 0 to "M − 1", we can restrict the address space so that the hash function only assigns addresses to the first "N" locations in the table. The remaining "M − N" buckets, called the "cellar", are used exclusively for storing items that collide during insertion. No coalescing can occur until the cellar is exhausted. 

The optimal choice of "N" relative to "M" depends upon the load factor (or fullness) of the table. A careful analysis shows that the value "N = 0.86 × M" yields near-optimum performance for most load factors.

Other variants for insertion are also possible that have improved search time. Deletion algorithms have been developed that preserve randomness, and thus the average search time analysis still holds after deletions.

Insertion in C:

One benefit of this strategy is that the search algorithm for separate chaining can be used without change in a coalesced hash table.

Lookup in C:

Deletion may be hard.

Coalesced chaining avoids the effects of primary and secondary clustering, and as a result can take advantage of the efficient search algorithm for separate chaining. If the chains are short, this strategy is very efficient and can be highly condensed, memory-wise. As in open addressing, deletion from a coalesced hash table is awkward and potentially expensive, and resizing the table is terribly expensive and should be done rarely, if ever.


</doc>
<doc id="268162" url="https://en.wikipedia.org/wiki?curid=268162" title="Perfect hash function">
Perfect hash function

In computer science, a perfect hash function for a set is a hash function that maps distinct elements in to a set of integers, with no collisions. In mathematical terms, it is an injective function.

Perfect hash functions may be used to implement a lookup table with constant worst-case access time. A perfect hash function has many of the same applications as other hash functions, but with the advantage that no collision resolution has to be implemented. In addition, if the keys are not the data, the keys do not need to be stored in the lookup table, saving space.

A perfect hash function with values in a limited range can be used for efficient lookup operations, by placing keys from (or other associated values) in a lookup table indexed by the output of the function. One can then test whether a key is present in , or look up a value associated with that key, by looking for it at its cell of the table. Each such lookup takes constant time in the worst case.

A perfect hash function for a specific set that can be evaluated in constant time, and with values in a small range, can be found by a randomized algorithm in a number of operations that is proportional to the size of S.
The original construction of uses a two-level scheme to map a set of elements to a range of indices, and then map each index to a range of hash values. The first level of their construction chooses a large prime (larger than the size of the universe from which is drawn), and a parameter , and maps each element of to the index
If is chosen randomly, this step is likely to have collisions, but the number of elements that are simultaneously mapped to the same index is likely to be small.
The second level of their construction assigns disjoint ranges of integers to each index . It uses a second set of linear modular functions, one for each index , to map each member of into the range associated with .

As show, there exists a choice of the parameter such that the sum of the lengths of the ranges for the different values of is . Additionally, for each value of , there exists a linear modular function that maps the corresponding subset of into the range associated with that value. Both , and the second-level functions for each value of , can be found in polynomial time by choosing values randomly until finding one that works.

The hash function itself requires storage space to store , , and all of the second-level linear modular functions. Computing the hash value of a given key may be performed in constant time by computing , looking up the second-level function associated with , and applying this function to .
A modified version of this two-level scheme with a larger number of values at the top level can be used to construct a perfect hash function that maps into a smaller range of length .

The use of words of information to store the function of is near-optimal: any perfect hash function that can be calculated in constant time
requires at least a number of bits that is proportional to the size of .

Using a perfect hash function is best in situations where there is a frequently queried large set, , which is seldom updated. This is because any modification of the set may cause the hash function to no longer be perfect for the modified set. Solutions which update the hash function any time the set is modified are known as dynamic perfect hashing, but these methods are relatively complicated to implement.

A minimal perfect hash function is a perfect hash function that maps keys to consecutive integers – usually the numbers from to or from to . A more formal way of expressing this is: Let and be elements of some finite set . is a minimal perfect hash function if and only if implies (injectivity) and there exists an integer such that the range of is . It has been proven that a general purpose minimal perfect hash scheme requires at least 1.44 bits/key. The best currently known minimal perfect hashing schemes can be represented using less than 2.1 bits/key if given enough time.

A minimal perfect hash function is "order preserving" if keys are given in some order and for any keys and , implies . In this case, the function value is just the position of each key in the sorted ordering of all of the keys. A simple implementation of order-preserving minimal perfect hash functions with constant access time is to use an (ordinary) perfect hash function or cuckoo hashing to store a lookup table of the positions of each key. If the keys to be hashed are themselves stored in a sorted array, it is possible to store a small number of additional bits per key in a data structure that can be used to compute hash values quickly. Order-preserving minimal perfect hash functions require necessarily bits to be represented.

A simple alternative to perfect hashing, which also allows dynamic updates, is cuckoo hashing. This scheme maps keys to two or more locations within a range (unlike perfect hashing which maps each key to a single location) but does so in such a way that the keys can be assigned one-to-one to locations to which they have been mapped. Lookups with this scheme are slower, because multiple locations must be checked, but nevertheless take constant worst-case time.




</doc>
<doc id="4024666" url="https://en.wikipedia.org/wiki?curid=4024666" title="Universal hashing">
Universal hashing

In mathematics and computing, universal hashing (in a randomized algorithm or data structure) refers to selecting a hash function at random from a family of hash functions with a certain mathematical property (see definition below). This guarantees a low number of collisions in expectation, even if the data is chosen by an adversary. Many universal families are known (for hashing integers, vectors, strings), and their evaluation is often very efficient. Universal hashing has numerous uses in computer science, for example in implementations of hash tables, randomized algorithms, and cryptography.

Assume we want to map keys from some universe formula_1 into formula_2 bins (labelled formula_3). The algorithm will have to handle some data set formula_4 of formula_5 keys, which is not known in advance. Usually, the goal of hashing is to obtain a low number of collisions (keys from formula_6 that land in the same bin). A deterministic hash function cannot offer any guarantee in an adversarial setting if the size of formula_1 is greater than formula_8, since the adversary may choose formula_6 to be precisely the preimage of a bin. This means that all data keys land in the same bin, making hashing useless. Furthermore, a deterministic hash function does not allow for "rehashing": sometimes the input data turns out to be bad for the hash function (e.g. there are too many collisions), so one would like to change the hash function.

The solution to these problems is to pick a function randomly from a family of hash functions. A family of functions formula_10 is called a universal family if, formula_11.

In other words, any two keys of the universe collide with probability at most formula_12 when the hash function formula_13 is drawn randomly from formula_14. This is exactly the probability of collision we would expect if the hash function assigned truly random hash codes to every key. Sometimes, the definition is relaxed to allow collision probability formula_15. This concept was introduced by Carter and Wegman in 1977, and has found numerous applications in computer science (see, for example ). If we have an upper bound of formula_16 on the collision probability, we say that we have formula_17-almost universality.

Many, but not all, universal families have the following stronger uniform difference property:

Note that the definition of universality is only concerned with whether formula_23, which counts collisions. The uniform difference property is stronger.

An even stronger condition is pairwise independence: we have this property when formula_18 we have the probability that formula_30 will hash to any pair of hash values formula_31 is as if they were perfectly random: formula_32. Pairwise independence is sometimes called strong universality.

Another property is uniformity. We say that a family is uniform if all hash values are equally likely: formula_33 for any hash value formula_34. Universality does not imply uniformity. However, strong universality does imply uniformity.

Given a family with the uniform distance property, one can produce a pairwise independent or strongly universal hash family by adding a uniformly distributed random constant with values in formula_22 to the hash functions. (Similarly, if formula_2 is a power of two, we can achieve pairwise independence from an XOR universal hash family by doing an exclusive or with a uniformly distributed random constant.) Since a shift by a constant is sometimes irrelevant in applications (e.g. hash tables), a careful distinction between the uniform distance property and pairwise independent is sometimes not made.

For some applications (such as hash tables), it is important for the least significant bits of the hash values to be also universal. When a family is strongly universal, this is guaranteed: if formula_14 is a strongly universal family with formula_38, then the family made of the functions formula_39 for all formula_40 is also strongly universal for formula_41. Unfortunately, the same is not true of (merely) universal families. For example, the family made of the identity function formula_42 is clearly universal, but the family made of the function formula_43 fails to be universal.

UMAC and Poly1305-AES and several other message authentication code algorithms are based on universal hashing.
In such applications, the software chooses a new hash function for every message, based on a unique nonce for that message.

Several hash table implementations are based on universal hashing.
In such applications, typically the software chooses a new hash function only after it notices that "too many" keys have collided; until then, the same hash function continues to be used over and over.
(Some collision resolution schemes, such as dynamic perfect hashing, pick a new hash function every time there is a collision. Other collision resolution schemes, such as cuckoo hashing and 2-choice hashing, allow a number of collisions before picking a new hash function). A survey of fastest known universal and strongly universal hash functions for integers, vectors, and
strings is found in.

For any fixed set formula_6 of formula_45 keys, using a universal family guarantees the following properties.

As the above guarantees hold for any fixed set formula_6, they hold if the data set is chosen by an adversary. However, the adversary has to make this choice before (or independent of) the algorithm's random choice of a hash function. If the adversary can observe the random choice of the algorithm, randomness serves no purpose, and the situation is the same as deterministic hashing.

The second and third guarantee are typically used in conjunction with rehashing. For instance, a randomized algorithm may be prepared to handle some formula_60 number of collisions. If it observes too many collisions, it chooses another random formula_13 from the family and repeats. Universality guarantees that the number of repetitions is a geometric random variable.

Since any computer data can be represented as one or more machine words, one generally needs hash functions for three types of domains: machine words ("integers"); fixed-length vectors of machine words; and variable-length vectors ("strings").

This section refers to the case of hashing integers that fit in machines words; thus, operations like multiplication, addition, division, etc. are cheap machine-level instructions. Let the universe to be hashed be formula_71.

The original proposal of Carter and Wegman was to pick a prime formula_72 and define

where formula_74 are randomly chosen integers modulo formula_75 with formula_76. (This is a single iteration of a linear congruential generator.)

To see that formula_77 is a universal family, note that formula_54 only holds when

for some integer formula_80 between formula_81 and formula_82. If formula_83, their difference, formula_84 is nonzero and has an inverse modulo formula_75. Solving for formula_86 yields

There are formula_88 possible choices for formula_86 (since formula_90 is excluded) and, varying formula_80 in the allowed range, formula_92 possible non-zero values for the right hand side. Thus the collision probability is

Another way to see formula_14 is a universal family is via the notion of statistical distance. Write the difference formula_95 as

Since formula_97 is nonzero and formula_86 is uniformly distributed in formula_99, it follows that formula_100 modulo formula_75 is also uniformly distributed in formula_99. The distribution of formula_103 is thus almost uniform, up to a difference in probability of formula_104 between the samples. As a result, the statistical distance to a uniform family is formula_105, which becomes negligible when formula_106.

The family of simpler hash functions
is only "approximately" universal: formula_108 for all formula_109. Moreover, this analysis is nearly tight; Carter and Wegman show that formula_110 whenever formula_111.

The state of the art for hashing integers is the multiply-shift scheme described by Dietzfelbinger et al. in 1997. By avoiding modular arithmetic, this method is much easier to implement and also runs significantly faster in practice (usually by at least a factor of four). The scheme assumes the number of bins is a power of two, formula_112. Let formula_113 be the number of bits in a machine word. Then the hash functions are parametrised over odd positive integers formula_114 (that fit in a word of formula_113 bits). To evaluate formula_116, multiply formula_46 by formula_86 modulo formula_119 and then keep the high order formula_120 bits as the hash code. In mathematical notation, this is

and it can be implemented in C-like programming languages by

This scheme does "not" satisfy the uniform difference property and is only "formula_123-almost-universal"; for any formula_109, formula_108.

To understand the behavior of the hash function, 
notice that, if formula_126 and formula_127 have the same highest-order 'M' bits, then formula_128 has either all 1's or all 0's as its highest order M bits (depending on whether formula_126 or formula_130 is larger).
Assume that the least significant set bit of formula_84 appears on position formula_132. Since formula_86 is a random odd integer and odd integers have inverses in the ring formula_134, it follows that formula_135 will be uniformly distributed among formula_113-bit integers with the least significant set bit on position formula_132. The probability that these bits are all 0's or all 1's is therefore at most formula_138.
On the other hand, if formula_139, then higher-order M bits of 
formula_128 contain both 0's and 1's, so 
it is certain that formula_141. Finally, if formula_142 then bit formula_143 of 
formula_128 is 1 and formula_145 if and only if bits formula_146 are also 1, which happens with probability formula_147.

This analysis is tight, as can be shown with the example formula_148 and formula_149. To obtain a truly 'universal' hash function, one can use the multiply-add-shift scheme

which can be implemented in C-like programming languages by

where formula_86 is a random odd positive integer with formula_114 and formula_154 is a random non-negative integer with formula_155. With these choices of formula_86 and formula_154, formula_158 for all formula_159. This differs slightly but importantly from the mistranslation in the English paper.

This section is concerned with hashing a fixed-length vector of machine words. Interpret the input as a vector formula_160 of formula_161 machine words (integers of formula_113 bits each). If formula_14 is a universal family with the uniform difference property, the following family (dating back to Carter and Wegman) also has the uniform difference property (and hence is universal):

If formula_2 is a power of two, one may replace summation by exclusive or.

In practice, if double-precision arithmetic is available, this is instantiated with the multiply-shift hash family of. Initialize the hash function with a vector formula_167 of random odd integers on formula_168 bits each. Then if the number of bins is formula_112 for formula_170:

It is possible to halve the number of multiplications, which roughly translates to a two-fold speed-up in practice. Initialize the hash function with a vector formula_167 of random odd integers on formula_168 bits each. The following hash family is universal:

If double-precision operations are not available, one can interpret the input as a vector of half-words (formula_175-bit integers). The algorithm will then use formula_176 multiplications, where formula_161 was the number of half-words in the vector. Thus, the algorithm runs at a "rate" of one multiplication per word of input.

The same scheme can also be used for hashing integers, by interpreting their bits as vectors of bytes. In this variant, the vector technique is known as tabulation hashing and it provides a practical alternative to multiplication-based universal hashing schemes.

Strong universality at high speed is also possible. Initialize the hash function with a vector formula_178 of random integers on formula_168 bits. Compute

The result is strongly universal on formula_113 bits. Experimentally, it was found to run at 0.2 CPU cycle per byte on recent Intel processors for formula_182.

This refers to hashing a "variable-sized" vector of machine words. If the length of the string can be bounded by a small number, it is best to use the vector solution from above (conceptually padding the vector with zeros up to the upper bound). The space required is the maximal length of the string, but the time to evaluate formula_183 is just the length of formula_184. As long as zeroes are forbidden in the string, the zero-padding can be ignored when evaluating the hash function without affecting universality. Note that if zeroes are allowed in the string, then it might be best to append a fictitious non-zero (e.g., 1) character to all strings prior to padding: this will ensure that universality is not affected.

Now assume we want to hash formula_185, where a good bound on formula_186 is not known a priori. A universal family proposed by 
treats the string formula_46 as the coefficients of a polynomial modulo a large prime. If formula_188, let formula_189 be a prime and define:

Using properties of modular arithmetic, above can be computed without producing large numbers for large strings as follows:

This Rabin-Karp rolling hash is based on a linear congruential generator.
Above algorithm is also known as "Multiplicative hash function". In practice, the "mod" operator and the parameter "p" can be avoided altogether by simply allowing integer to overflow because it is equivalent to "mod" ("Max-Int-Value" + 1) in many programming languages. Below table shows values chosen to initialize "h" and a for some of the popular implementations.

Consider two strings formula_194 and let formula_186 be length of the longer one; for the analysis, the shorter string is conceptually padded with zeros up to length formula_186. A collision before applying formula_192 implies that formula_86 is a root of the polynomial with coefficients formula_199. This polynomial has at most formula_186 roots modulo formula_75, so the collision probability is at most formula_202. The probability of collision through the random formula_192 brings the total collision probability to formula_204. Thus, if the prime formula_75 is sufficiently large compared to the length of strings hashed, the family is very close to universal (in statistical distance).

Other universal families of hash functions used to hash unknown-length strings to fixed-length hash values include the Rabin fingerprint and the Buzhash.

To mitigate the computational penalty of modular arithmetic, three tricks are used in practice:




</doc>
<doc id="2981764" url="https://en.wikipedia.org/wiki?curid=2981764" title="Linear hashing">
Linear hashing

Linear hashing (LH) is a dynamic data structure which implements a hash table and grows or shrinks one bucket at a time. It was invented by Witold Litwin in 1980.
It is the first in a number of schemes known as dynamic hashing
such as Larson's Linear Hashing with Partial Extensions, 

Linear Hashing with Priority Splitting,

Linear Hashing with Partial Expansions and Priority Splitting,

or Recursive Linear Hashing.
The file structure of a dynamic hashing data structure adapts itself to changes in the size of the file, so expensive periodic file reorganization is avoided. A Linear Hashing file expends by splitting
a pre-determined bucket into two and contracts by merging two predetermined buckets into one. The trigger for a reconstruction depends on the flavor of the scheme; it could be an overflow at a bucket or load factor (number of records over the number of buckets) moving outside of a predetermined range.

Linear Hashing has also been made into a scalable distributed data structure, LH*. In LH*, each bucket resides at a different server.
failed buckets.
LH* take maximum constant time independent of the number of buckets and hence of records.

Records in LH or LH* consists of a key and a content, the latter basically all the other attributes of the record. They are stored in buckets. For example, in Ellis' implementation, a bucket is a linked list of records. The file allows the key based CRUD operations create or insert, read, update, and delete as well as a scan operations that scans all records, for example to do a database select operation on a non-key attribute. Records are stored in buckets whose numbering starts with 0.

In order to access a record with key formula_1, a family of hash functions, called
collectively a dynamic hash function is applied to the key formula_1. At any time, 
at most two hash functions formula_3 and formula_4 are used. A typical
example uses the division modulo x operation. If the original number of buckets is
formula_5, then the family of hash functions is 

formula_6

As the file grows through insertions, it expands gracefully through the splitting
of one bucket into two buckets. The sequence of buckets to split is predetermined.
This is the fundamental difference to schemes like Fagin's extendible hashing.

For the two new buckets, the hash function formula_3 is replaces with 
formula_4. The number of the bucket to be split is part of the 
file state and called the split pointer formula_9.

A split can be performed whenever a bucket overflows. This is an uncontrolled split.
Alternatively, the file can monitor the load factor and performs a split whenever
the load factor exceeds a threshold. This was controlled splitting.

Addressing is based on the file state, consisting of the split pointer formula_9
and the level formula_11. If the level is formula_11, then the hash functions
used are formula_13 and formula_14.

The LH algorithm for hashing key formula_1 is

formula_16

if formula_17

if formula_18: formula_19

If under controlled splitting the load factor sinks below a threshold, a merge operation
is triggered. The merge operation undoes the last split, also resetting the file state.
The file state consists of split pointer formula_9 and level formula_11. If the 
original file started with formula_22 buckets, then the number of buckets 
formula_23 and the file state are related via
formula_24

The main contribution of LH* is to allow a client of an LH* file to find the bucket where
the record resides even if the client does not know the file state. Clients in fact store
their version of the file state, which is initially just the knowledge of the first bucket, namely Bucket 0. Based on their file state, a client calculates the address of a
key and sends a request to that bucket. At the bucket, the request is checked and if
the record is not at the bucket, it is forwarded. In a reasonably stable system, that is, 
if there is only one split or merge going on while the request is processed, it can
be shown that there are at most two forwards. After a forward, the final bucket sends an 
Image Adjustment Message to the client whose state is now closer to the state of the distributed file. While forwards are reasonably rare for active clients, 
their number can be even further reduced by additional information exchange between 
servers and clients 

Griswold and Townsend discussed the adoption of linear hashing in the Icon language. They discussed the implementation alternatives of dynamic array algorithm used in linear hashing, and presented performance comparisons using a list of Icon benchmark applications.

Linear hashing is used in the Berkeley database system (BDB), which in turn is used by many software systems such as OpenLDAP, using a C implementation derived from the CACM article and first published on the Usenet in 1988 by Esmond Pitt.




</doc>
<doc id="8019200" url="https://en.wikipedia.org/wiki?curid=8019200" title="Extendible hashing">
Extendible hashing

Extendible hashing is a type of hash system which treats a hash as a bit string and uses a trie for bucket lookup. Because of the hierarchical nature of the system, re-hashing is an incremental operation (done one bucket at a time, as needed). This means that time-sensitive applications are less affected by table growth than by standard full-table rehashes.

Extendible hashing was described by Ronald Fagin in 1979.
Practically all modern filesystems use either extendible hashing or B-trees.
In particular, the Global File System, ZFS, and the SpadFS filesystem use extendible hashing.

Assume that the hash function formula_1 returns a string of bits. The first i bits of each string will be used as indices to figure out where they will go in the "directory" (hash table). Additionally, i is the smallest number such that the index of every item in the table is unique.

Keys to be used:

Let's assume that for this particular example, the bucket size is 1. The first two keys to be inserted, k and k, can be distinguished by the most significant bit, and would be inserted into the table as follows:

Now, if k were to be hashed to the table, it wouldn't be enough to distinguish all three keys by one bit (because both k and k have 1 as their leftmost bit). Also, because the bucket size is one, the table would overflow. Because comparing the first two most significant bits would give each key a unique location, the directory size is doubled as follows:

And so now k and k have a unique location, being distinguished by the first two leftmost bits. Because k is in the top half of the table, both 00 and 01 point to it because there is no other key to compare to that begins with a 0.

The above example is from .

Now, k needs to be inserted, and it has the first two bits as 01..(1110), and using a 2 bit depth in the directory, this maps from 01 to Bucket A. Bucket A is full (max size 1), so it must be split; because there is more than one pointer to Bucket A, there is no need to increase the directory size.

What is needed is information about:


In order to distinguish the two action cases:


Examining the initial case of an extendible hash structure, if each directory entry points to one bucket, then the local depth should be equal to the global depth.

The number of directory entries is equal to 2, and the initial number of buckets
is equal to 2.

Thus if global depth = local depth = 0, then 2 = 1, so an initial directory of one pointer to one bucket.

Back to the two action cases; if the bucket is full:

Key 01 points to Bucket A, and Bucket A's local depth of 1 is less than the directory's global depth of 2, which means keys hashed to Bucket A have only used a 1 bit prefix (i.e. 0), and the bucket needs to have its contents split using keys 1 + 1 = 2 bits in length; in general, for any local depth d where d is less than D, the global depth, then d must be incremented after a bucket split, and the new d used as the number of bits of each entry's key to redistribute the entries of the former bucket into the new buckets.

Now, 
is tried again, with 2 bits 01.., and now key 01 points to a new bucket but there is still in it (formula_5 and also begins with 01).

If had been 000110, with key 00, there would have been no problem, because would have remained in the new bucket A' and bucket D would have been empty.

So Bucket D needs to be split, but a check of its local depth, which is 2, is the same as the global depth, which is 2, so the directory must be split again, in order to hold keys of sufficient detail, e.g. 3 bits.


Now, formula_5 is in D and formula_3 is tried again, with 3 bits 011.., and it points to bucket D which already contains so is full; D's local depth is 2 but now the global depth is 3 after the directory doubling, so now D can be split into bucket's D' and E, the contents of D, has its formula_8 retried with a new global depth bitmask of 3 and ends up in D', then the new entry is retried with formula_9 bitmasked using the new global depth bit count of 3 and this gives 011 which now points to a new bucket E which is empty. So goes in Bucket E.

Below is the extendible hashing algorithm in Python, with the disc block / memory page association, caching and consistency issues removed. Note a problem exists if the depth exceeds the bit size of an integer, because then doubling of the directory or splitting of a bucket won't allow entries to be rehashed to different buckets.

The code uses the "least significant bits", which makes it more efficient to expand the table, as the entire directory can be copied as one block ().





</doc>
<doc id="4436335" url="https://en.wikipedia.org/wiki?curid=4436335" title="2-choice hashing">
2-choice hashing

2-choice hashing, also known as 2-choice chaining, is "a variant of a hash table in which keys are added by hashing with two hash functions. The key is put in the array position with the fewer (colliding) keys. Some collision resolution scheme is needed, unless keys are kept in buckets. The average-case cost of a successful search is O(2 + (m-1)/n), where m is the number of keys and n is the size of the array. The most collisions is formula_1 with high probability."

2-choice hashing utilizes two hash functions h1(x) and h2(x) which work as hash functions are expected to work (i.e. mapping integers from the universe into a specified range). The two hash functions should be independent and have no correlation to each other. Having two hash functions allows any integer x to have up to two potential locations to be stored based on the values of the respective outputs, h1(x) and h2(x). It is important to note that, although there are two hash functions, there is only one table; both hash functions map to locations on that table.

The most important functions of the hashing implementation in this case are insertion and search.

As is true with all hash tables, the performance is based on the largest bucket. Although there are instances where bucket sizes happen to be large based on the values and the hash functions used, this is rare. Having two hash functions and, therefore, two possible locations for any one value, makes the possibility of large buckets even more unlikely to happen.

The expected bucket size while using 2-choice hashing is: . This improvement is due to the randomized concept known as The Power of Two Choices.

Using two hash functions offers substantial benefits over a single hash function. There is little improvement (and no change to the expected order statistics) if more than two hash functions are used: "Additional hash functions only decrease the maximum by a constant factor."

Some people recommend a type of 2-choice hashing called two-way skewed-associative cache in some CPU caches.
2-left hashing -- using two hash tables of equal size n/2, and asymmetrically resolving ties by putting the key in the left hash table -- has fewer collisions and therefore better performance than 2-choice hashing with one large hash table of size n.


</doc>
<doc id="101219" url="https://en.wikipedia.org/wiki?curid=101219" title="Pearson hashing">
Pearson hashing

Pearson hashing is a hash function designed for fast execution on processors with 8-bit registers. Given an input consisting of any number of bytes, it produces as output a single byte that is strongly dependent on every byte of the input. Its implementation requires only a few instructions, plus a 256-byte lookup table containing a permutation of the values 0 through 255.

This hash function is a CBC-MAC that uses an 8-bit substitution cipher implemented via the substitution table. An 8-bit cipher has negligible cryptographic security, so the Pearson hash function is not cryptographically strong, but it is useful for implementing hash tables or as a data integrity check code, for which purposes it offers these benefits:


One of its drawbacks when compared with other hashing algorithms designed for 8-bit processors is the suggested 256 byte lookup table, which can be prohibitively large for a small microcontroller with a program memory size on the order of hundreds of bytes. A workaround to this is to use a simple permutation function instead of a table stored in program memory. However, using a too simple function, such as codice_1, partly defeats the usability as a hash function as anagrams will result in the same hash value; using a too complex function, on the other hand, will affect speed negatively. Using a function rather than a table also allows extending the block size. Such functions naturally have to be bijective, like their table variants.

The algorithm can be described by the following pseudocode, which computes the hash of message "C" using the permutation table "T":

The hash variable () may be initialized differently, e.g. to the length of the data () modulo 256; this particular choice is used in the Python implementation example below.

The 'table' parameter requires a pseudo-randomly shuffled list of range [0..255]. This may easily be generated by using python's builtin function and using to permutate it:
The scheme used above is a very straightforward implementation of the algorithm, with a simple extension to generate a hash longer than 8 bits. That extension comprises the outer loop (i.e. all statement lines that include the variable ) and the array .

For a given string or chunk of data, Pearson's original algorithm produces only an 8-bit byte or integer, 0-255. However, the algorithm makes it extremely easy to generate a hash of whatever length is desired. As Pearson noted, a change to any bit in the string causes his algorithm to create a completely different hash (0-255). In the code above, following every completion of the inner loop, the first byte of the string is effectively incremented by one (without modifying the string itself).

Every time that simple change to the first byte of the data is made, a different Pearson hash, , is generated. The C function builds a 16 hex character hash by concatenating a series of 8-bit Pearson hashes (collected in ). Instead of producing a value from 0 to 255, this function generates a value from 0 to 18,446,744,073,709,551,615 (= 2 - 1).

This shows that Pearson's algorithm can be made to generate hashes of any desired length by concatenating a sequence of 8-bit hash values, each of which is computed simply by slightly modifying the string each time the hash function is computed. Thus the same core logic can be made to generate 32-bit or 128-bit hashes.



</doc>
<doc id="1096165" url="https://en.wikipedia.org/wiki?curid=1096165" title="Fowler–Noll–Vo hash function">
Fowler–Noll–Vo hash function

Fowler–Noll–Vo is a non-cryptographic hash function created by Glenn Fowler, Landon Curt Noll, and Kiem-Phong Vo.

The basis of the FNV hash algorithm was taken from an idea sent as reviewer comments to the IEEE POSIX P1003.2 committee by Glenn Fowler and Phong Vo in 1991. In a subsequent ballot round, Landon Curt Noll improved on their algorithm. In an email message to Landon, they named it the "Fowler/Noll/Vo" or FNV hash.

The current versions are FNV-1 and FNV-1a, which supply a means of creating non-zero FNV offset basis. FNV currently comes in 32-, 64-, 128-, 256-, 512-, and 1024-bit flavors. For pure FNV implementations, this is determined solely by the availability of FNV primes for the desired bit length; however, the FNV webpage discusses methods of adapting one of the above versions to a smaller length that may or may not be a power of two.

The FNV hash algorithms and reference FNV source code have been released into the public domain.

FNV is not a cryptographic hash.

One of FNV's key advantages is that it is very simple to implement. Start with an initial hash value of FNV offset basis. For each byte in the input, multiply "hash" by the FNV prime, then XOR it with the byte from the input. The alternate algorithm, FNV-1a, reverses the multiply and XOR steps.

The FNV-1 hash algorithm is as follows:

In the above pseudocode, all variables are unsigned integers. All variables, except for "byte_of_data", have the same number of bits as the FNV hash. The variable, "byte_of_data", is an 8 bit unsigned integer.

As an example, consider the 64-bit FNV-1 hash:


The FNV-1a hash differs from the FNV-1 hash by only the order in which the multiply and XOR is performed:

The above pseudocode has the same assumptions that were noted for the FNV-1 pseudocode. The small change in order leads to slightly better avalanche characteristics.

The FNV-0 hash differs from the FNV-1 hash only by the initialisation value of the "hash" variable:

The above pseudocode has the same assumptions that were noted for the FNV-1 pseudocode.

Use of the FNV-0 hash is deprecated except for the computing of the FNV offset basis for use as the FNV-1 and FNV-1a hash parameters.

There are several different FNV offset basis for various bit lengths. These FNV offset basis are computed by computing the FNV-0 from the following 32 octets when expressed in ASCII:
which is one of Landon Curt Noll's signature lines. This is the only current practical use for the deprecated FNV-0.

An FNV prime is a prime number and is determined as follows:

For a given formula_1:

where formula_4 is:

and where formula_6 is:

then the "n"-bit FNV prime is the smallest prime number formula_9 that is of the form:

such that:

Experimentally, FNV prime matching the above constraints tend to have better dispersion properties. They improve the polynomial feedback characteristic when an FNV prime multiplies an intermediate hash value. As such, the hash values produced are more scattered throughout the "n"-bit hash space.

The above FNV prime constraints and the definition of the FNV offset basis yield the following table of FNV hash parameters:

The FNV hash was designed for fast hash table and checksum use, not cryptography. The authors have identified the following properties as making the algorithm unsuitable as a cryptographic hash function:





</doc>
<doc id="21890341" url="https://en.wikipedia.org/wiki?curid=21890341" title="Bitstate hashing">
Bitstate hashing

Bitstate hashing is a hashing method invented in 1968 by Morris. It is used for state hashing, where each state (e.g. of an automaton) is represented by a number and it is passed to some hash function.

The result of the function is then taken as the index to an array of bits (a bit-field), where one looks for 1 if the state was already seen before or stores 1 by itself if not.

It usually serves as a yes–no technique without a need of storing whole state bit representation.

A shortcoming of this framework is losing precision like in other hashing techniques. Hence some tools use this technique with more than one hash function so that the bit-field gets widened by the number of used functions, each having its own row. And even after all functions return values (the indices) point to fields with contents equal to 1, the state may be uttered as visited with much higher probability.



</doc>
<doc id="602211" url="https://en.wikipedia.org/wiki?curid=602211" title="Bloom filter">
Bloom filter

A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not – in other words, a query returns either "possibly in set" or "definitely not in set". Elements can be added to the set, but not removed (though this can be addressed with the counting Bloom filter variant); the more elements that are added to the set, the larger the probability of false positives.

Bloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if "conventional" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses.

More generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set.

An "empty Bloom filter" is a bit array of bits, all set to 0. There must also be different hash functions defined, each of which maps or hashes some set element to one of the array positions, generating a uniform random distribution. Typically, is a constant, much smaller than , which is proportional to the number of elements to be added; the precise choice of and the constant of proportionality of are determined by the intended false positive rate of the filter.

To "add" an element, feed it to each of the hash functions to get array positions. Set the bits at all these positions to 1.

To "query" for an element (test whether it is in the set), feed it to each of the hash functions to get array positions. If any of the bits at these positions is 0, the element is definitely not in the set if it were, then all the bits would have been set to 1 when it was inserted. If all are 1, then either the element is in the set, or the bits have by chance been set to 1 during the insertion of other elements, resulting in a false positive. In a simple Bloom filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.

The requirement of designing different independent hash functions can be prohibitive for large . For a good hash function with a wide output, there should be little if any correlation between different bit-fields of such a hash, so this type of hash can be used to generate multiple "different" hash functions by slicing its output into multiple bit fields. Alternatively, one can pass different initial values (such as 0, 1, ...,  − 1) to a hash function that takes an initial value; or add (or append) these values to the key. For larger and/or , independence among the hash functions can be relaxed with negligible increase in false positive rate. (Specifically, show the effectiveness of deriving the indices using enhanced double hashing or triple hashing, variants of double hashing that are effectively simple random number generators seeded with the two or three hash values.)

Removing an element from this simple Bloom filter is impossible because false negatives are not permitted. An element maps to bits, and although setting any one of those bits to zero suffices to remove the element, it also results in removing any other elements that happen to map onto that bit. Since there is no way of determining whether any other elements have been added that affect the bits for an element to be removed, clearing any of the bits would introduce the possibility for false negatives.

One-time removal of an element from a Bloom filter can be simulated by having a second Bloom filter that contains items that have been removed. However, false positives in the second filter become false negatives in the composite filter, which may be undesirable. In this approach re-adding a previously removed item is not possible, as one would have to remove it from the "removed" filter.

It is often the case that all the keys are available but are expensive to enumerate (for example, requiring many disk reads). When the false positive rate gets too high, the filter can be regenerated; this should be a relatively rare event.

While risking false positives, Bloom filters have a strong space advantage over other data structures for representing sets, such as self-balancing binary search trees, tries, hash tables, or simple arrays or linked lists of the entries. Most of these require storing at least the data items themselves, which can require anywhere from a small number of bits, for small integers, to an arbitrary number of bits, such as for strings ( are an exception, since they can share storage between elements with equal prefixes). However, Bloom filters do not store the data items at all, and a separate solution must be provided for the actual storage. Linked structures incur an additional linear space overhead for pointers. A Bloom filter with 1% error and an optimal value of , in contrast, requires only about 9.6 bits per element, regardless of the size of the elements. This advantage comes partly from its compactness, inherited from arrays, and partly from its probabilistic nature. The 1% false-positive rate can be reduced by a factor of ten by adding only about 4.8 bits per element.

However, if the number of potential values is small and many of them can be in the set, the Bloom filter is easily surpassed by the deterministic bit array, which requires only one bit for each potential element. Note also that hash tables gain a space and time advantage if they begin ignoring collisions and store only whether each bucket contains an entry; in this case, they have effectively become Bloom filters with  = 1.

Bloom filters also have the unusual property that the time needed either to add items or to check whether an item is in the set is a fixed constant, O(), completely independent of the number of items already in the set. No other constant-space set data structure has this property, but the average access time of sparse hash tables can make them faster in practice than some Bloom filters. In a hardware implementation, however, the Bloom filter shines because its lookups are independent and can be parallelized.

To understand its space efficiency, it is instructive to compare the general Bloom filter with its special case when  = 1. If = 1, then in order to keep the false positive rate sufficiently low, a small fraction of bits should be set, which means the array must be very large and contain long runs of zeros. The information content of the array relative to its size is low. The generalized Bloom filter ( greater than 1) allows many more bits to be set while still maintaining a low false positive rate; if the parameters ( and ) are chosen well, about half of the bits will be set, and these will be apparently random, minimizing redundancy and maximizing information content.

Assume that a hash function selects each array position with equal probability. If "m" is the number of bits in the array, the probability that a certain bit is not set to 1 by a certain hash function during the insertion of an element is

If "k" is the number of hash functions and each has no significant correlation between each other, then the probability that the bit is not set to 1 by any of the hash functions is

If we have inserted "n" elements, the probability that a certain bit is still 0 is

the probability that it is 1 is therefore

Now test membership of an element that is not in the set. Each of the "k" array positions computed by the hash functions is 1 with a probability as above. The probability of all of them being 1, which would cause the algorithm to erroneously claim that the element is in the set, is often given as

This is not strictly correct as it assumes independence for the probabilities of each bit being set. However, assuming it is a close approximation we have that the probability of false positives decreases as "m" (the number of bits in the array) increases, and increases as "n" (the number of inserted elements) increases.

An alternative analysis arriving at the same approximation without the assumption of independence is given by Mitzenmacher and Upfal. After all "n" items have been added to the Bloom filter, let "q" be the fraction of the "m" bits that are set to 0. (That is, the number of bits still set to 0 is "qm".) Then, when testing membership of an element not in the set, for the array position given by any of the "k" hash functions, the probability that the bit is found set to 1 is formula_6. So the probability that all "k" hash functions find their bit set to 1 is formula_7. Further, the expected value of "q" is the probability that a given array position is left untouched by each of the "k" hash functions for each of the "n" items, which is (as above)
It is possible to prove, without the independence assumption, that "q" is very strongly concentrated around its expected value. In particular, from the Azuma–Hoeffding inequality, they prove that
Because of this, we can say that the exact probability of false positives is
as before.

The number of hash functions, "k", must be a positive integer. Putting this constraint aside, for a given "m" and "n", the value of "k" that minimizes the false positive probability is

The required number of bits, "m", given "n" (the number of inserted elements) and a desired false positive probability "p" (and assuming the optimal value of "k" is used) can be computed by substituting the optimal value of "k" in the probability expression above:
which can be simplified to:
This results in:
So the optimal number of bits per element is
with the corresponding number of hash functions "k" (ignoring integrality):

This means that for a given false positive probability "p", the length of a Bloom filter "m" is proportionate to the number of elements being filtered "n" and the required number of hash functions only depends on the target false positive probability "p".

The formula formula_14 is approximate for three reasons. First, and of least concern, it approximates
formula_18 as formula_19, which is a good asymptotic approximation (i.e., which holds as "m" →∞). Second, of more concern, it assumes that during the membership test the event that one tested bit is set to 1 is independent of the event that any other tested bit is set to 1. Third, of most concern, it assumes that formula_20 is fortuitously integral.

Goel and Gupta, however, give a rigorous upper bound that makes no approximations and requires no assumptions. They show that the false positive probability for a finite Bloom filter with "m" bits (formula_21), "n" elements, and "k" hash functions is at most

This bound can be interpreted as saying that the approximate formula formula_23 can be applied at a penalty of at most half an extra element and at most one fewer bit.

 showed that the number of items in a Bloom filter can be approximated with the following formula,

where formula_25 is an estimate of the number of items in the filter, m is the length (size) of the filter, k is the number of hash functions, and X is the number of bits set to one.

Bloom filters are a way of compactly representing a set of items. It is common to try to compute the size of the intersection or union between two sets. Bloom filters can be used to approximate the size of the intersection and union of two sets. showed that for two Bloom filters of length , their counts, respectively can be estimated as

and

The size of their union can be estimated as

where formula_29 is the number of bits set to one in either of the two Bloom filters. Finally, the intersection can be estimated as

using the three formulas together.



Classic Bloom filters use formula_31 bits of space per inserted key, where formula_32 is the false positive rate of the Bloom filter. However, the space that is strictly necessary for any data structure playing the same role as a Bloom filter is only formula_33 per key. Hence Bloom filters use 44% more space than an equivalent optimal data structure. Instead, Pagh et al. provide an optimal-space data structure. Moreover, their data structure has constant locality of reference independent of the false positive rate, unlike Bloom filters, where a smaller false positive rate formula_32 leads to a greater number of memory accesses per query, formula_35. Also, it allows elements to be deleted without a space penalty, unlike Bloom filters. The same improved properties of optimal space usage, constant locality of reference, and the ability to delete elements are also provided by the cuckoo filter of , an open source implementation of which is available.

The space efficient variant relies on using a single hash function that generates for each key a value in the range formula_36 where formula_32 is the requested false positive rate. The sequence of values is then sorted and compressed using Golomb coding (or some other compression technique) to occupy a space close to formula_38 bits. To query the Bloom filter for a given key, it will suffice to check if its corresponding value is stored in the Bloom filter. Decompressing the whole Bloom filter for each query would make this variant totally unusable. To overcome this problem the sequence of values is divided into small blocks of equal size that are compressed separately. At query time only half a block will need to be decompressed on average. Because of decompression overhead, this variant may be slower than classic Bloom filters but this may be compensated by the fact that a single hash function need to be computed.

Another alternative to classic Bloom filter is the one based on space efficient variants of cuckoo hashing. In this case once the hash table is constructed, the keys stored in the hash table are replaced with short signatures of the keys. Those signatures are strings of bits computed using a hash function applied on the keys.

There are over 60 variants of Bloom filters, many surveys of the field, and a continuing churn of applications (see e.g., Luo, "et al" ). Some of the variants differ sufficiently from the original proposal to be breaches from or forks of the original data structure and its philosophy. A treatment which unifies Bloom filters with other work on random projections, compressive sensing, and locality sensitive hashing remains to be done (though see Dasgupta, "et al" for one attempt inspired by neuroscience).

Content delivery networks deploy web caches around the world to cache and serve web content to users with greater performance and reliability. A key application of Bloom filters is their use in efficiently determining which web objects to store in these web caches. Nearly three-quarters of the URLs accessed from a typical web cache are "one-hit-wonders" that are accessed by users only once and never again. It is clearly wasteful of disk resources to store one-hit-wonders in a web cache, since they will never be accessed again. To prevent caching one-hit-wonders, a Bloom filter is used to keep track of all URLs that are accessed by users. A web object is cached only when it has been accessed at least once before, i.e., the object is cached on its second request. The use of a Bloom filter in this fashion significantly reduces the disk write workload, since one-hit-wonders are never written to the disk cache. Further, filtering out the one-hit-wonders also saves cache space on disk, increasing the cache hit rates.

Kiss "et al" described a new construction for the Bloom filter that avoids false positives in addition to the typical non-existence of false negatives. The construction applies to a finite universe from which set elements are taken. It relies on existing non-adaptive combinatorial group testing scheme by Eppstein, Goodrich and Hirschberg. Unlike the typical Bloom filter, elements are hashed to a bit array through deterministic, fast and simple-to-calculate functions. The maximal set size for which false positives are completely avoided is a function of the universe size and is controlled by the amount of allocated memory.

Counting filters provide a way to implement a "delete" operation on a Bloom filter without recreating the filter afresh. In a counting filter, the array positions (buckets) are extended from being a single bit to being an multibit counter. In fact, regular Bloom filters can be considered as counting filters with a bucket size of one bit. Counting filters were introduced by .

The insert operation is extended to "increment" the value of the buckets, and the lookup operation checks that each of the required buckets is non-zero. The delete operation then consists of decrementing the value of each of the respective buckets.

Arithmetic overflow of the buckets is a problem and the buckets should be sufficiently large to make this case rare. If it does occur then the increment and decrement operations must leave the bucket set to the maximum possible value in order to retain the properties of a Bloom filter.

The size of counters is usually 3 or 4 bits. Hence counting Bloom filters use 3 to 4 times more space than static Bloom filters. In contrast, the data structures of and also allow deletions but use less space than a static Bloom filter.

Another issue with counting filters is limited scalability. Because the counting Bloom filter table cannot be expanded, the maximal number of keys to be stored simultaneously in the filter must be known in advance. Once the designed capacity of the table is exceeded, the false positive rate will grow rapidly as more keys are inserted.

The space efficient variant by could also be used to implement counting filters by supporting insertions and deletions.

Kim et al. (2019) shows that false positive of Counting Bloom filter decreases from k=1 to a point defined formula_39, and increases from formula_39to positive infinity, and finds formula_39as a function of count threshold.

Bloom filters can be organized in distributed data structures to perform fully decentralized computations of aggregate functions. Decentralized aggregation makes collective measurements locally available in every node of a distributed network without involving a centralized computational entity for this purpose.

Bloom filters can be used for approximate data synchronization as in . Counting Bloom filters can be used to approximate the number of differences between two sets and this approach is described in .

 designed a generalization of Bloom filters that could associate a value with each element that had been inserted, implementing an associative array. Like Bloom filters, these structures achieve a small space overhead by accepting a small probability of false positives. In the case of "Bloomier filters", a "false positive" is defined as returning a result when the key is not in the map. The map will never return the wrong value for a key that "is" in the map.
 proposed a lattice-based generalization of Bloom filters. A compact approximator associates to each key an element of a lattice (the standard Bloom filters being the case of the Boolean two-element lattice). Instead of a bit array, they have an array of lattice elements. When adding a new association between a key and an element of the lattice, they compute the maximum of the current contents of the k array locations associated to the key with the lattice element. When reading the value associated to a key, they compute the minimum of the values found in the k locations associated to the key. The resulting value approximates from above the original value.

This implementation used a separate array for each hash function. This method allows for parallel hash calculations for both insertions and inquiries.

 proposed Stable Bloom filters as a variant of Bloom filters for streaming data. The idea is that since there is no way to store the entire history of a stream (which can be infinite), Stable Bloom filters continuously evict stale information to make room for more recent elements. Since stale information is evicted, the Stable Bloom filter introduces false negatives, which do not appear in traditional Bloom filters. The authors show that a tight upper bound of false positive rates is guaranteed, and the method is superior to standard Bloom filters in terms of false positive rates and time efficiency when a small space and an acceptable false positive rate are given.

 proposed a variant of Bloom filters that can adapt dynamically to the number of elements stored, while assuring a minimum false positive probability. The technique is based on sequences of standard Bloom filters with increasing capacity and tighter false positive probabilities, so as to ensure that a maximum false positive probability can be set beforehand, regardless of the number of elements to be inserted.

Spatial Bloom filters (SBF) were originally proposed by as a data structure designed to store location information, especially in the context of cryptographic protocols for location privacy. However, the main characteristic of SBFs is their ability to store multiple sets in a single data structure, which makes them suitable for a number of different application scenarios. Membership of an element to a specific set can be queried, and the false positive probability depends on the set: the first sets to be entered into the filter during construction have higher false positive probabilities than sets entered at the end. This property allows a prioritization of the sets, where sets containing more "important" elements can be preserved.

A layered Bloom filter consists of multiple Bloom filter layers. Layered Bloom filters allow keeping track of how many times an item was added to the Bloom filter by checking how many layers contain the item. With a layered Bloom filter a check operation will normally return the deepest layer number the item was found in.

An attenuated Bloom filter of depth D can be viewed as an array of D normal Bloom filters. In the context of service discovery in a network, each node stores regular and attenuated Bloom filters locally. The regular or local Bloom filter indicates which services are offered by the node itself. The attenuated filter of level i indicates which services can be found on nodes that are i-hops away from the current node. The i-th value is constructed by taking a union of local Bloom filters for nodes i-hops away from the node.

Let's take a small network shown on the graph below as an example. Say we are searching for a service A whose id hashes to bits 0,1, and 3 (pattern 11010). Let n1 node to be the starting point. First, we check whether service A is offered by n1 by checking its local filter. Since the patterns don't match, we check the attenuated Bloom filter in order to determine which node should be the next hop. We see that n2 doesn't offer service A but lies on the path to nodes that do. Hence, we move to n2 and repeat the same procedure. We quickly find that n3 offers the service, and hence the destination is located.

By using attenuated Bloom filters consisting of multiple layers, services at more than one hop distance can be discovered while avoiding saturation of the Bloom filter by attenuating (shifting out) bits set by sources further away.

Bloom filters are often used to search large chemical structure databases (see chemical similarity). In the simplest case, the elements added to the filter (called a fingerprint in this field) are just the atomic numbers present in the molecule, or a hash based on the atomic number of each atom and the number and type of its bonds. This case is too simple to be useful. More advanced filters also encode atom counts, larger substructure features like carboxyl groups, and graph properties like the number of rings. In hash-based fingerprints, a hash function based on atom and bond properties is used to turn a subgraph into a PRNG seed, and the first output values used to set bits in the Bloom filter.

Molecular fingerprints started in the late 1940s as way to search for chemical structures searched on punched cards. However, it wasn't until around 1990 that Daylight Chemical Information Systems, Inc. introduced a hash-based method to generate the bits, rather than use a precomputed table. Unlike the dictionary approach, the hash method can assign bits for substructures which hadn't previously been seen. In the early 1990s, the term "fingerprint" was considered different from "structural keys", but the term has since grown to encompass most molecular characteristics which can be used for a similarity comparison, including structural keys, sparse count fingerprints, and 3D fingerprints. Unlike Bloom filters, the Daylight hash method allows the number of bits assigned per feature to be a function of the feature size, but most implementations of Daylight-like fingerprints use a fixed number of bits per feature, which makes them a Bloom filter. The original Daylight fingerprints could be used for both similarity and screening purposes. Many other fingerprint types, like the popular ECFP2, can be used for similarity but not for screening because they include local environmental characteristics that introduce false negatives when used as a screen. Even if these are constructed with the same mechanism, these are not Bloom filters because they cannot be used to filter.





</doc>
<doc id="1963880" url="https://en.wikipedia.org/wiki?curid=1963880" title="Zobrist hashing">
Zobrist hashing

Zobrist hashing (also referred to as Zobrist keys or Zobrist signatures ) is a hash function construction used in computer programs that play abstract board games, such as chess and Go, to implement transposition tables, a special kind of hash table that is indexed by a board position and used to avoid analyzing the same position more than once. Zobrist hashing is named for its inventor, Albert Lindsey Zobrist. It has also been applied as a method for recognizing substitutional alloy configurations in simulations of crystalline materials.

Zobrist hashing starts by randomly generating bitstrings for each possible element of a board game, i.e. for each combination of a piece and a position (in the game of chess, that's 12 pieces × 64 board positions, or 16 x 64 if a king that may still castle and a pawn that may capture "en passant" are treated separately for both colors). Now any board configuration can be broken up into independent piece/position components, which are mapped to the random bitstrings generated earlier. The final Zobrist hash is computed by combining those bitstrings using bitwise XOR. Example pseudocode for the game of chess:

If the bitstrings are long enough, different board positions will almost certainly hash to different values; however longer bitstrings require proportionally more computer resources to manipulate. The most commonly used bitstring (key) length is 64 bits. Many game engines store only the hash values in the transposition table, omitting the position information itself entirely to reduce memory usage, and assuming that hash collisions will not occur, or will not greatly influence the results of the table if they do.

Zobrist hashing is the first known instance of tabulation hashing. The result is a 3-wise independent hash family. In particular, it is strongly universal.

As an example, in chess, each of the 64 squares can at any time be empty, or contain one of the 6 game pieces, which are either black or white. That is, each square can be in one of 1 + 6 x 2 = 13 possible states at any time. Thus one needs to generate at most 13 x 64 = 832 random bitstrings. Given a position, one obtains its Zobrist hash by finding out which pieces are on which squares, and combining the relevant bitstrings together.

Rather than computing the hash for the entire board every time, as the pseudocode above does, the hash value of a board can be updated simply by XORing out the bitstring(s) for positions that have changed, and XORing in the bitstrings for the new positions. For instance, if a pawn on a chessboard square is replaced by a rook from another square, the resulting position would be produced by XORing the existing hash with the bitstrings for:
This makes Zobrist hashing very efficient for traversing a game tree.

In computer go, this technique is also used for superko detection.

The same method has been used to recognize substitutional alloy configurations during Monte Carlo simulations in order to prevent wasting computational effort on states that have already been calculated.



</doc>
<doc id="4071549" url="https://en.wikipedia.org/wiki?curid=4071549" title="Rolling hash">
Rolling hash

A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input.

A few hash functions allow a rolling hash to be computed very quickly—the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window—similar to the way a moving average function can be computed much more quickly than other low-pass filters.

One of the main applications is the Rabin–Karp string search algorithm, which uses the rolling hash described below. Another popular application is the rsync program, which uses a checksum based on Mark Adler's adler-32 as its rolling hash. Low Bandwidth Network Filesystem (LBFS) uses a Rabin fingerprint as its rolling hash.

At best, rolling hash values are pairwise independent or strongly universal. They cannot be 3-wise independent, for example.

The Rabin–Karp string search algorithm is often explained using a very simple rolling hash function that only uses multiplications and additions:
where formula_2 is a constant, and formula_3 are the input characters (but this function is not a Rabin fingerprint, see below).

In order to avoid manipulating huge formula_4 values, all math is done modulo formula_5. The choice of formula_2 and formula_5 is critical to get good hashing; see linear congruential generator for more discussion.

Removing and adding characters simply involves adding or subtracting the first or last term. Shifting all characters by one position to the left requires multiplying the entire sum formula_4 by formula_2. Shifting all characters by one position to the right requires dividing the entire sum formula_4 by formula_2. Note that in modulo arithmetic, formula_2 can be chosen to have a multiplicative inverse formula_13 by which formula_4 can be multiplied to get the result of the division without actually performing a division.

The Rabin fingerprint is another hash, which also interprets the input as a polynomial, but over the Galois field GF(2). Instead of seeing the input as a polynomial of bytes, it is seen as a polynomial of bits, and all arithmetic is done in GF(2) (similarly to CRC32). The hash is the result of the division of that polynomial by an irreducible polynomial over GF(2). It is possible to update a Rabin fingerprint using only the entering and the leaving byte, making it effectively a rolling hash.

Because it shares the same author as the Rabin–Karp string search algorithm, which is often explained with another, simpler rolling hash, and because this simpler rolling hash is also a polynomial, both rolling hashes are often mistaken for each other. The backup software restic uses a Rabin fingerprint for splitting files, with blob size varying between 512 bytes and 8MiB.

Hashing by cyclic polynomial—sometimes called Buzhash—is also simple, but it has the benefit of avoiding multiplications, using barrel shifts instead. It is a form of tabulation hashing: it presumes that there is some hash function formula_15 from characters to integers in the interval formula_16. This hash function might be simply an array or a hash table mapping characters to random integers. Let the function formula_17 be a cyclic binary rotation (or circular shift): it rotates the bits by 1 to the left, pushing the latest bit in the first position. E.g., formula_18. Let formula_19 be the bitwise exclusive or. The hash values are defined as

where the multiplications by powers of two can be implemented by binary shifts. The result is a number in formula_16.

Computing the hash values in a rolling fashion is done as follows. Let formula_4 be the previous hash value. Rotate formula_4 once: formula_24. If formula_25 is the character to be removed, rotate it formula_26 times: formula_27. Then simply set 

where formula_29 is the new character.

Hashing by cyclic polynomials is strongly universal or pairwise independent: simply keep the first formula_30 bits. That is, take the result formula_4 and dismiss any formula_32 consecutive bits. In practice, this can be achieved by an integer division formula_33.

The backup software Attic uses a Buzhash algorithm with a customizable chunk size range for splitting file streams.

One of the interesting use cases of the rolling hash function is that it can create dynamic, content-based chunks of a stream or file. This is especially useful when it is required to send only the changed chunks of a large file over a network and a simple byte addition at the front of the file would cause all the fixed size windows to become updated, while in reality, only the first "chunk" has been modified.

The simplest approach to calculate the dynamic chunks is to calculate the rolling hash and if it matches a pattern (like the lower "N" bits are all zeroes) then it’s a chunk boundary. This approach will ensure that any change in the file will only affect its current and possibly the next chunk, but nothing else.

When the boundaries are known, the chunks need to be compared by their hash values to detect which one was modified and needs transfer across the network.

Several programs, including gzip (with the codice_1 option) and rsyncrypto, do content-based slicing based on this specific (unweighted) moving sum:

where

Shifting the window by one byte simply involves adding the new character to the sum and subtracting the oldest character (no longer in the window) from the sum.

For every formula_5 where formula_43, these programs cut the file between formula_5 and formula_45.
This approach will ensure that any change in the file will only affect its current and possibly the next chunk, but no other chunk.

All rolling hash functions are linear in the number of characters, but their complexity with respect to the length of the window (formula_26) varies. Rabin–Karp rolling hash requires the multiplications of two formula_26-bit numbers, integer multiplication is in formula_48. Hashing ngrams by cyclic polynomials can be done in linear time.





</doc>
<doc id="2497149" url="https://en.wikipedia.org/wiki?curid=2497149" title="Hash list">
Hash list

In computer science, a hash list is typically a list of hashes of the data blocks in a file or set of files. Lists of hashes are used for many different purposes, such as fast table lookup (hash tables) and distributed databases (distributed hash tables).

A hash list is an extension of the concept of hashing an item (for instance, a file). A hash list is a subtree of a Merkle tree.

Often, an additional hash of the hash list itself (a "top hash", also called "root hash" or "master hash") is used. Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash list can be received from any non-trusted source, like any peer in the p2p network. Then the received hash list is checked against the trusted top hash, and if the hash list is damaged or fake, another hash list from another source will be tried until the program finds one that matches the top hash.

In some systems (for example, BitTorrent), instead of a top hash the whole hash list is available on a web site in a small file. Such a "torrent file" contains a description, file names, a hash list and some additional data.

Hash lists can be used to protect any kind of data stored, handled and transferred in and between computers. An important use of hash lists is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and to check that the other peers do not "lie" and send fake blocks. 

Usually a cryptographic hash function such as SHA-256 is used for the hashing. If the hash list only needs to protect against unintentional damage unsecured checksums such as CRCs can be used.

Hash lists are better than a simple hash of the entire file since, in the case of a data block being damaged, this is noticed, and only the damaged block needs to be redownloaded. With only a hash of the file, many undamaged blocks would have to be redownloaded, and the file reconstructed and tested until the correct hash of the entire file is obtained. Hash lists also protect against nodes that try to sabotage by sending fake blocks, since in such a case the damaged block can be acquired from some other source.




</doc>
<doc id="39393557" url="https://en.wikipedia.org/wiki?curid=39393557" title="Hash tree">
Hash tree

In computer science, hash tree may refer to:



</doc>
<doc id="11576493" url="https://en.wikipedia.org/wiki?curid=11576493" title="Prefix hash tree">
Prefix hash tree

A prefix hash tree (PHT) is a distributed data structure that enables more sophisticated queries over a distributed hash table (DHT). The prefix hash tree uses the lookup interface of a DHT to construct a trie-based data structure that is both efficient (updates are doubly logarithmic in the size of the domain being indexed), and resilient (the failure of any given node in a prefix hash tree does not affect the availability of data stored at other nodes).



</doc>
<doc id="3568843" url="https://en.wikipedia.org/wiki?curid=3568843" title="Hash trie">
Hash trie

In computer science, hash trie can refer to:




</doc>
<doc id="14514583" url="https://en.wikipedia.org/wiki?curid=14514583" title="Hash array mapped trie">
Hash array mapped trie

A hash array mapped trie (HAMT) is an implementation of an associative array that combines the characteristics of a hash table and an array mapped trie.
It is a refined version of the more general notion of a hash tree.

A HAMT is an array mapped trie where the keys are first hashed to ensure an even distribution of keys and a constant key length.

In a typical implementation of HAMT's array mapped trie, each node contains a table with some fixed number N of slots with each slot containing either a nil pointer or a pointer to another node. N is commonly 32. As allocating space for N pointers for each node would be expensive, each node instead contains a bitmap which is N bits long where each bit indicates the presence of a non-nil pointer. This is followed by an array of pointers equal in length to the number of ones in the bitmap, (its Hamming weight).

The hash array mapped trie achieves almost hash table-like speed while using memory much more economically. Also, a hash table may have to be periodically resized, an expensive operation, whereas HAMTs grow dynamically. Generally, HAMT performance is improved by a larger root table with some multiple of N slots; some HAMT variants allow the root to grow lazily with negligible impact on performance.

Implementation of a HAMT involves the use of the population count function, which counts the number of ones in the binary representation of a number. This operation is available in many instruction set architectures, but it is available in only some high-level languages. Although population count can be implemented in software in O(1) time using a series of shift and add instructions, doing so may perform the operation an order of magnitude slower.

The programming languages Clojure, Scala, and Frege use a persistent variant of hash array mapped tries for their native hash map type. The Haskell library "unordered-containers" uses the same to implement persistent map and set data structures. Another Haskell library "stm-containers" adapts the algorithm for use in the context of software transactional memory. A Javascript HAMT library based on the Clojure implementation is also available. The Rubinius implementation of Ruby includes a HAMT, mostly written in Ruby but with 3 primitives. Large maps in Erlang use a HAMT representation internally since release 18.0. The Pony programming language uses a HAMT for the hash map in its persistent collections package.

The concurrent lock-free version of the hash trie called Ctrie is a mutable thread-safe implementation which ensures progress. The data-structure has been proven to be correct - Ctrie operations have been shown to have the atomicity, linearizability and lock-freedom properties.


</doc>
<doc id="192141" url="https://en.wikipedia.org/wiki?curid=192141" title="Distributed hash table">
Distributed hash table

A distributed hash table (DHT) is a distributed system that provides a lookup service similar to a hash table: ("key", "value") pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. The main advantage of a DHT is that nodes can be added/removed with minimum work around re-distributting keys. "Keys" are unique identifiers which map to particular "values", which in turn can be anything from addresses, to documents, to arbitrary data. Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption. This allows a DHT to scale to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures.

DHTs form an infrastructure that can be used to build more complex services, such as anycast, cooperative Web caching, distributed file systems, domain name services, instant messaging, multicast, and also peer-to-peer file sharing and content distribution systems. Notable distributed networks that use DHTs include BitTorrent's distributed tracker, the Coral Content Distribution Network, the Kad network, the Storm botnet, the Tox instant messenger, Freenet, the YaCy search engine, and the InterPlanetary File System.

DHT research was originally motivated, in part, by peer-to-peer systems such as Freenet, gnutella, BitTorrent, and Napster, which took advantage of resources distributed across the Internet to provide a single useful application. In particular, they took advantage of increased bandwidth and hard disk capacity to provide a file-sharing service.

These systems differed in how they located the data offered by their peers. Napster, the first large-scale P2P content delivery system, required a central index server: each node, upon joining, would send a list of locally held files to the server, which would perform searches and refer the queries to the nodes that held the results. This central component left the system vulnerable to attacks and lawsuits.

Gnutella and similar networks moved to a flooding query model in essence, each search would result in a message being broadcast to every other machine in the network. While avoiding a single point of failure, this method was significantly less efficient than Napster. Later versions of Gnutella clients moved to a dynamic querying model which vastly improved efficiency.

Freenet is fully distributed, but employs a heuristic key-based routing in which each file is associated with a key, and files with similar keys tend to cluster on a similar set of nodes. Queries are likely to be routed through the network to such a cluster without needing to visit many peers. However, Freenet does not guarantee that data will be found.

Distributed hash tables use a more structured key-based routing in order to attain both the decentralization of Freenet and gnutella, and the efficiency and guaranteed results of Napster. One drawback is that, like Freenet, DHTs only directly support exact-match search, rather than keyword search, although Freenet's routing algorithm can be generalized to any key type where a closeness operation can be defined.

In 2001, four systems—CAN, Chord, Pastry, and Tapestry—ignited DHTs as a popular research topic. 
A project called the Infrastructure for Resilient Internet Systems (Iris) was funded by a $12 million grant from the US National Science Foundation in 2002.
Researchers included Sylvia Ratnasamy, Ion Stoica, Hari Balakrishnan and Scott Shenker.
Outside academia, DHT technology has been adopted as a component of BitTorrent and in the Coral Content Distribution Network.

DHTs characteristically emphasize the following properties:


A key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system – most commonly, O(log "n") of the formula_1 participants (see below) – so that only a limited amount of work needs to be done for each change in membership.

Some DHT designs seek to be secure against malicious participants and to allow participants to remain anonymous, though this is less common than in many other peer-to-peer (especially file sharing) systems; see anonymous P2P.

Finally, DHTs must deal with more traditional distributed systems issues such as load balancing, data integrity, and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly).

The structure of a DHT can be decomposed into several main components. The foundation is an abstract keyspace, such as the set of 160-bit strings. A keyspace partitioning scheme splits ownership of this keyspace among the participating nodes. An overlay network then connects the nodes, allowing them to find the owner of any given key in the keyspace.

Once these components are in place, a typical use of the DHT for storage and retrieval might proceed as follows. Suppose the keyspace is the set of 160-bit strings. To index a file with given and in the DHT, the SHA-1 hash of is generated, producing a 160-bit key , and a message is sent to any node participating in the DHT. The message is forwarded from node to node through the overlay network until it reaches the single node responsible for key as specified by the keyspace partitioning. That node then stores the key and the data. Any other client can then retrieve the contents of the file by again hashing to produce and asking any DHT node to find the data associated with with a message . The message will again be routed through the overlay to the node responsible for , which will reply with the stored .

The keyspace partitioning and overlay network components are described below with the goal of capturing the principal ideas common to most DHTs; many designs differ in the details.

Most DHTs use some variant of consistent hashing or rendezvous hashing to map keys to nodes. The two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem.

Both consistent hashing and rendezvous hashing have the essential property that removal or addition of one node changes only the set of keys owned by the nodes with adjacent IDs, and leaves all other nodes unaffected. Contrast this with a traditional hash table in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to bandwidth-intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure).

Consistent hashing employs a function formula_2 that defines an abstract notion of the distance between the keys formula_3 and formula_4, which is unrelated to geographical distance or network latency. Each node is assigned a single key called its "identifier" (ID). A node with ID formula_5 owns all the keys formula_6 for which formula_5 is the closest ID, measured according to formula_8.

For example, the Chord DHT uses consistent hashing, which treats nodes as points on a circle, and formula_2 is the distance traveling clockwise around the circle from formula_3 to formula_4. Thus, the circular keyspace is split into contiguous segments whose endpoints are the node identifiers. If formula_12 and formula_13 are two adjacent IDs, with a shorter clockwise distance from formula_12 to formula_13, then the node with ID formula_13 owns all the keys that fall between formula_12 and formula_13.

In rendezvous hashing, also called highest random weight (HRW) hashing, all clients use the same hash function formula_19 (chosen ahead of time) to associate a key to one of the "n" available servers.
Each client has the same list of identifiers , one for each server.
Given some key "k", a client computes "n" hash weights .
The client associates that key with the server corresponding to the highest hash weight for that key.
A server with ID formula_20 owns all the keys formula_6 for which the hash weight formula_22 is higher than the hash weight of any other node for that key.

Locality-preserving hashing ensures that similar keys are assigned to similar objects. This can enable a more efficient execution of range queries. Self-Chord decouples object keys from peer IDs and sorts keys along the ring with a statistical approach based on the swarm intelligence paradigm. Sorting ensures that similar keys are stored by neighbour nodes and that discovery procedures, including range queries, can be performed in logarithmic time.

Each node maintains a set of links to other nodes (its "neighbors" or routing table). Together, these links form the overlay network. A node picks its neighbors according to a certain structure, called the network's topology.

All DHT topologies share some variant of the most essential property: for any key , each node either has a node ID that owns or has a link to a node whose node ID is "closer" to , in terms of the keyspace distance defined above. It is then easy to route a message to the owner of any key using the following greedy algorithm (that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest to . When there is no such neighbor, then we must have arrived at the closest node, which is the owner of as defined above. This style of routing is sometimes called key-based routing.

Beyond basic routing correctness, two important constraints on the topology are to guarantee that the maximum number of hops in any route (route length) is low, so that requests complete quickly; and that the maximum number of neighbors of any node (maximum node degree) is low, so that maintenance overhead is not excessive. Of course, having shorter routes requires higher maximum degree. Some common choices for maximum degree and route length are as follows, where is the number of nodes in the DHT, using Big O notation:

The most common choice, formula_23 degree/route length, is not optimal in terms of degree/route length tradeoff, but such topologies typically allow more flexibility in choice of neighbors. Many DHTs use that flexibility to pick neighbors that are close in terms of latency in the physical underlying network. In general, all DHTs construct navigable Small-World network topologies, which trade-off route length vs. network degree.

Maximum route length is closely related to diameter: the maximum number of hops in any shortest path between nodes. Clearly, the network's worst case route length is at least as large as its diameter, so DHTs are limited by the degree/diameter tradeoff that is fundamental in graph theory. Route length can be greater than diameter, since the greedy routing algorithm may not find shortest paths. 

Aside from routing, there exist many algorithms that exploit the structure of the overlay network for sending a message to all nodes, or a subset of nodes, in a DHT. These algorithms are used by applications to do overlay multicast, range queries, or to collect statistics. Two systems that are based on this approach are Structella, which implements flooding and random walks on a Pastry overlay, and DQ-DHT, which implements a dynamic querying search algorithm over a Chord network.

Because of the decentralization, fault tolerance, and scalability of DHTs, they are inherently more resilient against a hostile attacker than a centralized system.

Open systems for distributed data storage that are robust against massive hostile attackers are feasible.

A DHT system that is carefully designed to have Byzantine fault tolerance can defend against a security weakness, known as the Sybil attack, which affects all current DHT designs.

Petar Maymounkov, one of the original authors of Kademlia, has proposed a way to circumvent the weakness to the Sybil attack by incorporating social trust relationships into the system design. The new system, codenamed Tonika or also known by its domain name as 5ttt, is based on an algorithm design known as "electric routing" and co-authored with the mathematician Jonathan Kelner. Maymounkov has now undertaken a comprehensive implementation effort of this new system. However, research into effective defences against Sybil attacks is generally considered an open question, and wide variety of potential defences are proposed every year in top security research conferences.

Most notable differences encountered in practical instances of DHT implementations include at least the following:






</doc>
<doc id="2434041" url="https://en.wikipedia.org/wiki?curid=2434041" title="Consistent hashing">
Consistent hashing

In computer science, consistent hashing is a special kind of hashing such that when a hash table is resized, only formula_1 keys need to be remapped on average, where formula_2 is the number of keys, and formula_3 is the number of slots.

In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation. Consistent hashing is a particular case of rendezvous hashing, which has a conceptually simpler algorithm, and was first described in 1996.. Consistent hashing first appeared in 1997, uses a different algorithm.

The term "consistent hashing" was introduced by Karger "et al." at MIT for use in distributed caching. This academic paper from 1997 introduced the term "consistent hashing" as a way of distributing requests among a changing population of web servers. Each slot is then represented by a node in a distributed system. The addition (joins) and removal (leaves/failures) of nodes only requires formula_1 items to be re-shuffled when the number of slots/nodes change. The authors mention linear hashing and its ability to handle sequential node addition and removal, while consistent hashing allows buckets to be added and removed in arbitrary order.
Teradata used this technique in their distributed database, released in 1986, although they did not use this term. Teradata still use the concept of a hash table to fulfill exactly this purpose. Akamai Technologies was founded in 1998 by the scientists Daniel Lewin and F. Thomson Leighton (co-authors of the article coining "consistent hashing") to apply this algorithm, which gave birth to the content delivery network industry.

Consistent hashing has also been used to reduce the impact of partial system failures in large web applications to provide robust caching without incurring the system-wide fallout of a failure.

Consistent hashing is also the cornerstone of distributed hash tables (DHTs), which employ hash values to partition a keyspace across a distributed set of nodes, then construct an overlay network of connected nodes that provide efficient node retrieval by key.

Rendezvous hashing, designed at the same time as consistent hashing, achieves the same goals using the very different highest random weight (HRW) algorithm. 

While running collections of caching machines some limitations are experienced. A common way of load balancing formula_3 cache machines is to put object formula_6 in cache machine number formula_7. But this will not work if a cache machine is added or removed because formula_3 changes and every object is hashed to a new location. This can be disastrous since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping of servers.

Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared among the remaining machines.

The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed, its interval is taken over by a cache with an adjacent interval while all the remaining caches are unchanged.

Consistent hashing is based on mapping each object to a point on a circle (or equivalently, mapping each object to a real angle). The system maps each available machine (or other storage bucket) to many pseudo-randomly distributed points on the same circle.

To find where an object should be placed, the system finds the location of that object's key on the circle; then walks around the circle until falling into the first bucket it encounters (or equivalently, the first available bucket with a higher angle). The result is that each bucket contains all the resources located between each one of its points and the previous points that belong to other buckets.

If a bucket becomes unavailable (for example because the computer it resides on is not reachable), then the points it maps to will be removed. Requests for resources that would have mapped to each of those points now map to the next highest points. Since each bucket is associated with many pseudo-randomly distributed points, the resources that were held by that bucket will now map to many different buckets. The items that mapped to the lost bucket must be redistributed among the remaining ones, but values mapping to other buckets will still do so and do not need to be moved.

A similar process occurs when a bucket is added. By adding new bucket points, we make any resources between those and the points corresponding to the next smaller angles map to the new bucket. These resources will no longer be associated with the previous buckets, and any value previously stored there will not be found by the selection method described above.

The portion of the keys associated with each bucket can be altered by altering the number of angles that bucket maps to.

If key values will always increase monotonically, an alternative approach using a hash table with monotonic keys may be more suitable.

The formula_9 complexity for consistent hashing comes from the fact that a binary search among nodes angles is required to find the next node on the ring.

Known examples of consistent hashing use include:



</doc>
<doc id="14947011" url="https://en.wikipedia.org/wiki?curid=14947011" title="Koorde">
Koorde

In peer-to-peer networks, Koorde is a Distributed hash table (DHT) system based on the Chord DHT and the De Bruijn graph (De Bruijn sequence). Inheriting the simplicity of Chord, Koorde meets O(log n) hops per node (where n is the number of nodes in the DHT), and O(log n/ log log n) hops per lookup request with O(log n) neighbors per node.

The Chord concept is based on a wide range of identifiers (e.g. 2^160) in a structure of a ring where an identifier can stand for both node and data. Node-successor is responsible for the whole range of IDs between itself and its predecessor.

Koorde is based on Chord but also on De Bruijn graph (De Bruijn sequence).
In a d-dimensional de Bruijn graph, there are 2 nodes, each of which has a unique d-bit ID. The node with ID i is connected to nodes 2i modulo 2 and 2i+1 modulo 2. Thanks to this property, the routing algorithm can route to any destination in d hops by successively "shifting in" the bits of the destination ID but only if the dimensions of the distance between modulo 1d and 3d are equal.

Routing a message from node m to node k is accomplished by taking the number m and shifting in the bits of k one at a time until the number has been replaced by k. Each shift corresponds to a routing hop to the next intermediate address; the hop is valid because each node's neighbors are the two possible outcomes of shifting a 0 or 1 onto its own address. Because of the structure of de Bruijn graphs, when the last bit of k has been shifted, the query will be at node k. Node k responds whether key k exists.

For example, when a message needs to be routed from node “2” (which is “010”) to “6” (which is “110”), the steps are following:

Step 1)
Node #2 routes the message to Node #5 (using its connection to 2i+1 mod8), shifts the bits left and puts “1” as the youngest bit (right side).

Step 2)
Node #5 routes the message to Node #3 (using its connection to 2i+1 mod8), shifts the bits left and puts “1” as the youngest bit (right side).

Step 3)
Node #3 routes the message to Node #6 (using its connection to 2i mod8), shifts the bits left and puts “0” as the youngest bit (right side).

The d-dimensional de Bruijn can be generalized to base k, in which case node i is connected to nodes k * i + j modulo kd, 0 ≤ j < k. The diameter is reduced to Θ(logk n). Koorde node i maintains pointers to k consecutive nodes beginning at the predecessor of k * i modulo kd. Each de Bruijn routing step can be emulated with an expected constant number of messages, so routing uses O(logk n) expected hops- For k = Θ(log n), we get Θ(log n) degree and Θ(log n/ log log n) diameter.



</doc>
<doc id="48589" url="https://en.wikipedia.org/wiki?curid=48589" title="Graph">
Graph

Graph may refer to:





</doc>
<doc id="392431" url="https://en.wikipedia.org/wiki?curid=392431" title="Adjacency list">
Adjacency list

In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. Each list describes the set of neighbors of a vertex in the graph. This is one of several commonly used representations of graphs for use in computer programs.

An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighboring vertices or edges. There are many variations of this basic idea, differing in the details of how they implement the association between vertices and collections, in how they implement the collections, in whether they include both vertices and edges or only vertices as first class objects, and in what kinds of objects are used to represent the vertices and edges.

The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex. Using any of the implementations detailed above, this can be performed in constant time per neighbor. In other words, the total time to report all of the neighbors of a vertex "v" is proportional to the degree of "v".

It is also possible, but not as efficient, to use adjacency lists to test whether an edge exists or does not exist between two specified vertices. In an adjacency list in which the neighbors of each vertex are unsorted, testing for the existence of an edge may be performed in time proportional to the minimum degree of the two given vertices, by using a sequential search through the neighbors of this vertex. If the neighbors are represented as a sorted array, binary search may be used instead, taking time proportional to the logarithm of the degree.

The main alternative to the adjacency list is the adjacency matrix, a matrix whose rows and columns are indexed by vertices and whose cells contain a Boolean value that indicates whether an edge is present between the vertices corresponding to the row and column of the cell. For a sparse graph (one in which most pairs of vertices are not connected by edges) an adjacency list is significantly more space-efficient than an adjacency matrix (stored as an array): the space usage of the adjacency list is proportional to the number of edges and vertices in the graph, while for an adjacency matrix stored in this way the space is proportional to the square of the number of vertices. However, it is possible to store adjacency matrices more space-efficiently, matching the linear space usage of an adjacency list, by using a hash table indexed by pairs of vertices rather than an array.

The other significant difference between adjacency lists and adjacency matrices is in the efficiency of the operations they perform. In an adjacency list, the neighbors of each vertex may be listed efficiently, in time proportional to the degree of the vertex. In an adjacency matrix, this operation takes time proportional to the number of vertices in the graph, which may be significantly higher than the degree. On the other hand, the adjacency matrix allows testing whether two vertices are adjacent to each other in constant time; the adjacency list is slower to support this operation.

For use as a data structure, the main alternative to the adjacency list is the adjacency matrix. Because each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only bytes of contiguous space, where is the number of vertices of the graph. Besides avoiding wasted space, this compactness encourages locality of reference.

However, for a sparse graph, adjacency lists require less space, because they do not waste any space to represent edges that are not present. Using a naïve array implementation on a 32-bit computer, an adjacency list for an undirected graph requires about bytes of space, where is the number of edges of the graph.

Noting that an undirected simple graph can have at most edges, allowing loops, we can let denote the density of the graph. Then, when , that is the adjacency list representation occupies more space than the adjacency matrix representation when . Thus a graph must be sparse enough to justify an adjacency list representation.

Besides the space trade-off, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list. With an adjacency matrix, an entire row must instead be scanned, which takes time. Whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.



</doc>
<doc id="244463" url="https://en.wikipedia.org/wiki?curid=244463" title="Adjacency matrix">
Adjacency matrix

In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.

In the special case of a finite simple graph, the adjacency matrix is a (0,1)-matrix with zeros on its diagonal. If the graph is undirected, the adjacency matrix is symmetric. 
The relationship between a graph and the eigenvalues and eigenvectors of its adjacency matrix is studied in spectral graph theory.

The adjacency matrix should be distinguished from the incidence matrix for a graph, a different matrix representation whose elements indicate whether vertex–edge pairs are incident or not, and degree matrix which contains information about the degree of each vertex.

For a simple graph with vertex set "V", the adjacency matrix is a square  ×  matrix "A" such that its element "A" is one when there is an edge from vertex "i" to vertex "j", and zero when there is no edge. The diagonal elements of the matrix are all zero, since edges from a vertex to itself (loops) are not allowed in simple graphs. It is also sometimes useful in algebraic graph theory to replace the nonzero elements with algebraic variables.

The same concept can be extended to multigraphs and graphs with loops by storing the number of edges between each two vertices in the corresponding matrix element, and by allowing nonzero diagonal elements. Loops may be counted either once (as a single edge) or twice (as two vertex-edge incidences), as long as a consistent convention is followed. Undirected graphs often use the latter convention of counting loops twice, whereas directed graphs typically use the former convention.

The adjacency matrix "A" of a bipartite graph whose two parts have "r" and "s" vertices can be written in the form 
where "B" is an matrix, and 0 and 0 represent the and zero matrices. In this case, the smaller matrix "B" uniquely represents the graph, and the remaining parts of "A" can be discarded as redundant. "B" is sometimes called the biadjacency matrix.

Formally, let be a bipartite graph with parts } and }. The "biadjacency matrix" is the 0–1 matrix "B" in which if and only if ∈ "E". 

If "G" is a bipartite multigraph or weighted graph then the elements "b" are taken to be the number of edges between the vertices or the weight of the edge , respectively.

An -"adjacency matrix" "A" of a simple graph has "A" = "a" if ("i", "j") is an edge, "b" if it is not, and "c" on the diagonal. The Seidel adjacency matrix is a -"adjacency matrix". This matrix is used in studying strongly regular graphs and two-graphs.

The distance matrix has in position ("i", "j") the distance between vertices "v" and "v". The distance is the length of a shortest path connecting the vertices. Unless lengths of edges are explicitly provided, the length of a path is the number of edges in it. The distance matrix resembles a high power of the adjacency matrix, but instead of telling only whether or not two vertices are connected (i.e., the connection matrix, which contains boolean values), it gives the exact distance between them.

The convention followed here (for undirected graphs) is that each edge adds 1 to the appropriate cell in the matrix, and each loop adds 2. This allows the degree of a vertex to be easily found by taking the sum of the values in either its respective row or column in the adjacency matrix.

In directed graphs, the in-degree of a vertex can be computed by summing the entries of the corresponding column, and the out-degree can be computed by summing the entries of the corresponding row.
The adjacency matrix of a complete graph contains all ones except along the diagonal where there are only zeros. The adjacency matrix of an empty graph is a zero matrix.

The adjacency matrix of an undirected simple graph is symmetric, and therefore has a complete set of real eigenvalues and an orthogonal eigenvector basis. The set of eigenvalues of a graph is the spectrum of the graph. It is common to denote the eigenvalues by formula_2

The greatest eigenvalue formula_3 is bounded above by the maximum degree. This can be seen as result of the Perron–Frobenius theorem, but it can be proved easily. Let "v" be one eigenvector associated to formula_3 and "x" the component in which "v" has maximum absolute value. Without loss of generality assume "v" is positive since otherwise you simply take the eigenvector formula_5, also associated to formula_3. Then

For "d"-regular graphs, "d" is the first eigenvalue of "A" for the vector (it is easy to check that it is an eigenvalue and it is the maximum because of the above bound). The multiplicity of this eigenvalue is the number of connected components of "G", in particular formula_8 for connected graphs. It can be shown that for each eigenvalue formula_9, its opposite formula_10 is also an eigenvalue of "A" if "G" is a bipartite graph. In particular −"d" is an eigenvalue of bipartite graphs.

The difference formula_11 is called the spectral gap and it is related to the expansion of "G". It is also useful to introduce the spectral radius of formula_12 denoted by formula_13. This number is bounded by formula_14. This bound is tight in the Ramanujan graphs, which have applications in many areas.

Suppose two directed or undirected graphs "G" and "G" with adjacency matrices "A" and "A" are given. "G" and "G" are isomorphic if and only if there exists a permutation matrix "P" such that
In particular, "A" and "A" are similar and therefore have the same minimal polynomial, characteristic polynomial, eigenvalues, determinant and trace. These can therefore serve as isomorphism invariants of graphs. However, two graphs may possess the same set of eigenvalues but not be isomorphic. Such linear operators are said to be isospectral.

If "A" is the adjacency matrix of the directed or undirected graph "G", then the matrix "A" (i.e., the matrix product of "n" copies of "A") has an interesting interpretation: the element gives the number of (directed or undirected) walks of length "n" from vertex "i" to vertex "j". If "n" is the smallest nonnegative integer, such that for some "i", "j", the element of "A" is positive, then "n" is the distance between vertex "i" and vertex "j". This implies, for example, that the number of triangles in an undirected graph "G" is exactly the trace of "A" divided by 6. Note that the adjacency matrix can be used to determine whether or not the graph is connected.

The adjacency matrix may be used as a data structure for the representation of graphs in computer programs for manipulating graphs. The main alternative data structure, also in use for this application, is the adjacency list.

Because each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only /8 bytes to represent a directed graph, or (by using a packed triangular format and only storing the lower triangular part of the matrix) approximately /16 bytes to represent an undirected graph. Although slightly more succinct representations are possible, this method gets close to the information-theoretic lower bound for the minimum number of bits needed to represent all -vertex graphs. For storing graphs in text files, fewer bits per byte can be used to ensure that all bytes are text characters, for instance by using a Base64 representation.
Besides avoiding wasted space, this compactness encourages locality of reference.
However, for a large sparse graph, adjacency lists require less storage space, because they do not waste any space to represent edges that are "not" present.

An alternative form of adjacency matrix (which, however, requires a larger amount of space) replaces the numbers in each element of the matrix with pointers to edge objects (when edges are present) or null pointers (when there is no edge).
It is also possible to store edge weights directly in the elements of an adjacency matrix.

Besides the space tradeoff, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list, and takes time proportional to the number of neighbors. With an adjacency matrix, an entire row must instead be scanned, which takes a larger amount of time, proportional to the number of vertices in the whole graph. On the other hand, testing whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.




</doc>
<doc id="4689919" url="https://en.wikipedia.org/wiki?curid=4689919" title="And-inverter graph">
And-inverter graph

An and-inverter graph (AIG) is a directed, acyclic graph that represents a structural implementation of the logical functionality of a circuit or network. An AIG consists of two-input nodes representing logical conjunction, terminal nodes labeled with variable names, and edges optionally containing markers indicating logical negation. This representation of a logic function is rarely structurally efficient for large circuits, but is an efficient representation for manipulation of boolean functions. Typically, the abstract graph is represented as a data structure in software.
Conversion from the network of logic gates to AIGs is fast and scalable. It only requires that every gate be expressed in terms of AND gates and inverters. This conversion does not lead to unpredictable increase in memory use and runtime. This makes the AIG an efficient representation in comparison with either the binary decision diagram (BDD) or the "sum-of-product" (ΣoΠ) form, that is, the canonical form in Boolean algebra known as the disjunctive normal form (DNF). The BDD and DNF may also be viewed as circuits, but they involve formal constraints that deprive them of scalability. For example, ΣoΠs are circuits with at most two levels while BDDs are canonical, that is, they require that input variables be evaluated in the same order on all paths.

Circuits composed of simple gates, including AIGs, are an "ancient" research topic. The interest in AIGs started with Alan Turing's seminal 1948 paper on neural networks, in which he described a randomized trainable network of NAND gates. Interest continued through the late 1950s and continued in the 1970s when various local transformations have been developed. These transformations were implemented in several
logic synthesis and verification systems, such as Darringer et al. and Smith et al., which reduce circuits to improve area and delay during synthesis, or to speed up formal equivalence checking. Several important techniques were discovered early at IBM, such as combining and reusing multi-input logic expressions and subexpressions, now known as structural hashing.

Recently there has been a renewed interest in AIGs as a functional representation for a variety of tasks in synthesis and verification. That is because representations popular in the 1990s (such as BDDs) have reached their limits of scalability in many of their applications. Another important development was the recent emergence of much more efficient boolean satisfiability (SAT) solvers. When coupled with "AIGs" as the circuit representation, they lead to remarkable speedups in solving a wide variety of boolean problems.

AIGs found successful use in diverse EDA applications. A well-tuned combination of "AIGs" and boolean satisfiability made an impact on formal verification, including both model checking and equivalence checking. Another recent work shows that efficient circuit compression techniques can be developed using AIGs. There is a growing understanding that logic and physical synthesis problems can be solved using simulation and boolean satisfiability to compute functional properties (such as symmetries) and node flexibilities (such as don't-care terms, resubstitutions, and SPFDs). Mishchenko et al. shows that AIGs are a promising "unifying" representation, which can bridge logic synthesis, technology mapping, physical synthesis, and formal verification. This is, to a large extent, due to the simple and uniform structure of AIGs, which allow rewriting, simulation, mapping, placement, and verification to share the same data structure.

In addition to combinational logic, AIGs have also been applied to sequential logic and sequential transformations. Specifically, the method of structural hashing was extended to work for AIGs with memory elements (such as D-type flip-flops with an initial state,
which, in general, can be unknown) resulting in a data structure that is specifically tailored for applications related to retiming.

Ongoing research includes implementing a modern logic synthesis system completely based on AIGs. The prototype called ABC features an AIG package, several AIG-based synthesis and equivalence-checking techniques, as well as an experimental implementation of sequential synthesis. One such technique combines technology mapping and retiming in a single optimization step. These optimizations can be implemented using networks composed of arbitrary gates, but the use of AIGs makes them more scalable and easier to implement.


"This article is adapted from a column in the ACM SIGDA e-newsletter by Alan Mishchenko <br>
Original text is available here."


</doc>
<doc id="576855" url="https://en.wikipedia.org/wiki?curid=576855" title="Binary decision diagram">
Binary decision diagram

In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. On a more abstract level, BDDs can be considered as a compressed representation of sets or relations. Unlike other compressed representations, operations are performed directly on the compressed representation, i.e. without decompression. Other data structures used to represent Boolean functions include negation normal form (NNF), Zhegalkin polynomials, and propositional directed acyclic graphs (PDAG).

A Boolean function can be represented as a rooted, directed, acyclic graph, which consists of several decision nodes and terminal nodes. There are two types of terminal nodes called 0-terminal and 1-terminal. Each decision node formula_1 is labeled by Boolean variable formula_2 and has two child nodes called low child and high child. The edge from node formula_2 to a low (or high) child represents an assignment of formula_2 to 0 (respectively 1).
Such a BDD is called 'ordered' if different variables appear in the same order on all paths from the root. A BDD is said to be 'reduced' if the following two rules have been applied to its graph:

In popular usage, the term BDD almost always refers to Reduced Ordered Binary Decision Diagram (ROBDD in the literature, used when the ordering and reduction aspects need to be emphasized). The advantage of an ROBDD is that it is canonical (unique) for a particular function and variable order. This property makes it useful in functional equivalence checking and other operations like functional technology mapping.

A path from the root node to the 1-terminal represents a (possibly partial) variable assignment for which the represented Boolean function is true. As the path descends to a low (or high) child from a node, then that node's variable is assigned to 0 (respectively 1).

The left figure below shows a binary decision "tree" (the reduction rules are not applied), and a truth table, each representing the function f (x1, x2, x3). In the tree on the left, the value of the function can be determined for a given variable assignment by following a path down the graph to a terminal. In the figures below, dotted lines represent edges to a low child, while solid lines represent edges to a high child. Therefore, to find (x1=0, x2=1, x3=1), begin at x1, traverse down the dotted line to x2 (since x1 has an assignment to 0), then down two solid lines (since x2 and x3 each have an assignment to one). This leads to the terminal 1, which is the value of f (x1=0, x2=1, x3=1).

The binary decision "tree" of the left figure can be transformed into a binary decision "diagram" by maximally reducing it according to the two reduction rules. The resulting BDD is shown in the right figure.

The basic idea from which the data structure was created is the Shannon expansion. A switching function is split into two sub-functions (cofactors) by assigning one variable (cf. "if-then-else normal form"). If such a sub-function is considered as a sub-tree, it can be represented by a "binary decision tree". Binary decision diagrams (BDD) were introduced by Lee, and further studied and made known by Akers and Boute.

The full potential for efficient algorithms based on the data structure was investigated by Randal Bryant at Carnegie Mellon University: his key extensions were to use a fixed variable ordering (for canonical representation) and shared sub-graphs (for compression). Applying these two concepts results in an efficient data structure and algorithms for the representation of sets and relations. By extending the sharing to several BDDs, i.e. one sub-graph is used by several BDDs, the data structure "Shared Reduced Ordered Binary Decision Diagram" is defined. The notion of a BDD is now generally used to refer to that particular data structure.

In his video lecture "Fun With Binary Decision Diagrams (BDDs)", Donald Knuth calls BDDs "one of the only really fundamental data structures that came out in the last twenty-five years" and mentions that Bryant's 1986 paper was for some time one of the most-cited papers in computer science.

Adnan Darwiche and his collaborators have shown that BDDs are one of several normal forms for Boolean functions, each induced by a different combination of requirements. Another important normal form identified by Darwiche is Decomposable Negation Normal Form or DNNF.

BDDs are extensively used in CAD software to synthesize circuits (logic synthesis) and in formal verification. There are several lesser known applications of BDD, including fault tree analysis, Bayesian reasoning, product configuration, and private information retrieval.

Every arbitrary BDD (even if it is not reduced or ordered) can be directly implemented in hardware by replacing each node with a 2 to 1 multiplexer; each multiplexer can be directly implemented by a 4-LUT in a FPGA. It is not so simple to convert from an arbitrary network of logic gates to a BDD (unlike the and-inverter graph).

The size of the BDD is determined both by the function being represented and the chosen ordering of the variables. There exist Boolean functions formula_5 for which depending upon the ordering of the variables we would end up getting a graph whose number of nodes would be linear (in "n") at the best and exponential at the worst case (e.g., a ripple carry adder). Consider the Boolean function formula_6
Using the variable ordering formula_7, the BDD needs 2 nodes to represent the function. Using the ordering formula_8, the BDD consists of 2"n" + 2 nodes.

It is of crucial importance to care about variable ordering when applying this data structure in practice.
The problem of finding the best variable ordering is NP-hard. For any constant "c" > 1 it is even NP-hard to compute a variable ordering resulting in an OBDD with a size that is at most c times larger than an optimal one. However, there exist efficient heuristics to tackle the problem.

There are functions for which the graph size is always exponential — independent of variable ordering. This holds e.g. for the multiplication function. In fact, the function computing the middle bit of the product of two formula_9-bit numbers does not have an OBDD smaller than formula_10 vertices. (If the multiplication function had polynomial-size OBDDs, it would show that integer factorization is in P/poly, which is not known to be true.)

For cellular automata with simple behavior, the minimal BDD typically grows linearly on successive steps. For rule 254, for example, it is 8t+2, while for rule 90 it is 4t+2. For cellular automata with more complex behavior, it typically grows roughly exponentially. Thus for rule 30 it is {7, 14, 29, 60, 129} and for rule 110 {7, 15, 27, 52, 88}. The size of the minimal BDD can depend on the order in which variables are specified; thus for example, just reflecting rule 30 to give rule 86 yields {6, 11, 20, 36, 63}.

Researchers have suggested refinements on the BDD data structure giving way to a number of related graphs, such as BMD (binary moment diagrams), ZDD (zero-suppressed decision diagram), FDD (free binary decision diagrams), PDD (parity decision diagrams), and MTBDDs (multiple terminal BDDs).

Many logical operations on BDDs can be implemented by polynomial-time graph manipulation algorithms:

However, repeating these operations several times, for example forming the conjunction or disjunction of a set of BDDs, may in the worst case result in an exponentially big BDD. This is because any of the preceding operations for two BDDs may result in a BDD with a size proportional to the product of the BDDs' sizes, and consequently for several BDDs the size may be exponential. Also, since constructing the BDD of a Boolean function solves the NP-complete Boolean satisfiability problem and the co-NP-complete tautology problem, constructing the BDD can take exponential time in the size of the Boolean formula even when the resulting BDD is small.

Computing existential abstraction over multiple variables of reduced BDDs is NP-complete.

Model-counting, counting the number of satisfying assignments of a Boolean formula, can be done in polynomial time for BDDs. For general propositional formulas the problem is ♯P-complete and the known best algorithms require an exponential time in the worst case.





</doc>
<doc id="4056695" url="https://en.wikipedia.org/wiki?curid=4056695" title="Binary moment diagram">
Binary moment diagram

A binary moment diagram (BMD) is a generalization of the binary decision diagram (BDD) to linear functions over domains such as booleans (like BDDs), but also to integers or to real numbers.

They can deal with boolean functions with complexity comparable to BDDs, but also some functions that are dealt with very inefficiently in a BDD are handled easily by BMD, most notably multiplication.

The most important properties of BMD is that, like with BDDs, each function has exactly one canonical representation, and many operations can be efficiently performed on these representations.

The main features that differentiate BMDs from BDDs are using linear instead of pointwise diagrams, and having weighted edges.

The rules that ensure the canonicity of the representation are:

In pointwise decomposition, like in BDDs, on each branch point we store result of all branches separately. An example of such decomposition for an integer function (2"x" + "y") is:

In linear decomposition we provide instead a default value and a difference:

It can easily be seen that the latter (linear) representation is much more efficient in case of additive functions, as when we add many elements the latter representation will have only O("n") elements, while the former (pointwise), even with sharing, exponentially many.

Another extension is using weights for edges. A value of function at given node is a sum of the true nodes below it (the node under always, and possibly the decided node) times the edges' weights.

For example, formula_3 can be represented as:

Without weighted nodes a much more complex representation would be required:


</doc>
<doc id="4057707" url="https://en.wikipedia.org/wiki?curid=4057707" title="Zero-suppressed decision diagram">
Zero-suppressed decision diagram

A zero-suppressed decision diagram (ZSDD or ZDD) is a type of binary decision diagram (BDD) based on a new reduction rule, devised by Shin-ichi Minato in 1993. This data structure brings unique and compact representation of sets, often appearing in combinatorial problems. With a modified reduction rule in node elimination, a ZDD distinguishes itself from a BDD by ensuring the canonicity of the representation of the sets. Canonical means that, under certain conditions, an object has only one representation of this kind. Whereas the node in BDDs is removed from the decision tree if both its edges point to the same node, the node in ZDDs is removed if its positive edge points to the constant node 0. The modifications in reduction rule especially improve performance when dealing with sparse sets.

In a Binary Decision Diagram, a Boolean function can be represented as a rooted, directed, acyclic graph, which consists of several decision nodes and terminal nodes. In 1993, Shin-ichi Minato from Japan modified Randal Bryant’s BDDs for solving combinatorial problems. His “Zero-Suppressed” BDDs aim to represent and manipulate sparse sets of bit vectors. If the data for a problem are represented as bit vectors of length n, then any subset of the vectors can be represented by the Boolean function over n variables yielding 1 when the vector corresponding to the variable assignment is in the set.
According to Bryant, it is possible to use forms of logic functions to express problems involving sum-of-products. Such forms are often represented as sets of “cubes”, each denoted by a string containing symbols 0, 1, and -. For instance, the function formula_1 can be illustrated by the set formula_2. By using bits 10, 01, and 00 to denote symbols 1, 0, and – respectively, one can represent the above set with bit vectors in the form of formula_3. Notice that the set of bit vectors is sparse, in that the number of vectors is fewer than 2, which is the maximum number of bit vectors, and the set contains many elements equal to zero. In this case, a node can be omitted if setting the node variable to 1 causes the function to yield 0. This is seen in the condition that a 1 at some bit position implies that the vector is not in the set. For sparse sets, this condition is common, and hence many node eliminations are possible. 

Minato has proved that ZDDs are especially suitable for combinatorial problems, such as the classical problems in two-level logic minimization, knight’s tour problem, fault simulation, timing analysis, the N-queens problem, as well as weak division. By using ZDDs, one can reduce the size of the representation of a set of n-bit vectors in OBDDs by at most a factor of n. In practice, the optimization is statistically significant.

We define a Zero-Suppressed Decision Diagram (ZDD) to be any directed acyclic graph such that: 

We call Z an unreduced ZDD, if a HI edge points to a ⊥ node or condition 4 fails to hold. 
In computer programs, Boolean functions can be expressed in bits, so the ⊤ node and ⊥ node can be represented by 1 and 0. From the definition above, we can represent combination sets efficiently by applying two rules to the BDDs:
If the number and the order of input variables are fixed, a zero-suppressed BDD represents a Boolean function uniquely (as proved in Figure 2, it is possible to use a BDD to represent a Boolean binary tree). 

Let F be a ZDD. Let v be its root node. Then:

One may represent the LO branch as the sets in F that don't contain "v":
formula_5

And the HI branch as the sets in F that do contain "v":
formula_6

Figure 3: The family formula_7. We may call this formula_8, an elementary family. Elementary families consist of the form formula_9, and are denoted by formula_10.

Figure 4: The family formula_11

Figure 5: The family formula_12

Figure 6: The family formula_13

One feature of ZDDs is that the form does not depend on the number of input variables as long as the combination sets are the same. It is unnecessary to fix the number of input variables before generating graphs. ZDDs automatically suppress the variables for objects which never appear in combination, hence the efficiency for manipulating sparse combinations. 
Another advantage of ZDDs is that the number of 1-paths in the graph is exactly equal to the number of elements in the combination set. In original BDDs, the node elimination breaks this property. Therefore, ZDDs are better than simple BDDs to represent combination sets. It is, however, better to use the original BDDs when representing ordinary Boolean functions, as shown in Figure 7.
Here we have the basic operations for ZDDs, as they are slightly different from those of the original BDDs. One may refer to Figure 8 for examples generated from the table below. 

In ZDDs, there is no NOT operation, which is an essential operation in original BDDs. The reason is that the complement set formula_18 cannot be computed without defining the universal set formula_19. In ZDDs, formula_18 can be computed as Diff(U, P).

Supposeformula_21 , we can recursively compute the number of sets in a ZDD, enabling us to get the 34th set out a 54-member family. Random access is fast, and any operation possible for an array of sets can be done with efficiency on a ZDD. 

According to Minato, the above operations for ZDDs can be executed recursively like original BDDs. To describe the algorithms simply, we define the procedure codice_1 that returns a node for a variable top and two subgraphs P0 and P1. We may use a hash table, called uniq-table, to keep each node unique. Node elimination and sharing are managed only by codice_2. 

Using codice_2, we can then represent other basic operations as follows:

These algorithms take an exponential time for the number of variables in the worst case; however, we can improve the performance by using a cache that memorizes results of recent operations in a similar fashion in BDDs. The cache prevents duplicate executions for equivalent sub-graphs. Without any duplicates, the algorithms can operate in a time that is proportional to the size of graphs, as shown in Figure 9 and 10.

ZDDs can be used to represent the five-letter words of English, the set WORDS (of size 5757) from the Stanford GraphBase for instance.
One way to do this is to consider the function formula_22 that is defined to be 1 if and only if the five numbers formula_23, formula_24, ..., formula_25 encode the letters of an English word, where formula_26, ..., formula_27. For example,
formula_28. The function of 25 variables has Z(f) = 6233 nodes – which is not too bad for representing 5757 words.
Compared to binary trees, tries, or hash tables, a ZDD may not be the best to complete simple searches, yet it is efficient in retrieving data that is only partially specified, or data that is only supposed to match a key approximately. Complex queries can be handled with ease. Moreover, ZDDs do not involve as many variables. In fact, by using a ZDD, one can represent those five letter words as a sparse function formula_29that has 26×5 = 130 variables, where variable formula_30 for example determines whether the second letter is “a”. To represent the word “crazy”, one can make F true when formula_31 and all other variables are 0.
Thus, F can be considered as a family consisting of the 5757 subsets formula_32, etc. With these 130 variables the ZDD size Z(F) is in fact 5020 instead of 6233. According to Knuth, the equivalent size of B(F) using a BDD is 46,189—significantly larger than Z(F). In spite of having similar theories and algorithms, ZDDs outperform BDDs for this problem with quite a large margin.
Consequently, ZDDs allow us to perform certain queries that are too onerous for BDDs. Complex families of subset can readily be constructed from elementary families. To search words containing a certain pattern, one may use family algebra on ZDDs to compute formula_33 where P is the pattern, e.g formula_34.

One may use ZDDs to represent simple paths in an undirected graph. For example, there are 12 ways to go from the upper left corner of a three by three grid (shown in Figure 11) to the lower right corner, without visiting any point twice.

These paths can be represented by the ZDD shown in Figure 13. In this ZDD, we get the first path by taking the HI branches at node 13 , node 36 , node 68 , and node89 of the ZDD (LO branches that simply go to ⊥ are omitted). Although the ZDD in Figure 13 may not seem significant by any means, the advantages of a ZDD become obvious as the grid gets larger. For example, for an eight by eight grid, the number of simple paths from corner to corner turns out to be
789, 360,053,252 (Knuth). The paths can be illustrated with 33580 nodes using a ZDD.
A real world example for simple paths was proposed by Randal Bryant, “Suppose I wanted to take a driving tour of the Continental U.S., visiting all of the state capitols, and passing through each state only once. What route should I take to minimize the total distance?” Figure 14 shows an undirected graph for this roadmap, the numbers indicating the shortest distances between neighboring capital cities. The problem is to choose a subset of these edges that form a Hamiltonian path of smallest total length. Every Hamiltonian path in this graph must either start or end at Augusta, Maine(ME). Suppose one starts in CA. One can find a ZDD that characterizes all paths from CA to ME. According to Knuth, this ZDD turns out to have only 7850 nodes, and it effectively shows that exactly 437,525,772,584 simple paths from CA to ME are possible. By number of edges, the generating function is 
so the longest such paths are Hamiltonian, with a size of 2,707,075. ZDDs in this case, are efficient for simple paths and Hamiltonian paths.
Define 64 input variables to represent the squares on a chess board. Each variable denotes the presence or absence of a queen on that square. Consider that, 
Although one can solve this problem by constructing OBDDs, it is more efficient to use ZDDs. Constructing a ZDD for the 8-Queens problem requires 8 steps from S1 to S8.
Each step can be defined as follows:

 The Knight’s tour problem has a historical significance. The knight’s graph contains n2 vertices to depict the squares of the chessboard. The edges illustrate the legal moves of a knight. The knight can visit each square of the board exactly once. Olaf Schröer, M. Löbbing, and Ingo Wegener approached this problem, namely on a board, by assigning Boolean variables for each edge on the graph, with a total of 156 variables to designate all the edges. A solution of the problem can be expressed by a 156-bit combination vector. According to Minato, the construction of a ZDD for all solutions is too large to solve directly. It is easier to divide and conquer. By dividing the problems into two parts of the board, and constructing ZDDs in subspaces, one can solve The Knight’s tour problem with each solution containing 64 edges. However, since the graph is not very sparse, the advantage of using ZDDs is not so obvious. 

 N. Takahashi et al suggested a fault simulation method given multiple faults by using OBDDs. This deductive method transmits the fault sets from primary inputs to primary outputs, and captures the faults at primary outputs. Since this method involves unate cube set expressions, ZDDs are more efficient. The optimizations from ZDDs in unate cube set calculations indicate that ZDDs could be useful in developing VLSI CAD systems and in a myriad of other applications.






</doc>
<doc id="4477141" url="https://en.wikipedia.org/wiki?curid=4477141" title="Propositional directed acyclic graph">
Propositional directed acyclic graph

A propositional directed acyclic graph (PDAG) is a data structure that is used to represent a Boolean function. A Boolean function can be represented as a rooted, directed acyclic graph of the following form:

Leaves labeled with formula_1 (formula_2) represent the constant Boolean function which always evaluates to 1 (0). A leaf labeled with a Boolean variable formula_11 is interpreted as the assignment formula_12, i.e. it represents the Boolean function which evaluates to 1 if and only if formula_12. The Boolean function represented by a formula_3-node is the one that evaluates to 1, if and only if the Boolean function of all its children evaluate to 1. Similarly, a formula_4-node represents the Boolean function that evaluates to 1, if and only if the Boolean function of at least one child evaluates to 1. Finally, a formula_5-node represents the complementary Boolean function its child, i.e. the one that evaluates to 1, if and only if the Boolean function of its child evaluates to 0.

Every binary decision diagram (BDD) and every negation normal form (NNF) are also a PDAG with some particular properties. The following pictures represent the Boolean function formula_17:




</doc>
<doc id="672499" url="https://en.wikipedia.org/wiki?curid=672499" title="Graph-structured stack">
Graph-structured stack

In computer science, a graph-structured stack (GSS) is a directed acyclic graph where each directed path represents a stack.
The graph-structured stack is an essential part of Tomita's algorithm, where it replaces the usual stack of a pushdown automaton. This allows the algorithm to encode the nondeterministic choices in parsing an ambiguous grammar, sometimes with greater efficiency. 

In the following diagram, there are four stacks: {7,3,1,0}, {7,4,1,0}, {7,5,2,0}, and {8,6,2,0}.

Another way to simulate nondeterminism would be to duplicate the stack as needed. The duplication would be less efficient since vertices would not be shared. For this example, 16 vertices would be needed instead of 9.



</doc>
<doc id="161944" url="https://en.wikipedia.org/wiki?curid=161944" title="Scene graph">
Scene graph

A scene graph is a general data structure commonly used by vector-based graphics editing applications and modern computer games, which arranges the logical and often spatial representation of a graphical scene. 

A scene graph is a collection of nodes in a graph or tree structure. A tree node may have many children but only a single parent, with the effect of a parent applied to all its child nodes; an operation performed on a group automatically propagates its effect to all of its members. In many programs, associating a geometrical transformation matrix (see also transformation and matrix) at each group level and concatenating such matrices together is an efficient and natural way to process such operations. A common feature, for instance, is the ability to group related shapes and objects into a compound object that can then be moved, transformed, selected, etc. as easily as a single object.

In vector-based graphics editing, each leaf node in a scene graph represents some atomic unit of the document, usually a shape such as an ellipse or Bezier path. Although shapes themselves (particularly paths) can be decomposed further into nodes such as spline nodes, it is practical to think of the scene graph as composed of shapes rather than going to a lower level of representation.

Another useful and user-driven node concept is the layer. A layer acts like a transparent sheet upon which any number of shapes and shape groups can be placed. The document then becomes a set of layers, any of which can be conveniently made invisible, dimmed, or locked (made read-only). Some applications place all layers in a linear list, while others support sublayers (i.e., layers within layers to any desired depth).

Internally, there may be no real structural difference between layers and groups at all, since they are both just nodes of a scene graph. If differences are needed, a common type declaration in C++ would be to make a generic node class, and then derive layers and groups as subclasses. A visibility member, for example, would be a feature of a layer, but not necessarily of a group.

Scene graphs are useful for modern games using 3D graphics and increasingly large worlds or levels. In such applications, nodes in a scene graph (generally) represent entities or objects in the scene.

For instance, a game might define a logical relationship between a knight and a horse so that the knight is considered an extension to the horse. The scene graph would have a 'horse' node with a 'knight' node attached to it.

The scene graph may also describe the spatial, as well as the logical, relationship of the various entities: the knight moves through 3D space as the horse moves.

In these large applications, memory requirements are major considerations when designing a scene graph. For this reason, many large scene graph systems use geometry instancing to reduce memory costs and increase speed. In our example above, each knight is a separate scene node, but the graphical representation of the knight (made up of a 3D mesh, textures, materials and shaders) is instanced. This means that only a single copy of the data is kept, which is then referenced by any 'knight' nodes in the scene graph. This allows a reduced memory budget and increased speed, since when a new knight node is created, the appearance data needs not be duplicated.

The simplest form of scene graph uses an array or linked list data structure, and displaying its shapes is simply a matter of linearly iterating the nodes one by one. Other common operations, such as checking to see which shape intersects the mouse pointer are also done via linear searches. For small scene graphs, this tends to suffice.

Applying an operation on a scene graph requires some way of dispatching an operation based on a node's type. For example, in a render operation, a transformation group node would accumulate its transformation by matrix multiplication, vector displacement, quaternions or Euler angles. After which a leaf node sends the object off for rendering to the renderer. Some implementations might render the object directly, which invokes the underlying rendering API, such as DirectX or OpenGL. But since the underlying implementation of the rendering API usually lacks portability, one might separate the scene graph and rendering systems instead. In order to accomplish this type of dispatching, several different approaches can be taken.

In object-oriented languages such as C++, this can easily be achieved by virtual functions, where each represents an operation that can be performed on a node. Virtual functions are simple to write, but it is usually impossible to add new operations to nodes without access to the source code. Alternatively, the "visitor pattern" can be used. This has a similar disadvantage in that it is similarly difficult to add new node types.

Other techniques involve the use of RTTI (Run-Time Type Information). The operation can be realised as a class that is passed to the current node; it then queries the node's type using RTTI and looks up the correct operation in an array of callbacks or functors. This requires that the map of types to callbacks or functors be initialized at runtime, but offers more flexibility, speed and extensibility.

Variations on these techniques exist, and new methods can offer added benefits. One alternative is scene graph rebuilding, where the scene graph is rebuilt for each of the operations performed. This, however, can be very slow, but produces a highly optimised scene graph. It demonstrates that a good scene graph implementation depends heavily on the application in which it is used.

Traversals are the key to the power of applying operations to scene graphs. A traversal generally consists of starting at some arbitrary node (often the root of the scene graph), applying the operation(s) (often the updating and rendering operations are applied one after the other), and recursively moving down the scene graph (tree) to the child nodes, until a leaf node is reached. At this point, many scene graph engines then traverse back up the tree, applying a similar operation. For example, consider a render operation that takes transformations into account: while recursively traversing down the scene graph hierarchy, a pre-render operation is called. If the node is a transformation node, it adds its own transformation to the current transformation matrix. Once the operation finishes traversing all the children of a node, it calls the node's post-render operation so that the transformation node can undo the transformation. This approach drastically reduces the necessary amount of matrix multiplication.

Some scene graph operations are actually more efficient when nodes are traversed in a different order – this is where some systems implement scene graph rebuilding to reorder the scene graph into an easier-to-parse format or tree.

For example, in 2D cases, scene graphs typically render themselves by starting at the tree's root node and then recursively draw the child nodes. The tree's leaves represent the most foreground objects. Since drawing proceeds from back to front with closer objects simply overwriting farther ones, the process is known as employing the Painter's algorithm. In 3D systems, which often employ depth buffers, it is more efficient to draw the closest objects first, since farther objects often need only be depth-tested instead of actually rendered, because they are occluded by nearer objects.

Bounding Volume Hierarchies (BVHs) are useful for numerous tasks – including efficient culling and speeding up collision detection between objects. A BVH is a spatial structure, but doesn't have to partition the geometry (see spatial partitioning below).

A BVH is a tree of bounding volumes (often spheres, axis-aligned bounding boxes or oriented bounding boxes). At the bottom of the hierarchy, the size of the volume is just large enough to encompass a single object tightly (or possibly even some smaller fraction of an object in high resolution BVHs). As one ascends the hierarchy, each node has its own volume that tightly encompasses all the volumes beneath it. At the root of the tree is a volume that encompasses all the volumes in the tree (the whole scene).

BVHs are useful for speeding up collision detection between objects. If an object's bounding volume does not intersect a volume higher in the tree, it cannot intersect any object below that node (so they are all rejected very quickly).

There are some similarities between BVHs and scene graphs. A scene graph can easily be adapted to include/become a BVH – if each node has a volume associated or there is a purpose-built "bound node" added in at convenient location in the hierarchy. This may not be the typical view of a scene graph, but there are benefits to including a BVH in a scene graph.

An effective way of combining spatial partitioning and scene graphs is by creating a scene leaf node that contains the spatial partitioning data. This can increase computational efficiency of rendering.

Spatial data is usually static and generally contains non-moving scene data in some partitioned form. Some systems may have the systems and their rendering separately. This is fine and there are no real advantages to either method. In particular, it is bad to have the scene graph contained within the spatial partitioning system, as the scene graph is better thought of as the grander system to the spatial partitioning.

Very large drawings, or scene graphs that are generated solely at runtime (as happens in ray tracing rendering programs), require defining of group nodes in a more automated fashion. A raytracer, for example, will take a scene description of a 3D model and build an internal representation that breaks up its individual parts into bounding boxes (also called bounding slabs). These boxes are grouped hierarchically so that ray intersection tests (as part of visibility determination) can be efficiently computed. A group box that does not intersect an eye ray, for example, can entirely skip testing any of its members.

A similar efficiency holds in 2D applications as well. If the user has magnified a document so that only part of it is visible on his computer screen, and then scrolls in it, it is useful to use a bounding box (or in this case, a bounding rectangle scheme) to quickly determine which scene graph elements are visible and thus actually need to be drawn.

Depending on the particulars of the application's drawing performance, a large part of the scene graph's design can be impacted by rendering efficiency considerations. In 3D video games such as Quake, binary space partitioning (BSP) trees are heavily favored to minimize visibility tests. BSP trees, however, take a very long time to compute from design scene graphs, and must be recomputed if the design scene graph changes, so the levels tend to remain static, and dynamic characters aren't generally considered in the spatial partitioning scheme.

Scene graphs for dense regular objects such as heightfields and polygon meshes tend to employ quadtrees and octrees, which are specialized variants of a 3D bounding box hierarchy. Since a heightfield occupies a box volume itself, recursively subdividing this box into eight subboxes (hence the 'oct' in octree) until individual heightfield elements are reached is efficient and natural. A quadtree is simply a 2D octree.

PHIGS was the first commercial scene graph specification, and became an ANSI standard in 1988. Disparate implementations were provided by Unix hardware vendors. The HOOPS 3D Graphics System appears to have been the first commercial scene graph library provided by a single software vendor. It was designed to run on disparate lower-level 2D and 3D interfaces, with the first major production version (v3.0) completed in 1991. Shortly thereafter, Silicon Graphics released IRIS Inventor 1.0 (1992), which was a scene graph built on top of the IRIS GL 3D API. It was followed up with Open Inventor in 1994, a portable scene graph built on top of OpenGL. More 3D scene graph libraries can be found in .

X3D is a royalty-free open standards file format and run-time architecture to represent and communicate 3D scenes and objects using XML. It is an ISO-ratified standard that provides a system for the storage, retrieval and playback of real-time graphics content embedded in applications, all within an open architecture to support a wide array of domains and user scenarios.






</doc>
<doc id="2501" url="https://en.wikipedia.org/wiki?curid=2501" title="Appendix">
Appendix

Appendix may refer to:





</doc>
<doc id="44578" url="https://en.wikipedia.org/wiki?curid=44578" title="Big O notation">
Big O notation

Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

In computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows. In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem.

Big O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation.

The letter O is used because the growth rate of a function is also referred to as the order of the function. A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function. Associated with big O notation are several related notations, using the symbols , to describe other kinds of bounds on asymptotic growth rates.

Big O notation is also used in many other fields to provide similar estimates.

Let "f" be a real or complex valued function and "g" a real valued function, both defined on some unbounded subset of the real positive numbers, such that "g"("x") is strictly positive for all large enough values of "x". One writes

if and only if for all sufficiently large values of "x", the absolute value of "f"("x") is at most a positive constant multiple of "g"("x"). That is, "f"("x") = "O"("g"("x")) if and only if there exists a positive real number "M" and a real number "x" such that

In many contexts, the assumption that we are interested in the growth rate as the variable "x" goes to infinity is left unstated, and one writes more simply that

The notation can also be used to describe the behavior of "f" near some real number "a" (often, "a" = 0): we say

if and only if there exist positive numbers "δ" and "M" such that

As "g"("x") is chosen to be non-zero for values of "x" sufficiently close to "a", both of these definitions can be unified using the limit superior:

if and only if

In typical usage the "O" notation is asymptotical, that is, it refers to very large "x". In this setting, the contribution of the terms that grow "most quickly" will eventually make the other ones irrelevant. As a result, the following simplification rules can be applied:
For example, let "f"("x") = 6"x" − 2"x" + 5, and suppose we wish to simplify this function, using "O" notation, to describe its growth rate as "x" approaches infinity. This function is the sum of three terms: 6"x", −2"x", and 5. Of these three terms, the one with the highest growth rate is the one with the largest exponent as a function of "x", namely 6"x". Now one may apply the second rule: 6"x" is a product of 6 and "x" in which the first factor does not depend on "x". Omitting this factor results in the simplified form "x". Thus, we say that "f"("x") is a "big-oh" of ("x"). Mathematically, we can write "f"("x") = "O"("x").
One may confirm this calculation using the formal definition: let "f"("x") = 6"x" − 2"x" + 5 and "g"("x") = "x". Applying the formal definition from above, the statement that "f"("x") = "O"("x") is equivalent to its expansion,
for some suitable choice of "x" and "M" and for all "x" > "x". To prove this, let "x" = 1 and "M" = 13. Then, for all "x" > "x":
so

Big O notation has two main areas of application:

In both applications, the function "g"("x") appearing within the "O"(...) is typically chosen to be as simple as possible, omitting constant factors and lower order terms.

There are two formally close, but noticeably different, usages of this notation:

This distinction is only in application and not in principle, however—the formal definition for the "big O" is the same for both cases, only with different limits for the function argument.

Big O notation is useful when analyzing algorithms for efficiency. For example, the time (or the number of steps) it takes to complete a problem of size "n" might be found to be "T"("n") = 4"n" − 2"n" + 2.
As "n" grows large, the "n" term will come to dominate, so that all other terms can be neglected—for instance when "n" = 500, the term 4"n" is 1000 times as large as the 2"n" term. Ignoring the latter would have negligible effect on the expression's value for most purposes.
Further, the coefficients become irrelevant if we compare to any other order of expression, such as an expression containing a term "n" or "n". Even if "T"("n") = 1,000,000"n", if "U"("n") = "n", the latter will always exceed the former once "n" grows larger than 1,000,000 ("T"(1,000,000) = 1,000,000= "U"(1,000,000)). Additionally, the number of steps depends on the details of the machine model on which the algorithm runs, but different types of machines typically vary by only a constant factor in the number of steps needed to execute an algorithm.
So the big O notation captures what remains: we write either
or
and say that the algorithm has "order of n" time complexity.
The sign "=" is not meant to express "is equal to" in its normal mathematical sense, but rather a more colloquial "is", so the second expression is sometimes considered more accurate (see the "Equals sign" discussion below) while the first is considered by some as an abuse of notation.

Big O can also be used to describe the error term in an approximation to a mathematical function. The most significant terms are written explicitly, and then the least-significant terms are summarized in a single big O term. Consider, for example, the exponential series and two expressions of it that are valid when "x" is small:
The second expression (the one with "O"("x")) means the absolute-value of the error "e" − (1 + "x" + "x"/2) is at most some constant times |"x"| when "x" is close enough to 0.

If the function "f" can be written as a finite sum of other functions, then the fastest growing one determines the order of "f"("n"). For example,
In particular, if a function may be bounded by a polynomial in "n", then as "n" tends to "infinity", one may disregard "lower-order" terms of the polynomial.
The sets "O"("n") and "O"("c") are very different. If "c" is greater than one, then the latter grows much faster. A function that grows faster than "n" for any "c" is called "superpolynomial". One that grows more slowly than any exponential function of the form "c" is called "subexponential". An algorithm can require time that is both superpolynomial and subexponential; examples of this include the fastest known algorithms for integer factorization and the function "n".

We may ignore any powers of "n" inside of the logarithms. The set "O"(log "n") is exactly the same as "O"(log("n")). The logarithms differ only by a constant factor (since
log("n") = "c" log "n") and thus the big O notation ignores that. Similarly, logs with different constant bases are equivalent. On the other hand, exponentials with different bases are not of the same order. For example, 2 and 3 are not of the same order.

Changing units may or may not affect the order of the resulting algorithm. Changing units is equivalent to multiplying the appropriate variable by a constant wherever it appears. For example, if an algorithm runs in the order of "n", replacing "n" by "cn" means the algorithm runs in the order of "c""n", and the big O notation ignores the constant "c". This can be written as "c""n" = O("n"). If, however, an algorithm runs in the order of 2, replacing "n" with "cn" gives 2 = (2). This is not equivalent to 2 in general.
Changing variables may also affect the order of the resulting algorithm. For example, if an algorithm's run time is "O"("n") when measured in terms of the number "n" of "digits" of an input number "x", then its run time is "O"(log "x") when measured as a function of the input number "x" itself, because "n" = "O"(log "x").

This implies formula_18, which means that formula_19 is a convex cone.

Big "O" (and little o, Ω, etc.) can also be used with multiple variables.
To define big "O" formally for multiple variables, suppose formula_22 and formula_23 are two functions defined on some subset of formula_24. We say
if and only if
Equivalently, the condition that formula_27 for some formula_28 can be replaced with the condition that formula_29, where formula_30 denotes the Chebyshev norm. For example, the statement
asserts that there exist constants "C" and "M" such that
where "g"("n","m") is defined by
This definition allows all of the coordinates of formula_34 to increase to infinity. In particular, the statement
(i.e., formula_36) is quite different from
(i.e., formula_38).

Under this definition, the subset on which a function is defined is significant when generalizing statements from the univariate setting to the multivariate setting. For example, if formula_39 and formula_40, then formula_41 if we restrict formula_22 and formula_23 to formula_44, but not if they are defined on formula_45.

This is not the only generalization of big O to multivariate functions, and in practice, there is some inconsistency in the choice of definition.

The statement ""f"("x") is "O"("g"("x"))" as defined above is usually written as "f"("x") = "O"("g"("x")). Some consider this to be an abuse of notation, since the use of the equals sign could be misleading as it suggests a symmetry that this statement does not have. As de Bruijn says, "O"("x") = "O"("x") is true but "O"("x") = "O"("x") is not. Knuth describes such statements as "one-way equalities", since if the sides could be reversed, "we could deduce ridiculous things like "n" = "n" from the identities "n" = "O"("n") and "n" = "O"("n")."

For these reasons, it would be more precise to use set notation and write "f"("x") ∈ "O"("g"("x")), thinking of "O"("g"("x")) as the class of all functions "h"("x") such that |"h"("x")| ≤ "C"|"g"("x")| for some constant "C". However, the use of the equals sign is customary. Knuth pointed out that "mathematicians customarily use the = sign as they use the word 'is' in English: Aristotle is a man, but a man isn't necessarily Aristotle."

Big O notation can also be used in conjunction with other arithmetic operators in more complicated equations. For example, "h"("x") + "O"("f"("x")) denotes the collection of functions having the growth of "h"("x") plus a part whose growth is limited to that of "f"("x"). Thus,
expresses the same as

Suppose an algorithm is being developed to operate on a set of "n" elements. Its developers are interested in finding a function "T"("n") that will express how long the algorithm will take to run (in some arbitrary measurement of time) in terms of the number of elements in the input set. The algorithm works by first calling a subroutine to sort the elements in the set and then perform its own operations. The sort has a known time complexity of "O"("n"), and after the subroutine runs the algorithm must take an additional 55"n" + 2"n" + 10 steps before it terminates. Thus the overall time complexity of the algorithm can be expressed as "T"("n") = 55"n" + "O"("n").
Here the terms 2"n"+10 are subsumed within the faster-growing "O"("n"). Again, this usage disregards some of the formal meaning of the "=" symbol, but it does allow one to use the big O notation as a kind of convenient placeholder.

In more complicated usage, "O"(...) can appear in different places in an equation, even several times on each side. For example, the following are true for formula_48
The meaning of such statements is as follows: for "any" functions which satisfy each "O"(...) on the left side, there are "some" functions satisfying each "O"(...) on the right side, such that substituting all these functions into the equation makes the two sides equal. For example, the third equation above means: "For any function "f"("n") = "O"(1), there is some function "g"("n") = "O"("e") such that "n" = "g"("n")." In terms of the "set notation" above, the meaning is that the class of functions represented by the left side is a subset of the class of functions represented by the right side. In this use the "=" is a formal symbol that unlike the usual use of "=" is not a symmetric relation. Thus for example "n" = "O"("e") does not imply the false statement "O"("e") = "n"

Big O consists of just an uppercase "O". Unlike Greek-named Bachmann–Landau notations, it needs no special symbol. Yet, commonly used calligraphic variants, like formula_52, are available, in LaTeX and derived typesetting systems.

Here is a list of classes of functions that are commonly encountered when analyzing the running time of an algorithm. In each case, "c" is a positive constant and "n" increases without bound. The slower-growing functions are generally listed first.
The statement formula_53 is sometimes weakened to formula_54 to derive simpler formulas for asymptotic complexity.
For any formula_55 and formula_56, formula_57 is a subset of formula_58 for any formula_59, so may be considered as a polynomial with some bigger order.

Big "O" is the most commonly used asymptotic notation for comparing functions. Together with some other related notations it forms the family of Bachmann–Landau notations.

Intuitively, the assertion " is " (read " is little-o of ") means that grows much faster than . Let as before "f" be a real or complex valued function and "g" a real valued function, both defined on some unbounded subset of the real positive numbers, such that "g(x)" is strictly positive for all large enough values of "x". One writes
if for every positive constant there exists a constant such that
For example, one has

The difference between the earlier definition for the big-O notation and the present definition of little-o, is that while the former has to be true for "at least one" constant "M", the latter must hold for "every" positive constant , however small. In this way, little-o notation makes a "stronger statement" than the corresponding big-O notation: every function that is little-o of "g" is also big-O of "g", but not every function that is big-O of "g" is also little-o of "g". For example, formula_64 but formula_65

As "g"("x") is nonzero, or at least becomes nonzero beyond a certain point, the relation is equivalent to

Little-o respects a number of arithmetic operations. For example,
It also satisfies a transitivity relation:

There are two very widespread and incompatible definitions of the statement

where "a" is some real number, ∞, or −∞, where "f" and "g" are real functions defined in a neighbourhood of "a", and where "g" is positive in this neighbourhood.

The first one (chronologically) is used in analytic number theory, and the other one in computational complexity theory. When the two subjects meet, this situation is bound to generate confusion.

In 1914 Godfrey Harold Hardy and John Edensor Littlewood introduced the new symbol formula_76, which is defined as follows:

Thus formula_78 is the negation of formula_79.

In 1916 the same authors introduced the two new symbols formula_80 and formula_81, defined as:

These symbols were used by Edmund Landau, with the same meanings, in 1924. After Landau, the notations were never used again exactly thus; formula_80 became formula_85 and formula_81 became formula_87.

These three symbols formula_88, as well as formula_89 (meaning that formula_90 and formula_91 are both satisfied), are now currently used in analytic number theory.

We have

and more precisely

We have

and more precisely

however

In 1976 Donald Knuth published a paper to justify his use of the formula_76-symbol to describe a stronger property. Knuth wrote: "For all the applications I have seen so far in computer science, a stronger requirement […] is much more appropriate". He defined

with the comment: "Although I have changed Hardy and Littlewood's definition of formula_76, I feel justified in doing so because their definition is by no means in wide use, and because there are other ways to say what they want to say in the comparatively rare cases when their definition applies."

The limit definitions assume formula_118 for sufficiently large . The table is (partly) sorted from smallest to largest, in the sense that o, O, Θ, ∼, (Knuth's version of) Ω, ω on functions correspond to <, ≤, ≈, =, ≥, > on the real line (the Hardy-Littlewood version of Ω, however, doesn't correspond to any such description).

Computer science uses the big "O", big Theta Θ, little "o", little omega ω and Knuth's big Omega Ω notations. Analytic number theory often uses the big "O", small "o", Hardy–Littlewood's big Omega Ω (with or without the +, - or ± subscripts) and formula_119 notations. The small omega ω notation is not used as often in analysis.

Informally, especially in computer science, the big "O" notation often can be used somewhat differently to describe an asymptotic tight bound where using big Theta Θ notation might be more factually appropriate in a given context. For example, when considering a function "T"("n") = 73"n" + 22"n" + 58, all of the following are generally acceptable, but tighter bounds (such as numbers 2 and 3 below) are usually strongly preferred over looser bounds (such as number 1 below).
The equivalent English statements are respectively:
So while all three statements are true, progressively more information is contained in each. In some fields, however, the big O notation (number 2 in the lists above) would be used more commonly than the big Theta notation (bullets number 3 in the lists above). For example, if "T"("n") represents the running time of a newly developed algorithm for input size "n", the inventors and users of the algorithm might be more inclined to put an upper asymptotic bound on how long it will take to run without making an explicit statement about the lower asymptotic bound.

In their book "Introduction to Algorithms", Cormen, Leiserson, Rivest and Stein consider the set of functions "f" which satisfy

In a correct notation this set can, for instance, be called "O"("g"), where

The authors state that the use of equality operator (=) to denote set membership rather than the set membership operator (∈) is an abuse of notation, but that doing so has advantages. Inside an equation or inequality, the use of asymptotic notation stands for an anonymous function in the set "O"("g"), which eliminates lower-order terms, and helps to reduce inessential clutter in equations, for example:

Another notation sometimes used in computer science is Õ (read "soft-O"): "f"("n") = "Õ"("g"("n")) is shorthand
for "f"("n") = "O"("g"("n") log "g"("n")) for some "k". Essentially, it is big O notation, ignoring logarithmic factors because the growth-rate effects of some other super-logarithmic function indicate a growth-rate explosion for large-sized input parameters that is more important to predicting bad run-time performance than the finer-point effects contributed by the logarithmic-growth factor(s). This notation is often used to obviate the "nitpicking" within growth-rates that are stated as too tightly bounded for the matters at hand (since log "n" is always "o"("n") for any constant "k" and any ε > 0).

Also the L notation, defined as
is convenient for functions that are between polynomial and exponential in terms of formula_127.

The generalization to functions taking values in any normed vector space is straightforward (replacing absolute values by norms), where "f" and "g" need not take their values in the same space. A generalization to functions "g" taking values in any topological group is also possible.
The "limiting process" "x" → "x" can also be generalized by introducing an arbitrary filter base, i.e. to directed nets "f" and "g".
The "o" notation can be used to define derivatives and differentiability in quite general spaces, and also (asymptotical) equivalence of functions,
which is an equivalence relation and a more restrictive notion than the relationship ""f" is Θ("g")" from above. (It reduces to lim "f" / "g" = 1 if "f" and "g" are positive real valued functions.) For example, 2"x" is Θ("x"), but 2"x" − "x" is not "o"("x").

The symbol O was first introduced by number theorist Paul Bachmann in 1894, in the second volume of his book "Analytische Zahlentheorie" ("analytic number theory"), the first volume of which (not yet containing big O notation) was published in 1892. The number theorist Edmund Landau adopted it, and was thus inspired to introduce in 1909 the notation o; hence both are now called Landau symbols. These notations were used in applied mathematics during the 1950s for asymptotic analysis.
The symbol formula_76 (in the sense "is not an "o" of") was introduced in 1914 by Hardy and Littlewood. Hardy and Littlewood also introduced in 1918 the symbols formula_80 ("right") and formula_81 ("left"), precursors of the modern symbols formula_85 ("is not smaller than a small o of") and formula_87 ("is not larger than a small o of"). Thus the Omega symbols (with their original meanings) are sometimes also referred to as "Landau symbols". This notation formula_76 became commonly used in number theory at least since the 1950s.
In the 1970s the big O was popularized in computer science by Donald Knuth, who introduced the related Theta notation, and proposed a different definition for the Omega notation.

Landau never used the big Theta and small omega symbols.

Hardy's symbols were (in terms of the modern "O" notation)

(Hardy however never defined or used the notation formula_137, nor formula_138, as it has been sometimes reported).
Hardy introduced the symbols formula_139 and formula_140 (as well as some other symbols) in his 1910 tract "Orders of Infinity", and made use of them only in three papers (1910–1913). In his nearly 400 remaining papers and books he consistently used the Landau symbols O and o.

Hardy's notation is not used anymore. On the other hand, in the 1930s, the Russian number theorist Ivan Matveyevich Vinogradov introduced his notation formula_138, which has been increasingly used in number theory instead of the formula_142 notation. We have
and frequently both notations are used in the same paper.

The big-O originally stands for "order of" ("Ordnung", Bachmann 1894), and is thus a Latin letter. Neither Bachmann nor Landau ever call it "Omicron". The symbol was much later on (1976) viewed by Knuth as a capital omicron, probably in reference to his definition of the symbol Omega. The digit zero should not be used.





</doc>
<doc id="236683" url="https://en.wikipedia.org/wiki?curid=236683" title="Amortized analysis">
Amortized analysis

In computer science, amortized analysis is a method for analyzing a given algorithm's complexity, or how much of a resource, especially time or memory, it takes to execute. The motivation for amortized analysis is that looking at the worst-case run time "per operation", rather than "per algorithm", can be too pessimistic.

While certain operations for a given algorithm may have a significant cost in resources, other operations may not be as costly. The amortized analysis considers both the costly and less costly operations together over the whole series of operations of the algorithm. This may include accounting for different types of input, length of the input, and other factors that affect its performance.

Amortized analysis initially emerged from a method called aggregate analysis, which is now subsumed by amortized analysis. The technique was first formally introduced by Robert Tarjan in his 1985 paper "Amortized Computational Complexity", which addressed the need for a more useful form of analysis than the common probabilistic methods used. Amortization was initially used for very specific types of algorithms, particularly those involving binary trees and union operations. However, it is now ubiquitous and comes into play when analyzing many other algorithms as well.

Amortized analysis requires knowledge of which series of operations are possible. This is most commonly the case with data structures, which have state that persists between operations. The basic idea is that a worst-case operation can alter the state in such a way that the worst case cannot occur again for a long time, thus "amortizing" its cost.

There are generally three methods for performing amortized analysis: the aggregate method, the accounting method, and the potential method. All of these give correct answers; the choice of which to use depends on which is most convenient for a particular situation.


Consider a dynamic array that grows in size as more elements are added to it, such as in Java or in C++. If we started out with a dynamic array of size 4, it would take constant time to push four elements onto it. Yet pushing a fifth element onto that array would take longer as the array would have to create a new array of double the current size (8), copy the old elements onto the new array, and then add the new element. The next three push operations would similarly take constant time, and then the subsequent addition would require another slow doubling of the array size.

In general if we consider an arbitrary number of pushes "n" + 1 to an array of size "n", we notice that push operations take constant time except for the last one which takes time to perform the size doubling operation. Since there were "n" + 1 operations total we can take the average of this and find that pushing elements onto the dynamic array takes: formula_1, constant time.

Shown is a Ruby implementation of a Queue, a FIFO data structure:
The enqueue operation just pushes an element onto the input array; this operation does not depend on the lengths of either input or output and therefore runs in constant time.

However the dequeue operation is more complicated. If the output array already has some elements in it, then dequeue runs in constant time; otherwise, dequeue takes time to add all the elements onto the output array from the input array, where "n" is the current length of the input array. After copying "n" elements from input, we can perform "n" dequeue operations, each taking constant time, before the output array is empty again. Thus, we can perform a sequence of "n" dequeue operations in only time, which implies that the amortized time of each dequeue operation is .

Alternatively, we can charge the cost of copying any item from the input array to the output array to the earlier enqueue operation for that item. This charging scheme doubles the amortized time for enqueue but reduces the amortized time for dequeue to .



</doc>
<doc id="64028" url="https://en.wikipedia.org/wiki?curid=64028" title="Locality of reference">
Locality of reference

In computer science, locality of reference, also known as the principle of locality, is the tendency of a processor to access the same set of memory locations repetitively over a short period of time. There are two basic types of reference locality temporal and spatial locality. Temporal locality refers to the reuse of specific data, and/or resources, within a relatively small time duration. Spatial locality refers to the use of data elements within relatively close storage locations. Sequential locality, a special case of spatial locality, occurs when data elements are arranged and accessed linearly, such as, traversing the elements in a one-dimensional array.

Locality is a type of predictable behavior that occurs in computer systems. Systems that exhibit strong "locality of reference" are great candidates for performance optimization through the use of techniques such as the caching, prefetching for memory and advanced branch predictors at the pipelining stage of a processor core.

There are several different types of locality of reference:


In order to benefit from the very frequently occurring temporal and spatial locality, most of the information storage systems are hierarchical. The equidistant locality is usually supported by the diverse nontrivial increment instructions of the processors. For branch locality, the contemporary processors have sophisticated branch predictors, and on the basis of this prediction the memory manager of the processor tries to collect and preprocess the data of the plausible alternatives.

There are several reasons for locality. These reasons are either goals to achieve or circumstances to accept, depending on the aspect. The reasons below are not disjoint; in fact, the list below goes from the most general case to special cases:


If most of the time the substantial portion of the references aggregate into clusters, and if the shape of this system of clusters can be well predicted, then it can be used for performance optimization. There are several ways to benefit from locality using optimization techniques. Common techniques are:


Hierarchical memory is a hardware optimization that takes the benefits of spatial and temporal locality and can be used on several levels of the memory hierarchy. Paging obviously benefits from temporal and spatial locality. A cache is a simple example of exploiting temporal locality, because it is a specially designed, faster but smaller memory area, generally used to keep recently referenced data and data near recently referenced data, which can lead to potential performance increases.

Data elements in a cache do not necessarily correspond to data elements that are spatially close in the main memory; however, data elements are brought into cache one cache line at a time. This means that spatial locality is again important: if one element is referenced, a few neighboring elements will also be brought into cache. Finally, temporal locality plays a role on the lowest level, since results that are referenced very closely together can be kept in the machine registers. Some programming languages (such as C) allow the programmer to suggest that certain variables be kept in registers.

Data locality is a typical memory reference feature of regular programs (though many irregular memory access patterns exist). It makes the hierarchical memory layout profitable. In computers, memory is divided into a hierarchy in order to speed up data accesses. The lower levels of the memory hierarchy tend to be slower, but larger. Thus, a program will achieve greater performance if it uses memory while it is cached in the upper levels of the memory hierarchy and avoids bringing other data into the upper levels of the hierarchy that will displace data that will be used shortly in the future. This is an ideal, and sometimes cannot be achieved.

Typical memory hierarchy (access times and cache sizes are approximations of typical values used for the purpose of discussion; actual values and actual numbers of levels in the hierarchy vary):

Modern machines tend to read blocks of lower memory into the next level of the memory hierarchy. If this displaces used memory, the operating system tries to predict which data will be accessed least (or latest) and move it down the memory hierarchy. Prediction algorithms tend to be simple to reduce hardware complexity, though they are becoming somewhat more complicated.

A common example is matrix multiplication:
for i in 0..n
By switching the looping order for codice_1 and codice_2, the speedup in large matrix multiplications becomes dramatic, at least for languages that put contiguous array elements in the last dimension. This will not change the mathematical result, but it improves efficiency. In this case, "large" means, approximately, more than 100,000 elements in each matrix, or enough addressable memory such that the matrices will not fit in L1 and L2 caches.
for i in 0..n
The reason for this speedup is that in the first case, the reads of codice_3 are in cache (since the codice_2 index is the contiguous, last dimension), but codice_5 is not, so there is a cache miss penalty on codice_5. codice_7 is irrelevant, because it can be factored out of the inner loop. In the second case, the reads and writes of codice_7 are both in cache, the reads of codice_5 are in cache, and the read of codice_3 can be factored out of the inner loop. Thus, the second example has no cache miss penalty in the inner loop while the first example has a cache penalty.

On a year 2014 processor, the second case is approximately five times faster than the first case, when written in C and compiled with codice_11. (A careful examination of the disassembled code shows that in the first case, GCC uses SIMD instructions and in the second case it does not, but the cache penalty is much worse than the SIMD gain.)

Temporal locality can also be improved in the above example by using a technique called blocking. The larger matrix can be divided into evenly sized sub-matrices, so that the smaller blocks can be referenced (multiplied) several times while in memory.
for (ii = 0; ii < SIZE; ii += BLOCK_SIZE)
The temporal locality of the above solution is provided because a block can be used several times before moving on, so that it is moved in and out of memory less often. Spatial locality is improved because elements with consecutive memory addresses tend to be pulled up the memory hierarchy together.




</doc>
<doc id="156777" url="https://en.wikipedia.org/wiki?curid=156777" title="Standard Template Library">
Standard Template Library

The Standard Template Library (STL) is a software library for the C++ programming language that influenced many parts of the C++ Standard Library. It provides four components called "algorithms", "containers", "functions", and "iterators".

The STL provides a set of common classes for C++, such as containers and associative arrays, that can be used with any built-in type and with any user-defined type that supports some elementary operations (such as copying and assignment). STL algorithms are independent of containers, which significantly reduces the complexity of the library.

The STL achieves its results through the use of templates. This approach provides compile-time polymorphism that is often more efficient than traditional run-time polymorphism. Modern C++ compilers are tuned to minimize abstraction penalties arising from heavy use of the STL.

The STL was created as the first library of generic algorithms and data structures for C++, with four ideas in mind: generic programming, abstractness without loss of efficiency, the Von Neumann computation model, and value semantics.

In November 1993 Alexander Stepanov presented a library based on generic programming to the ANSI/ISO committee for C++ standardization. The committee's response was overwhelmingly favorable and led to a request from Andrew Koenig for a formal proposal in time for the March 1994 meeting. The committee had several requests for changes and extensions and the committee members met with Stepanov and Meng Lee to help work out the details. The requirements for the most significant extension (associative containers) had to be shown to be consistent by fully implementing them, a task Stepanov delegated to David Musser. A proposal received final approval at the July 1994 ANSI/ISO committee meeting. Subsequently, the Stepanov and Lee document 17 was incorporated into the ANSI/ISO C++ draft standard (1, parts of clauses 17 through 27).

The prospects for early widespread dissemination of STL were considerably improved with Hewlett-Packard's decision to make its implementation freely available on the Internet in August 1994. This implementation, developed by Stepanov, Lee, and Musser during the standardization process, became the basis of many implementations offered by compiler and library vendors today.

The STL contains sequence containers and associative containers. The containers are objects that store data. The standard sequence containers include , , and . The standard associative containers are , , , , , , and . There are also "container adaptors" , , and , that are containers with specific interface, using other containers as implementation.

The STL implements five different types of iterators. These are "input iterators" (that can only be used to read a sequence of values), "output iterators" (that can only be used to write a sequence of values), "forward iterators" (that can be read, written to, and move forward), "bidirectional iterators" (that are like forward iterators, but can also move backwards) and "s" (that can move freely any number of steps in one operation).

A fundamental STL concept is a "range" which is a pair of iterators that designate the beginning and end of the computation, and most of the library’s algorithmic templates that operate on data structures have interfaces that use ranges .

It is possible to have bidirectional iterators act like random access iterators, so moving forward ten steps could be done by simply moving forward a step at a time a total of ten times. However, having distinct random access iterators offers efficiency advantages. For example, a vector would have a random access iterator, but a list only a bidirectional iterator.

Iterators are the major feature that allow the generality of the STL. For example, an algorithm to reverse a sequence can be implemented using bidirectional iterators, and then the same implementation can be used on lists, vectors and deques. User-created containers only have to provide an iterator that implements one of the five standard iterator interfaces, and all the algorithms provided in the STL can be used on the container.

This generality also comes at a price at times. For example, performing a search on an associative container such as a map or set can be much slower using iterators than by calling member functions offered by the container itself. This is because an associative container's methods can take advantage of knowledge of the internal structure, which is opaque to algorithms using iterators.

A large number of algorithms to perform activities such as searching and sorting are provided in the STL, each implemented to require a certain level of iterator (and therefore will work on any container that provides an interface by iterators). Searching algorithms like and use binary search and like sorting algorithms require that the type of data must implement comparison operator or custom comparator function must be specified; such comparison operator or comparator function must guarantee strict weak ordering. Apart from these, algorithms are provided for making heap from a range of elements, generating lexicographically ordered permutations of a range of elements, merge sorted ranges and perform union, intersection, difference of sorted ranges.

The STL includes classes that overload the function call operator (). Instances of such classes are called functors or function objects. Functors allow the behavior of the associated function to be parameterized (e.g. through arguments passed to the functor's constructor) and can be used to keep associated per-functor state information along with the function. Since both functors and function pointers can be invoked using the syntax of a function call, they are interchangeable as arguments to templates when the corresponding parameter only appears in function call contexts.

A particularly common type of functor is the predicate. For example, algorithms like take a unary predicate that operates on the elements of a sequence. Algorithms like sort, partial_sort, nth_element and all sorted containers use a binary predicate that must provide a strict weak ordering, that is, it must behave like a membership test on a transitive, non reflexive and asymmetric binary relation. If none is supplied, these algorithms and containers use less by default, which in turn calls the less-than-operator <.

The Quality of Implementation (QoI) of the C++ compiler has a large impact on usability of STL (and templated code in general):







</doc>
