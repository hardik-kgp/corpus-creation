<doc id="920295" url="https://en.wikipedia.org/wiki?curid=920295" title="British Museum algorithm">
British Museum algorithm

The British Museum algorithm is a general approach to finding a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities is enormous.

Newell, Shaw, and Simon 
called this procedure the British Museum algorithm 




</doc>
<doc id="200877" url="https://en.wikipedia.org/wiki?curid=200877" title="Maze generation algorithm">
Maze generation algorithm

Maze generation algorithms are automated methods for the creation of mazes.

A maze can be generated by starting with a predetermined arrangement of cells (most commonly a rectangular grid but other arrangements are possible) with wall sites between them. This predetermined arrangement can be considered as a connected graph with the edges representing possible wall sites and the nodes representing cells. The purpose of the maze generation algorithm can then be considered to be making a subgraph in which it is challenging to find a route between two particular nodes.

If the subgraph is not connected, then there are regions of the graph that are wasted because they do not contribute to the search space. If the graph contains loops, then there may be multiple paths between the chosen nodes. Because of this, maze generation is often approached as generating a random spanning tree. Loops, which can confound naive maze solvers, may be introduced by adding random edges to the result during the course of the algorithm.

The animation shows the maze generation steps for a 
graph that is not on a rectangular grid.
First, the computer creates a random planar graph G
shown in blue, and its dual F
shown in yellow. Second, computer traverses F using a chosen
algorithm, such as a depth-first search, coloring the path red.
During the traversal, whenever a red edge crosses over a blue edge,
the blue edge is removed.
Finally, when all vertices of F have been visited, F is erased
and two edges from G, one for the entrance and one for the exit, are removed.

This algorithm is a randomized version of the depth-first search algorithm. Frequently implemented with a stack, this approach is one of the simplest ways to generate a maze using a computer. Consider the space for a maze being a large grid of cells (like a large chess board), each cell starting with four walls. Starting from a random cell, the computer then selects a random neighbouring cell that has not yet been visited. The computer removes the wall between the two cells and marks the new cell as visited, and adds it to the stack to facilitate backtracking. The computer continues this process, with a cell that has no unvisited neighbours being considered a dead-end. When at a dead-end it backtracks through the path until it reaches a cell with an unvisited neighbour, continuing the path generation by visiting this new, unvisited cell (creating a new junction). This process continues until every cell has been visited, causing the computer to backtrack all the way back to the beginning cell. We can be sure every cell is visited.

As given above this algorithm involves deep recursion which may cause stack overflow issues on some computer architectures. The algorithm can be rearranged into a loop by storing backtracking information in the maze itself. This also provides a quick way to display a solution, by starting at any given point and backtracking to the beginning.

Mazes generated with a depth-first search have a low branching factor and contain many long corridors, because the algorithm explores as far as possible along each branch before backtracking.

The depth-first search algorithm of maze generation is frequently implemented using backtracking. This can be described with a following recursive routine:


which is invoked once for any initial cell in the area.

A disadvantage of this approach is a large depth of recursion – in the worst case, the routine may need to recur on every cell of the area being processed, which may exceed the maximum recursion stack depth in many environments. As a solution, the same bactracking method can be implemented with an explicit stack, which is usually allowed to grow much bigger with no harm.


This algorithm is a randomized version of Kruskal's algorithm.


There are several data structures that can be used to model the sets of cells. An efficient implementation using a disjoint-set data structure can perform each union and find operation on two sets in nearly constant amortized time (specifically, formula_1 time; formula_2 for any plausible value of formula_3), so the running time of this algorithm is essentially proportional to the number of walls available to the maze.

It matters little whether the list of walls is initially randomized or if a wall is randomly chosen from a nonrandom list, either way is just as easy to code.

Because the effect of this algorithm is to produce a minimal spanning tree from a graph with equally weighted edges, it tends to produce regular patterns which are fairly easy to solve.

This algorithm is a randomized version of Prim's algorithm.


It will usually be relatively easy to find the way to the starting cell, but hard to find the way anywhere else.

Note that simply running classical Prim's on a graph with random edge weights would create mazes stylistically identical to Kruskal's, because they are both minimal spanning tree algorithms. Instead, this algorithm introduces stylistic variation because the edges closer to the starting point have a lower effective weight.

Although the classical Prim's algorithm keeps a list of edges, for maze generation we could instead maintain a list of adjacent cells. If the randomly chosen cell has multiple edges that connect it to the existing maze, select one of these edges at random. This will tend to branch slightly more than the edge-based version above.

All the above algorithms have biases of various sorts: depth-first search is biased toward long corridors, while Kruskal's/Prim's algorithms are biased toward many short dead ends. Wilson's algorithm, on the other hand, generates an "unbiased" sample from the uniform distribution over all mazes, using loop-erased random walks.

We begin the algorithm by initializing the maze with one cell chosen arbitrarily. Then we start at a new cell chosen arbitrarily, and perform a random walk until we reach a cell already in the maze—however, if at any point the random walk reaches its own path, forming a loop, we erase the loop from the path before proceeding. When the path reaches the maze, we add it to the maze. Then we perform another loop-erased random walk from another arbitrary starting cell, repeating until all cells have been filled.

This procedure remains unbiased no matter which method we use to arbitrarily choose starting cells. So we could always choose the first unfilled cell in (say) left-to-right, top-to-bottom order for simplicity.

Mazes can be created with "recursive division", an algorithm which works as follows: Begin with the maze's space with no walls. Call this a chamber. Divide the chamber with a randomly positioned wall (or multiple walls) where each wall contains a randomly positioned passage opening within it. Then recursively repeat the process on the subchambers until all chambers are minimum sized. This method results in mazes with long straight walls crossing their space, making it easier to see which areas to avoid.

For example, in a rectangular maze, build at random points two walls that are perpendicular to each other. These two walls divide the large chamber into four smaller chambers separated by four walls. Choose three of the four walls at random, and open a one cell-wide hole at a random point in each of the three. Continue in this manner recursively, until every chamber has a width of one cell in either of the two directions.

Other algorithms exist that require only enough memory to store one line of a 2D maze or one plane of a 3D maze. Eller's algorithm prevents loops by storing which cells in the current line are connected through cells in the previous lines, and never removes walls between any two cells already connected. The Sidewinder algorithm starts with an open passage along the entire the top row, and subsequent rows consist of shorter horizontal passages with one connection to the passage above. The Sidewinder algorithm is trivial to solve from the bottom up because it has no upward dead ends. Given a starting width, both algorithm create perfect mazes of unlimited height.

Most maze generation algorithms require maintaining relationships between cells within it, to ensure the end result will be solvable. Valid simply connected mazes can however be generated by focusing on each cell independently. A binary tree maze is a standard orthogonal maze where each cell always has a passage leading up or leading left, but never both. To create a binary tree maze, for each cell flip a coin to decide whether to add a passage leading up or left. Always pick the same direction for cells on the boundary, and the end result will be a valid simply connected maze that looks like a binary tree, with the upper left corner its root. As with Sidewinder, the binary tree maze has no dead ends in the directions of bias. 

A related form of flipping a coin for each cell is to create an image using a random mix of forward slash and backslash characters. This doesn't generate a valid simply connected maze, but rather a selection of closed loops and unicursal passages. (The manual for the Commodore 64 presents a BASIC program using this algorithm, but using PETSCII diagonal line graphic characters instead for a smoother graphic appearance.)

Certain types of cellular automata can be used to generate mazes. Two well-known such cellular automata, Maze and Mazectric, have rulestrings B3/S12345 and B3/S1234. In the former, this means that cells survive from one generation to the next if they have at least one and at most five neighbours. In the latter, this means that cells survive if they have one to four neighbours. If a cell has exactly three neighbours, it is born. It is similar to Conway's Game of Life in that patterns that do not have a living cell adjacent to 1, 4, or 5 other living cells in any generation will behave identically to it. However, for large patterns, it behaves very differently from Life.

For a random starting pattern, these maze-generating cellular automata will evolve into complex mazes with well-defined walls outlining corridors. Mazecetric, which has the rule B3/S1234 has a tendency to generate longer and straighter corridors compared with Maze, with the rule B3/S12345. Since these cellular automaton rules are deterministic, each maze generated is uniquely determined by its random starting pattern. This is a significant drawback since the mazes tend to be relatively predictable.

Like some of the graph-theory based methods described above, these cellular automata typically generate mazes from a single starting pattern; hence it will usually be relatively easy to find the way to the starting cell, but harder to find the way anywhere else.

Example implementation of a variant of Prim's algorithm in Python/NumPy. Prim's algorithm above starts with a grid full of walls and grows a single component of pathable tiles. In this example, we start with an open grid and grow multiple components of walls.

This algorithm works by creating "n" (density) islands of length "p" (complexity). An island is created by choosing a random starting point with odd coordinates, then a random direction is chosen. If the cell two steps in the direction is free, then a wall is added at both one step and two steps in this direction. The process is iterated for "n" steps for this island. "p" islands are created. "n" and "p" are expressed as float to adapt them to the size of the maze. With a low complexity, islands are very small and the maze is easy to solve. With low density, the maze has more "big empty rooms".

The code below is an example of depth-first search maze generator in C. 



</doc>
<doc id="390562" url="https://en.wikipedia.org/wiki?curid=390562" title="Tomasulo algorithm">
Tomasulo algorithm

Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. It was developed by Robert Tomasulo at IBM in 1967 and was first implemented in the IBM System/360 Model 91’s floating point unit.

The major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.

Robert Tomasulo received the Eckert–Mauchly Award in 1997 for his work on the algorithm.

The following are the concepts necessary to the implementation of Tomasulo's Algorithm:

The Common Data Bus (CDB) connects reservation stations directly to functional units. According to Tomasulo it "preserves precedence while encouraging concurrency". This has two important effects:

Instructions are issued sequentially so that the effects of a sequence of instructions, such as exceptions raised by these instructions, occur in the same order as they would on an in-order processor, regardless of the fact that they are being executed out-of-order (i.e. non-sequentially). 

Tomasulo's Algorithm uses register renaming to correctly perform out-of-order execution. All general-purpose and reservation station registers hold either a real value or a placeholder value. If a real value is unavailable to a destination register during the issue stage, a placeholder value is initially used. The placeholder value is a tag indicating which reservation station will produce the real value. When the unit finishes and broadcasts the result on the CDB, the placeholder will be replaced with the real value.

Each functional unit has a single reservation station. Reservation stations hold information needed to execute a single instruction, including the operation and the operands. The functional unit begins processing when it is free and when all source operands needed for an instruction are real.

Practically speaking, there may be exceptions for which not enough status information about an exception is available, in which case the processor may raise a special exception, called an "imprecise" exception. Imprecise exceptions cannot occur in in-order implementations, as processor state is changed only in program order (see RISC Pipeline Exceptions).

Programs that experience "precise" exceptions, where the specific instruction that took the exception can be determined, can restart or re-execute at the point of the exception. However, those that experience "imprecise" exceptions generally cannot restart or re-execute, as the system cannot determine the specific instruction that took the exception.

The three stages listed below are the stages through which each instruction passes from the time it is issued to the time its execution is complete.


In the issue stage, instructions are issued for execution if all operands and reservation stations are ready or else they are stalled. Registers are renamed in this step, eliminating WAR and WAW hazards.


In the execute stage, the instruction operations are carried out. Instructions are delayed in this step until all of their operands are available, eliminating RAW hazards. Program correctness is maintained through effective address calculation to prevent hazards through memory.


In the write Result stage, ALU operations results are written back to registers and store operations are written back to memory.

The concepts of reservation stations, register renaming, and the common data bus in Tomasulo's algorithm presents significant advancements in the design of high-performance computers.

Reservation stations take on the responsibility of waiting for operands in the presence of data dependencies and other inconsistencies such as varying storage access time and circuit speeds, thus freeing up the functional units. This improvement overcomes long floating point delays and memory accesses. In particular the algorithm is more tolerant of cache misses. Additionally, programmers are freed from implementing optimized code. This is a result of the common data bus and reservation station working together to preserve dependencies as well as encouraging concurrency.

By tracking operands for instructions in the reservation stations and register renaming in hardware the algorithm minimizes read-after-write (RAW) and eliminates write-after-write (WAW) and Write-after-Read (WAR) computer architecture hazards. This improves performance by reducing wasted time that would otherwise be required for stalls.

An equally important improvement in the algorithm is the design is not limited to a specific pipeline structure. This improvement allows the algorithm to be more widely adopted by multiple-issue processors. Additionally, the algorithm is easily extended to enable branch speculation. 

Tomasulo's algorithm, outside of IBM, was unused for several years after its implementation in the System/360 Model 91 architecture. However, it saw a vast increase in usage during the 1990s for 3 reasons:

Many modern processors implement dynamic scheduling schemes that are derivative of Tomasulo’s original algorithm, including popular Intel x86-64 chips.




</doc>
<doc id="1551981" url="https://en.wikipedia.org/wiki?curid=1551981" title="Medical algorithm">
Medical algorithm

A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty. A medical prescription is also a type of medical algorithm.

Medical algorithms are part of a broader field which is usually fit under the aims of medical informatics and medical decision-making. Medical decisions occur in several areas of medical activity including medical test selection, diagnosis, therapy and prognosis, and automatic control of medical equipment.

In relation to logic-based and artificial neural network-based clinical decision support systems, which are also computer applications used in the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.

A wealth of medical information exists in the form of published medical algorithms. These algorithms range from simple calculations to complex outcome predictions. Most clinicians use only a small subset routinely.

Examples of medical algorithms are:

A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the Texas Medication Algorithm Project or otherwise working on their production.

A grammar—the Arden syntax—exists for describing algorithms in terms of medical logic modules. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.

The intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of medical care. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm automation intended to reduce potential introduction of errors. Some attempt to predict the outcome, for example critical care scoring systems.

Computerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to evidence-based guidelines, and be a resource for education and research.

Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as protocols and it is a key task in training to ensure people step outside the protocol when necessary. In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.

In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.

Computations obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and physician judgment.



</doc>
<doc id="2935699" url="https://en.wikipedia.org/wiki?curid=2935699" title="Run-time algorithm specialisation">
Run-time algorithm specialisation

In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.

The idea is inspired by the use of partial evaluation in optimising program translation. 
Many core operations in theorem provers exhibit the following pattern.
Suppose that we need to execute some algorithm formula_1 in a situation where a value of formula_2 "is fixed for potentially many different values of" formula_3. In order to do this efficiently, we can try to find a specialization of formula_4 for every fixed formula_2, i.e., such an algorithm formula_6, that executing formula_7 is equivalent to executing formula_1.

The specialized algorithm may be more efficient than the generic one, since it can "exploit some particular properties" of the fixed value formula_2. Typically, formula_7 can avoid some operations that formula_1 would have to perform, if they are known to be redundant for this particular parameter formula_2. 
In particular, we can often identify some tests that are true or false for formula_2, unroll loops and recursion, etc.

The key difference between run-time specialization and partial evaluation is that the values of formula_2 on which formula_4 is specialised are not known statically, so the "specialization takes place at run-time".

There is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of formula_4. We only have to "imagine" formula_4 "when we program" the specialization procedure.
All we need is a concrete representation of the specialized version formula_6. This also means that we cannot use any universal methods for specializing algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialization procedure for every particular algorithm formula_4. An important advantage of doing so is that we can use some powerful "ad hoc" tricks exploiting peculiarities of formula_4 and the representation of formula_2 and formula_3, which are beyond the reach of any universal specialization methods.

The specialized algorithm has to be represented in a form that can be interpreted.
In many situations, usually when formula_7 is to be computed on many values formula_3 in a row, we can write formula_6 as a code of a special abstract machine, and we often say that formula_2 is "compiled". 
Then the code itself can be additionally optimized by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine.

Instructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional parameters of the instruction, for example a pointer to another
instruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree.

Interpretation is done by fetching instructions in some order, identifying their type
and executing the actions associated with this type. 
In C or C++ we can use a switch statement to associate 
some actions with different instruction tags. 
Modern compilers usually compile a switch statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value formula_27 in the formula_27-th cell of a special array. One can exploit this
by taking values for instruction tags from a small interval of integers.

There are situations when many instances of formula_2 are intended for long-term storage and the calls of formula_1 occur with different formula_3 in an unpredictable order.
For example, we may have to check formula_32 first, then formula_33, then formula_34, and so on.
In such circumstances, full-scale specialization with compilation may not be suitable due to excessive memory usage. 
However, we can sometimes find a compact specialized representation formula_35
for every formula_2, that can be stored with, or instead of, formula_2. 
We also define a variant formula_38 that works on this representation 
and any call to formula_1 is replaced by formula_40, intended to do the same job faster.





</doc>
<doc id="8286430" url="https://en.wikipedia.org/wiki?curid=8286430" title="Adaptive algorithm">
Adaptive algorithm

An adaptive algorithm is an algorithm that changes its behavior at the time it is run, based on information available and on "a priori" defined reward mechanism (or criterion). Such information could be the story of recently received data, information on the available computational resources, or other run-time acquired (or "a priori" known) information related to the environment in which it operates.

Among the most used adaptive algorithms is the Widrow-Hoff’s least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. In adaptive filtering the LMS is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired and the actual signal).

For example, stable partition, using no additional memory is "O"("n" lg "n") but given "O"("n") memory, it can be "O"("n") in time. As implemented by the C++ Standard Library, codice_1 is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory. Another example is adaptive sort, whose behavior changes upon the presortedness of its input.

An example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.

In machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, AdaBoost, and adaptive quadrature.

In data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.

In signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called "adaptive" because the window length (the size of an audio "chunk") can change according to the nature of the sound being compressed, to try to achieve the best-sounding compression strategy.



</doc>
<doc id="145555" url="https://en.wikipedia.org/wiki?curid=145555" title="XOR swap algorithm">
XOR swap algorithm

In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. "Distinct" means that the variables are stored at different, non-overlapping, memory addresses as the algorithm would set a single aliased value to zero; the actual values of the variables do not have to be different.

Conventional swapping requires the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:
X := X XOR Y
Y := Y XOR X
X := X XOR Y
The algorithm typically corresponds to three machine-code instructions. Since XOR is a commutative operation, X XOR Y can be replaced with Y XOR X in any of the lines. When coded in assembly language, this commutativity is often exercised in the second line:

In the above System/370 assembly code sample, R1 and R2 are distinct registers, and each operation leaves its result in the register named in the first argument. Using x86 assembly, values X and Y are in registers eax and ebx (respectively), and places the result of the operation in the first register.

However, the algorithm fails if "x" and "y" use the same storage location, since the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be "swapped with itself". Note that this is "not" the same as if "x" and "y" have the same values. The trouble only comes when "x" and "y" use the same storage location, in which case their values must already be equal. That is, if "x" and "y" use the same storage location, then the line:

X := X XOR Y

sets "x" to zero (because "x" = "y" so X XOR Y is zero) "and" sets "y" to zero (since it uses the same storage location), causing "x" and "y" to lose their original values.

The binary operation XOR over bit strings of length formula_1 exhibits the following properties (where formula_2 denotes XOR):


Suppose that we have two distinct registers codice_1 and codice_2 as in the table below, with initial values "A" and "B" respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.

As XOR can be interpreted as binary addition and a pair of bits can be interpreted as a vector in a two-dimensional vector space over the field with two elements, the steps in the algorithm can be interpreted as multiplication by 2×2 matrices over the field with two elements. For simplicity, assume initially that "x" and "y" are each single bits, not bit vectors.

For example, the step:

X := X XOR Y

which also has the implicit:

Y := Y

corresponds to the matrix formula_9 as
The sequence of operations is then expressed as:
(working with binary values, so formula_12), which expresses the elementary matrix of switching two rows (or columns) in terms of the transvections (shears) of adding one element to the other.

To generalize to where X and Y are not single bits, but instead bit vectors of length "n", these 2×2 matrices are replaced by 2"n"×2"n" block matrices such as formula_13

Note that these matrices are operating on "values," not on "variables" (with storage locations), hence this interpretation abstracts away from issues of storage location and the problem of both variables sharing the same storage location.

A C function that implements the XOR swap algorithm:
Note that the code does not swap the integers passed immediately, but first checks if their addresses are distinct. This is because, if the addresses are equal, the algorithm will fold to a triple *x ^= *x resulting in zero.

The XOR swap algorithm can also be defined with a macro:
In most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which XOR swapping may be practical include:


Because these situations are rare, most optimizing compilers do not generate XOR swap code.

Most modern compilers can optimize away the temporary variable in the three-way swap, in which case it will use the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster. In addition to that, the XOR swap is completely opaque to anyone unfamiliar with the technique.

On modern CPU architectures, the XOR technique can be slower than using a temporary variable to do swapping. At least on recent x86 CPUs, both by AMD and Intel, moving between registers regularly incurs zero latency. (This is called MOV-elimination.) Even if there is not any architectural register available to use, the codice_3 instruction will be at least as fast as the three XORs taken together. Another reason is that modern CPUs strive to execute instructions in parallel via instruction pipelines. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order, negating any benefits of instruction-level parallelism.

The XOR swap is also complicated in practice by aliasing. As noted above, if an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.

Similar problems occur with call by name, as in Jensen's Device, where swapping codice_4 and codice_5 via a temporary variable yields incorrect results due to the arguments being related: swapping via codice_6 changes the value for codice_4 in the second statement, which then results in the incorrect i value for codice_5 in the third statement.

The underlying principle of the XOR swap algorithm can be applied to any operation meeting criteria L1 through L4 above. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:

Unlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as modular arithmetic or bignums to guarantee that the computation of codice_9 cannot cause an error due to integer overflow. Therefore, it is seen even more rarely in practice than the XOR swap.

Note, however, that the implementation of codice_10 above in the C programming language always works even in case of integer overflow, since, according to the C standard, addition and subtraction of unsigned integers follow the rules of modular arithmetic, i. e. are done in the cyclic group formula_14 where formula_15 is the number of bits of codice_11. Indeed, the correctness of the algorithm follows from the fact that the formulas formula_16 and formula_17 hold in any abelian group. This is actually a generalization of the proof for the XOR swap algorithm: XOR is both the addition and subtraction in the abelian group formula_18 (which is the direct sum of "s" copies of formula_19).

Please note that the above doesn't hold when dealing with the codice_12 type (the default for codice_13). Signed integer overflow is an undefined behavior in C and thus modular arithmetic is not guaranteed by the standard (a standard-conforming compiler might optimize out such code, which leads to incorrect results).



</doc>
<doc id="15641067" url="https://en.wikipedia.org/wiki?curid=15641067" title="Super-recursive algorithm">
Super-recursive algorithm

In computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines. The term was introduced by Mark Burgin, whose book "Super-recursive algorithms" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.

Burgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the Church-Turing thesis, but this point of view has been criticized within the mathematical community and is not widely accepted.

Burgin (2005: 13) uses the term recursive algorithms for algorithms that can be implemented on Turing machines, and uses the word "algorithm" in a more general sense. Then a super-recursive class of algorithms is "a class of algorithms in which it is possible to compute functions not computable by any Turing machine" (Burgin 2005: 107).

Super-recursive algorithms are closely related to hypercomputation 
in a way similar to the relationship between ordinary computation and ordinary algorithms. Computation is a process, while an algorithm is a finite constructive description of such a process. Thus a super-recursive algorithm defines a "computational process (including processes of input and output) that cannot be realized by recursive algorithms." (Burgin 2005: 108). A more restricted definition demands that hypercomputation solves a supertask (see Copeland 2002; Hagar and Korolev 2007).

Super-recursive algorithms are also related to algorithmic schemes, which are more general than super-recursive algorithms. Burgin argues (2005: 115) that it is necessary to make a clear distinction between super-recursive algorithms and those algorithmic schemes that are not algorithms. Under this distinction, some types of hypercomputation are obtained by super-recursive algorithms, e.g., inductive Turing machines, while other types of hypercomputation are directed by algorithmic schemas, e.g., infinite time Turing machines. This explains how works on super-recursive algorithms are related to hypercomputation and vice versa. According to this argument, super-recursive algorithms are just one way of defining a hypercomputational process.

Examples of super-recursive algorithms include (Burgin 2005: 132):

Examples of algorithmic schemes include:


For examples of practical super-recursive algorithms, see the book of Burgin.

Inductive Turing machines implement an important class of super-recursive algorithms. An inductive Turing machine is a definite list of well-defined instructions for completing a task which, when given an initial state, will proceed through a well-defined series of successive states, eventually giving the final result. The difference between an inductive Turing machine and an ordinary Turing machine is that an ordinary Turing machine must stop when it has obtained its result, while in some cases an inductive Turing machine can continue to compute after obtaining the result, without stopping. Kleene called procedures that could run forever without stopping by the name "calculation procedure or algorithm" (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit "some object" (Kleene 1952:137). Burgin argues that this condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps. The reason that inductive Turing machines cannot be instructed to halt when their final output is produced is that in some cases inductive Turing machines may not be able to tell at which step the result has been obtained.

Simple inductive Turing machines are equivalent to other models of computation such as general Turing machines of Schmidhuber, trial and error predicates of Hilary Putnam, limiting partial recursive functions of Gold, and trial-and-error machines of Hintikka and Mutanen (1998). More advanced inductive Turing machines are much more powerful. There are hierarchies of inductive Turing machines that can decide membership in arbitrary sets of the arithmetical hierarchy (Burgin 2005). In comparison with other equivalent models of computation, simple inductive Turing machines and general Turing machines give direct constructions of computing automata that are thoroughly grounded in physical machines. In contrast, trial-and-error predicates, limiting recursive functions, and limiting partial recursive functions present only syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial-and-error predicates as Turing machines are related to partial recursive functions and lambda calculus.

The non-halting computations of inductive Turing machines should not be confused with infinite-time computations (see, for example, Potgieter 2006). First, some computations of inductive Turing machines do halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not. Even if it does not halt, an inductive Turing machine produces output from time to time. If this output stops changing, it is then considered the result of the computation.

There are two main distinctions between ordinary Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine will always determine (by coming to a final state) when the result is obtained, while a simple inductive Turing machine, in some cases (such as when "computing" something that cannot be computed by an ordinary Turing machine), will not be able to make this determination.

A symbol sequence is computable in the limit if there is a finite, possibly non-halting program on a universal Turing machine that incrementally outputs every symbol of the sequence. This includes the dyadic expansion of π but still excludes most of the real numbers, because most cannot be described by a finite program. Traditional Turing machines with a write-only output tape cannot edit their previous outputs; generalized Turing machines, according to Jürgen Schmidhuber, can edit their output tape as well as their work tape. He defines the constructively describable symbol sequences as those that have a finite, non-halting program running on a generalized Turing machine, such that any output symbol eventually converges, that is, it does not change any more after some finite initial time interval. Schmidhuber (2000, 2002) uses this approach to define the set of formally describable or constructively computable universes or constructive theories of everything. Generalized Turing machines and simple inductive Turing machines are two classes of super-recursive algorithms that are the closest to recursive algorithms (Schmidhuber 2000).

The Church–Turing thesis in recursion theory relies on a particular definition of the term "algorithm". Based on definitions that are more general than the one commonly used in recursion theory, Burgin argues that super-recursive algorithms, such as inductive Turing machines disprove the Church–Turing thesis. He proves furthermore that super-recursive algorithms could theoretically provide even greater efficiency gains than using quantum algorithms.

Burgin's interpretation of super-recursive algorithms has encountered opposition in the mathematical community. One critic is logician Martin Davis, who argues that Burgin's claims have been well understood "for decades". Davis states, 
Davis disputes Burgin's claims that sets at level formula_1 of the arithmetical hierarchy can be called computable, saying






</doc>
<doc id="3578575" url="https://en.wikipedia.org/wiki?curid=3578575" title="Randomization function">
Randomization function

In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.

Randomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.

Randomizing functions are used to turn algorithms that have good expected performance for "random" inputs, into algorithms that have the same performance for "any" input.

For example, consider a sorting algorithm like quicksort, which has small expected running time when the input items are presented in random order, but is very slow when they are presented in certain unfavorable orders. A randomizing function from the integers 1 to "n" to the integers 1 to "n" can be used to rearrange the "n" input items in "random" order, before calling that algorithm. This modified (randomized) algorithm will have small expected running time, whatever the input order.

In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed. The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time). With such a "pseudo-randomization" function, one could in principle construct a sequence of calls such that the function would always yield a "bad" case for the underlying deterministic algorithm. For that sequence of calls, the average cost would be closer to the worst-case cost, rather than the average cost for random inputs.

In practice, however, the main concern is that some "bad" cases for the deterministic algorithm may occur in practice much more often than it would be predicted by chance. For example, in a naive variant of quicksort, the worst case is when the input items are already sorted — which is a very common occurrence in many applications. For such algorithms, even a fixed pseudo-random permutation may be good enough. Even though the resulting "pseudo-randomized" algorithm would still have as many "bad" cases as the original, they will be certain peculiar orders that would be quite unlikely to arise in real applications. So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external "random" data such as the program's startup time.

The uniformity requirements for a randomizing function are usually much weaker than those of hash functions and pseudo-random generators. The minimum requirement is that it maps any input of the deterministic algorithm into a "good" input with a sufficiently high probability. (However, analysis is usually simpler if the randomizing function implements each possible mapping with uniform probability.)


</doc>
<doc id="22509875" url="https://en.wikipedia.org/wiki?curid=22509875" title="Simulation algorithms for atomic DEVS">
Simulation algorithms for atomic DEVS

Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to "lifespan" formula_1 and "elapsed time" formula_2 by introducing two other time variables, "last event time", formula_3, and "next event time" formula_4 with the following relations: 
formula_5

and

formula_6

where formula_7 denotes the "current time". And the "remaining time",

formula_9, apparently formula_10.

Since the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.

Regardless of two different views of total states, algorithms for initialization and internal transition cases are commonly defined as below.

As addressed in Behavior of Atomic DEVS, when DEVS receives an input event, right calling formula_24, the last event time,formula_11 is set by the current time,formula_14, thus the elapsed timeformula_27 becomes zero because formula_28.

Notice that as addressed in Behavior of Atomic DEVS, depending on the value of formula_36 return by formula_37, last event time,formula_11, and next event time,formula_12,consequently, elapsed time, formula_27, and lifespanformula_12, are updated (if formula_42) or preserved (if formula_43).




</doc>
<doc id="22513037" url="https://en.wikipedia.org/wiki?curid=22513037" title="Simulation algorithms for coupled DEVS">
Simulation algorithms for coupled DEVS

Given a coupled DEVS model, simulation algorithms are methods to generate the model's "legal" behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.) [Zeigler84] originally introduced the algorithms that handle time variables related to "lifespan" formula_1 and "elapsed time" formula_2 by introducing two other time variables, "last event time", formula_3, and "next event time" formula_4 with the following relations: formula_5

and

formula_6

where formula_7 denotes the "current time". And the "remaining time",

formula_9, apparently formula_10.
Based on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.

 DEVS-coordinator




</doc>
<doc id="505526" url="https://en.wikipedia.org/wiki?curid=505526" title="HAKMEM">
HAKMEM

HAKMEM, alternatively known as AI Memo 239, is a February 1972 "memo" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, "a bizarre and eclectic potpourri of technical trivia".
Contributors included about two dozen members and associates of the AI Lab. The title of the report is short for "hacks memo", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).

HAKMEM is notable as an early compendium of algorithmic technique, particularly for its practical bent, and as an illustration of the wide-ranging interests of AI Lab people of the time, which included almost anything other than AI research.

HAKMEM contains original work in some fields, notably continued fractions.




</doc>
<doc id="14609233" url="https://en.wikipedia.org/wiki?curid=14609233" title="Holographic algorithm">
Holographic algorithm

In computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them "holographic" because "their effect can be viewed as that of producing interference patterns among the solution fragments". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.

Holographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory. Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.

Holographic algorithms have some similarities with quantum computation, but are completely classical.

Holographic algorithms exist in the context of Holant problems, which generalize counting constraint satisfaction problems (#CSP). A #CSP instance is a hypergraph "G"=("V","E") called the constraint graph. Each hyperedge represents a variable and each vertex formula_1 is assigned a constraint formula_2 A vertex is connected to an hyperedge if the constraint on the vertex involves the variable on the hyperedge. The counting problem is to compute
which is a sum over all variable assignments, the product of every constraint, where the inputs to the constrain formula_4 are the variables on the incident hyperedges of formula_1.

A Holant problem is like a #CSP except the input must be a graph, not a hypergraph. Restricting the class of input graphs in this way is indeed a generalization. Given a #CSP instance, replace each hyperedge "e" of size "s" with a vertex "v" of degree "s" with edges incident to the vertices contained in "e". The constraint on "v" is the equality function of arity "s". This identifies all of the variables on the edges incident to "v", which is the same effect as the single variable on the hyperedge "e".

In the context of Holant problems, the expression in (1) is called the Holant after a related exponential sum introduced by Valiant.

A standard technique in complexity theory is a many-one reduction, where an instance of one problem is reduced to an instance of another (hopefully simpler) problem.
However, holographic reductions between two computational problems preserve the sum of solutions without necessarily preserving correspondences between solutions. For instance, the total number of solutions in both sets can be preserved, even though individual problems do not have matching solutions. The sum can also be weighted, rather than simply counting the number of solutions, using linear basis vectors.

It is convenient to consider holographic reductions on bipartite graphs. A general graph can always be transformed it into a bipartite graph while preserving the Holant value. This is done by replacing each edge in the graph by a path of length 2, which is also known as the 2-stretch of the graph. To keep the same Holant value, each new vertex is assigned the binary equality constraint.

Consider a bipartite graph "G"=("U","V","E") where the constraint assigned to every vertex formula_6 is formula_7 and the constraint assigned to every vertex formula_8 is formula_4. Denote this counting problem by formula_10 If the vertices in "U" are viewed as one large vertex of degree |"E"|, then the constraint of this vertex is the tensor product of formula_7 with itself |"U"| times, which is denoted by formula_12 Likewise, if the vertices in "V" are viewed as one large vertex of degree |"E"|, then the constraint of this vertex is formula_13 Let the constraint formula_7 be represented by its weighted truth table as a row vector and the constraint formula_4 be represented by its weighted truth table as a column vector. Then the Holant of this constraint graph is simply formula_16

Now for any complex 2-by-2 invertible matrix "T" (the columns of which are the linear basis vectors mentioned above), there is a holographic reduction between formula_17 and formula_18 To see this, insert the identity matrix formula_19 in between formula_20 to get
Thus, formula_17 and formula_25 have exactly the same Holant value for every constraint graph. They essentially define the same counting problem.

Let "G" be a graph. There is a 1-to-1 correspondence between the vertex covers of "G" and the independent sets of "G". For any set "S" of vertices of "G", "S" is a vertex cover in "G" if and only if the complement of "S" is an independent set in "G". Thus, the number of vertex covers in "G" is exactly the same as the number of independent sets in "G".

The equivalence of these two counting problems can also be proved using a holographic reduction. For simplicity, let "G" be a 3-regular graph. The 2-stretch of "G" gives a bipartite graph "H"=("U","V","E"), where "U" corresponds to the edges in "G" and "V" corresponds to the vertices in "G". The Holant problem that naturally corresponds to counting the number of vertex covers in "G" is formula_26 The truth table of OR as a row vector is (0,1,1,1). The truth table of EQUAL as a column vector is formula_27. Then under a holographic transformation by formula_28
which is formula_35 the Holant problem that naturally corresponds to counting the number of independent sets in "G".

As with any type of reduction, a holographic reduction does not, by itself, yield a polynomial time algorithm. In order to get a polynomial time algorithm, the problem being reduced to must also have a polynomial time algorithm. Valiant's original application of holographic algorithms used a holographic reduction to a problem where every constraint is realizable by matchgates, which he had just proved is tractable by a further reduction to counting the number of perfect matchings in a planar graph. The latter problem is tractable by the FKT algorithm, which dates to the 1960s.

Soon after, Valiant found holographic algorithms with reductions to matchgates for #Pl-Rtw-Mon-3CNF and #Pl-3/2Bip-VC. These problems may appear somewhat contrived, especially with respect to the modulus. Both problems were already known to be #P-hard when ignoring the modulus and Valiant supplied proofs of #P-hardness modulo 2, which also used holographic reductions. Valiant found these two problems by a computer search that looked for problems with holographic reductions to matchgates. He called their algorithms "accidental algorithms", saying "when applying the term accidental to an algorithm we intend to point out that the algorithm arises from satisfying an apparently onerous set of constraints." The "onerous" set of constraints in question are polynomial equations that, if satisfied, imply the existence of a holographic reduction to matchgate realizable constraints.

After several years of developing (what is known as) matchgate signature theory, Jin-Yi Cai and Pinyan Lu were able to explain the existence of Valiant's two accidental algorithms. These two problems are just special cases of two much larger families of problems: #Pl-Rtw-Mon-kCNF and #Pl-k/2Bip-VC for any positive integer "k". The modulus 7 is just the third Mersenne number and Cai and Lu showed that these types of problems with parameter "k" can be solved in polynomial time exactly when the modulus is the "k"th Mersenne number by using holographic reductions to matchgates and the Chinese remainder theorem.

Around the same time, Jin-Yi Cai, Pinyan Lu and Mingji Xia gave the first holographic algorithm that did not reduce to a problem that is tractable by matchgates. Instead, they reduced to a problem that is tractable by Fibonacci gates, which are symmetric constraints whose truth tables satisfy a recurrence relation similar to one that defines the Fibonacci numbers. They also used holographic reductions to prove that certain counting problems are #P-hard. Since then, holographic reductions have been used extensively as ingredients in both polynomial time algorithms and proofs of #P-hardness.


</doc>
<doc id="23868049" url="https://en.wikipedia.org/wiki?curid=23868049" title="Sequential algorithm">
Sequential algorithm

In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with "concurrent algorithm" or "parallel algorithm;" most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus "sequential" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.

"Sequential algorithm" may also refer specifically to an algorithm for decoding a convolutional code.



</doc>
<doc id="8756788" url="https://en.wikipedia.org/wiki?curid=8756788" title="One-pass algorithm">
One-pass algorithm

In computing, a one-pass algorithm is a streaming algorithm which reads its input exactly once, in order, without unbounded buffering. A one-pass algorithm generally requires "O"("n") (see 'big O' notation) time and less than "O"("n") storage (typically "O"(1)), where "n" is the size of the input.

Basically one-pass algorithm operates as follows:

Given any list as an input:

Given a list of numbers:

Given a list of symbols from an alphabet of "k" symbols, given in advance.

Given any list as an input:

Given a list of numbers:


</doc>
<doc id="417534" url="https://en.wikipedia.org/wiki?curid=417534" title="Algorism">
Algorism

Algorism is the technique of performing basic arithmetic by writing numbers in place value form and applying a set of memorized rules and facts to the digits. One who practices algorism is known as an algorist. This positional notation system largely superseded earlier calculation systems that used a different set of symbols for each numerical magnitude, such as Roman numerals, and in some cases required a device such as an abacus.

The word "algorism" comes from the name Al-Khwārizmī (c. 780–850), a Persian mathematician, astronomer, geographer and scholar in the House of Wisdom in Baghdad, whose name means "the native of Khwarezm", a city that was part of the Greater Iran during his era and now is in modern-day Uzbekistan He wrote a treatise in Arabic language in the 9th century, which was translated into Latin in the 12th century under the title "Algoritmi de numero Indorum". This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra. In late medieval Latin, "algorismus", the corruption of his name, simply meant the "decimal number system" that is still the meaning of modern English algorism. During the 17th century, the French form for the word –but not its meaning– was changed to "algorithm", following the model of the word "logarithm", this form alluding to the ancient Greek . English adopted the French very soon afterwards, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English. In English, it was first used about 1230 and then by Chaucer in 1391. Another early use of the word is from 1240, in a manual titled "Carmen de Algorismo" composed by Alexandre de Villedieu. It begins thus:

which translates as:
The word "algorithm" also derives from "algorism", a generalization of the meaning to any set of rules specifying a computational procedure. Occasionally "algorism" is also used in this generalized meaning, especially in older texts.

Starting with the integer arithmetic developed in India using base 10 notation, Al-Khwārizmī along with other mathematicians in medieval Islam, both Iranian and Arabic, documented new arithmetic methods and made many other contributions to decimal arithmetic (see the articles linked below). These included the concept of the decimal fractions as an extension of the notation, which in turn led to the notion of the decimal point. This system was popularized in Europe by Leonardo of Pisa, now known as Fibonacci.



</doc>
<doc id="12242679" url="https://en.wikipedia.org/wiki?curid=12242679" title="Ping-pong scheme">
Ping-pong scheme

Algorithms said to employ a Ping-Pong scheme exist in different fields of software engineering. They are characterized by an alternation between two entities. In the examples described below, these entities are communication partners, network paths or file blocks.

In most database management systems durable database transactions are supported through a log file. However, multiple writes to the same page of that file can produce a slim chance of data loss. Assuming for simplicity that the log file is organized in pages whose size matches the block size of its underlying medium, the following problem can occur:

If the very last page of the log file is only partially filled with data and has to be written to permanent storage in this state, the very same page will have to be overwritten during the next write operation. If a crash happens during that later write operation, previously stored log data may be lost.

The Ping-Pong scheme described in "Transaction Processing" eliminates this problem by alternately writing the contents of said (logical) last page to two different physical pages inside the log file (the actual last page "i" and its empty successor "i+1"). Once said logical log page is no longer the last page (i.e. it is completely filled with log data), it is written one last time to the regular physical position ("i") inside the log file.

This scheme requires the usage of time stamps for each page in order to distinguish the most recent version of the logical last page one from its predecessor.

A functionality which lets a computer A find out whether a computer B is reachable and responding is built into the Internet Control Message Protocol (ICMP). Through an "echo request" Computer A asks B to send back an "Echo response". These two messages are also sometimes called "ping" and "pong".

In Routing, a Ping-Pong scheme is a simple algorithm for distributing data packets across
two paths.

If you had two paths codice_1 and codice_2, then the algorithm
would randomly start with one of the paths and then switch back and forth 
between the two.

If you were to get the next path from a function call, it would look like
this in Python:


</doc>
<doc id="201154" url="https://en.wikipedia.org/wiki?curid=201154" title="Divide-and-conquer algorithm">
Divide-and-conquer algorithm

In computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.

This divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).

Understanding and designing divide-and-conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These divide-and-conquer complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.

The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.

The divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. 
For example, to sort a given list of "n" natural numbers, split it into two lists of about "n"/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture). This approach is known as the merge sort algorithm.

The name "divide and conquer" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analog in numerical computing, the bisection algorithm for root finding). These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops. Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a "divide-and-conquer algorithm". Therefore, some authors consider that the name "divide and conquer" should be used only when each problem may generate two or more subproblems. The name decrease and conquer has been proposed instead for the single-subproblem class.

An important application of divide and conquer is in optimization, where if the search space is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.

Early examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into "single" subproblems, and indeed can be solved iteratively.

Binary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC. Another ancient decrease-and-conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.

An early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm, although he did not analyze its operation count quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.

An early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.

Another notable example is the algorithm invented by Anatolii A. Karatsuba in 1960 that could multiply two "n"-digit numbers in formula_1 operations (in Big O notation). This algorithm disproved Andrey Kolmogorov's 1956 conjecture that formula_2 operations would be required for that task.

As another example of a divide-and-conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered. This is related to a radix sort, described for punch-card sorting machines as early as 1929.

Divide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem. Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic Tower of Hanoi puzzle, which reduces moving a tower of height "n" to moving a tower of height "n" − 1.

The divide-and-conquer paradigm often helps in the discovery of efficient algorithms. It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.

In all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution.
For example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size "n", and (b) there is a bounded number "p" of subproblems of size ~ "n"/"p" at each stage, then the cost of the divide-and-conquer algorithm will be O("n" log"n").

Divide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.

Divide-and-conquer algorithms naturally tend to make efficient use of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called "cache-oblivious", because it does not contain the cache size as an explicit parameter.
Moreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be "optimal" cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is "blocking", as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.

The same advantage exists with regards to other hierarchical storage systems, such as NUMA or virtual memory, as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.

In computations with rounded arithmetic, e.g. with floating-point numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add "N" numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums. While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.

Divide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.

Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue. This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch-and-bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.

In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow. D&C algorithms that are time-efficient often have relatively small recursion depth. For example, the quicksort algorithm can be implemented so that it never requires more than formula_3 nested recursive calls to sort formula_4 items.

Stack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it. Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure. Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.

In any recursive algorithm, there is considerable freedom in the choice of the "base cases", the small subproblems that are solved directly in order to terminate the recursion.

Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve. For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.

On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is "short-circuiting the base case", also known as "arm's-length recursion". In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.

Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small. Note that, if the empty list were the only base case, sorting a list with "n" entries would entail maximally "n" quicksort calls that would do nothing but return immediately. Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.

Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation). For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes. Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.

The generalized version of this idea is known as recursion "unrolling" or "coarsening", and various techniques have been proposed for automating the procedure of enlarging the base case.

For some problems, the branched recursion may end up evaluating the same sub-problem many times over. In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as memoization. Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming and chart parsing.



</doc>
<doc id="26754386" url="https://en.wikipedia.org/wiki?curid=26754386" title="Randomized rounding">
Randomized rounding

Within computer science and operations research,
many combinatorial optimization problems are computationally intractable to solve exactly (to optimality).
Many such problems do admit fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.

Randomized rounding

is a widely used approach for designing and analyzing such approximation algorithms. 
The basic idea is to use the probabilistic method
to convert an optimal solution of a relaxation
of the problem into an approximately optimal solution to the original problem.

The basic approach has three steps:

(Although the approach is most commonly applied with linear programs,
other kinds of relaxations are sometimes used.
For example, see Goeman's and Williamson's semi-definite programming-based
Max-Cut approximation algorithm.)

The challenge in the first step is to choose a suitable integer linear program.
Familiarity with linear programming is required, in particular, familiarity with
how to model problems using linear programs and integer linear programs.
But, for many problems, there is a natural integer linear program that works well,
such as in the Set Cover example below. (The integer linear program should have a small
integrality gap;
indeed randomized rounding is often used to prove bounds on integrality gaps.)

In the second step, the optimal fractional solution can typically be computed
in polynomial time
using any standard linear programming algorithm.

In the third step, the fractional solution must be converted into an integer solution
(and thus a solution to the original problem).
This is called "rounding" the fractional solution.
The resulting integer solution should (provably) have cost
not much larger than the cost of the fractional solution.
This will ensure that the cost of the integer solution
is not much larger than the cost of the optimal integer solution.

The main technique used to do the third step (rounding) is to use randomization,
and then to use probabilistic arguments to bound the increase in cost due to the rounding
(following the probabilistic method from combinatorics).
There, probabilistic arguments are used to show the existence of discrete structures with
desired properties. In this context, one uses such arguments to show the following:

Finally, to make the third step computationally efficient,
one either shows that formula_3 approximates formula_1
with high probability (so that the step can remain randomized)
or one derandomizes the rounding step,
typically using the method of conditional probabilities.
The latter method converts the randomized rounding process
into an efficient deterministic process that is guaranteed
to reach a good outcome.

The randomized rounding step differs from most applications of the probabilistic method in two respects:

The following example illustrates how randomized rounding can be used to design an approximation algorithm for the Set Cover problem.

Fix any instance formula_14 of set cover over a universe formula_15.

For step 1, let IP be the standard integer linear program for set cover for this instance.

For step 2, let LP be the linear programming relaxation of IP,
and compute an optimal solution formula_16 to LP
using any standard linear programming algorithm.

(The feasible solutions to LP are the vectors formula_1
that assign each set formula_18
a non-negative weight formula_19,
such that, for each element formula_20,
formula_3 "covers" formula_22
-- the total weight assigned to the sets containing formula_22
is at least 1, that is,
The optimal solution formula_16
is a feasible solution whose cost
is as small as possible.)
Note that any set cover formula_27 for formula_28
gives a feasible solution formula_1
(where formula_30 for formula_31,
formula_32 otherwise).
The cost of this formula_27 equals the cost of formula_1, that is,
In other words, the linear program LP is a relaxation
of the given set-cover problem.

Since formula_16 has minimum cost among feasible solutions to the LP,
"the cost of formula_16 is a lower bound on the cost of the optimal set cover".

Here is a description of the third step—the rounding step,
which must convert the minimum-cost fractional set cover formula_16
into a feasible integer solution formula_3 (corresponding to a true set cover).

The rounding step should produce an formula_3 that, with positive probability,
has cost within a small factor of the cost of formula_16.
Then (since the cost of formula_16 is a lower bound on the cost of the optimal set cover),
the cost of formula_3 will be within a small factor of the optimal cost.

As a starting point, consider the most natural rounding scheme:

With this rounding scheme,
the expected cost of the chosen sets is at most formula_48,
the cost of the fractional cover.
This is good. Unfortunately the coverage is not good.
When the variables formula_49 are small,
the probability that an element formula_22 is not covered is about

So only a constant fraction of the elements will be covered in expectation.

To make formula_3 cover every element with high probability,
the standard rounding scheme
first "scales up" the rounding probabilities
by an appropriate factor formula_53.
Here is the standard rounding scheme:

Scaling the probabilities up by formula_59
increases the expected cost by formula_59,
but makes coverage of all elements likely.
The idea is to choose formula_59 as small
as possible so that all elements are provably
covered with non-zero probability.
Here is a detailed analysis.

(Note: with care the formula_65
can be reduced to formula_67.)

The output formula_3 of the random rounding scheme has the desired properties
as long as none of the following "bad" events occur:

The expectation of each formula_75 is at most formula_76.
By linearity of expectation,
the expectation of formula_69
is at most formula_78.
Thus, by Markov's inequality, the probability of the first bad event
above is at most formula_79.

For the remaining bad events (one for each element formula_22), note that,
since formula_81 for any given element formula_22,
the probability that formula_22 is not covered is

(This uses the inequality formula_85,
which is strict for formula_86.)

Thus, for each of the formula_87 elements,
the probability that the element is not covered is less than formula_88.

By the naive union bound,
the probability that one of the formula_89 bad events happens
is less than formula_90.
Thus, with positive probability there are no bad events
and formula_3 is a set cover of cost at most formula_71.
QED

The lemma above shows the "existence" of a set cover
of cost formula_93).
In this context our goal is an efficient approximation algorithm,
not just an existence proof, so we are not done.

One approach would be to increase formula_59
a little bit, then show that the probability of success is at least, say, 1/4.
With this modification, repeating the random rounding step a few times
is enough to ensure a successful outcome with high probability.

That approach weakens the approximation ratio.
We next describe a different approach that yields
a deterministic algorithm that is guaranteed to
match the approximation ratio of the existence proof above.
The approach is called the method of conditional probabilities.

The deterministic algorithm emulates the randomized rounding scheme:
it considers each set formula_44 in turn,
and chooses formula_96.
But instead of making each choice "randomly" based on formula_16,
it makes the choice "deterministically", so as to
"keep the conditional probability of failure, given the choices so far, below 1".

We want to be able to set each variable formula_75 in turn
so as to keep the conditional probability of failure below 1.
To do this, we need a good bound on the conditional probability of failure.
The bound will come by refining the original existence proof.
That proof implicitly bounds the probability of failure
by the expectation of the random variable
where
is the set of elements left uncovered at the end.

The random variable formula_101 may appear a bit mysterious,
but it mirrors the probabilistic proof in a systematic way.
The first term in formula_101 comes from applying Markov's inequality
to bound the probability of the first bad event (the cost is too high).
It contributes at least 1 to formula_101 if the cost of formula_3 is too high.
The second term
counts the number of bad events of the second kind (uncovered elements).
It contributes at least 1 to formula_101 if formula_3 leaves any element uncovered.
Thus, in any outcome where formula_101 is less than 1,
formula_3 must cover all the elements
and have cost meeting the desired bound from the lemma.
In short, if the rounding step fails, then formula_109.
This implies (by Markov's inequality) that
"formula_110 is an upper bound on the probability of failure."
Note that the argument above is implicit already in the proof of the lemma,
which also shows by calculation that formula_111.

To apply the method of conditional probabilities,
we need to extend the argument to bound the "conditional" probability of failure
as the rounding step proceeds.
Usually, this can be done in a systematic way,
although it can be technically tedious.

So, what about the "conditional" probability of failure as the rounding step iterates through the sets?
Since formula_109 in any outcome where the rounding step fails,
by Markov's inequality, the "conditional" probability of failure
is at most the "conditional" expectation of formula_101.

Next we calculate the conditional expectation of formula_101,
much as we calculated the unconditioned expectation of formula_101 in the original proof.
Consider the state of the rounding process at the end of some iteration formula_116.
Let formula_117 denote the sets considered so far
(the first formula_116 sets in formula_28).
Let formula_120 denote the (partially assigned) vector formula_3
(so formula_122 is determined only if formula_123).
For each set formula_124,
let formula_125
denote the probability with which formula_75 will be set to 1.
Let formula_127 contain the not-yet-covered elements.
Then the conditional expectation of formula_101,
given the choices made so far, that is, given formula_120, is

Note that formula_131 is determined only after iteration formula_116.

To keep the conditional probability of failure below 1,
it suffices to keep the conditional expectation of formula_101 below 1.
To do this, it suffices to keep the conditional expectation of formula_101 from increasing.
This is what the algorithm will do.
It will set formula_75 in each iteration to ensure that
(where formula_137).

In the formula_116th iteration,
how can the algorithm set formula_139
to ensure that formula_140?
It turns out that it can simply set formula_139
so as to "minimize" the resulting value of formula_142.

To see why, focus on the point in time when iteration formula_116 starts.
At that time, formula_144 is determined,
but formula_142 is not yet determined
--- it can take two possible values depending on how formula_139
is set in iteration formula_116.
Let formula_148 denote the value of formula_149.
Let formula_150 and formula_151,
denote the two possible values of formula_142,
depending on whether formula_139 is set to 0, or 1, respectively.
By the definition of conditional expectation,
Since a weighted average of two quantities
is always at least the minimum of those two quantities,
it follows that
Thus, setting formula_139
so as to minimize the resulting value of
formula_131
will guarantee that
formula_158.
This is what the algorithm will do.

In detail, what does this mean?
Considered as a function of formula_139
formula_131
is a linear function of formula_139,
and the coefficient of formula_139 in that function is

Thus, the algorithm should set formula_139 to 0 if this expression is positive,
and 1 otherwise. This gives the following algorithm.

input: set system formula_28, universe formula_15, cost vector formula_167

output: set cover formula_3 (a solution to the standard integer linear program for set cover)

The algorithm ensures that the conditional expectation of formula_101,
formula_185, does not increase at each iteration.
Since this conditional expectation is initially less than 1 (as shown previously),
the algorithm ensures that the conditional expectation stays below 1.
Since the conditional probability of failure
is at most the conditional expectation of formula_101,
in this way the algorithm
ensures that the conditional probability of failure stays below 1.
Thus, at the end, when all choices are determined,
the algorithm reaches a successful outcome.
That is, the algorithm above returns a set cover formula_3
of cost at most formula_183 times
the minimum cost of any (fractional) set cover.

In the example above, the algorithm was guided by the conditional expectation of a random variable formula_101.
In some cases, instead of an exact conditional expectation,
an "upper bound" (or sometimes a lower bound)
on some conditional expectation is used instead.
This is called a pessimistic estimator.





</doc>
<doc id="25190127" url="https://en.wikipedia.org/wiki?curid=25190127" title="Reservoir sampling">
Reservoir sampling

Reservoir sampling is a family of randomized algorithms for choosing a simple random sample without replacement of items from a population of unknown size in a single pass over the items. The size of the population is not known to the algorithm and is typically too large to fit all items into main memory. The population is revealed to the algorithm over time, and the algorithm cannot look back at previous items. At any point, the current state of the algorithm must permit extraction of a simple random sample without replacement of size over the part of the population seen so far.

Suppose we see a sequence of items, one at a time. We want to keep ten items in memory, and we want them to be selected at random from the sequence. If we know the total number of items and can access the items arbitrarily, then the solution is easy: select 10 distinct indices between 1 and with equal probability, and keep the -th elements. The problem is that we do not always know the exact in advance.

A simple and popular but slow algorithm, commonly known as "Algorithm R", is due to Alan Waterman.

The algorithm works by maintaining a "reservoir" of size , which initially contains the first items of the input. It then iterates over the remaining items until the input is exhausted. Using one-based array indexing, let formula_1 be the index of the item currently under consideration. The algorithm then generates a random number between (and including) 1 and . If is at most , then the item is selected and replaces whichever item currently occupies the -th position in the reservoir. Otherwise, the item is discarded. In effect, for all , the element of the input is chosen to be included in the reservoir with probability formula_2. Similarly, at each iteration the element of the reservoir array is chosen to be replaced with probability formula_3. It can be shown that when the algorithm has finished executing, each item in the input population has equal probability (i.e., formula_4) of being chosen for the reservoir.

While conceptually simple and easy to understand, this algorithm needs to generate a random number for each item of the input, including the items that are discarded. Its asymptotic running time is thus formula_5. This causes the algorithm to be unnecessarily slow if the input population is large.

"Algorithm L" improves upon this algorithm by computing how many items are discarded before the next item enters the reservoir. The key observation is that this number follows a geometric distribution and can therefore be computed in constant time.

This algorithm computes three random numbers for each item that becomes part of the reservoir, and does not spend any time on items that do not. Its expected running time is thus formula_6, which is optimal. At the same time, it is simple to implement efficiently and does not depend on random deviates from exotic or hard-to-compute distributions.

If we associate with each item of the input a uniformly generated random number, the items with the largest (or, equivalently, smallest) associated values form a simple random sample. A simple reservoir-sampling thus maintains the items with the currently largest associated values in a priority queue.

The expected running time of this algorithm is formula_7 and it is relevant mainly because it can easily be extended to items with weights.

Some applications require items' sampling probabilities to be according to weights associated with each item. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. Let the weight of item be formula_8, and the sum of all weights be . There are two ways to interpret weights assigned to each item in the set:

The following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:
ReservoirSample(S[1..?])
This algorithm is identical to the algorithm given in Reservoir Sampling with Random Sort except for the generation of the items' keys. The algorithm is equivalent to assigning each item a key formula_12 where is the random number and then selecting the items with the largest keys. Equivalently, a more numerically stable formulation of this algorithm computes the keys as formula_13 and select the items with the "smallest" keys.

Following algorithm was given by M. T. Chao uses interpretation 2:
WeightedReservoir-Chao(S[1..n], R[1..k])

For each item, its relative weight is calculated and used to randomly decide if the item will be added into the reservoir. If the item is selected, then one of the existing items of the reservoir is uniformly selected and replaced with the new item. The trick here is that, if the probabilities of all items in the reservoir are already proportional to their weights, then by selecting uniformly which item to replace, the probabilities of all items remain proportional to their weight after the replacement.

Suppose one wanted to draw random cards from a deck of cards.
A natural approach would be to shuffle the deck and then take the top cards.
In the general case, the shuffle also needs to work even if the number of cards in the deck is not known in advance, a condition which is satisfied by the inside-out version of the Fisher–Yates shuffle:
Shuffle(S[1..n], R[1..n])
Note that although the rest of the cards are shuffled, only the first are important in the present context.
Therefore, the array need only track the cards in the first positions while performing the shuffle, reducing the amount of memory needed.
Truncating to length , the algorithm is modified accordingly:

ReservoirSample(S[1..n], R[1..k])
Since the order of the first cards is immaterial, the first loop can be removed and can be initialized to be the first items of the input.
This yields "Algorithm R".

Probabilities of selection of the reservoir methods are discussed in Chao (1982) and Tillé (2006). While the first-order selection probabilities are equal to formula_4 (or, in case of Chao's procedure, to an arbitrary set of unequal probabilities), the second order selection probabilities depend on the order in which the records are sorted in the original reservoir. The problem is overcome by the cube sampling method of Deville and Tillé (2004).

Reservoir sampling makes the assumption that the desired sample fits into main memory, often implying that is a constant independent of . In applications where we would like to select a large subset of the input list (say a third, i.e. formula_15), other methods need to be adopted. Distributed implementations for this problem have been proposed.


</doc>
<doc id="219861" url="https://en.wikipedia.org/wiki?curid=219861" title="In-place algorithm">
In-place algorithm

In computer science, an in-place algorithm is an algorithm which transforms input using no auxiliary data structure. However a small amount of extra storage space is allowed for auxiliary variables. The input is usually overwritten by the output as the algorithm executes. In-place algorithm updates input sequence only through replacement or swapping of elements. An algorithm which is not in-place is sometimes called not-in-place or out-of-place.

In-place can have slightly different meanings. In its strictest form, the algorithm can only have a constant amount of extra space, counting everything including function calls and pointers. However, this form is very limited as simply having an index to a length n array requires O(log "n") bits. More broadly, in-place means that the algorithm does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log "n"), though sometimes anything in o("n") is allowed. Note that space complexity also has varied choices in whether or not to count the index lengths as part of the space used. Often, the space complexity is given in terms of the number of indices or pointers needed, ignoring their length. In this article, we refer to total space complexity (DSPACE), counting pointer lengths. Therefore, the space requirements here have an extra log "n" factor compared to an analysis that ignores the length of indices and pointers. 

An algorithm may or may not count the output as part of its space usage. Since in-place algorithms usually overwrite their input with output, no additional space is needed. When writing the output to write-only memory or a stream, it may be more appropriate to only consider the working space of the algorithm. In theory applications such as log-space reductions, it is more typical to always ignore output space (in these cases it is more essential that the output is "write-only").

Given an array codice_1 of "n" items, suppose we want an array that holds the same elements in reversed order and dispose of the original. One seemingly simple way to do this is to create a new array of equal size, fill it with copies from codice_1 in appropriate order and then delete codice_1.

Unfortunately, this requires O("n") extra space for having the arrays codice_1 and codice_5 available simultaneously. Also, allocation and deallocation are often slow operations. Since we no longer need codice_1, we can instead overwrite it with its own reversal using this in-place algorithm which will only need constant number (2) of integers for the auxiliary variables codice_7 and codice_8, no matter how large the array is.

As another example, many sorting algorithms rearrange arrays into sorted order in-place, including: bubble sort, comb sort, selection sort, insertion sort, heapsort, and Shell sort. These algorithms require only a few pointers, so their space complexity is .

Quicksort operates in-place on the data to be sorted. However, quicksort requires stack space pointers to keep track of the subarrays in its divide and conquer strategy. Consequently, quicksort needs additional space. Although this non-constant space technically takes quicksort out of the in-place category, quicksort and other algorithms needing only additional pointers are usually considered in-place algorithms.

Most selection algorithms are also in-place, although some considerably rearrange the input array in the process of finding the final, constant-sized result.

Some text manipulation algorithms such as trim and reverse may be done in-place.

In computational complexity theory, the strict definition of in-place algorithms includes all algorithms with O(1) space complexity, the class DSPACE(1). This class is very limited; it equals the regular languages. In fact, it does not even include any of the examples listed above.

We usually consider algorithms in L, the class of problems requiring O(log "n") additional space, to be in-place. This class is more in line with the practical definition, as it allows numbers of size n as pointers or indices. This expanded definition still excludes quicksort, however, because of its recursive calls. 

Identifying the in-place algorithms with L has some interesting implications; for example, it means that there is a (rather complex) in-place algorithm to determine whether a path exists between two nodes in an undirected graph, a problem that requires O("n") extra space using typical algorithms such as depth-first search (a visited bit for each node). This in turn yields in-place algorithms for problems such as determining if a graph is bipartite or testing whether two graphs have the same number of connected components. See SL for more information.

In many cases, the space requirements for an algorithm can be drastically cut by using a randomized algorithm. For example, say we wish to know if two vertices in a graph of "n" vertices are in the same connected component of the graph. There is no known simple, deterministic, in-place algorithm to determine this, but if we simply start at one vertex and perform a random walk of about 20"n" steps, the chance that we will stumble across the other vertex provided that it is in the same component is very high. Similarly, there are simple randomized in-place algorithms for primality testing such as the Miller-Rabin primality test, and there are also simple in-place randomized factoring algorithms such as Pollard's rho algorithm. See RL and BPL for more discussion of this phenomenon.

Functional programming languages often discourage or don't support explicit in-place algorithms that overwrite data, since this is a type of side effect; instead, they only allow new data to be constructed. However, good functional language compilers will often recognize when an object very similar to an existing one is created and then the old one thrown away, and will optimize this into a simple mutation "under-the-hood".

Note that it is possible in principle to carefully construct in-place algorithms that don't modify data (unless the data is no longer being used), but this is rarely done in practice. See purely functional data structures.



</doc>
<doc id="416776" url="https://en.wikipedia.org/wiki?curid=416776" title="Timeline of algorithms">
Timeline of algorithms

The following timeline outlines the development of algorithms (mainly "mathematical recipes") since their inception.












</doc>
<doc id="6901703" url="https://en.wikipedia.org/wiki?curid=6901703" title="Algorithm characterizations">
Algorithm characterizations

Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the "characterizations" of the notion of "algorithm" in more detail.

Over the last 200 years the definition of algorithm has become more complicated and detailed as researchers have tried to pin down the term. Indeed, there may be more than one type of "algorithm". But most agree that algorithm has something to do with defining generalized processes for the creation of "output" integers from other "input" integers – "input parameters" arbitrary and infinite in extent, or limited in extent but still variable—by the manipulation of distinguishable symbols (counting numbers) with finite collections of rules that a person can perform with paper and pencil.

The most common number-manipulation schemes—both in formal mathematics and in routine life—are: (1) the recursive functions calculated by a person with paper and pencil, and (2) the Turing machine or its Turing equivalents—the primitive register machine or "counter machine" model, the Random Access Machine model (RAM), the Random access stored program machine model (RASP) and its functional equivalent "the computer".

When we are doing "arithmetic" we are really calculating by the use of "recursive functions" in the shorthand algorithms we learned in grade-school, for example, adding and subtracting.

The proofs that every "recursive function" we can "calculate by hand" we can "compute by machine" and vice versa—note the usage of the words "calculate" versus "compute"—is remarkable. But this equivalence together with the "thesis" (unproven assertion) that this includes "every" calculation/computation indicates why so much emphasis has been placed upon the use of Turing-equivalent machines in the definition of specific algorithms, and why the definition of "algorithm" itself often refers back to "the Turing machine". This is discussed in more detail under Stephen Kleene's characterization.

The following are summaries of the more famous characterizations (Kleene, Markov, Knuth) together with those that introduce novel elements—elements that further expand the definition or contribute to a more precise definition.

There is more consensus on the "characterization" of the notion of "simple algorithm".

All algorithms need to be specified in a formal language, and the "simplicity notion" arises from the simplicity of the language. The Chomsky (1956) hierarchy is a containment hierarchy of classes of formal grammars that generate formal languages. It is used for classifying of programming languages and abstract machines.

From the "Chomsky hierarchy" perspective, if the algorithm can be specified on a simpler language (than unrestricted), it can be characterized by this kind of language, else it is a typical "unrestricted algorithm".

Examples: a "general purpose" macro language, like M4 is unrestricted (Turing complete), but the C preprocessor macro language is not, so any algorithm expressed in "C preprocessor" is a "simple algorithm".

See also Relationships between complexity classes.

The following are the features of a good algorithm;



In early 1870 W. Stanley Jevons presented a "Logical Machine" (Jevons 1880:200) for analyzing a syllogism or other logical form e.g. an argument reduced to a Boolean equation. By means of what Couturat (1914) called a "sort of "logical piano" [,] ... the equalities which represent the premises ... are "played" on a keyboard like that of a typewriter. ... When all the premises have been "played", the panel shows only those constituents whose sum is equal to 1, that is, ... its logical whole. This mechanical method has the advantage over VENN's geometrical method..." (Couturat 1914:75).

For his part John Venn, a logician contemporary to Jevons, was less than thrilled, opining that "it does not seem to me that any contrivances at present known "or likely to be discovered" really deserve the name of logical machines" (italics added, Venn 1881:120). But of historical use to the developing notion of "algorithm" is his explanation for his negative reaction with respect to a machine that "may subserve a really valuable purpose by enabling us to avoid otherwise inevitable labor":
He concludes that "I cannot see that any machine can hope to help us except in the third of these steps; so that it seems very doubtful whether any thing of this sort really deserves the name of a logical engine."(Venn 1881:119–121).

This section is longer and more detailed than the others because of its importance to the topic: Kleene was the first to propose that "all" calculations/computations—of "every" sort, the "totality" of—can "equivalently" be (i) "calculated" by use of five "primitive recursive operators" plus one special operator called the mu-operator, or be (ii) "computed" by the actions of a Turing machine or an equivalent model.

Furthermore, he opined that either of these would stand as a definition of algorithm.

A reader first confronting the words that follow may well be confused, so a brief explanation is in order. "Calculation" means done by hand, "computation" means done by Turing machine (or equivalent). (Sometimes an author slips and interchanges the words). A "function" can be thought of as an "input-output box" into which a person puts natural numbers called "arguments" or "parameters" (but only the counting numbers including 0—the nonnegative integers) and gets out a single nonnegative integer (conventionally called "the answer"). Think of the "function-box" as a little man either calculating by hand using "general recursion" or computing by Turing machine (or an equivalent machine).

"Effectively calculable/computable" is more generic and means "calculable/computable by "some" procedure, method, technique ... whatever...". "General recursive" was Kleene's way of writing what today is called just "recursion"; however, "primitive recursion"—calculation by use of the five recursive operators—is a lesser form of recursion that lacks access to the sixth, additional, mu-operator that is needed only in rare instances. Thus most of life goes on requiring only the "primitive recursive functions."

In 1943 Kleene proposed what has come to be known as Church's thesis:

In a nutshell: to calculate "any" function the only operations a person needs (technically, formally) are the 6 primitive operators of "general" recursion (nowadays called the operators of the mu recursive functions).

Kleene's first statement of this was under the section title "12. Algorithmic theories". He would later amplify it in his text (1952) as follows:

This is not as daunting as it may sound – "general" recursion is just a way of making our everyday arithmetic operations from the five "operators" of the primitive recursive functions together with the additional mu-operator as needed. Indeed, Kleene gives 13 examples of primitive recursive functions and Boolos–Burgess–Jeffrey add some more, most of which will be familiar to the reader—e.g. addition, subtraction, multiplication and division, exponentiation, the CASE function, concatenation, etc., etc.; for a list see Some common primitive recursive functions.
Why general-recursive functions rather than primitive-recursive functions?

Kleene et al. (cf §55 General recursive functions p. 270 in Kleene 1952) had to add a sixth recursion operator called the minimization-operator (written as μ-operator or mu-operator) because Ackermann (1925) produced a hugely growing function—the Ackermann function—and Rózsa Péter (1935) produced a general method of creating recursive functions using Cantor's diagonal argument, neither of which could be described by the 5 primitive-recursive-function operators. With respect to the Ackermann function:
But the need for the mu-operator is a rarity. As indicated above by Kleene's list of common calculations, a person goes about their life happily computing primitive recursive functions without fear of encountering the monster numbers created by Ackermann's function (e.g. super-exponentiation ).

Turing's Thesis hypothesizes the computability of "all computable functions" by the Turing machine model and its equivalents.

To do this in an effective manner, Kleene extended the notion of "computable" by casting the net wider—by allowing into the notion of "functions" both "total functions" and "partial functions". A "total function" is one that is defined "for all natural numbers" (positive integers including 0). A partial function is defined for "some" natural numbers but not all—the specification of "some" has to come "up front". Thus the inclusion of "partial function" extends the notion of function to "less-perfect" functions. Total- and partial-functions may either be calculated by hand or computed by machine.

We now observe Kleene's definition of "computable" in a formal sense:

Thus we have arrived at "Turing's Thesis":

Although Kleene did not give examples of "computable functions" others have. For example, Davis (1958) gives Turing tables for the Constant, Successor and Identity functions, three of the five operators of the primitive recursive functions:

Boolos–Burgess–Jeffrey (2002) give the following as prose descriptions of Turing machines for:

With regards to the counter machine, an abstract machine model equivalent to the Turing machine:

Demonstrations of computability by abacus machine (Boolos–Burgess–Jeffrey (2002)) and by counter machine (Minsky 1967):

The fact that the abacus/counter machine models can simulate the recursive functions provides the proof that: If a function is "machine computable" then it is "hand-calculable by partial recursion". Kleene's Theorem XXIX :

The converse appears as his Theorem XXVIII. Together these form the proof of their equivalence, Kleene's Theorem XXX.

With his Theorem XXX Kleene proves the "equivalence" of the two "Theses"—the Church Thesis and the Turing Thesis. (Kleene can only hypothesize (conjecture) the truth of both thesis – "these he has not proven"):

Thus by Kleene's Theorem XXX: either method of making numbers from input-numbers—recursive functions calculated by hand or computated by Turing-machine or equivalent—results in an ""effectively calculable/computable" function". If we accept the hypothesis that "every" calculation/computation can be done by either method equivalently we have accepted both Kleene's Theorem XXX (the equivalence) and the Church–Turing Thesis (the hypothesis of "every").

The notion of separating out Church's and Turing's theses from the "Church–Turing thesis" appears not only in Kleene (1952) but in Blass-Gurevich (2003) as well. But while there are agreements, there are disagreements too:

Andrey Markov Jr. (1954) provided the following definition of algorithm:

He admitted that this definition "does not pretend to mathematical precision" (p. 1). His 1954 monograph was his attempt to define algorithm more accurately; he saw his resulting definition—his "normal" algorithm—as "equivalent to the concept of a recursive function" (p. 3). His definition included four major components (Chapter II.3 pp. 63ff):

In his Introduction Markov observed that "the entire significance for mathematics" of efforts to define algorithm more precisely would be "in connection with the problem of a constructive foundation for mathematics" (p. 2). Ian Stewart (cf Encyclopædia Britannica) shares a similar belief: "...constructive analysis is very much in the same algorithmic spirit as computer science...". For more see constructive mathematics and Intuitionism.

Distinguishability and Locality: Both notions first appeared with Turing (1936–1937) --

Locality appears prominently in the work of Gurevich and Gandy (1980) (whom Gurevich cites). Gandy's "Fourth Principle for Mechanisms" is "The Principle of Local Causality":

1936: A rather famous quote from Kurt Gödel appears in a "Remark added in proof [of the original German publication] in his paper "On the Length of Proofs" translated by Martin Davis appearing on pp. 82–83 of "The Undecidable". A number of authors—Kleene, Gurevich, Gandy etc. -- have quoted the following:

1963: In a "Note" dated 28 August 1963 added to his famous paper "On Formally Undecidable Propositions" (1931) Gödel states (in a footnote) his belief that "formal systems" have "the characteristic property that reasoning in them, in principle, can be completely replaced by mechanical devices" (p. 616 in van Heijenoort). ". . . due to "A. M. Turing's work a precise and unquestionably adequate definition of the general notion of formal system can now be given [and] a completely general version of Theorems VI and XI is now possible." (p. 616). In a 1964 note to another work he expresses the same opinion more strongly and in more detail.

1964: In a Postscriptum, dated 1964, to a paper presented to the Institute for Advanced Study in spring 1934, Gödel amplified his conviction that "formal systems" are those that can be mechanized:

The * indicates a footnote in which Gödel cites the papers by Alan Turing (1937) and Emil Post (1936) and then goes on to make the following intriguing statement:

Church's definitions encompass so-called "recursion" and the "lambda calculus" (i.e. the λ-definable functions). His footnote 18 says that he discussed the relationship of "effective calculatibility" and "recursiveness" with Gödel but that he independently questioned "effectively calculability" and "λ-definability":

It would appear from this, and the following, that far as Gödel was concerned, the Turing machine was sufficient and the lambda calculus was "much less suitable." He goes on to make the point that, with regards to limitations on human reason, the jury is still out:

Minsky (1967) baldly asserts that "an algorithm is "an effective procedure" and declines to use the word "algorithm" further in his text; in fact his index makes it clear what he feels about "Algorithm, "synonym" for Effective procedure"(p. 311):

Other writers (see Knuth below) use the word "effective procedure". This leads one to wonder: What is Minsky's notion of "an effective procedure"? He starts off with:

But he recognizes that this is subject to a criticism:

His refinement? To "specify, along with the statement of the rules, "the details of the mechanism that is to interpret them"". To avoid the "cumbersome" process of "having to do this over again for each individual procedure" he hopes to identify a "reasonably "uniform" family of rule-obeying mechanisms". His "formulation":

In the end, though, he still worries that "there remains a subjective aspect to the matter. Different people may not agree on whether a certain procedure should be called effective" (p. 107)

But Minsky is undeterred. He immediately introduces "Turing's Analysis of Computation Process" (his chapter 5.2). He quotes what he calls "Turing's "thesis""

After an analysis of "Turing's Argument" (his chapter 5.3)
he observes that "equivalence of many intuitive formulations" of Turing, Church, Kleene, Post, and Smullyan "...leads us to suppose that there is really here an 'objective' or 'absolute' notion. As Rogers [1959] put it:

In his 1967 "Theory of Recursive Functions and Effective Computability" Hartley Rogers' characterizes "algorithm" roughly as "a clerical (i.e., deterministic, bookkeeping) procedure . . . applied to . . . symbolic "inputs" and which will eventually yield, for each such input, a corresponding symbolic "output""(p. 1). He then goes on to describe the notion "in approximate and intuitive terms" as having 10 "features", 5 of which he asserts that "virtually all mathematicians would agree [to]" (p. 2). The remaining 5 he asserts "are less obvious than *1 to *5 and about which we might find less general agreement" (p. 3).

The 5 "obvious" are:

The remaining 5 that he opens to debate, are:

Knuth (1968, 1973) has given a list of five properties that are widely accepted as requirements for an algorithm:


Knuth offers as an example the Euclidean algorithm for determining the greatest common divisor of two natural numbers (cf. Knuth Vol. 1 p. 2).

Knuth admits that, while his description of an algorithm may be intuitively clear, it lacks formal rigor, since it is not exactly clear what "precisely defined" means, or "rigorously and unambiguously specified" means, or "sufficiently basic", and so forth. He makes an effort in this direction in his first volume where he defines "in detail" what he calls the "machine language" for his "mythical MIX...the world's first polyunsaturated computer" (pp. 120ff). Many of the algorithms in his books are written in the MIX language. He also uses tree diagrams, flow diagrams and state diagrams.

"Goodness" of an algorithm, "best" algorithms: Knuth states that "In practice, we not only want algorithms, we want "good" algorithms..." He suggests that some criteria of an algorithm's goodness are the number of steps to perform the algorithm, its "adaptability to computers, its simplicity and elegance, etc." Given a number of algorithms to perform the same computation, which one is "best"? He calls this sort of inquiry "algorithmic analysis: given an algorithm, to determine its performance characteristcis" (all quotes this paragraph: Knuth Vol. 1 p. 7)

Stone (1972) and Knuth (1968, 1973) were professors at Stanford University at the same time so it is not surprising if there are similarities in their definitions (boldface added for emphasis):

Stone is noteworthy because of his detailed discussion of what constitutes an “effective” rule – his robot, or person-acting-as-robot, must have some information and abilities within them, and if not the information and the ability must be provided in "the algorithm":

Furthermore, "...not all instructions are acceptable, because they may require the robot to have abilities beyond those that we consider reasonable.” He gives the example of a robot confronted with the question is “Henry VIII a King of England?” and to print 1 if yes and 0 if no, but the robot has not been previously provided with this information. And worse, if the robot is asked if Aristotle was a King of England and the robot only had been provided with five names, it would not know how to answer. Thus:

After providing us with his definition, Stone introduces the Turing machine model and states that the set of five-tuples that are the machine’s instructions are “an algorithm ... known as a Turing machine program” (p. 9). Immediately thereafter he goes on say that a “"computation" of a Turing machine is "described" by stating:

This precise prescription of what is required for "a computation" is in the spirit of what will follow in the work of Blass and Gurevich.

While a student at Princeton in the mid-1960s, David Berlinski was a student of Alonzo Church (cf p. 160). His year-2000 book "The Advent of the Algorithm: The 300-year Journey from an Idea to the Computer" contains the following definition of algorithm:

A careful reading of Gurevich 2000 leads one to conclude (infer?) that he believes that "an algorithm" is actually "a Turing machine" or "a pointer machine" doing a computation. An "algorithm" is not just the symbol-table that guides the behavior of the machine, nor is it just one instance of a machine doing a computation given a particular set of input parameters, nor is it a suitably programmed machine with the power off; rather "an algorithm is the machine actually doing any computation of which it is capable". Gurevich does not come right out and say this, so as worded above this conclusion (inference?) is certainly open to debate:

In Blass and Gurevich 2002 the authors invoke a dialog between "Quisani" ("Q") and "Authors" (A), using Yiannis Moshovakis as a foil, where they come right out and flatly state:

This use of the word "implementation" cuts straight to the heart of the question. Early in the paper, Q states his reading of Moshovakis:

But the authors waffle here, saying "[L]et's stick to "algorithm" and "machine", and the reader is left, again, confused. We have to wait until Dershowitz and Gurevich 2007 to get the following footnote comment:

Blass and Gurevich describe their work as evolved from consideration of Turing machines and pointer machines, specifically Kolmogorov-Uspensky machines (KU machines), Schönhage Storage Modification Machines (SMM), and linking automata as defined by Knuth. The work of Gandy and Markov are also described as influential precursors.

Gurevich offers a 'strong' definition of an algorithm (boldface added):

The above phrase computation as an evolution of the state differs markedly from the definition of Knuth and Stone—the "algorithm" as a Turing machine program. Rather, it corresponds to what Turing called "the complete configuration" (cf Turing's definition in Undecidable, p. 118) -- and includes "both" the current instruction (state) "and" the status of the tape. [cf Kleene (1952) p. 375 where he shows an example of a tape with 6 symbols on it—all other squares are blank—and how to Gödelize its combined table-tape status].

In Algorithm examples we see the evolution of the state first-hand.

Philosopher Daniel Dennett analyses the importance of evolution as an algorithmic process in his 1995 book "Darwin's Dangerous Idea". Dennett identifies three key features of an algorithm:

It is on the basis of this analysis that Dennett concludes that "According to Darwin, evolution is an algorithmic process". (p. 60).

However, in the previous page he has gone out on a much-further limb. In the context of his chapter titled "Processes as Algorithms", he states:

It is unclear from the above whether Dennett is stating that the physical world by itself and without observers is intrinsically algorithmic (computational) or whether a symbol-processing observer is what is adding "meaning" to the observations.

Daniel Dennett is a proponent of strong artificial intelligence: the idea that the logical structure of an algorithm is sufficient to explain mind. John Searle, the creator of the Chinese room thought experiment, claims that "syntax [that is, logical structure] is by itself not sufficient for semantic content [that is, meaning]" . In other words, the "meaning" of symbols is relative to the mind that is using them; an algorithm—a logical construct—by itself is insufficient for a mind.

Searle cautions those who claim that algorithmic (computational) processes are intrinsic to nature (for example, cosmologists, physicists, chemists, etc.):

An example in Boolos-Burgess-Jeffrey (2002) (pp. 31–32) demonstrates the precision required in a complete specification of an algorithm, in this case to add two numbers: m+n. It is similar to the Stone requirements above.

(i) They have discussed the role of "number format" in the computation and selected the "tally notation" to represent numbers:

(ii) At the outset of their example they specify the machine to be used in the computation as a Turing machine. They have previously specified (p. 26) that the Turing-machine will be of the 4-tuple, rather than 5-tuple, variety. For more on this convention see Turing machine.

(iii) Previously the authors have specified that the tape-head's position will be indicated by a subscript to the "right" of the scanned symbol. For more on this convention see Turing machine. (In the following, boldface is added for emphasis):

This specification is incomplete: it requires the location of where the instructions are to be placed and their format in the machine--

This later point is important. Boolos-Burgess-Jeffrey give a demonstration (p. 36) that the predictability of the entries in the table allow one to "shrink" the table by putting the entries in sequence and omitting the input state and the symbol. Indeed, the example Turing machine computation required only the 4 columns as shown in the table below (but note: these were presented to the machine in "rows"):

Sipser begins by defining '"algorithm" as follows:

Does Sipser mean that "algorithm" is just "instructions" for a Turing machine, or is the combination of "instructions + a (specific variety of) Turing machine"? For example, he defines the two standard variants (multi-tape and non-deterministic) of his particular variant (not the same as Turing's original) and goes on, in his Problems (pages 160-161), to describes four more variants (write-once, doubly infinite tape (i.e. left- and right-infinite), left reset, and "stay put instead of left). In addition, he imposes some constraints. First, the input must be encoded as a string (p. 157) and says of numeric encodings in the context of complexity theory:

van Emde Boas comments on a similar problem with respect to the random access machine (RAM) abstract model of computation sometimes used in place of the Turing machine when doing "analysis of algorithms":
"The absence or presence of multiplicative and parallel bit manipulation operations is of relevance for the correct understanding of some results in the analysis of algorithms.

". . . [T]here hardly exists such as a thing as an "innocent" extension of the standard RAM model in the uniform time measures; either one only has additive arithmetic or one might as well include all reasonable multiplicative and/or bitwise Boolean instructions on small operands." (van Emde Boas, 1990:26)

With regards to a "description language" for algorithms Sipser finishes the job that Stone and Boolos-Burgess-Jeffrey started (boldface added). He offers us three levels of description of Turing machine algorithms (p. 157):



</doc>
<doc id="2842189" url="https://en.wikipedia.org/wiki?curid=2842189" title="Hyphenation algorithm">
Hyphenation algorithm

A hyphenation algorithm is a set of rules, especially one codified for implementation in a computer program, that decides at which points a word can be broken over two lines with a hyphen. For example, a hyphenation algorithm might decide that "impeachment" can be broken as "impeach-ment" or "im-peachment" but not "impe-achment".

One of the reasons for the complexity of the rules of word-breaking is that different "dialects" of English tend to differ on hyphenation: American English tends to work on sound, but British English tends to look to the origins of the word and then to sound. There are also a large number of exceptions, which further complicates matters.

Some rules of thumb can be found in the Major Keary's: "On Hyphenation – Anarchy of Pedantry." Among the algorithmic approaches to hyphenation, the one implemented in the TeX typesetting system is widely used. It is thoroughly documented in the first two volumes of "Computers and Typesetting" and in Francis Mark Liang's dissertation. The aim of Liang's work was to get the algorithm as accurate as he practically could and to keep any exception dictionary small.

In TeX's original hyphenation patterns for American English, the exception list contains only 14 words.

Ports of the TeX hyphenation algorithm are available as libraries for several programming languages, including Haskell, JavaScript, Perl, PostScript, Python, Ruby, C#, and TeX can be made to show hyphens in the log by the command codice_1.

In LaTeX, hyphenation correction can be added by users by using:

The codice_2 command declares allowed hyphenation points in which words is a list of words, separated by spaces, in which each hyphenation point is indicated by a codice_3 character. For example,

declares that in the current job "fortran" should not be hyphenated and that if "ergonomic" must be hyphenated, it will be at one of the indicated points.

However, there are several limits. For example, the stock codice_2 command accepts only ASCII letters by default and so it cannot be used to correct hyphenation for words with non-ASCII characters (like "ä", "é", "ç"), which are very common in almost all languages except English. Simple workarounds exist, however.




</doc>
<doc id="7347241" url="https://en.wikipedia.org/wiki?curid=7347241" title="Spreading activation">
Spreading activation

Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or "activation" and then iteratively propagating or "spreading" that activation out to other nodes linked to the source nodes. Most often these "weights" are real values that decay as activation propagates through the network. When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.

Spreading activation models are used in cognitive psychology to model the fan out effect.

Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.

As it relates to cognitive psychology, spreading activation is the theory of how the brain iterates through a network of associated ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept. For memory psychology, Spreading activation model means people organize their knowledge of the world based on their personal experience, which is saying those personal experiences form the network of ideas that is the person's knowledge of the world.

When a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word "doctor" when it is preceded by "nurse" than when it is preceded by an unrelated word like "carrot". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.

As another example, if the original concept is "red" and the concept "vehicles" is primed, they are much more likely to say "fire engine" instead of something unrelated to vehicles, such as "cherries". If instead "fruits" was primed, they would likely name "cherries" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.

A directed graph is populated by Nodes[ 1...N ] each having an associated activation value A [ i ] which is a real number in the range [ 0.0 ... 1.0]. A Link[ i, j ] connects source node[ i ] with target node[ j ]. Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].

Parameters:

Steps:




</doc>
<doc id="26784161" url="https://en.wikipedia.org/wiki?curid=26784161" title="Manhattan address algorithm">
Manhattan address algorithm

The Manhattan address algorithm refers to the formulas used to estimate the closest east–west cross street for building numbers on north–south avenues in the New York City borough of Manhattan.

To find the approximate number of the closest cross street, divide the building number by a divisor (generally 20) and add (or subtract) the "magic number" from the table below:


</doc>
<doc id="35685954" url="https://en.wikipedia.org/wiki?curid=35685954" title="Generalized distributive law">
Generalized distributive law

The generalized distributive law (GDL) is a generalization of the distributive property which gives rise to a general message passing algorithm. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. The law and algorithm were introduced in a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.

"The distributive law in mathematics is the law relating the operations of multiplication and addition, stated symbolically, formula_1; that is, the monomial factor formula_2 is distributed, or separately applied, to each term of the binomial factor formula_3, resulting in the product formula_4" - Britannica

As it can be observed from the definition, application of distributive law to an arithmetic expression reduces the number of operations in it. In the previous example the total number of operations reduced from three (two multiplications and an addition in formula_4) to two (one multiplication and one addition in formula_6). Generalization of distributive law leads to a large family of fast algorithms. This includes the FFT and Viterbi algorithm.

This is explained in a more formal way in the example below:

formula_7 where formula_8 and formula_9 are real-valued functions, formula_10 and formula_11 (say)

Here we are "marginalizing out" the independent variables (formula_12, formula_13, and formula_14) to obtain the result. When we are calculating the computational complexity, we can see that for each formula_15 pairs of formula_16, there are formula_17 terms due to the triplet formula_18 which needs to take part in the evaluation of formula_19 with each step having one addition and one multiplication. Therefore, the total number of computations needed is formula_20. Hence the asymptotic complexity of the above function is formula_21.

If we apply the distributive law to the RHS of the equation, we get the following:

This implies that formula_23 can be described as a product formula_24 where formula_25 and formula_26

Now, when we are calculating the computational complexity, we can see that there are formula_17 additions in formula_28 and formula_29 each and there are formula_30 multiplications when we are using the product formula_24 to evaluate formula_23. Therefore, the total number of computations needed is formula_33. Hence the asymptotic complexity of calculating formula_34 reduces to formula_35 from formula_36. This shows by an example that applying distributive law reduces the computational complexity which is one of the good features of a "fast algorithm".

Some of the problems that used distributive law to solve can be grouped as follows

1. Decoding algorithms<br>
A GDL like algorithm was used by Gallager's for decoding low density parity-check codes. Based on Gallager's work Tanner introduced the Tanner graph and expressed Gallagers work in message passing form. The tanners graph also helped explain the Viterbi algorithm.

It is observed by Forney that Viterbi's maximum likelihood decoding of convolutional codes also used algorithms of GDL-like generality.

2. Forward-backward algorithm<br>
The forward backward algorithm helped as an algorithm for tracking the states in the markov chain. And this also was used the algorithm of GDL like generality

3. Artificial intelligence<br>
The notion of junction trees has been used to solve many problems in AI. Also the concept of bucket elimination used many of the concepts.

MPF or marginalize a product function is a general computational problem which as special case includes many classical problems such as computation of discrete Hadamard transform, maximum likelihood decoding of a linear code over a memory-less channel, and matrix chain multiplication. The power of the GDL lies in the fact that it applies to situations in which additions and multiplications are generalized.
A commutative semiring is a good framework for explaining this behavior. It is defined over a set formula_37 with operators "formula_38" and "formula_39" where formula_40 and formula_41 are a commutative monoids and the distributive law holds.

Let formula_42 be variables such that formula_43 where formula_44 is a finite set and formula_45. Here formula_46. If formula_47 and formula_48, let
formula_49,
formula_50, 
formula_51, 
formula_52, and
formula_53

Let formula_54 where formula_55. Suppose a function is defined as formula_56, where formula_57 is a commutative semiring. Also, formula_58 are named the "local domains" and formula_59 as the "local kernels".

Now the global kernel formula_60 is defined as :
formula_61

"Definition of MPF problem": For one or more indices formula_62, compute a table of the values of formula_63-"marginalization" of the global kernel formula_64, which is the function formula_65 defined as formula_66

Here formula_67 is the complement of formula_63 with respect to formula_69 and the formula_70 is called the formula_71 "objective function", or the "objective function" at formula_72. It can observed that the computation of the formula_71 objective function in the obvious way needs formula_74 operations. This is because there are formula_75 additions and formula_76 multiplications needed in the computation of the formula_77 objective function. The GDL algorithm which is explained in the next section can reduce this computational complexity.

The following is an example of the MPF problem. 
Let formula_78 and formula_79 be variables such that formula_80 and formula_81. Here formula_82 and formula_83. The given functions using these variables are formula_84 and formula_85 and we need to calculate formula_86 and formula_87 defined as:

Here local domains and local kernels are defined as follows: 
where formula_90 is the formula_91 objective function and formula_87 is the formula_93 objective function.

Consider another example where formula_94 and formula_95 is a real valued function. Now, we shall consider the MPF problem where the commutative semiring is defined as the set of real numbers with ordinary addition and multiplication and the local domains and local kernels are defined as follows:

Now since the global kernel is defined as the product of the local kernels, it is

and the objective function at the local domain formula_97 is

This is the Hadamard transform of the function formula_8. Hence we can see that the computation of Hadamard transform is a special case of the MPF problem. More examples can be demonstrated to prove that the MPF problem forms special cases of many classical problem as explained above whose details can be found at

If one can find a relationship among the elements of a given set formula_100, then one can solve the MPF problem basing on the notion of belief propagation which is a special use of "message passing" technique. The required relationship is that the given set of local domains can be organised into a junction tree. In other words, we create a graph theoretic tree with the elements of formula_100 as the vertices of the tree formula_102, such that for any two arbitrary vertices say formula_103 and formula_104 where formula_105 and there exists an edge between these two vertices, then the intersection of corresponding labels, viz formula_106, is a subset of the label on each vertex on the unique path from formula_103 to formula_104.

For example,

Example 1: Consider the following nine local domains:


For the above given set of local domains, one can organize them into a junction tree as shown below:

Similarly If another set like the following is given

Example 2: Consider the following four local domains:


Then constructing the tree only with these local domains is not possible since this set of values has no common domains which can be placed between any two values of the above set. But however if add the two dummy domains as shown below then organizing the updated set into a junction tree would be possible and easy too.

5.formula_122,formula_123
6.formula_124,formula_123

Similarly for these set of domains, the junction tree looks like shown below:
Input: A set of local domains.
Output: For the given set of domains, possible minimum number of operations that is required to solve the problem is computed. 
So, if formula_103 and formula_104 are connected by an edge in the junction tree, then a message from formula_103 to formula_104 is a set/table of values given by a function: formula_130:formula_131. To begin with all the functions i.e. for all combinations of formula_132 and formula_133 in the given tree, formula_130 is defined to be identically formula_135 and when a particular message is update, it follows the equation given below.

where formula_138 means that formula_139 is an adjacent vertex to formula_103 in tree.

Similarly each vertex has a state which is defined as a table containing the values from the function formula_141, Just like how messages initialize to 1 identically, state of formula_103 is defined to be local kernel formula_143, but whenever formula_144 gets updated, it follows the following equation:

For the given set of local domains as input, we find out if we can create a junction tree, either by using the set directly or by adding dummy domains to the set first and then creating the junction tree, if construction junction is not possible then algorithm output that there is no way to reduce the number of steps to compute the given equation problem, but once we have junction tree, algorithm will have to schedule messages and compute states, by doing these we can know where steps can be reduced, hence will be discusses this below.

There are two special cases we are going to talk about here namely "Single Vertex Problem" in which the objective function is computed at only one vertex formula_146 and the second one is "All Vertices Problem" where the goal is to compute the objective function at all vertices.

Lets begin with the single-vertex problem, GDL will start by directing each edge towards the targeted vertex formula_147. Here messages are sent only in the direction towards the targeted vertex. Note that all the directed messages are sent only once. The messages are started from the leaf nodes(where the degree is 1) go up towards the target vertex formula_147. The message travels from the leaves to its parents and then from there to their parents and so on until it reaches the target vertex formula_147. The target vertex formula_147 will compute its state only when it receives all messages from all its neighbors. Once we have the state, We have got the answer and hence the algorithm terminates.

For Example, Lets consider a junction tree constructed from the set of local domains given above i.e. the set from example 1, Now the Scheduling table for these domains is (where the target vertex is formula_151).

formula_152
formula_153
formula_154
formula_155
formula_156
formula_157
formula_158
formula_159
formula_160
formula_161

Thus the complexity for Single Vertex GDL can be shown as

formula_162 arithmetic operations
Where (Note: The explanation for the above equation is explained later in the article )
formula_163 is the label of formula_164.
formula_165 is the degree of formula_164 (i.e. number of vertices adjacent to v).

To solve the All-Vertices problem, we can schedule GDL in several ways, some of them are parallel implementation where in each round, every state is updated and every message is computed and transmitted at the same time. In this type of implementation the states and messages will stabilizes after number of rounds that is at most equal to the diameter of the tree. At this point all the all states of the vertices will be equal to the desired objective function.

Another way to schedule GDL for this problem is serial implementation where its similar to the Single vertex problem except that we don't stop the algorithm until all the vertices of a required set have not got all the messages from all their neighbors and have compute their state. 
Thus the number of arithmetic this implementation requires is at most formula_167 arithmetic operations.

The key to constructing a junction tree lies in the local domain graph formula_168, which is a weighted complete graph with formula_169 vertices formula_170 i.e. one for each local domain, having the weight of the edge formula_171 defined by
formula_172.
if formula_173, then we say formula_174 is contained informula_175. Denoted by formula_176 (the weight of a maximal-weight spanning tree of formula_168), which is defined by

where "n" is the number of elements in that set. For more clarity and details, please refer to these.

Let formula_179 be a junction tree with vertex set formula_180 and edge set formula_181. In this algorithm, the messages are sent in both the direction on any edge, so we can say/regard the edge set E as set of ordered pairs of vertices. For example, from Figure 1 formula_181 can be defined as follows

NOTE:formula_184 above gives you all the possible directions that a message can travel in the tree.

The schedule for the GDL is defined as a finite sequence of subsets offormula_184. Which is generally represented by 
formula_186{formula_187}, Where formula_188 is the set of messages updated during the formula_189 round of running the algorithm.

Having defined/seen some notations, we will see want the theorem says,
When we are given a schedule formula_190, the corresponding message trellis as a finite directed graph with Vertex set of formula_191, in which a typical element is denoted by formula_192 for formula_193, Then after completion of the message passing, state at vertex formula_104 will be the formula_195 objective defined in

and <nowiki>iff</nowiki> there is a path from formula_197 to formula_198

Here we try to explain the complexity of solving the MPF problem in terms of the number of mathematical operations required for the calculation. i.e. We compare the number of operations required when calculated using the normal method (Here by normal method we mean by methods that do not use message passing or junction trees in short methods that do not use the concepts of GDL)and the number of operations using the generalized distributive law.

Example: Consider the simplest case where we need to compute the following expression formula_199.

To evaluate this expression naively requires two multiplications and one addition. The expression when expressed using the distributive law can be written as formula_200 a simple optimization that reduces the number of operations to one addition and one multiplication.

Similar to the above explained example we will be expressing the equations in different forms to perform as few operation as possible by applying the GDL.

As explained in the previous sections we solve the problem by using the concept of the junction trees. The optimization obtained by the use of these trees is comparable to the optimization obtained by solving a semi group problem on trees. For example, to find the minimum of a group of numbers we can observe that if we have a tree and the elements are all at the bottom of the tree, then we can compare the minimum of two items in parallel and the resultant minimum will be written to the parent. When this process is propagated up the tree the minimum of the group of elements will be found at the root.

The following is the complexity for solving the junction tree using message passing

We rewrite the formula used earlier to the following form. This is the eqn for a message to be sent from vertex "v" to "w"

Similarly we rewrite the equation for calculating the state of vertex v as follows

We first will analyze for the single-vertex problem and assume the target vertex is formula_147 and hence we have one edge from formula_164 to formula_205. 
Suppose we have an edge formula_206 we calculate the message using the message equation. To calculate formula_207 requires

additions and

multiplications.

But there will be many possibilities for formula_212 hence 
formula_213 possibilities for formula_214.
Thus the entire message will need

additions and

multiplications

The total number of arithmetic operations required to send a message towards formula_217along the edges of tree will be

additions and

multiplications.

Once all the messages have been transmitted the algorithm terminates with the computation of state at formula_147 The state computation requires formula_221 more multiplications.
Thus number of calculations required to calculate the state is given as below

additions and

multiplications

Thus the grand total of the number of calculations is

where formula_226 is an edge and its size is defined by formula_227

The formula above gives us the upper bound.

If we define the complexity of the edge formula_226 as

Therefore, formula_225 can be written as

We now calculate the edge complexity for the problem defined in Figure 1 as follows

The total complexity will be formula_240 which is considerably low compared to the direct method. (Here by direct method we mean by methods that do not use message passing. The time taken using the direct method will be the equivalent to calculating message at each node and time to calculate the state of each of the nodes.)

Now we consider the all-vertex problem where the message will have to be sent in both the directions and state must be computed at both the vertexes. This would take formula_241 but by precomputing we can reduce the number of multiplications to formula_242. Here formula_13 is the degree of the vertex. Ex : If there is a set formula_244 with formula_245 numbers. It is possible to compute all the d products of formula_246 of the formula_247 with at most formula_242 multiplications rather than the obvious formula_249. 
We do this by precomputing the quantities 
formula_250 and formula_251 this takes formula_252 multiplications. Then if formula_253 denotes the product of all formula_254 except for formula_255 we have formula_256 and so on will need another formula_257 multiplications making the total formula_258

There is not much we can do when it comes to the construction of the junction tree except that we may have many maximal weight spanning tree and we should choose the spanning tree with the least formula_259 and sometimes this might mean adding a local domain to lower the junction tree complexity.

It may seem that GDL is correct only when the local domains can be expressed as a junction tree. But even in cases where there are cycles and a number of iterations the messages will approximately be equal to the objective function. The experiments on Gallager–Tanner–Wiberg algorithm for low density parity-check codes were supportive of this claim.


</doc>
<doc id="36033877" url="https://en.wikipedia.org/wiki?curid=36033877" title="Bisection (software engineering)">
Bisection (software engineering)

Bisection is a method used in software development to identify change sets that result in a specific behavior change. It is mostly employed for finding the patch that introduced a bug. Another application area is finding the patch that indirectly fixed a bug.

The process of locating the changeset that introduced a specific regression was described as "source change isolation" in 1997 by Brian Ness and Viet Ngo of Cray Research. Regression testing was performed on Cray's compilers in editions comprising one or more changesets. Editions with known regressions could not be validated until developers addressed the problem. Source change isolation narrowed the cause to a single changeset that could then be excluded from editions, unblocking them with respect to this problem, while the author of the change worked on a fix. Ness and Ngo outlined linear search and binary search methods of performing this isolation.

Code bisection has the goal of minimizing the effort to find a specific change set.
It employs a divide and conquer algorithm that
depends on having access to the code history which is usually preserved by
revision control in a code repository.

Code history has the structure of a directed acyclic graph which can be topologically sorted. This makes it possible to use a divide and conquer search algorithm which:

Bisection is in LSPACE having an algorithmic complexity of formula_1 with formula_2 denoting the number of revisions in the search space, and is similar to a binary search.

For code bisection it is desirable that each revision in the search space can be built and tested independently.

Although the bisection method can be completed manually, one of its main advantages is that it can be easily automated. It can thus fit into existing test automation processes: failures in exhaustive automated regression tests can trigger automated bisection to localize faults. Ness and Ngo focused on its potential in Cray's continuous delivery-style environment in which the automatically-isolated bad changeset could be automatically excluded from builds.

The revision control systems Git and Mercurial have built-in functionality for code bisection. The user can start a bisection session with a specified range of revisions from which the revision control system proposes a revision to test, the user tells the system whether the revision tested as "good" or "bad", and the process repeats until the specific "bad" revision has been identified. Other revision control systems, such as Bazaar or Subversion, support bisection through plugins or external scripts.

Phoronix Test Suite can do bisection automatically to find performance regressions.



</doc>
<doc id="37606787" url="https://en.wikipedia.org/wiki?curid=37606787" title="Run to completion scheduling">
Run to completion scheduling

Run-to-completion scheduling or nonpreemptive scheduling is a scheduling model in which each task runs until it either finishes, or explicitly yields control back to the scheduler. Run to completion systems typically have an event queue which is serviced either in strict order of admission by an event loop, or by an admission scheduler which is capable of scheduling events out of order, based on other constraints such as deadlines.

Some preemptive multitasking scheduling systems behave as run-to-completion schedulers in regard to scheduling tasks at one particular process priority level, at the same time as those processes still preempt other lower priority tasks and are themselves preempted by higher priority tasks.



</doc>
<doc id="19759995" url="https://en.wikipedia.org/wiki?curid=19759995" title="Kinodynamic planning">
Kinodynamic planning

In robotics and motion planning, kinodynamic planning is a class of problems for which velocity, acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles. The term was coined by Bruce Donald, Pat Xavier, John Canny, and John Reif. Donald et al. developed the first polynomial-time approximation schemes (PTAS) for the problem. By providing a provably polynomial-time ε-approximation algorithm, they resolved a long-standing open problem in optimal control. Their first paper considered time-optimal control ("fastest path") of a point mass under Newtonian dynamics, amidst polygonal (2D) or polyhedral (3D) obstacles, subject to state bounds on position, velocity, and acceleration. Later they extended the technique to many other cases, for example, to 3D open-chain kinematic robots under full Lagrangian dynamics. More recently, many practical heuristic algorithms based on stochastic optimization and iterative sampling were developed, by a wide range of authors, to address the kinodynamic planning problem. These techniques for kinodynamic planning have been shown to work well in practice. However, none of these heuristic techniques can guarantee the optimality of the computed solution (i.e., they have no performance guarantees), and none can be mathematically proven to be faster than the original PTAS algorithms (i.e., none have a provably lower computational complexity).


</doc>
<doc id="17876651" url="https://en.wikipedia.org/wiki?curid=17876651" title="Predictor–corrector method">
Predictor–corrector method

In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equationsto find an unknown function that satisfies a given differential equation. All such algorithms proceed in two steps: 


When considering the numerical solution of ordinary differential equations (ODEs), a predictor–corrector method typically uses an explicit method for the predictor step and an implicit method for the corrector step.

A simple predictor–corrector method (known as Heun's method) can be constructed from the Euler method (an explicit method) and the trapezoidal rule (an implicit method).

Consider the differential equation

and denote the step size by formula_2.

First, the predictor step: starting from the current value formula_3, calculate an initial guess value formula_4 via the Euler method,

Next, the corrector step: improve the initial guess using trapezoidal rule,

That value is used as the next step.

There are different variants of a predictor–corrector method, depending on how often the corrector method is applied. The Predict–Evaluate–Correct–Evaluate (PECE) mode refers to the variant in the above example:

It is also possible to evaluate the function "f" only once per step by using the method in Predict–Evaluate–Correct (PEC) mode:

Additionally, the corrector step can be repeated in the hope that this achieves an even better approximation to the true solution. If the corrector method is run twice, this yields the PECECE mode:

The PECEC mode has one fewer function evaluation than PECECE mode.

More generally, if the corrector is run "k" times, the method is in P(EC)
or P(EC)E mode. If the corrector method is iterated until it converges, this could be called PE(CE).





</doc>
<doc id="39226029" url="https://en.wikipedia.org/wiki?curid=39226029" title="HCS clustering algorithm">
HCS clustering algorithm

The HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm, and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for cluster analysis. It works by representing the similarity data in a similarity graph, and then finding all the highly connected subgraphs. It does not make any prior assumptions on the number of the clusters. This algorithm was published by Erez Hartuv and Ron Shamir in 1998.

The HCS algorithm gives a clustering solution, which is inherently meaningful in the application domain, since each solution cluster must have diameter 2 while a union of two solution clusters will have diameter 3.

The goal of cluster analysis is to group elements into disjoint subsets, or clusters, based on similarity between elements, so that elements in the same cluster are highly similar to each other (homogeneity), while elements from different clusters have low similarity to each other (separation). Similarity graph is one of the models to represent the similarity between elements, and in turn facilitate generating of clusters. To construct a similarity graph from similarity data, represent elements as vertices, and elicit edges between vertices when the similarity value between them is above some threshold.

In the similarity graph, the more edges exist for a given number of vertices, the more similar such a set of vertices are between each other. In other words, if we try to disconnect a similarity graph by removing edges, the more edges we need to remove before the graph becomes disconnected, the more similar the vertices in this graph. Minimum cut is a minimum set of edges without which the graph will become disconnected.

HCS clustering algorithm finds all the subgraphs with n vertices such that the minimum cut of those subgraphs contain more than n/2 edges, and identifies them as clusters. Such a subgraph is called a Highly Connected Subgraph (HCS). Single vertices are not considered clusters and are grouped into a singletons set S.

Given a similarity graph G(V,E), HCS clustering algorithm will check if it is already highly connected, if yes, returns G, otherwise uses the minimum cut of G to partition G into two subgraphs H and H', and recursively run HCS clustering algorithm on H and H'.

The following animation shows how the HCS clustering algorithm partitions a similarity graph into three clusters.

function HCS(G(V,E)) 
end
The step of finding the minimum cut on graph G is a subroutine that can be implemented using different algorithms for this problem. See below for an example algorithm for finding minimum cut using randomization.

The running time of the HCS clustering algorithm is bounded by N x f(n,m). f(n,m) is the time complexity of computing a minimum cut in a graph with n vertices and m edges, and N is the number of clusters found. In many applications N « n.

For fast algorithms for finding a minimum cut in an unweighted graph: 

The clusters produced by the HCS clustering algorithm possess several properties, which can demonstrate the homogeneity and the separation of the solution.

Theorem 1 The diameter of every highly connect graph is at most two.

"Proof:" We know the edges of minimum cut must be greater or equal than the minimum degree of the graph. If the graph G is highly connected, then the edges of the minimum cut must be greater than the number of vertices divided by 2. So the degree of vertices in the highly connected graph G must be greater than half the vertices. Therefore, for any two vertices in this graph G, there must be at least one common neighbor, as the distance between them is two.

Theorem 2 (a) The number of edges in a highly connected subgraph is quadratic. (b) The number of edges removed by each iteration of the HCS algorithm is at most linear.

"Proof:" (For a) From Theorem 1 we know every vertex must have more than half of the total vertices as neighbors. Therefore, the total number of edges in a highly connect subgraph must be at least (n/2) x n x 1/2, where we sum all the degrees of each vertex and divide by 2.

(For b) Each iteration HCS algorithm will separate a graph containing n vertices into two subgraphs, so the number of edges between those two components is at most n/2.

Theorem 1 and 2a provide a strong indication to the homogeneity, as the only better possibility in terms of the diameter is that every two vertices of a cluster are connected by an edge, which is both too stringent and also a NP-hard problem.

Theorem 2b also indicates separation since the number of edges removed by each iteration of the HCS algorithm is at most linear in the size of the underlying subgraph, contrast to the quadratic number of edges within final clusters.

Singletons adoption: Elements left as singletons by the initial clustering process can be "adopted" by clusters based on similarity to the cluster. If the maximum number of neighbors to a specific cluster is large enough, then it can be added to that cluster.

Removing Low Degree Vertices: When the input graph has vertices with low degrees, it is not worthy to run the algorithm since it is computationally expensive and not informative. Alternatively, a refinement of the algorithm can first remove all vertices with a degree lower than certain threshold.





</doc>
<doc id="39456471" url="https://en.wikipedia.org/wiki?curid=39456471" title="Driver scheduling problem">
Driver scheduling problem

The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.

The DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods.

This very complex problem involves several constraints related to labour and company rules and also different evaluation criteria and objectives. Being able to solve this problem efficiently can have a great impact on costs and quality of service for public transportation companies. There is a large number of different rules that a feasible duty might be required to satisfy, such as
Operations research has provided optimization models and algorithms that lead to efficient solutions for this problem. Among the most common models proposed to solve the DSP are the Set Covering and Set Partitioning Models (SPP/SCP). In the SPP model, each work piece (task) is covered by only one duty. In the SCP model, it is possible to have more than one duty covering a given work piece.
In both models, the set of work pieces that needs to be covered is laid out in rows, and the set of previously defined feasible duties available for covering specific work pieces is arranged in columns. The DSP resolution, based on either of these models, is the selection of the set of feasible duties that guarantees that there is one (SPP) or more (SCP) duties covering each work piece while minimizing the total cost of the final schedule.


</doc>
<doc id="39093307" url="https://en.wikipedia.org/wiki?curid=39093307" title="Chandy–Misra–Haas algorithm resource model">
Chandy–Misra–Haas algorithm resource model

The Chandy–Misra–Haas algorithm resource model checks for deadlock in a distributed system. It was developed by K. Mani Chandy, Jayadev Misra and Laura M Haas.

Consider the n processes "P", "P", "P", "P", "P", ... ,"P" which are performed in a single system(controller). "P" is locally dependent on "P", if "P" depends on "P", "P" on "P" so on and "P" on "P". That is, if formula_1, then formula_2 is locally dependent on formula_3. If "P" is said to be locally dependent to itself if it is locally dependent on "P" and "P" depends on "P": i.e. if formula_4, then formula_2 is locally dependent on itself.

The algorithm uses a message called probe(i,j,k) to transfer a message from controller of process "P" to controller of process "P". It specifies a message started by process "P" to find whether a deadlock has occurred or not. Every process "P" maintains a boolean array "dependent" which contains the information about the processes that depend on it. Initially the values of each array are all "false".

Before sending, the probe checks whether "P" is locally dependent on itself. If so, a deadlock occurs. Otherwise it checks whether "P", and "P" are in different controllers, are locally dependent and "P" is waiting for the resource that is locked by "P". Once all the conditions are satisfied it sends the probe.

On the receiving side, the controller checks whether "P" is performing a task. If so, it neglects the probe. Otherwise, it checks the responses given "P" to "P" and "dependent"(i) is false. Once it is verified, it assigns true to "dependent"(i). Then it checks whether k is equal to i. If both are equal, a deadlock occurs, otherwise it sends the probe to next dependent process.

In pseudocode, the algorithm works as follows:

 if "P" is locally dependent on itself

 if

"P" initiates deadlock detection. "C" sends the probe saying "P" depends on "P". Once the message is received by "C", it checks whether "P" is idle. "P" is idle because it is locally dependent on "P" and updates "dependent"(2) to True.

As above, "C" sends probe to "C" and "C" sends probe to "C". At "C", "P" is idle so it update "dependent"(1) to True. Therefore, deadlock can be declared.

Consider that there are "m" controllers and "p" process to perform, to declare whether a deadlock has occurred or not, the worst case for controllers and processes must be visited. Therefore, the solution is O(m+p). The time complexity is O(n).


</doc>
<doc id="40129720" url="https://en.wikipedia.org/wiki?curid=40129720" title="Devex algorithm">
Devex algorithm

In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Paula M. J. Harris. It identifies the steepest-edge approximately in its search for the optimal solution.


</doc>
<doc id="40338559" url="https://en.wikipedia.org/wiki?curid=40338559" title="Hybrid algorithm">
Hybrid algorithm

A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.

"Hybrid algorithm" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.

In computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of 
divide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.

A general procedure for a simple hybrid recursive algorithm is "short-circuiting the base case," also known as "arm's-length recursion." In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing. This is useful for efficiency when the algorithm usually encounters the base case many times, as in many tree algorithms, but is otherwise considered poor style, particularly in academia, due to the added complexity.

Another example of hybrid algorithms for performance reasons are introsort and introselect, which combine one algorithm for fast average performance, falling back on another algorithm to ensure (asymptotically) optimal worst-case performance. Introsort begins with a quicksort, but switches to a heap sort if quicksort is not progressing well; analogously introselect begins with quickselect, but switches to median of medians if quickselect is not progressing well.

Centralized distributed algorithms can often be considered as hybrid algorithms, consisting of an individual algorithm (run on each distributed processor), and a combining algorithm (run on a centralized distributor) – these correspond respectively to running the entire algorithm on one processor, or running the entire computation on the distributor, combining trivial results (a one-element data set from each processor). A basic example of these algorithms are distribution sorts, particularly used for external sorting, which divide the data into separate subsets, sort the subsets, and then combine the subsets into totally sorted data; examples include bucket sort and flashsort.

However, in general distributed algorithms need not be hybrid algorithms, as individual algorithms or combining or communication algorithms may be solving different problems. For example, in models such as MapReduce, the Map and Reduce step solve different problems, and are combined to solve a different, third problem.



</doc>
<doc id="41026219" url="https://en.wikipedia.org/wiki?curid=41026219" title="Pointer jumping">
Pointer jumping

Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. It can be used to find the roots of a forest of rooted trees, and can also be applied to parallelize many other graph algorithms including connected components, minimum spanning trees, and biconnected components.

One of the simpler tasks that can be solved by a pointer jumping algorithm is the "list ranking" problem. This problem is defined as follows: given a linked list of nodes, find the distance (measured in the number of nodes) of each node to the end of the list. The distance is defined as follows, for nodes that point to their successor by a pointer called :


This problem can easily be solved in linear time on a sequential machine, but a parallel algorithm can do better: given processors, the problem can be solved in logarithmic time, , by the following pointer jumping algorithm:

The pointer jumping occurs in the last line of the algorithm, where each node's pointer is reset to skip the node's direct successor. It is assumed, as in common in the PRAM model of computation, that memory access are performed in lock-step, so that each memory fetch is performed before each memory store; otherwise, processors may clobber each other's data, producing inconsistencies.

Analyzing the algorithm yields a logarithmic running time. The initialization loop takes constant time, because each of the processors performs a constant amount of work, all in parallel. The inner loop of the main loop also takes constant time, as does (by assumption) the termination check for the loop, so the running time is determined by how often this inner loop is executed. Since the pointer jumping in each iteration splits the list into two parts, one consisting of the "odd" elements and one of the "even" elements, the length of the list pointed to by each processor's is halved in each iteration, which can be done at most time before each list has a length of at most one.

Following a path in a graph is an inherently serial operation, but pointer jumping reduces the total amount of work by following all paths simultaneously and sharing results among dependent operations. Pointer jumping iterates and finds a "successor" — a vertex closer to the tree root — each time. By following successors computed for other vertices, the traversal down each path can be doubled every iteration, which means that the tree roots can be found in logarithmic time.

Pointer doubling operates on an array successor with an entry for every vertex in the graph. Each successor["i"] is initialized with the parent index of vertex "i" if that vertex is not a root or to "i" itself if that vertex is a root. At each iteration, each successor is updated to its successor's successor. The root is found when the successor's successor points to itself.

The following pseudocode demonstrates the algorithm.

The following image provides an example of using pointer jumping on a small forest. On each iteration the successor points to the vertex following one more successor. After two iterations, every vertex points to its root node.


</doc>
<doc id="40543215" url="https://en.wikipedia.org/wiki?curid=40543215" title="Rendezvous hashing">
Rendezvous hashing

Rendezvous or highest random weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of "k" options out of a possible set of "n" options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to.

Rendezvous hashing is more general than consistent hashing, which becomes a special case (for formula_1) of rendezvous hashing.

Rendezvous hashing was invented by David Thaler and Chinya Ravishankar at the University of Michigan in 1996. Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE) to identify multicast rendezvous points in a distributed fashion. It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing. Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.

Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching, router design, secure key establishment, and sharding and distributed databases.

Rendezvous hashing solves the distributed hash table problem: How can a set of clients, given an object "O", agree on where in a set of "n" sites (servers, say) to place "O"? Each client is to select a site independently, but all clients must end up picking the same site. This is non-trivial if we add a "minimal disruption" constraint, and require that only objects mapping to a removed site may be reassigned to other sites.

The basic idea is to give each site "S" a score (a "weight") for each object "O", and assign the object to the highest scoring site. All clients first agree on a hash function "h()". For object "O", the site "S" is defined to have weight "w = h(O, S)". HRW assigns "O" to the site "S" whose weight "w" is the largest. Since "h()" is agreed upon, each client can independently compute the weights "w, w, ..., w" and pick the largest. If the goal is distributed "k"-agreement, the clients can independently pick the sites with the "k" largest hash values.

If a site "S" is added or removed, only the objects mapping to "S" are remapped to different sites, satisfying the minimal disruption constraint above. The HRW assignment can be computed independently by any client, since it depends only on the identifiers for the set of sites "S, S, ..., S" and the object being assigned.

HRW easily accommodates different capacities among sites. If site "S" has twice the capacity of the other sites, we simply represent "S" twice in the list, say, as "S" and "S". Clearly, twice as many objects will now map to "S" as to the other sites.

It might first appear sufficient to treat the "n" sites as buckets in a hash table and hash the object name "O" into this table. However, if any of the sites fails or is unreachable, the hash table size changes, requiring all objects to be remapped. This massive disruption makes such direct hashing unworkable. Under rendezvous hashing, however, clients handle site failures by picking the site that yields the next largest weight. Remapping is required only for objects currently mapped to the failed site, and disruption is minimal.

Rendezvous hashing has the following properties:


Consistent hashing operates by mapping sites uniformly and randomly to points on a unit circle called tokens. Objects are also mapped to the unit circle and placed in the site whose token is the first encountered traveling clockwise from the object's location. When a site is removed, the objects it owns are transferred to the site owning the next token encountered moving clockwise. Provided each site is mapped to a large number (100-200, say) of tokens this will reassign objects in a relatively uniform fashion among the remaining sites.

If sites are mapped to points on the circle randomly by hashing 200 variants of the site ID, say, the assignment of any object requires storing or recalculating 200 hash values for each site. However, the tokens associated with a given site can be precomputed and stored in a sorted list, requiring only a single application of the hash function to the object, and a binary search to compute the assignment. Even with many tokens per site, however, the basic version of consistent hashing may not balance objects uniformly over sites, since when a site is removed each object assigned to it is distributed only over as many other sites as the site has tokens (say 100-200).

Variants of consistent hashing (such as Amazon's Dynamo) that use more complex logic to distribute tokens on the unit circle offer better load balancing than basic consistent hashing, reduce the overhead of adding new sites, and reduce metadata overhead and offer other benefits.

In contrast, rendezvous hashing (HRW) is much simpler conceptually and in practice. It also distributes objects uniformly over all sites, given a uniform hash function. Unlike consistent hashing, HRW requires no precomputing or storage of tokens. An object "O" is placed into one of "n" sites "S, ..., S" by computing the "n" hash values "h(O,S)" and picking the site "S" that yields the highest hash value. If a new site "S" is added, new object placements or requests will compute "n+1" hash values, and pick the largest of these. If an object already in the system at "S" maps to this new site "S", it will be fetched afresh and cached at "S". All clients will henceforth obtain it from this site, and the old cached copy at "S" will ultimately be replaced by the local cache management algorithm. If "S" is taken offline, its objects will be remapped uniformly to the remaining "n-1" sites.

Variants of the HRW algorithm, such as the use of a skeleton (see below), can reduce the formula_2 time for object location to formula_3, at the cost of less global uniformity of placement. When "n" is not too large, however, the formula_2 placement cost of basic HRW is not likely to be a problem. HRW completely avoids all the overhead and complexity associated with correctly handling multiple tokens for each site and associated metadata.

Rendezvous hashing also has the great advantage that it provides simple solutions to other important problems, such as distributed "k"-agreement.

Consistent hashing can be reduced to an instance of HRW by an appropriate choice of a two-place hash function. From the site identifier formula_5 the simplest version of consistent hashing computes a list of token positions, e.g., formula_6 where formula_7 hashes values to locations on the unit circle. Define the two place hash function formula_8 to be formula_9 where formula_10 denotes the distance along the unit circle from formula_11 to formula_12 (since formula_10 has some minimal non-zero value there is no problem translating this value to a unique integer in some bounded range). This will duplicate exactly the assignment produced by consistent hashing.

It is not possible, however, to reduce HRW to consistent hashing (assuming the number of tokens per site is bounded), since HRW potentially reassigns the objects from a removed site to an unbounded number of other sites.

In the standard implementation of rendezvous hashing, every node receives a statically equal proportion of the keys. This behavior, however, is undesirable when the nodes have different capacities for processing or holding their assigned keys. For example, if one of the nodes had twice the storage capacity as the others, it would be beneficial if the algorithm could take this into account such that this more powerful node would receive twice the number of keys as each of the others.

A straightforward mechanism to handle this case is to assign two virtual locations to this node, so that if either of that larger node's virtual locations has the highest hash, that node receives the key. But this strategy does not work when the relative weights are not integer multiples. For example, if one node had 42% more storage capacity, it would require adding many virtual nodes in different proportions, leading to greatly reduced performance. Several modifications to rendezvous hashing have been proposed to overcome this limitation.

The Cache Array Routing Protocol (CARP) is a 1998 IETF draft that describes a method for computing "load factors" which can be multiplied by each node's hash score to yield an arbitrary level of precision for weighting nodes differently. However, one disadvantage of this approach is that when any node's weight is changed, or when any node is added or removed, all the load factors must be re-computed and relatively scaled. When the load factors change relative to one another, it triggers movement of keys between nodes whose weight was not changed, but whose load factor did change relative to other nodes in the system. This results in excess movement of keys.

Controlled replication under scalable hashing or CRUSH is an extension to RUSH that improves upon rendezvous hashing by constructing a tree where a pseudo-random function (hash) is used to navigate down the tree to find which node is ultimately responsible for a given key. It permits perfect stability for adding nodes however it is not perfectly stable when removing or re-weighting nodes, with the excess movement of keys being proportional to the height of the tree.

The CRUSH algorithm is used by the ceph data storage system to map data objects to the nodes responsible for storing them.

When "n" is extremely large, a skeleton-based variant can improve running time. This approach creates a virtual hierarchical structure (called a "skeleton"), and achieves formula_14 running time by applying HRW at each level while descending the hierarchy. The idea is to first choose some constant "m" and organize the "n" sites into "c =" ceiling"(n/m)" clusters "C = {S, S, ... ,S}, C = {S, S, ... ,S}, ..." Next, build a virtual hierarchy by choosing a constant "f" and imagining these "c" clusters placed at the leaves of a tree "T" of virtual nodes, each with fanout "f".

In the accompanying diagram, the cluster size is "m = 4", and the skeleton fanout is formula_15. Assuming 108 sites (real nodes) for convenience, we get a three-tier virtual hierarchy. Since "f = 3", each virtual node has a natural numbering in octal. Thus, the 27 virtual nodes at the lowest tier would be numbered 000, 001, 002, ..., 221, 222, in octal. (We can, of course, vary the fanout at each level. In that case, each node will be identified with the corresponding mixed-radix number.)

Instead of applying HRW to all 108 real nodes, we can first apply HRW to the 27 lowest-tier virtual nodes, selecting one. We then apply HRW to the four real nodes in its cluster, and choose the winning site. We only need formula_16 hashes, rather than 108. If we apply this method starting one level higher in the hierarchy, we would need formula_17 hashes to get to the winning site. The figure shows how, if we proceed starting from the root of the skeleton, we may successively choose the virtual nodes (2), (20), and (200), and finally end up with site 74.

We can start at any level in the virtual hierarchy, not just at the root. Starting lower in the hierarchy requires more hashes, but may improve load distribution in the case of failures. Also, the virtual hierarchy need not be stored, but can be created on demand, since the virtual nodes names are simply prefixes of base-"f" (or mixed-radix) representations. We can easily create appropriately sorted strings from the digits, as required. In the example, we would be working with the strings 0, 1, 2 (at tier 1), 20, 21, 22 (at tier 2), and 200, 201, 202 (at tier 3). Clearly, "T" has height formula_18, since "m" and "f" are both constants. The work done at each level is "O (1)", since "f" is a constant.

For any given object "O", it is clear that the method chooses each cluster, and hence each of the "n" sites, with equal probability. If the site finally selected is unavailable, we can select a different site within the same cluster, in the usual manner. Alternatively, we could go up one or more tiers in the skeleton and select an alternate from among the sibling virtual nodes at that tier, and once again descend the hierarchy to the real nodes, as above.

The value of "m" can be chosen based on factors like the anticipated failure rate and the degree of desired load balancing. A higher value of "m" leads to less load skew in the event of failure at the cost of higher search overhead.

The choice formula_19 is equivalent to non-hierarchical rendezvous hashing. In practice, the hash function formula_20 is very cheap, so formula_21 can work quite well unless "n" is very high.

In 2005, Christian Schindelhauer and Gunnar Schomaker described a logarithmic method for re-weighting hash scores in a way that does not require relative scaling of load factors when a node's weight changes or when nodes are added or removed. This enabled the dual benefits of perfect precision when weighting nodes, along with perfect stability, as only a minimum number of keys needed to be remapped to new nodes.

A similar logarithm-based hashing strategy is used to assign data to storage nodes in Cleversafe's data storage system, now IBM Cloud Object Storage.

Implementation is straightforward once a hash function formula_20 is chosen (the original work on the HRW method makes a hash function recommendation). Each client only needs to compute a hash value for each of the "n" sites, and then pick the largest. This algorithm runs in formula_2 time. If the hash function is efficient, the formula_2 running time is not a problem unless "n" is very large.

Python code implementing a weighted rendezvous hash:

Example outputs of WRH:


</doc>
<doc id="22074859" url="https://en.wikipedia.org/wiki?curid=22074859" title="Maze solving algorithm">
Maze solving algorithm

There are a number of different maze solving algorithms, that is, automated methods for the solving of mazes. The random mouse, wall follower, Pledge, and Trémaux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.

Mazes containing no loops are known as "simply connected", or "perfect" mazes, and are equivalent to a "tree" in graph theory. Thus many maze solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.

This is a trivial method that can be implemented by a very unintelligent robot or perhaps a mouse. It is simply to proceed following the current passage until a junction is reached, and then to make a random decision about the next direction to follow. Although such a method would always eventually find the right solution, this algorithm can be extremely slow.

The wall follower, the best-known rule for traversing mazes, is also known as either the "left-hand rule" or the "right-hand rule". If the maze is "simply connected", that is, all its walls are connected together or to the maze's outer boundary, then by keeping one hand in contact with one wall of the maze the solver is guaranteed not to get lost and will reach a different exit if there is one; otherwise, the algorithm will return to the entrance having traversed every corridor next to that connected section of walls at least once.

Another perspective into why wall following works is topological. If the walls are connected, then they may be deformed into a loop or circle. Then wall following reduces to walking around a circle from start to finish. To further this idea, notice that by grouping together connected components of the maze walls, the boundaries between these are precisely the solutions, even if there is more than one solution (see figures on the right).

If the maze is not simply-connected (i.e. if the start or endpoints are in the center of the structure surrounded by passage loops, or the pathways cross over and under each other and such parts of the solution path are surrounded by passage loops), this method will not reach the goal.

Another concern is that care should be taken to begin wall-following at the entrance to the maze. If the maze is not simply-connected and one begins wall-following at an arbitrary point inside the maze, one could find themselves trapped along a separate wall that loops around on itself and containing no entrances or exits. Should it be the case that wall-following begins late, attempt to mark the position in which wall-following began. Because wall-following will always lead you back to where you started, if you come across your starting point a second time, you can conclude the maze is not simply-connected, and you should switch to an alternative wall not yet followed. See the "Pledge Algorithm", below, for an alternative methodology.

Wall-following can be done in 3D or higher-dimensional mazes if its higher-dimensional passages can be projected onto the 2D plane in a deterministic manner. For example, if in a 3D maze "up" passages can be assumed to lead Northwest, and "down" passages can be assumed to lead southeast, then standard wall following rules can apply. However, unlike in 2D, this requires that the current orientation is known, to determine which direction is the first on the left or right.

Disjoint mazes can be solved with the wall follower method, so long as the entrance and exit to the maze are on the outer walls of the maze. If however, the solver starts inside the maze, it might be on a section disjoint from the exit, and wall followers will continually go around their ring. The Pledge algorithm (named after Jon Pledge of Exeter) can solve this problem.

The Pledge algorithm, designed to circumvent obstacles, requires an arbitrarily chosen direction to go toward, which will be preferential. When an obstacle is met, one hand (say the right hand) is kept along the obstacle while the angles turned are counted (clockwise turn is positive, counter-clockwise turn is negative). When the solver is facing the original preferential direction again, and the angular sum of the turns made is 0, the solver leaves the obstacle and continues moving in its original direction.

The hand is removed from the wall only when both "sum of turns made" and "current heading" are at zero. This allows the algorithm to avoid traps shaped like an upper case letter "G". Assuming the algorithm turns left at the first wall, one gets turned around a full 360 degrees by the walls. An algorithm that only keeps track of "current heading" leads into an infinite loop as it leaves the lower rightmost wall heading left and runs into the curved section on the left hand side again. The Pledge algorithm does not leave the rightmost wall due to the "sum of turns made" not being zero at that point (note 360 degrees is not equal to 0 degrees). It follows the wall all the way around, finally leaving it heading left outside and just underneath the letter shape.

This algorithm allows a person with a compass to find their way from any point inside to an outer exit of any finite two-dimensional maze, regardless of the initial position of the solver. However, this algorithm will not work in doing the reverse, namely finding the way from an entrance on the outside of a maze to some end goal within it.

Trémaux's algorithm, invented by Charles Pierre Trémaux, is an efficient method to find the way out of a maze that requires drawing lines on the floor to mark a path, and is guaranteed to work for all mazes that have well-defined passages, but it is not guaranteed to find the shortest route.

A path from a junction is either unvisited, marked once or marked twice. The algorithm works according to the following rules:
When you finally reach the solution, paths marked exactly once will indicate a way back to the start. If there is no exit, this method will take you back to the start where all paths are marked twice.
In this case each path is walked down exactly twice, once in each direction. The resulting walk is called a bidirectional double-tracing.

Essentially, this algorithm, which was discovered in the 19th century, has been used about a hundred years later as depth-first search.

Dead-end filling is an algorithm for solving mazes that fills all dead ends, leaving only the correct ways unfilled. It can be used for solving mazes on paper or with a computer program, but it is not useful to a person inside an unknown maze since this method looks at the entire maze at once. The method is to 1) find all of the dead-ends in the maze, and then 2) "fill in" the path from each dead-end until the first junction is met. Note that some passages won't become parts of dead end passages until other dead ends are removed first. A video of dead-end filling in action can be seen here: .

Dead-end filling cannot accidentally "cut off" the start from the finish since each step of the process preserves the topology of the maze. Furthermore, the process won't stop "too soon" since the end result cannot contain any dead-ends. Thus if dead-end filling is done on a perfect maze (maze with no loops), then only the solution will remain. If it is done on a partially braid maze (maze with some loops), then every possible solution will remain but nothing more. 

If given an omniscient view of the maze, a simple recursive algorithm can tell one how to get to the end. The algorithm will be given a starting X and Y value. If the X and Y values are not on a wall, the method will call itself with all adjacent X and Y values, making sure that it did not already use those X and Y values before. If the X and Y values are those of the end location, it will save all the previous instances of the method as the correct path. Here is a sample code in Java:
The maze-routing algorithm is a low overhead method to find the way between any two locations of the maze. The algorithm is initially proposed for chip multiprocessors (CMPs) domain and guarantees to work for any grid-based maze. In addition to finding paths between two location of the grid (maze), the algorithm can detect when there is no path between the source and destination. Also, the algorithm is to be used by an inside traveler with no prior knowledge of the maze with fixed memory complexity regardless of the maze size; requiring 4 variables in total for finding the path and detecting the unreachable locations. Nevertheless, the algorithm is not to find the shortest path.

Maze-routing algorithm uses the notion of Manhattan distance (MD) and relies on the property of grids that the MD increments/decrements "exactly" by 1 when moving from one location to any 4 neighboring locations. Here is the pseudocode without the capability to detect unreachable locations.

When a maze has multiple solutions, the solver may want to find the shortest path from start to finish. There are several algorithms to find shortest paths, most of them coming from graph theory. One such algorithm finds the shortest path by implementing a breadth-first search, while another, the A* algorithm, uses a heuristic technique. The breadth-first search algorithm uses a queue to visit cells in increasing distance order from the start until the finish is reached. Each visited cell needs to keep track of its distance from the start or which adjacent cell nearer to the start caused it to be added to the queue. When the finish location is found, follow the path of cells backwards to the start, which is the shortest path. The breadth-first search in its simplest form has its limitations, like finding the shortest path in weighted graphs.




</doc>
<doc id="41457976" url="https://en.wikipedia.org/wiki?curid=41457976" title="METIS">
METIS

METIS is a software package for graph partitioning that implements various multilevel algorithms. METIS' multilevel approach has three phases and comes with several algorithms for each phase:
The final partition computed during the third phase (the refined partition projected onto G) is a partition of the original graph.



</doc>
<doc id="13830115" url="https://en.wikipedia.org/wiki?curid=13830115" title="Jump-and-Walk algorithm">
Jump-and-Walk algorithm

Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.

Jump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).

Jump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.

 


</doc>
<doc id="42360188" url="https://en.wikipedia.org/wiki?curid=42360188" title="Algorithmic logic">
Algorithmic logic

Algorithmic logic is a calculus of programs which allows the expression of semantic properties of programs by appropriate logical formulas. It provides a framework that enables proving the formulas from the axioms of program constructs such as assignment, iteration and composition instructions and from the axioms of the data structures in question see , .

The following diagram helps to locate algorithmic logic among other logics.
formula_1
The formalized language of algorithmic logic (and of algorithmic theories of various data structures) contains three types of well formed expressions: "Terms" - i.e. expressions denoting operations on elements of data structures, 
"formulas" - i.e. expressions denoting the relations among elements of data structures, "programs" - i.e. algorithms - these expressions describe the computations.
For semantics of terms and formulas consult pages on first order logic and Tarski's semantic. The meaning of a program formula_2 is the set of possible computations of the program.

Algorithmic logic is one of many logics of programs.
Another logic of programs is dynamic logic, see dynamic logic, .



</doc>
<doc id="42563034" url="https://en.wikipedia.org/wiki?curid=42563034" title="KiSAO">
KiSAO

The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.

KiSAO consists of three main branches:
The elements of each algorithm branch are linked to characteristic and parameter branches using "has characteristic" and "has parameter" relationships accordingly. The algorithm branch itself is hierarchically structured using relationships which denote that the descendant algorithms were derived from, or specify, more general ancestors.



</doc>
<doc id="42923391" url="https://en.wikipedia.org/wiki?curid=42923391" title="Kleene's algorithm">
Kleene's algorithm

In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given nondeterministic finite automaton (NFA) into a regular expression. 
Together with other conversion algorithms, it establishes the equivalence of several description formats for regular languages. Alternative presentations of the same method include the "elimination method" attributed to Brzozowski and McCluskey, the algorithm of McNaughton and Yamada, and the use of Arden's lemma.

According to Gross and Yellen (2004), the algorithm can be traced back to Kleene (1956). A presentation of the algorithm in the case of deterministic finite automata (DFAs) is given in Hopcroft and Ullman (1979). The presentation of the algorithm for NFAs below follows Gross and Yellen (2004).

Given a nondeterministic finite automaton "M" = ("Q", Σ, δ, "q", "F"), with "Q" = { "q"...,"q" } its set of states, the algorithm computes 
Here, "going through a state" means entering "and" leaving it, so both "i" and "j" may be higher than "k", but no intermediate state may.
Each set "R" is represented by a regular expression; the algorithm computes them step by step for "k" = -1, 0, ..., "n". Since there is no state numbered higher than "n", the regular expression "R" represents the set of all strings that take "M" from its start state "q" to "q". If "F" = { "q"...,"q" } is the set of accept states, the regular expression "R" | ... | "R" represents the language accepted by "M".

The initial regular expressions, for "k" = -1, are computed as follows for "i"≠"j":
and as follows for "i"="j":

In other words, "R" mentions all letters that label a transition from "i" to "j", and we also include ε in the case where "i"="j".

After that, in each step the expressions "R" are computed from the previous ones by

Another way to understand the operation of the algorithm is as an "elimination method", where the states from 0 to "n" are successively removed: when state "k" is removed, the regular expression "R", which describes the words that label a path from state "i">"k" to state "j">"k", is rewritten into "R" so as to take into account the possibility of going via the "eliminated" state "k".

By induction on "k", it can be shown that the length of each expression "R" is at most (4(6"s"+7) - 4) symbols, where "s" denotes the number of characters in Σ.
Therefore, the length of the regular expression representing the language accepted by "M" is at most (4(6"s"+7)"f" - "f" - 3) symbols, where "f" denotes the number of final states.
This exponential blowup is inevitable, because there exist families of DFAs for which any equivalent regular expression must be of exponential size.

In practice, the size of the regular expression obtained by running the algorithm can be very different depending on the order in which the states are considered by the procedure, i.e., the order in which they are numbered from 0 to "n".

The automaton shown in the picture can be described as "M" = ("Q", Σ, δ, "q", "F") with

Kleene's algorithm computes the initial regular expressions as

After that, the "R" are computed from the "R" step by step for "k" = 0, 1, 2.
Kleene algebra equalities are used to simplify the regular expressions as much as possible.




Since "q" is the start state and "q" is the only accept state, the regular expression "R" denotes the set of all strings accepted by the automaton.



</doc>
<doc id="45194398" url="https://en.wikipedia.org/wiki?curid=45194398" title="Domain reduction algorithm">
Domain reduction algorithm

Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.


</doc>
<doc id="35457560" url="https://en.wikipedia.org/wiki?curid=35457560" title="RNA22">
RNA22

Rna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes.

The algorithm is conceptually distinct from other methods for predicting in that it does "not" use experimentally validated heteroduplexes for training, instead relying only on the sequences of
known mature miRNAs that are found in the public databases. The key idea of rna22 is that the reverse complement of any salient sequence features that one can identify in mature microRNA sequences (using pattern discovery techniques) should allow one to identify candidate microRNA target sites in a sequence of interest: rna22 makes use of the Teiresias algorithm to discover such salient features. Once a candidate microRNA target site has been located, the targeting microRNA can be identified with the help of any of several algorithms able to compute RNA:RNA heteroduplexes. A new version (v2.0) of the algorithm is now available: v2.0-beta adds probability estimates to each prediction, gives users the ability to choose the sensitivity/specificity settings on-the-fly, is significantly faster than the original, and can be accessed through http://cm.jefferson.edu/rna22/Interactive/.

Rna22 neither relies on nor imposes any cross-organism conservation constraints to filter out unlikely candidates; this gives it the ability to discover microRNA binding sites that may not be conserved in phylogenetically proximal organisms. Also, as mentioned above, rna22 can identify putative microRNA binding sites without needing to know the identity of the targeting microRNA. A notable property of rna22 is that it does "not" require the presence of the exact reverse complement of a microRNA's seed in a putative target permitting bulges and G:U wobbles in the seed region of the heteroduplex. Lastly, the algorithm has been shown to achieve high signal-to-noise ratio.

Use of rna22 led to the discovery of "non-canonical" microRNA targets in the coding regions of the mouse "Nanog", "Oct4" and "Sox2". Most of these targets are not conserved in the human orthologues of these three transcription factors even though they reside in the coding region of the corresponding mRNAs. Moreover, most of these targets contain G:U wobbles, one or more bulges, or both, in the seed region of the heteroduplex. In addition to coding regions, rna22 has helped discover non-canonical targets in 3'UTRs.

A recent study examined the problem of non-canonical miRNA targets using molecular dynamics simulations of the crystal structure of the Argonaute-miRNA:mRNA ternary complex. The study found that several kinds of modifications, including combinations of multiple G:U wobbles and mismatches in the seed region, are admissible and result in only minor structural fluctuations that do not affect the stability of the ternary complex. The study also showed that the findings of the molecular dynamics simulation are supported by HITS-CLIP (CLIP-seq) data. These results suggest that "bona fide" miRNA targets transcend the canonical seed-model in turn making target prediction tools like rna22 an ideal choice for exploring the newly augmented spectrum of miRNA targets.


</doc>
<doc id="44995795" url="https://en.wikipedia.org/wiki?curid=44995795" title="AVT Statistical filtering algorithm">
AVT Statistical filtering algorithm

AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources. It is most effective in cases when there is inband noise present. In those cases AVT is better at filtering data then, band-pass filter or any digital filtering based on variation of.

Conventional filtering is useful when signal/data has different frequency than noise and signal/data is separated/filtered by frequency discrimination of noise. Frequency discrimination filtering is done using Low Pass, High Pass and Band Pass filtering which refers to relative frequency filtering criteria target for such configuration. Those filters are created using passive and active components and sometimes are implemented using software algorithms based on Fast Fourier transform (FFT).

AVT filtering is implemented in software and its inner working is based on statistical analysis of raw data.

When signal frequency/(useful data distribution frequency) coincides with noise frequency/(noisy data distribution frequency) we have inband noise. In this situations frequency discrimination filtering does not work since the noise and useful signal are indistinguishable and where AVT excels. To achieve filtering in such conditions there are several methods/algorithms available which are briefly described below.



AVT algorithm stands for Antonyan Vardan Transform and its implementation explained below.

This algorithm is based on amplitude discrimination and can easily reject any noise that is not like actual signal, otherwise statistically different then 1 standard deviation of the signal. Note that this type of filtering can be used in situations where the actual environmental noise is not known in advance.

Using a system that has signal value of 1 and has noise added at 0.1% and 1% levels will simplify quantification of algorithm performance. The R script is used to create pseudo random noise added to signal and analyze the results of filtering using several algorithms. Please refer to "Reduce Inband Noise with the AVT Algorithm" article for details.
This graphs show that AVT algorithm provides best results compared with Median and Averaging algorithms while using data sample size of 32, 64 and 128 values. Note that this graph was created by analyzing random data array of 10000 values. Sample of this data is graphically represented below.

In some situations better results can be obtained by cascading several stages of AVT filtering. This will produce singular constant value which can be used for equipment that has known stable characteristics like thermometers, thermistors and other slow acting sensors.

This is useful for detecting minute signals that are close to background noise level.



</doc>
<doc id="44308703" url="https://en.wikipedia.org/wiki?curid=44308703" title="Flajolet–Martin algorithm">
Flajolet–Martin algorithm

The Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption logarithmic in the maximal number of possible distinct elements in the stream (the count-distinct problem). The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 article "Probabilistic Counting Algorithms for Data Base Applications". Later it has been refined in "LogLog counting of large cardinalities" by Marianne Durand and Philippe Flajolet, and "HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm" by Philippe Flajolet et al.

In their 2010 article "An optimal algorithm for the distinct elements problem", Daniel M. Kane, Jelani Nelson and David P. Woodruff give an improved algorithm, which uses nearly optimal space and has optimal "O"(1) update and reporting times.

Assume that we are given a hash function formula_1 that maps input formula_2 to integers in the range formula_3, and where the outputs are sufficiently uniformly distributed. Note that the set of integers from 0 to formula_4 corresponds to the set of binary strings of length formula_5. For any non-negative integer formula_6, define formula_7 to be the formula_8-th bit in the binary representation of formula_6, such that:

We then define a function formula_11 that outputs the position of the least-significant set bit in the binary representation of formula_6:

where formula_14. Note that with the above definition we are using 0-indexing for the positions. For example, formula_15, since the least significant bit is a 1 (0th position), and formula_16, since the least significant bit is at the 3rd position. At this point, note that under the assumption that the output of our hash function is uniformly distributed, then the probability of observing a hash output ending with formula_17 (a one, followed by formula_8 zeroes) is formula_19, since this corresponds to flipping formula_8 heads and then a tail with a fair coin.

Now the Flajolet–Martin algorithm for estimating the cardinality of a multiset formula_21 is as follows:

The idea is that if formula_33 is the number of distinct elements in the multiset formula_21, then formula_35 is accessed approximately formula_36 times, formula_37 is accessed approximately formula_38 times and so on. Consequently, if formula_39, then formula_40 is almost certainly 0, and if formula_41, then formula_40 is almost certainly 1. If formula_43, then formula_40 can be expected to be either 1 or 0.

The correction factor formula_32 is found by calculations, which can be found in the original article.

A problem with the Flajolet–Martin algorithm in the above form is that the results vary significantly. A common solution has been to run the algorithm multiple times with formula_8 different hash functions and combine the results from the different runs. One idea is to take the mean of the formula_8 results together from each hash function, obtaining a single estimate of the cardinality. The problem with this is that averaging is very susceptible to outliers (which are likely here). A different idea is to use the median, which is less prone to be influences by outliers. The problem with this is that the results can only take form formula_31, where formula_27 is integer. A common solution is to combine both the mean and the median: Create formula_50 hash functions and split them into formula_8 distinct groups (each of size formula_52). Within each group use the median for aggregating together the formula_52 results, and finally take the mean of the formula_8 group estimates as the final estimate.

The 2007 HyperLogLog algorithm splits the multiset into subsets and estimates their cardinalities, then it uses the harmonic mean to combine them into an estimate for the original cardinality.



</doc>
<doc id="46900918" url="https://en.wikipedia.org/wiki?curid=46900918" title="Xulvi-Brunet–Sokolov algorithm">
Xulvi-Brunet–Sokolov algorithm

Xulvi-Brunet and Sokolov’s algorithm generates networks with chosen degree correlations. This method is based on link rewiring, in which the desired degree is governed by parameter ρ. By varying this single parameter it is possible to generate networks from random (when ρ = 0) to perfectly assortative or disassortative (when ρ = 1). This algorithm allows to keep network’s degree distribution unchanged when changing the value of ρ.

In assortative networks, well-connected nodes are likely to be connected to other highly connected nodes. Social networks are examples of assortative networks. This means that an assortative network has the property that almost all nodes with the same degree are linked only between themselves.

The Xulvi-Brunet–Sokolov algorithm for this type of networks is the following. 
In a given network, two links connecting four different nodes are chosen randomly. These nodes are ordered by their degrees. Then, with probability ρ, the links are randomly rewired in such a way that one link connects the two nodes with the smaller degrees and the other connects the two nodes with the larger degrees. If one or both of these links already existed in the network, the step is discarded and is repeated again. Thus, there will be no self-connected nodes or multiple links connecting the same two nodes. Different degrees of assortativity of a network can be achieved by changing the parameter ρ. 
Assortative networks are characterized by highly connected groups of nodes with similar degree. As assortativity grows,the average path length and clustering coefficient increase.

In disassortative networks, highly connected nodes tend to connect to less-well-connected nodes with larger probability than in uncorrelated networks. Examples of such networks include biological networks.
The Xulvi-Brunet and Sokolov’s algorithm for this type of networks is similar to the one for assortative networks with one minor change. As before, two links of four nodes are randomly chose and the nodes are ordered with respect to their degrees. However, in this case, the links are rewired (with probability p) such that one link connects the highest connected node with the node with the lowest degree and the other link connects the two remaining nodes randomly with probability 1 − ρ. Similarly, if the new links already existed, the previous step is repeated. This algorithm does not change the degree of nodes and thus the degree distribution of the network.


</doc>
<doc id="46953393" url="https://en.wikipedia.org/wiki?curid=46953393" title="Zassenhaus algorithm">
Zassenhaus algorithm

In mathematics, the Zassenhaus algorithm
is a method to calculate a basis for the intersection and sum of two subspaces of a vector space.
It is named after Hans Zassenhaus, but no publication of this algorithm by him is known. It is used in computer algebra systems.

Let be a vector space and , two finite-dimensional subspaces of with the following spanning sets:
and
Finally, let formula_3 be linearly independent vectors so that formula_4 and formula_5 can be written as
and

The algorithm computes the base of the sum formula_8 and a base of the intersection formula_9.

The algorithm creates the following block matrix of size formula_10:

Using elementary row operations, this matrix is transformed to the row echelon form. Then, it has the following shape:
Here, formula_13 stands for arbitrary numbers, and the vectors 
formula_14 for every formula_15 and formula_16 for every formula_17 are nonzero.

Then formula_18 with
is a basis of formula_20
and formula_21 with
is a basis of formula_9.

First, we define formula_24 to be the projection to the first component.

Let
formula_25
Then formula_26 and
formula_27.

Also, formula_28 is the kernel of formula_29, the projection restricted to .
Therefore, formula_30.

The Zassenhaus Algorithm calculates a basis of . In the first columns of this matrix, there is a basis formula_31 of formula_20.

The rows of the form formula_33 (with formula_34) are obviously in formula_28. Because the matrix is in row echelon form, they are also linearly independent.
All rows which are different from zero (formula_36 and formula_33) are a basis of , so there are formula_38 such formula_39s. Therefore, the formula_39s form a basis of formula_9.

Consider the two subspaces formula_42 and formula_43 of the vector space formula_44.

Using the standard basis, we create the following matrix of dimension formula_45:

Using elementary row operations, we transform this matrix into the following matrix:

Therefore,
formula_49 is a basis of formula_20, and
formula_51 is a basis of formula_9.



</doc>
<doc id="46877898" url="https://en.wikipedia.org/wiki?curid=46877898" title="Chinese Whispers (clustering method)">
Chinese Whispers (clustering method)

Chinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.

Chinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.

The algorithm works in the following way in an undirected unweighted graph:

The predetermined threshold for the number of the iterations is needed because it is possible that process does not converge. On the other hand in a network with approximately 10000 nodes the clusters does not change significantly after 40-50 iterations even if there is no convergence.

The main strength of Chinese Whispers lies in its time linear property. Because of the processing time increases linearly with the number of nodes, the algorithm is capable of identifying communities in a network very fast. For this reason Chinese Whispers is a good tool to analyze community structures in graph with a very high number of nodes. The effectiveness of the method increases further if the network has the small world property.

On the other hand because the algorithm is not deterministic in the case of small node number the resulting clusters often significantly differ from each other. The reason for this is that in the case of a small network it matters more from which node the iteration process starts while in large networks the relevance of starting points disappears. For this reason for small graphs other clustering methods are recommended.

Chinese Whispers is used in many subfield of network science. Most frequently it is mentioned in the context of natural language processing problems. On the other hand the algorithm is applicable to any kind of community identification problem which is related to a network framework. Chinese Whispers is available for personal use as an extension package for Gephi which is an open source program designed for network analysis.



</doc>
<doc id="47341174" url="https://en.wikipedia.org/wiki?curid=47341174" title="Collaborative diffusion">
Collaborative diffusion

Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of "antiobjects", objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.

Collaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.


</doc>
<doc id="10140499" url="https://en.wikipedia.org/wiki?curid=10140499" title="Algorithm engineering">
Algorithm engineering

Algorithm engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering.
It is a general methodology for algorithmic research.

In 1995, a report from an NSF-sponsored workshop "with the purpose of assessing the current goals and directions of the Theory of Computing (TOC) community" identified the slow speed of adoption of theoretical insights by practitioners as an important issue and suggested measures to
But also, promising algorithmic approaches have been neglected due to difficulties in mathematical analysis.
The term "algorithm engineering" was first used with specificity in 1997, with the first Workshop on Algorithm Engineering (WAE97), organized by Giuseppe F. Italiano.

Algorithm engineering does not intend to replace or compete with algorithm theory, but tries to enrich, refine and reinforce its formal approaches with experimental algorithmics (also called empirical algorithmics).

This way it can provide new insights into the efficiency and performance of algorithms in cases where

Some researchers describe algorithm engineering's methodology as a cycle consisting of algorithm design, analysis, implementation and experimental evaluation, joined by further aspects like machine models or realistic inputs.
They argue that equating algorithm engineering with experimental algorithmics is too limited, because viewing design and analysis, implementation and experimentation as separate activities ignores the crucial feedback loop between those elements of algorithm engineering.

While specific applications are outside the methodology of algorithm engineering, they play an important role in shaping realistic models of the problem and the underlying machine, and supply real inputs and other design parameters for experiments.

Compared to algorithm theory, which usually focuses on the asymptotic behavior of algorithms, algorithm engineers need to keep further requirements in mind: Simplicity of the algorithm, implementability in programming languages on real hardware, and allowing code reuse.
Additionally, constant factors of algorithms have such a considerable impact on real-world inputs that sometimes an algorithm with worse asymptotic behavior performs better in practice due to lower constant factors.

Some problems can be solved with heuristics and randomized algorithms in a simpler and more efficient fashion than with deterministic algorithms. Unfortunately, this makes even simple randomized algorithms "difficult to analyze because there are subtle dependencies to be taken into account".

Huge semantic gaps between theoretical insights, formulated algorithms, programming languages and hardware pose a challenge to efficient implementations of even simple algorithms, because small implementation details can have rippling effects on execution behavior.
The only reliable way to compare several implementations of an algorithm is to spend an considerable amount of time on tuning and profiling, running those algorithms on multiple architectures, and looking at the generated machine code.

See: Experimental algorithmics

Implementations of algorithms used for experiments differ in significant ways from code usable in applications.
While the former prioritizes fast prototyping, performance and instrumentation for measurements during experiments, the latter requires "thorough testing, maintainability, simplicity, and tuning for particular classes of inputs".

Stable, well-tested algorithm libraries like LEDA play an important role in technology transfer by speeding up the adoption of new algorithms in applications. 
Such libraries reduce the required investment and risk for practitioners, because it removes the burden of understanding and implementing the results of academic research.

Two main conferences on Algorithm Engineering are organized annually, namely:

The 1997 Workshop on Algorithm Engineering (WAE'97) was held in Venice (Italy) on September 11–13, 1997. The Third International Workshop on Algorithm Engineering (WAE'99) was held in London, UK in July 1999.
The first Workshop on Algorithm Engineering and Experimentation (ALENEX99) was held in Baltimore, Maryland on January 15–16, 1999. It was sponsored by DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science (at Rutgers University), with additional support from SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory, and SIAM, the Society for Industrial and Applied Mathematics.


</doc>
<doc id="47108875" url="https://en.wikipedia.org/wiki?curid=47108875" title="Jumble algorithm">
Jumble algorithm

Each clue in a Jumble word puzzle is a word that has been “jumbled” by permuting the letters of each word to make an anagram. A dictionary of such anagrams may be used to solve puzzles or verify that a jumbled word is unique when creating puzzles.

Algorithms have been designed to solve Jumbles, using a dictionary. Common algorithms work by printing all words that can be formed from a set of letters. The solver then chooses the right word.

First algorithm:


Second algorithm:


Algorithm to find the permutations of J:


J(1)J(2)

J(2)J(1)


J(1)J(2)J(3)

J(1)J(3)J(2)

J(3)J(1)J(2)

J(2)J(1)J(3)

J(2)J(3)J(1)

J(3)J(2)J(1)


Though the algorithm looks complex it is easy to program.

Douglas Hofstadter developed a program called Jumbo that tries to solve Jumble problems as a human mind would.
The program doesn't rely on a dictionary and doesn't try to find real English words, but rather words that could be English, exploiting a database of plausibilities for various combinations of letters.
Letters are combined non-deterministically, following a strategy inspired by chemical reactions and free associations.


</doc>
<doc id="46900869" url="https://en.wikipedia.org/wiki?curid=46900869" title="Label propagation algorithm">
Label propagation algorithm

Label propagation is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At the start of the algorithm, a (generally small) subset of the data points have labels (or classifications). These labels are propagated to the unlabeled points throughout the course of the algorithm.

Within complex networks, real networks tend to have community structure. Label propagation is an algorithm for finding communities. In comparison with other algorithms label propagation has advantages in its running time and amount of a priori information needed about the network structure (no parameter is required to be known beforehand). The disadvantage is that it produces no unique solution, but an aggregate of many solutions.

At initial condition, the nodes carry a label that denotes the community they belong to. Membership in a community changes based on the labels that the neighboring nodes possess. This change is subject to the maximum number of labels within one degree of the nodes. Every node is initialized with a unique label, then the labels diffuse through the network. Consequently, densely connected groups reach a common label quickly. When many such dense (consensus) groups are created throughout the network, they continue to expand outwards until it is impossible to do so.

The process has 5 steps:

1. Initialize the labels at all nodes in the network. For a given node x, C (0) = x.

2. Set t = 1.

3. Arrange the nodes in the network in a random order and set it to X.

4. For each x ∈ X chosen in that specific order, let C(t) = f(C(t), ...,C(t),C (t − 1), ...,C (t − 1)). f here returns the label occurring with the highest frequency among neighbours. Select a label at random if there are multiple highest frequency labels.

5. If every node has a label that the maximum number of their neighbours have, then stop the algorithm. Else, set t = t + 1 and go to (3).

In contrast with other algorithms label propagation can result in various community structures from the same initial condition. The range of solutions can be narrowed if some nodes are given preliminary labels while others are held unlabelled. Consequently, unlabelled nodes will be more likely to adapt to the labelled ones. For a more accurate finding of communities, Jaccard’s index is used to aggregate multiple community structures, containing all important information.




</doc>
<doc id="38090349" url="https://en.wikipedia.org/wiki?curid=38090349" title="EdgeRank">
EdgeRank

EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the EdgeRank system and uses a machine learning algorithm that, as of 2013, takes more than 100,000 factors into account.

EdgeRank was developed and implemented by Serkan Piantino.

In 2010, a simplified version of the EdgeRank algorithm was presented as:

where:


Some of the methods that Facebook uses to adjust the parameters are proprietary and not available to the public.

EdgeRank and its successors have a broad impact on what users actually see out of what they ostensibly follow: for instance, the selection can produce a filter bubble (if users are exposed to updates which confirm their opinions etc.) or alter people's mood (if users are shown a disproportionate amount of positive or negative updates).

As a result, for Facebook pages, the typical engagement rate is less than 1% (or less than 0.1% for the bigger ones) and organic reach 10% or less for most non-profits.

As a consequence, for pages it may be nearly impossible to reach any significant audience without paying to promote their content.




</doc>
<doc id="46493377" url="https://en.wikipedia.org/wiki?curid=46493377" title="The Algorithm Auction">
The Algorithm Auction

The Algorithm Auction is the world’s first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.

Five lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OKCupid’s original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.

Two other lots were “living algorithms,” including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.

Exhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktivist Richard Jones.

Sebastian Chan, Director of Digital & Emerging Media at the Cooper–Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.

Following the auction, the Museum of Modern Art held a salon titled "The Way of the Algorithm" highlighting algorithms as "a ubiquitous and indispensable component of our lives."


</doc>
<doc id="48786651" url="https://en.wikipedia.org/wiki?curid=48786651" title="Communication-avoiding algorithms">
Communication-avoiding algorithms

Communication-Avoiding Algorithms minimize movement of data within a memory hierarchy for improving its running-time and energy consumption. These minimize the total of two costs (in terms of time and energy): arithmetic and communication. Communication, in this context refers to moving data, either between levels of memory or between multiple processors over a network. It is much more expensive than arithmetic.

Consider the following running-time model:
⇒ Total running time = γ*(no. of FLOPs) + β*(no. of words)

From the fact that β » γ as measured in time and energy, communication cost dominates computation cost. Technological trends indicate that the relative cost of communication is increasing on a variety of platforms, from cloud computing to supercomputers to mobile devices. The report also predicts that gap between DRAM access time and FLOPs will increase 100x over coming decade to balance power usage between processors and DRAM.

Energy consumption increases by orders of magnitude as we go higher in the memory hierarchy. United States president Barack Obama cited Communication-Avoiding Algorithms in the FY 2012 Department of Energy budget request to Congress: "“New Algorithm Improves Performance and Accuracy on Extreme-Scale Computing Systems. On modern computer architectures, communication between processors takes longer than the performance of a floating point arithmetic operation by a given processor. ASCR researchers have developed a new method, derived from commonly used linear algebra methods, to minimize communications between processors and the memory hierarchy, by reformulating the communication patterns specified within the algorithm. This method has been implemented in the TRILINOS framework, a highly-regarded suite of software, which provides functionality for researchers around the world to solve large scale, complex multi-physics problems.”"

Communication-Avoiding algorithms are designed with the following objectives:

The following simple example demonstrates how these are achieved.

Let A, B and C be square matrices of order n x n. The following naive algorithm implements C = C + A * B:

Arithmetic cost (time-complexity): n² (2n-1) for sufficiently large n or O(n³).

Rewriting this algorithm with communication cost labelled at each step

Fast memory may be defined as the local processor memory (CPU cache) of size M and slow memory may be defined as the DRAM.

Communication cost (reads/writes): n³ + 3n² or O(n³)

Since total running time = γ*O(n³) + β*O(n³) and β » γ the communication cost is dominant. The Blocked (Tiled) Matrix Multiplication algorithm reduces this dominant term.

Consider A,B,C to be n/b-by-n/b matrices of b-by-b sub-blocks where b is called the block size; assume 3 b-by-b blocks fit in fast memory.

Communication cost: 2n³/b + 2n² reads/writes « 2n³ arithmetic cost

Making b as large possible:
3b ≤ M 
We achieve the following communication lowerbound:
3n/M + 2n or Ω(no. of FLOPs / M )

Most of the approaches investigated in the past to address this problem rely on scheduling or tuning techniques that aim at overlapping communication with computation. However, this approach can lead to an improvement of at most a factor of two. Ghosting is a different technique for reducing communication, in which a processor stores and computes redundantly data from neighboring processors for future computations. Cache-oblivious algorithms represent a different approach introduced in 1999 for Fast Fourier Transforms, and then extended to graph algorithms, dynamic programming, etc. They were also applied to several operations in linear algebra as dense LU and QR factorizations. The design of architecture specific algorithms is another approach that can be used for reducing the communication in parallel algorithms, and there are many examples in the literature of algorithms that are adapted to a given communication topology.


</doc>
<doc id="48768665" url="https://en.wikipedia.org/wiki?curid=48768665" title="Non-malleable code">
Non-malleable code

The notion of non-malleable codes was introduced in 2010 by Dziembowski, Pietrzak, and Wichs, for relaxing the notion of error-correction and error-detection. Informally, a code is non-malleable if the message contained in a modified code-word is either the original message, or a completely unrelated value. Non-malleable codes provide a useful and meaningful security guarantee in situations where traditional error-correction and error-detection is impossible; for example, when the attacker can completely overwrite the encoded message. Although such codes do not exist if the family of "tampering functions" F is completely unrestricted, they are known to exist for many broad tampering families F.

To know the operation schema of non-malleable code, we have to have a knowledge of the basic experiment it based on. The following is the three step method of tampering experiment.

The tampering experiment can be used to model several interesting real-world settings, such as data transmitted over a noisy channel, or adversarial tampering of data stored in the memory of a physical device. Having this experimental base, we would like to build special encoding/decoding procedures formula_12, which give us some meaningful guarantees about the results of the above tampering experiment, for large and interesting families formula_13 of tampering functions. The following are several possibilities for the type of guarantees that we may hope for.

One very natural guarantee, called error-correction, would be to require that for any tampering function and any "source-message s", the tampering experiment always produces the correct decoded message formula_14.

A weaker guarantee, called error-detection, requires that the tampering-experiment always results in either the correct value formula_14 or a special symbol formula_16 indicating that tampering has been detected. This notion of error-detection is a weaker guarantee than error-correction, and achievable for larger F of tampering functions.

A non-malleable code ensures that either the tampering experiment results in a correct decoded-message formula_14, or the decoded-message formula_10 is completely independent of and unrelated to the "source-message" formula_1. In other word, the notion of non-malleability for codes is similar, in spirit, to notions of non-malleability for cryptographic primitives (such as encryption2, commitments and zero-knowledge proofs), introduced by the seminal work of Dolev, Dwork and Naor.

Compared to error correction or error detection, the "right" formalization of non-malleable codes is somewhat harder to define. Let formula_20 be a random variable for the value of the decoded-message, which results when we run the tampering experiment with source-message formula_1 and tampering-function formula_22, over the randomness of the encoding procedure. Intuitively, we wish to say that the distribution of formula_20 is independent of the encoded message formula_1. Of course, we also want to allow for the case where the tampering experiment results in formula_14 (for example, if the tampering function is identity), which clearly depends on formula_1.

Thus, we require that for every tampering-function formula_5, there exists a distribution formula_28 which outputs either concrete values formula_10 or a special same formula_30 symbol, and faithfully models the distribution of formula_20 for all formula_1 in the following sense: for every source message formula_1, the distributions of formula_20 and formula_28 are statistically close when the formula_30 symbol is interpreted as formula_1. That is, formula_28 correctly simulates the "outcome" of the tampering-experiment with a function formula_5 without knowing the source-messages formula_1, but it is allowed some ambiguity by outputting a same formula_30 symbol to indicate that the decoded-message should be the same as the source-message, without specifying what the exact value is. The fact that formula_28 depends on only formula_22 and not on formula_1, shows that the outcome of formula_20 is independent of formula_1, exempting equality.

Notice that non-malleability is a weaker guarantee than error correction/detection; the latter ensure that any change in the code-word can be corrected or at least detected by the decoding procedure, whereas the former does allow the message to be modified, but only to an unrelated value. However, when studying error correction/detection we usually restrict ourselves to limited forms of tampering which preserve some notion of distance (e.g., usually hamming distance) between the original and tampered code-word. 
For example, it is already impossible to achieve error correction/detection for the simple family of functions formula_47 which, for every constant formula_6, includes a "constant" function formula_49 that maps all inputs to formula_6. There is always some function in formula_47 that maps everything to a valid code-word formula_6. In contrast, it is trivial to construct codes that are non-malleable w.r.t formula_47, as the output of a constant function is clearly independent of its input. The prior works on non-malleable codes show that one can construct non-malleable codes for highly complex tampering function families formula_13 for which error correction/detection can not be achievable.

As one very concrete example, we study non-malleability with respect to the family of functions formula_22 which specify, for each bit of the code-word formula_3, whether to keep it as is, flip it, set it to 0, set it to 1. That is, each bit of the code-word is modified arbitrarily but independently of the value of the other bits of the code-word. We call this the “bit-wise independent tampering” family formula_57. Note that this family contains constant functions formula_47 and constant-error functions formula_59 as subsets. Therefore, as we have mentioned, error-correction and error-detection cannot be achieved w.r.t. this family. Nevertheless, the following can show an efficient non-malleable code for this powerful family.

With formula_57 we denote the family which contains all tampering functions that tamper every bit independently. Formally, this family contains all functions <math>f_i: \left\


</doc>
<doc id="49589765" url="https://en.wikipedia.org/wiki?curid=49589765" title="Kunstweg">
Kunstweg

Bürgi's Kunstweg is a set of algorithms invented by Jost Bürgi at the end of the 16th century. They can be used for the calculation of sines to an arbitrary precision. Bürgi used these algorithms to calculate a Canon Sinuum, a table of sines in steps of 2 arc seconds. It is thought that this table had 8 sexagesimal places. Some authors have speculated that this table only covered the range from 0 to 45 degrees, but nothing seems to support this claim. Such tables were extremely important for navigation at sea. Johannes Kepler called the Canon Sinuum the most precise known table of sines (reference?). Bürgi explained his algorithms in his work Fundamentum Astronomiae which he presented to Emperor Rudolf II. in 1592.

The principles of iterative sine table calculation through the Kunstweg are as follows: cells in a column sum up the values of the two previous cells in the same column. The final cell's value is divided by two, and the next iteration starts. Finally, the values of the last column get normalized. Rather accurate approximations of sines are obtained after few iterations.

As recently as 2015, Folkerts et al. showed that this simple process converges indeed towards the true sines. According to Folkerts, this was the first step towards difference calculus.


</doc>
<doc id="49914674" url="https://en.wikipedia.org/wiki?curid=49914674" title="Online optimization">
Online optimization

Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). These kind of problems are denoted as online problems and are seen as opposed to the classical optimization problems where complete information is assumed (offline). The research on online optimization can be distinguished into online problems where multiple decisions are made sequentially based on a piece-by-piece input and those where a decision is made only once. A famous online problem where a decision is made only once is the Ski rental problem. In general, the output of an online algorithm is compared to the solution of a corresponding offline algorithm which is necessarily always optimal and knows the entire input in advance (competitive analysis).

In many situations, present decisions (for example, resources allocation) must be made with incomplete knowledge of the future or distributional assumptions on the future are not reliable. In such cases, online optimization can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes.

A problem exemplifying the concepts of online algorithms is the Canadian traveller problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed ("failed") is only revealed to "the traveller" when she/he reaches one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual shortest path problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.

There are many formal problems that offer more than one "online algorithm" as solution:


</doc>
<doc id="32612385" url="https://en.wikipedia.org/wiki?curid=32612385" title="Hindley–Milner type system">
Hindley–Milner type system

A Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.

Among HM's more notable properties are its completeness and its ability to infer the most general type of a given program without programmer-supplied type annotations or other hints. Algorithm W is an efficient type inference method that performs in almost linear time with respect to the size of the source, making it practically useful to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell.

One and the same thing can be used for many purposes. A chair might be used to support a sitting person but also as a ladder to stand on while changing a light bulb or as a clothes valet. Beside having particular material qualities, which make a chair usable as such, it also has the particular designation for its use. When no chair is at hand, other things might be used as a seat, and so the designation of a thing can be changed as fast as one can turn an empty bottle crate upside down to change its purpose from a container to that of a support.

Different uses of physically near-identical things are usually accompanied by giving those things different names to emphasize the intended purpose. Depending on the use, seamen have a dozen or more words for a rope though it might materially be the same thing. The same in everyday language, where a leash indicates a use different to a line.

In computer science, this practice of naming things by its intended use is put to an extreme called "typing" and the names or expressions called "types":


Beside structuring objects, (data) types serve as means to validate that these objects are used as intended. Much like a crate that could only be used as a support or a container at a time, a particular arrangement of bytes designated for one purpose might exclude other possible uses.

In programming, these uses are expressed as "functions" or "procedures" which serve the role of verbs in natural language. As an example for typing verbs, an English dictionary might define "gift" as "to give someone something", indicating that the object must be a person and the indirect object a physical thing. In programming, "someone" and "something" would be called types. Using a physical thing in the place of "someone" would be indicated as a programming error by a type checker.

Beside checking, one can use the types in this example to gain knowledge about an unknown word. Reading the sentence "Mary gifts John a bilber" the types could be used to conclude that a "bilber" is likely a physical thing. This activity and conclusion is called "type inference". As the story unfolds, more and more information about the unknown "bilber" may be gained, and eventually enough details become known to form a complete image of that kind of thing.

The type inference method designed by Hindley and Milner does just this for programming languages. The advantage of type inference over type checking is that it allows a more natural and dense style of programming. Instead of starting a program text with a glossary defining what a bilber and everything else is, one can distribute this information over the text simply by using the yet undefined words and let a program collect all the details about them. The method works for both nouns (data types) and for verbs (functions types). As a consequence, a programmer can proceed without ever mentioning types at all, while still having the full support of a type checker that validates their writing. When reading a program, the programmer can use type inference to query the full definition of anything named in the program whenever needed.

Historically, type inference to this extent was developed for a particular group of programming languages, called functional languages. These started in 1958 with Lisp, a programming language based on the lambda calculus and that compares well with modern scripting languages like Python or Lua. Lisp was mainly used for computer science research, often for symbol manipulation purposes where large, tree-like data structures were common.

Data in Lisp is dynamically typed and the types are only available to some degree while running a program. Debugging type errors was no less of a concern than it is with modern script languages. But, being completely untyped, i.e. written without any explicit type information, maintaining large programs written in Lisp soon became a problem because the many complicated types of the data were mentioned only in the program documentation and comments at best.

Thus, the need to have a Lisp-like language with machine-checkable types became more and more pressing. At some point, programming language development faced two challenges:


As an example, polymorphically constructing the list "(1 2)" of two numbers would mean writing:

This example was quite typical. Every third word a type, monotonously serving the type checker in every step. This worsens when the types become more complex. Then, the methods to be expressed in code become buried in types.

To handle this issue, effective methods for type inference were the subject of research, and Hindley–Milner's method was one of them. Their method was first used in ML (1973) and is also used in an extended form in Haskell (1990). The HM type inference method is strong enough to infer types not only for expressions, but for whole programs including the procedures and local definitions, providing a type-less style of programming.

The following text gives an impression of the resulting programming style for the quicksort procedure in Haskell:

Though all of the functions in the above example need type parameters, types are nowhere mentioned. The code is statically type-checked even though the type of the function defined is unknown and must be inferred to type-check the applications in the body.

Over the years, other programming languages added their own version of parametric types. C++ templates were introduced in 1998 and Java introduced generics in 2004. As programming with type parameters became more common, problems similar to the ones sketched for Lisp surfaced in imperative languages too, perhaps not as pressing as it was for the functional languages. As a consequence, these languages obtained support for some type inference techniques, for instance "auto" in C++11 (2014). Typically, the stronger type inference methods developed for functional programming cannot easily be integrated in the imperative languages, as their type systems' features are in part incompatible. However, through the support of additional techniques, it is actually possible to provide Haskell- and ML-style type inference even for a language like C which was designed decades ago, without any consideration for such a mechanism.

Before presenting the HM type system and related algorithms, the following sections make some features of HM more formal and precise.

In a typing, an expression E is opposed to a type T, formally written as E : T. Usually a typing only makes sense within some context, which is omitted here.

In this setting, the following questions are of particular interest:


For the simply typed lambda calculus, all three questions are decidable. The situation is not as comfortable when more expressive types are allowed. Additionally, the simply typed lambda calculus makes the types of the parameters of each function explicit, while they are not needed in HM. While HM is a method for type inference, it can be used also for type checking and answer the first question. To do that, a type is first inferred from E and then compared with the type wanted. The third question becomes of interest when looking at recursively-defined functions at the end of this article.

In the simply typed lambda calculus, types formula_1 are either atomic type constants or function types of form formula_2. Such types are "monomorphic". Typical examples are the types used in arithmetic values:

Contrary to this, the untyped lambda calculus is neutral to typing at all, and many of its functions can be meaningfully applied to all type of arguments. The trivial example is the identity function

which simply returns whatever value it is applied to. Less trivial examples include parametric types like lists.

While polymorphism in general means that operations accept values of more than one type, the polymorphism used here is parametric. One finds the notation of "type schemes" in the literature, too, emphasizing the parametric nature of the polymorphism. Additionally, constants may be typed with (quantified) type variables. E.g.:

Polymorphic types can become monomorphic by consistent substitution of their variables. Examples of monomorphic "instances" are:

More generally, types are polymorphic when they contain type variables, while types without them are monomorphic.

Contrary to the type systems used for example in Pascal (1970) or C (1972), which only support monomorphic types, HM is designed with emphasis on parametric polymorphism. The successors of the languages mentioned, like C++ (1985), focused on different types of polymorphism, namely subtyping in connection with object-oriented programming and overloading. While subtyping is incompatible with HM, a variant of systematic overloading is available in the HM-based type system of Haskell.

When extending the simply-typed lambda calculus towards polymorphism, one has to define when deriving an instance of a value is admissible. Ideally, this would be allowed with any use of a bound variable, as in:

Unfortunately, type inference in polymorphic lambda calculus is not decidable. Instead, HM provides a "let-polymorphism" of the form

restricting the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e. are polymorphic, while the parameters in lambda-abstractions are treated as being monomorphic.

The remainder of the article is more technical as it has to present the HM method as it is handled in the literature. It proceeds as follows:


The same description of the deduction system is used throughout, even for the two algorithms, to make the various forms in which the HM method is presented directly comparable.

The type system can be formally described by syntax rules that fix a language for the expressions, types, etc. The presentation here of such a syntax is not too formal, in that it is written down not to study the surface grammar, but rather the depth grammar, and leaves some syntactical details open. This form of presentation is usual. Building on this, type rules are used to define how expressions and types are related. As before, the form used is a bit liberal.

The expressions to be typed are exactly those of the lambda calculus extended with a let-expression as shown in the adjacent table. Parentheses can be used to disambiguate an expression. The application is left-binding and binds stronger than abstraction or the let-in construct.

Types are syntactically split into two groups, monotypes and polytypes.

Monotypes always designate a particular type. Monotypes formula_4 are syntactically represented as terms.

Examples of monotypes include type constants like formula_5 or formula_6, and parametric types like formula_7. The latter types are examples of "applications" of type functions, for example, from the set
formula_8, 
where the superscript indicates the number of type parameters. The complete set of type functions formula_9 is arbitrary in HM, except that it "must" contain at least formula_10, the type of functions. It is often written in infix notation for convenience. For example, a function mapping integers to strings has type formula_11. Again, parentheses can be used to disambiguate a type expression. The application binds stronger than the infix arrow, which is right-binding.

Type variables are admitted as monotypes. Monotypes are not to be confused with monomorphic types, which exclude variables and allow only ground terms.

Two monotypes are equal if they have identical terms.

"Polytypes" (or "type schemes") are types containing variables bound by one or more for-all quantifiers, e.g. formula_12.

A function with polytype formula_12 can map "any" value of the same type to itself,
and the identity function is a value for this type.

As another example, formula_14 is the type of a function mapping all finite sets to integers. A function which returns the cardinality of a set would be a value of this type.

Quantifiers can only appear top level. For instance, a type formula_15 is excluded by the syntax of types. Also monotypes are included in the polytypes, thus a type has the general form formula_16, where formula_4 is a monotype.

Equality of polytypes is up to reordering the quantification and renaming the quantified variables (formula_18-conversion). Further, quantified variables not occurring in the monotype can be dropped.

To meaningfully bring together the still disjoint parts (syntax expressions and types) a third part is needed: context. Syntactically, a context is a list of pairs formula_19, called assignments, assumptions or bindings, each pair stating that value variable formula_20has type formula_21. All three parts combined give a "typing judgment" of the form formula_22, stating that under assumptions formula_23, the expression formula_24 has type formula_25.

In a type formula_26, the symbol formula_27 is the quantifier binding the type variables formula_28 in the monotype formula_4. The variables formula_28 are called "quantified" and any occurrence of a quantified type variable in formula_4 is called "bound" and all unbound type variables in formula_4 are called "free". Additionally to the quantification formula_27 in polytypes, type variables can also be bound by occurring in the context, but with the inverse effect on the right hand side of the formula_34. Such variables then behave like type constants there. Finally, a type variable may legally occur unbound in a typing, in which case they are implicitly all-quantified.

The presence of both bound and unbound type variables is a bit uncommon in programming languages. Often, all type variables are implicitly treated all-quantified. For instance, one does not have clauses with free variables in Prolog. Likely in Haskell, in the absence of the ScopedTypeVariables language extension, all type variables implicitly occur quantified, i.e. a Haskell type codice_4 means formula_12 here.

Polymorphism means that one and the same expression can have (perhaps
infinitely) many types. But in this type system, these types are not completely
unrelated, but rather orchestrated by the parametric polymorphism.

As an example, the identity formula_36 can have formula_37 as its type as well as
formula_38 or formula_39 and many others, but not formula_40. The most general type for this function is
formula_41, while the
others are more specific and can be derived from the general one by consistently
replacing another type for the "type parameter", i.e. the quantified
variable formula_18. The counter-example fails because the
replacement is not consistent.

The consistent replacement can be made formal by applying a substitution formula_43 to the term of a type formula_4, written formula_45. As the example suggests, substitution is not only strongly related to an order, that expresses that a type is more or less special, but also with the all-quantification which allows the substitution to be applied.
Formally, in HM, a type formula_25 is 
more general than formula_47, formally formula_48 if some quantified variable in formula_25 is
consistently substituted such that one gains formula_47 as shown in the side bar.
This order is part of the type definition of the type system.

While substituting a monomorphic (ground) type for a quantified variable is
straight forward, substituting a polytype has some pitfalls caused by the
presence of free variables. Most particularly, unbound variables must not be
replaced. They are treated as constants here. Additionally, quantifications can only occur top-level. Substituting a parametric type,
one has to lift its quantors. The table on the right makes the rule precise.

Alternatively, consider an equivalent notation for the polytypes without
quantors in which quantified variables are represented by a different set of
symbols. In such a notation, the specialization reduces to plain consistent
replacement of such variables.

The relation formula_51 is a partial order
and formula_52 is its smallest element.

While specialization of a type scheme is one use of the order, it plays a
crucial second role in the type system. Type inference with polymorphism
faces the challenge of summarizing all possible types an expression may have.
The order guarantees that such a summary exists as the most general type
of the expression.

The type order defined above can be extended to typings because the implied all-quantification of typings enables consistent replacement:
Contrary to the specialisation rule, this is not part of the definition, but like the implicit all-quantification rather a consequence of the type rules defined next.
Free type variables in a typing serve as placeholders for possible refinement. The binding effect of the environment to free type
variables on the right hand side of formula_34 that prohibits their substitution in the specialisation rule is again
that a replacement has to be consistent and would need to include the whole typing.

The syntax of HM is carried forward to the syntax of the inference rules that form the body of the formal system, by using the typings as judgments. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.

A proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. The examples below show a possible format of proofs. From left to right, each line shows the conclusion, the formula_55 of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.

The side box shows the deduction rules of the HM type system. One can roughly divide the rules into two groups:

The first four rules formula_56 (variable or function access), formula_57 ("application", i.e. function call with one parameter), formula_58 ("abstraction", i.e. function declaration) and formula_59 (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.

The second group is formed by the remaining two rules formula_60 and formula_61.
They handle specialization and generalization of types. While the rule formula_60 should be clear from the section on specialization above, formula_61 complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables not bound in the context.
The following two examples exercise the rule system in action. Since both the expression and the type are given, they are a type-checking use of the rules.

Example: A proof for formula_64 where formula_65,
could be written

Example: To demonstrate generalization,
formula_67
is shown below:

Not visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules formula_58 and formula_59. Remember that formula_25 and formula_4 denote poly- and monotypes respectively.

In rule formula_58, the value variable of the parameter of the function formula_74 is added to the context with a monomorphic type through the premise formula_75, while in the rule formula_59, the variable enters the environment in polymorphic form formula_77. Though in both cases the presence of formula_78 in the context prevents the use of the generalisation rule for any free variable in the assignment, this regulation forces the type of parameter formula_78 in a formula_80-expression to remain monomorphic, while in a let-expression, the variable could be introduced polymorphic, making specializations possible.

As a consequence of this regulation, formula_81 cannot be typed,
since the parameter formula_82 is in a monomorphic position, while formula_83 has type formula_84, because formula_82 has been introduced in a let-expression and is treated polymorphic therefore.

The generalisation rule is also worth for closer look. Here, the all-quantification implicit in the premise formula_86 is simply moved to the right hand side of formula_87 in the conclusion. This is possible, since formula_18 does not occur free in the context. Again, while this makes the generalisation rule plausible, it is not really a consequence. Vis versa, the generalisation rule is part of the definition of HM's type system and the implicit all-quantification a consequence.

Now that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.
Alternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof are
formed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.

Isolating the points in a proof, where no decision is possible at all,
the first group of rules centered around the syntax leaves no choice since
to each syntactical rule corresponds a unique typing rule, which determines
a part of the proof, while between the conclusion and the premises of these
fixed parts chains of formula_60 and formula_61
could occur. Such a chain could also exist between the conclusion of the
proof and the rule for topmost expression. All proofs must have
the so sketched shape.

Because the only choice in a proof with respect of rule selection are the
formula_60 and formula_61 chains, the
form of the proof suggests the question whether it can be made more precise,
where these chains might be needed. This is in fact possible and leads to a
variant of the rules system with no such rules.

A contemporary treatment of HM uses a purely syntax-directed rule system due to
Clement
as an intermediate step. In this system, the specialization is located directly after the original formula_56 rule
and merged into it, while the generalization becomes part of the formula_59 rule. There the generalization is
also determined to always produce the most general type by introducing the function formula_95, which quantifies
all monotype variables not bound in formula_23.

Formally, to validate, that this new rule system formula_97 is equivalent to the original formula_87, one has
to show that formula_99, which falls apart into two sub-proofs:


While consistency can be seen by decomposing the rules formula_59 and formula_56
of formula_97 into proofs in formula_87, it is likely visible that formula_97 is incomplete, as
one cannot show formula_107 in formula_97, for instance, but only
formula_109. An only slightly weaker version of completeness is provable


implying, one can derive the principal type for an expression in formula_97 allowing us to generalize the proof in the end.

Comparing formula_87 and formula_97, now only monotypes appear in the judgments of all rules. Additionally, the shape of any possible proof with the deduction system is now identical to the shape of the expression (both seen as trees). Thus the expression fully determines the shape of the proof. In formula_87 the shape would likely be determined with respect to all rules except formula_60 and formula_61, which allow building arbitrarily long branches (chains) between the other nodes.

Now that the shape of the proof is known, one is already close to formulating a type inference algorithm.
Because any proof for a given expression must have the same shape, one can assume the monotypes in the
proof's judgements to be undetermined and consider how to determine them.

Here, the substitution (specialisation) order comes into play. Although at the first glance one cannot determine the types locally, the hope is that it is possible to refine them with the help of the order while traversing the proof tree, additionally assuming, because the resulting algorithm is to become an inference method, that the type in any premise will be determined as the best possible. And in fact, one can, as looking at the rules of formula_97 suggests:


The first premise forces the outcome of the inference to be of the form formula_129.

The second premise requires that the inferred type is equal to formula_4 of the first premise. Now there are two possibly different types, perhaps with open type variables, at hand to compare and to make equal if it is possible. If it is, a refinement is found, and if not, a type error is detected again. An effective method is known to "make two terms equal" by substitution, Robinson's Unification in combination with the so-called Union-Find algorithm.

To briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one to group them together into equivalence classes by means of a formula_132
procedure and to pick a representative for each such class using a formula_133 procedure. Emphasizing the word procedure in the sense of side effect, we're clearly leaving the realm of logic in order to prepare an effective algorithm. The representative of a formula_134 is determined such that, if both formula_135 and formula_136 are type variables then the representative is arbitrarily one of them, but while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:

Now having a sketch of an inference algorithm at hand, a more formal presentation is given in the next section. It is described in Milner P. 370 ff. as algorithm J.

The presentation of Algorithm J is a misuse of the notation of logical rules, since it includes side effects but allows a direct comparison with formula_97 while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters formula_138 yielding formula_4 in the conclusion where the execution of the premises proceeds from left to right.

The procedure formula_140 specializes the polytype formula_25 by copying the term and replacing the bound type variables consistently by new monotype variables. 'formula_142' produces a new monotype variable. Likely, formula_95 has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted above, the final result formula_4 has to be generalized to formula_95 in the end, to gain the most general type for a given expression.

Because the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be NP-hard, if not undecidable with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.

Efficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of formula_95 and enable an occurs check to prevent the building of recursive types during formula_147.
An example of such a case is formula_148, for which no type can be derived using HM. Practically, types are only small terms and do not build up expanding structures. Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.

In the previous section, while sketching the algorithm its proof was hinted at with metalogical argumentation. While this leads to an efficient algorithm J, it is
not clear whether the algorithm properly reflects the deduction systems D or S
which serve as a semantic base line.

The most critical point in the above argumentation is the refinement of monotype
variables bound by the context. For instance, the algorithm boldly changes the
context while inferring e.g. formula_149,
because the monotype variable added to the context for the parameter formula_82 later needs to be refined
to formula_151 when handling application.
The problem is that the deduction rules do not allow such a refinement.
Arguing that the refined type could have been added earlier instead of the
monotype variable is an expedient at best.

The key to reaching a formally satisfying argument is to properly include
the context within the refinement. Formally,
typing is compatible with substitution of free type variables.

To refine the free variables thus means to refine the whole typing.

From there, a proof of algorithm J leads to algorithm W, which only makes the
side effects imposed by the procedure formula_153 explicit by
expressing its serial composition by means of the substitutions
formula_154. The presentation of algorithm W in the sidebar still makes use of side effects
in the operations set in italic, but these are now limited to generating
fresh symbols. The form of judgement is formula_155,
denoting a function with a context and expression as parameter producing a monotype together with
a substitution. formula_156 is a side-effect free version
of formula_153 producing a substitution which is the most general unifier.

While algorithm W is normally considered to be "the" HM algorithm and is
often directly presented after the rule system in literature, its purpose is
described by Milner on P. 369 as follows:

While he considered W more complicated and less efficient, he presented it 
in his publication before J. It has its merits when side effects are unavailable or unwanted.
By the way, W is also needed to prove completeness, which is factored by him into the soundness proof.

Before formulating the proof obligations, a deviation between the rules systems
D and S and the algorithms presented needs to be emphasized.

While the development above sort of misused the monotypes as "open" proof variables, the possibility that proper monotype variables might be harmed was sidestepped by introducing fresh variables and hoping for the best. But there's a catch: One of the promises made was that these fresh variables would be "kept in mind" as such. This promise is not fulfilled by the algorithm.

Having a context formula_158, the expression formula_159
cannot be typed in either formula_87 or formula_97, but the algorithms come up with
the type formula_162, where W additionally delivers the substitution formula_163,
meaning that the algorithm fails to detect all type errors. This omission can easily be fixed by more carefully distinguishing proof
variables and monotype variables.

The authors were well aware of the problem but decided not to fix it. One might assume a pragmatic reason behind this.
While more properly implementing the type inference would have enabled the algorithm to deal with abstract monotypes,
they were not needed for the intended application where none of the items in a preexisting context have free
variables. In this light, the unneeded complication was dropped in favor of a simpler algorithm.
The remaining downside is that the proof of the algorithm with respect to the rule system is less general and can only be made
for contexts with formula_164 as a side condition.

formula_165

The side condition in the completeness obligation addresses how the deduction may give many types, while the algorithm always produces one. At the same time, the side condition demands that the type inferred is actually the most general.

To properly prove the obligations one needs to strengthen them first to allow activating the substitution lemma threading the substitution formula_166 through formula_97 and formula_168. From there, the proofs are by induction over the expression.

Another proof obligation is the substitution lemma itself, i.e. the substitution of the typing, which finally establishes the all-quantification. The later cannot formally be proven, since no such syntax is at hand.

To make programming practical recursive functions are needed.
A central property of the lambda calculus is that recursive definitions
are not directly available, but can instead be expressed with a fixed point combinator.
But unfortunately, the fixpoint combinator cannot be formulated in a typed version
of the lambda calculus without having a disastrous effect on the system as outlined
below.

The original paper shows recursion can be realized by a combinator
formula_169. A possible recursive definition could thus be formulated as
formula_170.

Alternatively an extension of the expression syntax and an extra typing rule is possible:

where
basically merging formula_58 and formula_59 while including the recursively defined
variables in monotype positions where they occur to the left of the formula_176 but as polytypes to the right of it. This
formulation perhaps best summarizes the essence of let-polymorphism.

While the above is straightforward it does come at a price.

Type theory connects lambda calculus computation and logic.
The easy modification above has effects on both:


Programs in simply typed lambda calculus are guaranteed to always terminate. Moreover, they
are even guaranteed to terminate under any evaluation strategy, be it top down, bottom up, breadth first, whatever. The same is true for expressions that have types in HM. It is well-known that separating
terminating from non-terminating programs is most difficult, and especially in lambda calculus,
which is so expressive that it can formulate recursion with just a few symbols. Thus the initial
inability of HM to provide recursive functions was not an omission, but a feature. Adding
recursion enables normal programming but the guarantee is not longer valid.

Another reading of the typing is given by the Curry–Howard isomorphism. Here
the types are interpreted as logical expressions. Let's look at the type of the fixpoint combinator from this
perspective, assuming the variables to have logical value:
But this is invalid.
Adding an invalid axiom will break the logic in the sense that
every formula can then be shown to be true in it, e.g. formula_178.
Thus the ability to distinguish even two simple things is no longer given. Everything is the same and
collapses into 42. The fixpoint
combinator that came in so handy above also plays a role in Curry's paradox.

Logic aside, does this matter for typing programs? It does. Since one is now able to formulate
non-terminating functions, one can make a function that would return whatever one wants but never really returns:
In practical programming such a function can come in handy when breaking out of a computation,
like with codice_5 in C, while silencing the type checker in
the current branch by returning essentially nothing but with a suitable type.

Less desirable is that the type checker (type inferencer) now succeeds with a type for a function that in fact never returns any value, like formula_180. The function "would" return a value of this type, but it "cannot" because no terminating function with this type exists. The type checker's claim that everything is ok thus has to be taken with a grain of salt. The types might only be "claimed" to be checked, but the program can still be typed wrong. Only if all functions are terminating does formula_18 in the logic above have a "true" value, and the assertions of the type checker become strong again.

Overloading means, that different functions still can be defined and used with the same name. Most programming languages at least provide overloading with the built-in arithmetic operations (+,<,etc.), to allow the programmer to write arithmetic expressions in the same form, even for different numerical types like codice_6 or codice_7. Because a mixture of these different types within the same expression also demands for implicit conversion, overloading especially for these operations is often built into the programming language itself. In some languages, this feature is generalized and made available to the user, e.g. in C++.

While ad-hoc overloading has been avoided in functional programming for the computation costs both in type checking and inference, a means to systematise overloading has been introduced that resembles both in form and naming to object oriented programming, but works one level upwards. "Instances" in this systematic are not objects (i.e. on value level), but rather types.
The quicksort example mentioned in the introduction uses the overloading in the orders, having the following type annotation in Haskell:

Herein, the type codice_8 is not only polymorphic, but also restricted to be an instance of some type class codice_9, that provides the order predicates codice_10 and codice_11 used in the functions body. The proper implementations of these predicates are then passed to quicksorts as additional parameters, as soon as quicksort is used on more concrete types providing a single implementation of the overloaded function quickSort.

Because the "classes" only allow a single type as their argument, the resulting type system can still provide inference. Additionally, the type classes can then be equipped with some kind of overloading order allowing one to arrange the classes as a lattice.

Parametric polymorphism implies that types themselves are passed as parameters as if they were proper values. Passed as arguments into a proper functions as in the introduction, but also into "type functions" as in the "parametric" type constants, leads to the question how to more properly type types themselves. A meta type, the "type of types" would be useful to create an even more expressive type system.

Though this would be a straight forward extension, unfortunately, only unification is not longer decidable in the presence of meta types, rendering type inference impossible in this extend of generality.
Additionally, assuming a type of all types that includes itself as type leads into a paradox, as in the set of all sets, so one must proceed in steps of levels of abstraction.
Research in second order lambda calculus, one step upwards, showed, that type inference is undecidable in this generality.

Parts of one extra level has been introduced into Haskell named kind, where it is used helping to type monads. Kinds are left implicit, working behind the scenes in the inner mechanics of the extended type system.

Attempts to combine subtyping and type inference have caused quite some frustration. While type inference is needed in object-oriented programming for the same reason as in functional languages, methods like HM cannot be made going for this purpose. It is not difficult to set up a type system with subtyping enabling object-oriented style, as e.g. Cardelli


Such objects would be immutable in a functional language context, but the type system would enable object-oriented programming style and the type inference method could be reused in imperative languages.

The subtyping rule for the record types is:
Syntatically, record expressions would have form
and have a type rule leading to the above type.
Such record values could then be used the same way as objects in object-oriented programming.




</doc>
<doc id="50546680" url="https://en.wikipedia.org/wiki?curid=50546680" title="Distributed tree search">
Distributed tree search

Distributed tree search (DTS) algorithm is a class of algorithms for searching values in an efficient and distributed manner. Their purpose is to iterate through a tree by working along multiple branches in parallel and merging the results of each branch into one common solution, in order to minimize time spent searching for a value in a tree-like data structure.

The original paper was written in 1988 by Chris Ferguson and Richard E. Korf, from the University of California's Computer Science Department. They used multiple other chess AIs to develop this wider range algorithm.

The Distributed Tree Search Algorithm (also known as Korf–Ferguson algorithm) was created to solve the following problem: "Given a tree with non-uniform branching factor and depth, search it in parallel with an arbitrary number of processors as fast as possible."

The top-level part of this algorithm is general and does not use a particular existing type of tree-search, but it can be easily specialized to fit any type of non-distributed tree-search.

DTS consists of using multiple processes, each with a node and a set of processors attached, with the goal of searching the sub-tree below the said node. Each process then divides itself into multiple coordinated sub-processes which recursively divide themselves again until an optimal way to search the tree has been found based on the number of processors available to each process. Once a process finishes, DTS dynamically reassigns the processors to other processes as to keep the efficiency to a maximum through good load-balancing, especially in irregular trees.

Once a process finishes searching, it recursively sends and merges a resulting signal to its parent-process, until all the different sub-answers have been merged and the entire problem has been solved.

DTS is only applicable under two major conditions: the data structure to search through is a tree, and the algorithm can make use of at least one computation unit (Although it cannot be considered as distributed if there is only one).

One major example of the everyday use of DTS is network routing. The Internet can be seen as a tree of IP addresses, and an analogy to a routing protocol could be how post offices work in the real world. Since there are over 4.3 billion IP addresses currently, society heavily depends on the time the data takes to find its way to its destination. As such, IP-routing divides the work into multiple sub-units which each have different scales of calculation capabilities and use each other's result to find the route in a very efficient manner. This is an instance of DTS that affects over 43% of the world's population, for reasons going from entertainment to national security.

Although DTS is currently one of the most widely used algorithms, many of its applications have alternatives to them which could potentially develop into more efficient, less resource-demanding solutions, were they more researched.

One of the more controversial examples is Big-Data processing. In applications like Google Search Engine, Facebook, YouTube, search needs to be optimized to keep waiting time inside a reasonable window. This could be achieved through the plain use of DTS, but other algorithms are used in place (for example data-hashing in SQL databases), or in conjunction (Facebook's Haystack algorithm groups parallel tree-search, data-hashing and memory-ordering/sorting).

One of the more important limits of DTS is the fact that it requires a tree as input. Trees are a sub-instance of a data structure known as Graphs, which means every Graph can be converted into a tree. Although there currently exists no better way to search through trees than Korf-Ferguson's algorithm, each task has different particularities and in most cases, there will exist more efficient data structures to represent the problem and solve it than through tree-search. And so there exist instances of tree structures with cycles that cannot possibly be faster than a graph-search on the same structure with the same processing power.

There are few controversies around Korf-Ferguson's DTS algorithm, since it is recognized as very complete, but simple. It is very often used as a stepping stone for students to discover the fundamentals and key concepts of distributed problem-solving.

The most important challenge to this algorithmic concept was an article by Kröll B, « Balanced Distributed Search Trees Do Not Exist », which does not attack the veracity or current efficiency of the algorithm, but rather the fact that DTS itself, no matter how many improvements are made to it (for example balancing the input tree before-hand), will never be able to reach optimal resolution-time. This opens a new view point: are too many resources used into the completion of DTS, which blocks new algorithms with higher efficiency-potential from getting researched and developed? Another limit of DTS is the fact that no matter how efficient the division, coordination and merging of the solutions is, it will always be limited by the material number or processors and their processing power. Until recently, this was admitted as being a limit to nearly every computation, but new-generation algorithms like Euclideon might one day be able to crush DFS's efficiency through processing-power-independent problem resolution.




</doc>
<doc id="51017812" url="https://en.wikipedia.org/wiki?curid=51017812" title="DONE">
DONE

The Data-based Online Nonlinear Extremumseeker (DONE) algorithm is a black-box optimization algorithm.
DONE models the unknown cost function and attempts to find an optimum of the underlying function.
The DONE algorithm is suitable for optimizing costly and noisy functions and does not require derivatives.
An advantage of DONE over similar algorithms, such as Bayesian optimization, is that the computational cost per iteration is independent of the number of function evaluations.

The DONE algorithm was first proposed by Hans Verstraete and Sander Wahls. The algorithm fits a surrogate model based on random Fourier features and then uses a well-known L-BFGS algorithm to find an optimum of the surrogate model.

DONE was first demonstrated for maximizing the signal in optical coherence tomography measurements, but has since then been applied to various other applications. For example, it was used to help extending the field of view in light sheet fluorescence microscopy.


</doc>
<doc id="50959785" url="https://en.wikipedia.org/wiki?curid=50959785" title="KHOPCA clustering algorithm">
KHOPCA clustering algorithm

KHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. KHOPCA (formula_1-hop clustering algorithm) provides a fully distributed and localized approach to group elements such as nodes in a network according to their distance from each other. KHOPCA operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.

KHOPCA's clustering process explicitly supports joining and leaving of nodes, which makes KHOPCA suitable for highly dynamic networks. However, it has been demonstrated that KHOPCA also performs in static networks.

Besides applications in ad hoc and wireless sensor networks, KHOPCA can be used in localization and navigation problems, networked swarming, and real-time data clustering and analysis.

KHOPCA (formula_1-hop clustering algorithm) operates proactively through a simple set of rules that defines clusters with variable formula_1-hops. A set of local rules describes the state transition between nodes. A node's weight is determined only depending on the current state of its neighbors in communication range. Each node of the network is continuously involved in this process. As result, formula_1-hop clusters are formed and maintained in static as well as dynamic networks.

KHOPCA does not require any predetermined initial configuration. Therefore, a node can potentially choose any weight (between formula_5 and formula_6). However, the choice of the initial configuration does influence the convergence time.

The prerequisites in the start configuration for the application of the rules are the following.
The following rules describe the state transition for a node formula_9 with weight formula_25. These rules have to be executed on each node in the order described here.

The first rule has the function of constructing an order within the cluster. This happens through a node formula_9 detects the direct neighbor with the highest weight formula_27, which is higher than the node's own weight formula_25. If such a direct neighbor is detected, the node formula_9 changes its own weight to be the weight of the highest weight within the neighborhood subtracted by 1. Applied iteratively, this process creates a top-to-down hierarchical cluster structure.
if max(W(N(n))) > w_n

The second rule deals with the situation where nodes in a neighborhood are on the minimum weight level. This situation can happen if, for instance, the initial configuration assigns the minimum weight to all nodes. If there is a neighborhood with all nodes having the minimum weight level, the node formula_9 declares itself as cluster center. Even if coincidently all nodes declare themselves as cluster centers, the conflict situation will be resolved by one of the other rules.
if max(W(N(n)) == MIN & w_n == MIN

The third rule describes situations where nodes with leveraged weight values, which are not cluster centers, attract surrounding nodes with lower weights. This behavior can lead to fragmented clusters without a cluster center. In order to avoid fragmented clusters, the node with higher weight value is supposed to successively decrease its own weight with the objective to correct the fragmentation by allowing the other nodes to reconfigure according to the rules. 
if max(W(N(n))) <= w_n && w_n != MAX

The fourth rule resolves the situation where two cluster centers connect in 1-hop neighborhood and need to decide which cluster center should continue its role as cluster center. Given any specific criterion (e.g., device ID, battery power), one cluster center remains while the other cluster center is hierarchized in 1-hop neighborhood to that new cluster center. The choice of the specific criterion to resolve the decision-making depends on the used application scenario and on the available information. 
if max(W(N(n)) == MAX && w_n == MAX

An exemplary sequence of state transitions applying the described four rules is illustrated below.

KHOPCA acting in a dynamic 2-D simulation. The geometry is based on a geometric random graph; all existing links are drawn in this network.

KHOPCA also works in a dynamic 3-D environment. The cluster connections are illustrated with bold lines.

It has been demonstrated that KHOPCA terminates after a finite number of state transitions in static networks.


</doc>
<doc id="51386092" url="https://en.wikipedia.org/wiki?curid=51386092" title="Certifying algorithm">
Certifying algorithm

In theoretical computer science, a certifying algorithm is an algorithm that outputs, together with a solution to the problem it solves, a proof that the solution is correct. A certifying algorithm is said to be "efficient" if the combined runtime of the algorithm and a proof checker is slower by at most a constant factor than the best known non-certifying algorithm for the same problem.

The proof produced by a certifying algorithm should be in some sense simpler than the algorithm itself, for otherwise any algorithm could be considered certifying (with its output verified by running the same algorithm again). Sometimes this is formalized by requiring that a verification of the proof take less time than the original algorithm, while for other problems (in particular those for which the solution can be found in linear time) simplicity of the output proof is considered in a less formal sense. For instance, the validity of the output proof may be more apparent to human users than the correctness of the algorithm, or a checker for the proof may be more amenable to formal verification.

Implementations of certifying algorithms that also include a checker for the proof generated by the algorithm may be considered to be more reliable than non-certifying algorithms. For, whenever the algorithm is run, one of three things happens: it produces a correct output (the desired case), it detects a bug in the algorithm or its implication (undesired, but generally preferable to continuing without detecting the bug), or both the algorithm and the checker are faulty in a way that masks the bug and prevents it from being detected (undesired, but unlikely as it depends on the existence of two independent bugs).

Many examples of problems with checkable algorithms come from graph theory.
For instance, a classical algorithm for testing whether a graph is bipartite would simply output a Boolean value: true if the graph is bipartite, false otherwise. In contrast, a certifying algorithm might output a 2-coloring of the graph in the case that it is bipartite, or a cycle of odd length if it is not. Any graph is bipartite if and only if it can be 2-colored, and non-bipartite if and only if it contains an odd cycle. Both checking whether a 2-coloring is valid and checking whether a given odd-length sequence of vertices is a cycle may be performed more simply than testing bipartiteness.

Analogously, it is possible to test whether a given directed graph is acyclic by a certifying algorithm that outputs either a topological order or a directed cycle. It is possible to test whether an undirected graph is a chordal graph by a certifying algorithm that outputs either an elimination ordering (an ordering of all vertices such that, for every vertex, the neighbors that are later in the ordering form a clique) or a chordless cycle. And it is possible to test whether a graph is planar by a certifying algorithm that outputs either a planar embedding or a Kuratowski subgraph.

The extended Euclidean algorithm for the greatest common divisor of two integers and is certifying: it outputs three integers (the divisor), , and , such that . This equation can only be true of multiples of the greatest common divisor, so testing that is the greatest common divisor may be performed by checking that divides both and and that this equation is correct.



</doc>
<doc id="51411922" url="https://en.wikipedia.org/wiki?curid=51411922" title="Algorithmic paradigm">
Algorithmic paradigm

An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.





</doc>
<doc id="52242050" url="https://en.wikipedia.org/wiki?curid=52242050" title="Multiplicative weight update method">
Multiplicative weight update method

The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.

"Multiplicative weights" implies the iterative rule used in algorithms derived from the multiplicative weight update method. It is given with different names in the different fields where it was discovered or rediscovered.

The earliest known version of this technique was in an algorithm named "fictitious play" which was proposed in game theory in the early 1950s. Grigoriadis and Khachiyan applied a randomized variant of "fictitious play" to solve two-player zero-sum games efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In machine learning, Littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm, which is similar to Minsky and Papert's earlier perceptron learning algorithm. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm.

The multiplicative weights algorithm is also widely applied in computational geometry such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find set covers for hypergraphs with small VC dimension.

In operation research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.

In computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavan's method of pessimistic estimators for derandomization of randomized rounding algorithms; Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yao's XOR Lemma; Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases.

A binary decision needs to be made based on n experts’ opinions to attain an associated payoff. In the first round, all experts’ opinions have the same weight. The decision maker will make the first decision based on the majority of the experts' prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each expert's opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down.

Given a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistakes will be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most mistakes.

Unlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same "expert advice" setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts.
The very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm.

If formula_10, the weight of the expert's advice will remain the same. When formula_11 increases, the weight of the expert's advice will decrease. Note that some researchers fix formula_12 in weighted majority algorithm.

After formula_6 steps, let formula_14 be the number of mistakes of expert i and formula_15 be the number of mistakes our algorithm has made. Then we have the following bound for every formula_16:

In particular, this holds for i which is the best expert. Since the best expert will have the least formula_14, it will give the best bound on the number of mistakes made by the algorithm as a whole.

Given the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction:

predict 

where 

The number of mistakes made by the randomized weighted majority algorithm is bounded as: 

where formula_22 and formula_23.

Note that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction.
In this randomized algorithm, formula_24 if formula_25. Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people define formula_12 in weighted majority algorithm and allow formula_27 in randomized weighted majority algorithm.

The multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event.

Suppose we were given the distribution formula_28 on experts. Let formula_29 = payoff matrix of a finite two-player zero-sum game, with formula_30 rows.

When the row player formula_31 uses plan formula_16 and the column player formula_33 uses plan formula_34, the payoff of player formula_33 is formula_36≔formula_37, assuming formula_38.

If player formula_31 chooses action formula_16 from a distribution formula_28 over the rows, then the expected result for player formula_33 selecting action formula_34 is formula_44.

To maximize formula_45, player formula_33 is should choose plan formula_34. Similarly, the expected payoff for player formula_48 is formula_49. Choosing plan formula_16 would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain:

where P and i changes over the distributions over rows, Q and j changes over the columns.

Then, let formula_52 denote the common value of above quantities, also named as the "value of the game". Let formula_53 be an error parameter. To solve the zero-sum game bounded by additive error of formula_54,

So there is an algorithm solving zero-sum game up to an additive factor of δ using O(/formula_57) calls to ORACLE, with an additional processing time of O(n) per call

Bailey and Piliouras showed that although the time average behavior of multiplicative weights update converges to Nash equilibria in zero-sum games the day-to-day (last iterate) behavior diverges away from it.

In machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method.

Based on current knowledge in algorithms, multiplicative weight update method was first used in Littlestone's winnow algorithm. It is used in machine learning to solve a linear program.

Given formula_58 labeled examples formula_59 where formula_60 are feature vectors, and formula_61 are their labels.

The aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that formula_62 for all formula_34. Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine formula_64 to be formula_65, the problem reduces to finding a solution to the following LP:

This is general form of LP.

The hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different.
It is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update.

Assume the learning rate formula_69 and for formula_70, formula_71 is picked by Hedge. Then for all experts formula_16,

Initialization: Fix an formula_69. For each expert, associate the weight formula_75 ≔1
For t=1,2,…,T:

This algorithm maintains a set of weights formula_80 over the training examples. On every iteration formula_3, a distribution formula_71 is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis formula_83 that (hopefully) has small error with respect to the distribution. Using the new hypothesis formula_83, AdaBoost generates the next weight vector formula_85. The process repeats. After T such iterations, the final hypothesis formula_86 is the output. The hypothesis formula_86 combines the outputs of the T weak hypotheses using a weighted majority vote.

Given a formula_109 matrix formula_29 and formula_111, is there a formula_112 such that formula_113?

Using the oracle algorithm in solving zero-sum problem, with an error parameter formula_115, the output would either be a point formula_112 such that formula_117 or a proof that formula_112 does not exist, i.e., there is no solution to this linear system of inequalities.

Given vector formula_119, solves the following relaxed problem

If there exists a x satisfying (1), then x satisfies (2) for all formula_121. The contrapositive of this statement is also true.
Suppose if oracle returns a feasible solution for a formula_122, the solution formula_112 it returns has bounded width formula_124.
So if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of formula_125. The algorithm makes at most formula_126 calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case.

Multiplicative weights update is the discrete-time variant of the replicator equation (replicator dynamics), which is a commonly used model in evolutionary game theory. It converges to Nash equilibrium when applied to a congestion game.

In operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.

The multiplicative weights algorithm is also widely applied in computational geometry, such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find Set Covers for hypergraphs with small VC dimension.


</doc>
<doc id="52280151" url="https://en.wikipedia.org/wiki?curid=52280151" title="Incremental learning">
Incremental learning

In computer science, incremental learning is a method of machine learning in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.

Many traditional machine learning algorithms inherently support incremental learning.
Other algorithms can be adapted to facilitate incremental learning. 
Examples of incremental algorithms include
decision trees
(IDE4,
ID5R),
decision rules,
artificial neural networks
(RBF networks,
Learn++,
Fuzzy ARTMAP,
TopoART, and
IGNG) or
the incremental SVM.

The aim of incremental learning is for the learning model to adapt to new data without forgetting its existing knowledge, it does not retrain the model. Some incremental learners have built-in some parameter or assumption that controls the relevancy of old data, while others, called stable incremental machine learning algorithms, learn representations of the training data that are not even partially forgotten over time. Fuzzy ART and TopoART are two examples for this second approach.

Incremental algorithms are frequently applied to data streams or big data, addressing issues in data availability and resource scarcity respectively. Stock trend prediction and user profiling are some examples of data streams where new data becomes continuously available. Applying incremental learning to big data aims to produce faster classification or forecasting times.



</doc>
<doc id="214269" url="https://en.wikipedia.org/wiki?curid=214269" title="Emergent algorithm">
Emergent algorithm

An emergent algorithm is an algorithm that exhibits emergent behavior. In essence an emergent algorithm implements a set of simple "building block" behaviors that when combined exhibit more complex behaviors. One example of this is the implementation of fuzzy motion controllers used to adapt robot movement in response to environmental obstacles.

An emergent algorithm has the following characteristics: 


Other examples of emergent algorithms and models include cellular automata, artificial neural networks and swarm intelligence systems (ant colony optimization, bees algorithm, etc.).



</doc>
<doc id="1773852" url="https://en.wikipedia.org/wiki?curid=1773852" title="Gutmann method">
Gutmann method

The Gutmann method is an algorithm for securely erasing the contents of computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper "Secure Deletion of Data from Magnetic and Solid-State Memory" in July 1996, it involved writing a series of 35 patterns over the region to be erased.

The selection of patterns assumes that the user does not know the encoding mechanism used by the drive, so it includes patterns designed specifically for three types of drives. A user who knows which type of encoding the drive uses can choose only those patterns intended for their drive. A drive with a different encoding mechanism would need different patterns.

Most of the patterns in the Gutmann method were designed for older MFM/RLL encoded disks. Gutmann himself has noted that more modern drives no longer use these older encoding techniques, making parts of the method irrelevant. He said "In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques".

Since about 2001, some ATA IDE and SATA hard drive manufacturer designs include support for the ATA Secure Erase standard, obviating the need to apply the Gutmann method when erasing an entire drive. However, a 2011 research found that 4 out of 8 manufacturers did not implement ATA Secure Erase correctly.

One standard way to recover data that has been overwritten on a hard drive is to capture and process the analog signal obtained from the drive's read/write head prior to this analog signal being digitized. This analog signal will be close to an ideal digital signal, but the differences will reveal important information. By calculating the ideal digital signal and then subtracting it from the actual analog signal, it is possible to amplify the obtained difference signal and use it to determine what had previously been written on the disk.

For example:

This can then be done again to see the previous data written:

However, even when overwriting the disk repeatedly with random data it is theoretically possible to recover the previous signal. The permittivity of a medium changes with the frequency of the magnetic field. This means that a lower frequency field will penetrate deeper into the magnetic material on the drive than a high frequency one. So a low frequency signal will, in theory, still be detectable even after it has been overwritten hundreds of times by a high frequency signal.

The patterns used are designed to apply alternating magnetic fields of various frequencies and various phases to the drive surface and thereby approximate degaussing the material below the surface of the drive.

An overwrite session consists of a lead-in of four random write patterns, followed by patterns 5 to 31 (see rows of table below), executed in a random order, and a lead-out of four more random patterns.

Each of patterns 5 to 31 was designed with a specific magnetic media encoding scheme in mind, which each pattern targets. The drive is written to for all the passes even though the table below only shows the bit patterns for the passes that are specifically targeted at each encoding scheme. The end result should obscure any data on the drive so that only the most advanced physical scanning (e.g., using a magnetic force microscope) of the drive is likely to be able to recover any data. 

The series of patterns is as follows:

Encoded bits shown in bold are what should be present in the ideal pattern, although due to the encoding the complementary bit is actually present at the start of the track.

The delete function in most operating systems simply marks the space occupied by the file as reusable (removes the pointer to the file) without immediately removing any of its contents. At this point the file can be fairly easily recovered by numerous recovery applications. However, once the space is overwritten with other data, there is no known way to use software to recover it. It cannot be done with software alone since the storage device only returns its current contents via its normal interface. Gutmann claims that intelligence agencies have sophisticated tools, including magnetic force microscopes, which together with image analysis, can detect the previous values of bits on the affected area of the media (for example hard disk).

Daniel Feenberg of the National Bureau of Economic Research, an American private nonprofit research organization, criticized Gutmann's claim that intelligence agencies are likely to be able to read overwritten data, citing a lack of evidence for such claims. Nevertheless, some published government security procedures consider a disk overwritten once to still be sensitive.

Gutmann himself has responded to some of these criticisms and also criticized how his algorithm has been abused in an epilogue to his original paper, in which he states:



</doc>
<doc id="45655492" url="https://en.wikipedia.org/wiki?curid=45655492" title="Wiener connector">
Wiener connector

In mathematics applied to the study of networks, the Wiener connector, named in honor of chemist Harry Wiener who first introduced the Wiener Index, is a means of maximizing efficiency in connecting specified "query vertices" in a network. Given a connected, undirected graph and a set of query vertices in a graph, the minimum Wiener connector is an induced subgraph that connects the query vertices and minimizes the sum of shortest path distances among all pairs of vertices in the subgraph. In combinatorial optimization, the minimum Wiener connector problem is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic Steiner tree problem (one of Karp's 21 NP-complete problems), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.

The minimum Wiener connector was first presented by Ruchansky, et al. in 2015.

The minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?

The Wiener index is the sum of shortest path distances in a (sub)graph. Using formula_1 to denote the shortest path between formula_2 and formula_3, the Wiener index of a (sub)graph formula_4, denoted formula_5, is defined as

The minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set formula_7 and edge set formula_8 and a set of query vertices formula_9, find a connector formula_10 of minimum Wiener index. More formally, the problem is to compute
that is, find a connector formula_12 that minimizes the sum of shortest paths in formula_12.

The minimum Wiener connector problem is related to the Steiner tree problem. In the former, the objective function in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.

The problem is NP-hard, and does not admit a polynomial-time approximation scheme unless P = NP. This can be proven using the inapproximability of vertex cover in bounded degree graphs. Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation—an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of complexity classes, the minimum Wiener connector problem is in APX but is not in PTAS unless P = NP.

An exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in formula_14 time (that is, exponential time) on graphs with "n" vertices. In the special case that there are exactly two query vertices, the optimum solution is the shortest path joining the two vertices, so the problem can be solved in polynomial time by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.

There is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time formula_15 on a graph with "n" vertices, "m" edges, and "q" query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph. The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.

The minimum Wiener connector behaves like betweenness centrality.

When the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community. Such vertices are likely to be influential vertices playing leadership roles in the community. In a social network, these influential vertices might be good users for spreading information or to target in a viral marketing campaign.

When the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a structural hole in the graph and are important.

The minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,


</doc>
<doc id="39045480" url="https://en.wikipedia.org/wiki?curid=39045480" title="Automate This">
Automate This

Automate This: How Algorithms Came to Rule Our World is a book written by Christopher Steiner and published by Penguin Group. Steiner begins his study of algorithms on Wall Street in the 1980s but also provides examples from other industries. For example, he explains the history of Pandora Radio and the use of algorithms in music identification. He expresses concern that such use of algorithms may lead to the homogenization of music over time. Steiner also discusses the algorithms that eLoyalty (now owned by Mattersight Corporation following divestiture of the technology) was created by dissecting 2 million speech patterns and can now identify a caller's personality style and direct the caller with a compatible customer support representative.

Steiner's book shares both the warning and the opportunity that algorithms bring to just about every industry in the world, and the pros and cons of the societal impact of automation (e.g. impact on employment).



</doc>
<doc id="46902242" url="https://en.wikipedia.org/wiki?curid=46902242" title="Lancichinetti–Fortunato–Radicchi benchmark">
Lancichinetti–Fortunato–Radicchi benchmark

Lancichinetti–Fortunato–Radicchi benchmark is an algorithm that generates benchmark networks (artificial networks that resemble real-world networks). They have "a priori" known communities and are used to compare different community detection methods. The advantage of the benchmark over other methods is that it accounts for the heterogeneity in the distributions of node degrees and of community sizes.

The node degrees and the community sizes are distributed according to a power law, with different exponents. The benchmark assumes that both the degree and the community size have power law distributions with different exponents, formula_1 and formula_2, respectively. formula_3 is the number of nodes and the average degree is formula_4. There is a mixing parameter formula_5, which is the average fraction of neighboring nodes of a node that do not belong to any community that the benchmark node belongs to. This parameter controls the fraction of edges that are between communities. Thus, it reflects the amount of noise in the network. At the extremes, when formula_6 all links are within community links, if formula_7 all links are between nodes belonging to different communities.

One can generate the benchmark network using the following steps.

Step 1: Generate a network with nodes following a power law distribution with exponent formula_1 and choose extremes of the distribution formula_9 and formula_10 to get desired average degree is formula_11.

Step 2: formula_12 fraction of links of every node is with nodes of the same community, while fraction formula_5 is with the other nodes.

Step 3: Generate community sizes from a power law distribution with exponent formula_2. The sum of all sizes must be equal to formula_3. The minimal and maximal community sizes formula_16 and formula_17 must satisfy the definition of community so that every non-isolated node is in at least in one community:

Step 4: Initially, no nodes are assigned to communities. Then, each node is randomly assigned to a community. As long as the number of neighboring nodes within the community does not exceed the community size a new node is added to the community, otherwise stays out. In the following iterations the “homeless” node is randomly assigned to some community. If that community is complete, i.e. the size is exhausted, a randomly selected node of that community must be unlinked. Stop the iteration when all the communities are complete and all the nodes belong to at least one community.

Step 5: Implement rewiring of nodes keeping the same node degrees but only affecting the fraction of internal and external links such that the number of links outside the community for each node is approximately equal to the mixing parameter formula_5.

Consider a partition into communities that do not overlap. The communities of randomly chosen nodes in each iteration follow a formula_21 distribution that represents the probability that a randomly picked node is from the community formula_22. Consider a partition of the same network that was predicted by some community finding algorithm and has formula_23 distribution. The benchmark partition has formula_24 distribution.
The joint distribution is formula_25. The similarity of these two partitions is captured by the normalized mutual information.

If formula_27 the benchmark and the detected partitions are identical, and if formula_28 then they are independent of each other.


</doc>
<doc id="53596792" url="https://en.wikipedia.org/wiki?curid=53596792" title="FGLM algorithm">
FGLM algorithm

FGLM is one of the main algorithms in computer algebra, named after its designers, Faugère, Gianni, Lazard and Mora. They introduced their algorithm in 1993. The input of the algorithm is a Gröbner basis of a zero-dimensional ideal in the ring of polynomials over a field with respect to a monomial order and a second monomial order; As its output, it returns a Gröbner basis of the ideal with respect to the second ordering. The algorithm is a fundamental tool in computer algebra and has been implemented in most of the computer algebra systems. The complexity of FGLM is "O"("nD"), where "n" is the number of variables of the polynomials and D is the degree of the ideal. There are several generalization and various application for FGLM.


</doc>
<doc id="53471572" url="https://en.wikipedia.org/wiki?curid=53471572" title="Monte Carlo polarization">
Monte Carlo polarization

In analytic business theory Monte Carlo Polarization is an opinion generation algorithm for a given prototype or design idea. 
The algorithm expands on traditional Monte Carlo aggregation which operates by placing candidates together and selecting a subset at random.
Each member of this subset is then asked for an opinion usually by filling out a form.
A resultant opinion scalar can be generated by application of the Softmax function over the generated form set.
However Monte Carlo Polarization goes a step further and attempts to construct the subset with the greatest standard deviation in response, referred to as the form data response eigen-norm vector scalar.
The idea of Monte-Carlo Polarization was firstly invented in Athens, (more commonly known as Thens), by Errikos Babudopoulos in 1978, but was mostly used in research in the 1990s by famous mathematicians, such as Grigori Perelman, in proving the soul conjecture.

The origins of Monte Carlo polarization came from the following observations made in early 1922:
Where the validity of an opinion is defined by the Emotional Intelligence Hierarchical metric space, using the obvious distance function.

Given an array "A" of "n" elements with values or records "A" ... "A", sorted such that "A" ≤ ... ≤ "A", and target value "T", the following subroutine uses Monte Carlo Polarization to find the index of "T" in "A".

Extract the emotional category isofunctor morphismvector and append this to the business manifesto.

Note: This can be done in matrix form. This is left as an exercise for the reader.

Although this a very recent cutting-edge technique, it has seen a couple of variations upon the basic algorithm in the last couple of month, most notably JSON driven resolution methods. the basic idea is that instead of supplying the algorithm with "n" records, it is more useful to provide the algorithm with emotional meta-data to guide its search and improve its complexity beyond the usual logarithmic bounds and this by a factor of "log(n)/2". It allows to select intermediate "m" values for the search index and skew them towards the wanted emotional value in the initial records.

The acceleration structures permitted by the Monte Carlo Polarization consist mainly in BVH and EBVH hierarchies. The logical subdivision of the kernel space leads to a logarithmic complexity, which is key to the scalability of the sentient analysis tools.

A key application is the direct targeting of hidden nodes in neural networks. By applying a Monte Carlo Polarization filter to the input layer of the neural system, hidden layers will be systematically and dynamically selected based on user-defined characteristics. Only the specified layers and units will receive and process the data.

Compared to standard drop-off methods, Monte Carlo Polarization is both more effective and more secure. Instead of having all nodes receiving the data and selecting output from a subset, the unnecessary nodes are directly filtered out. The result is a greatly increased level of accuracy and protection, as unreliable and malicious nodes will be left out, and a higher degree of efficiency.

The neural system that is created using the aforementioned method is the basis for many Computer Vision projects. A specific highlight is the American web-animated sitcom "F is for Family". 

Monte Carlo polarization can be easily deployed through Node.js.

The library provides a basic implementation of Monte Carlo polarization, and shows the kernel space learning algorithm applied to session tokens.

The native support of JSON files by NodeJS's JavaScript language is an example of the application of JSON Driven Monte Carlo Polarization.

Being a cutting edge technology, researchers are experimenting the expandability of the current technology to support Asynchronous Transport Protocol for JSON, and to provide an API for classic AJAX (Asynchronous Javascript and XML) interface by tunneling the data through Socket.IO packets secured by blockchain technology.


</doc>
<doc id="313384" url="https://en.wikipedia.org/wiki?curid=313384" title="Long division">
Long division

In arithmetic, long division is a standard division algorithm suitable for dividing multi-digit numbers that is simple enough to perform by hand. It breaks down a division problem into a series of easier steps.

As in all division problems, one number, called the dividend, is divided by another, called the divisor, producing a result called the quotient. It enables computations involving arbitrarily large numbers to be performed by following a series of simple steps. The abbreviated form of long division is called short division, which is almost always used instead of long division when the divisor has only one digit. Chunking (also known as the partial quotients method or the hangman method) is a less mechanical form of long division prominent in the UK which contributes to a more holistic understanding about the division process.

While related algorithms have existed since the 12th century AD, the specific algorithm in modern use was introduced by Henry Briggs 1600 AD.

Inexpensive calculators and computers have become the most common way to solve division problems, eliminating a traditional mathematical exercise, and decreasing the educational opportunity to show how to do so by paper and pencil techniques. (Internally, those devices use one of a variety of division algorithms, the faster ones of which relies on approximations and multiplications to achieve the tasks). In the United States, long division has been especially targeted for de-emphasis, or even elimination from the school curriculum, by reform mathematics, though traditionally introduced in the 4th or 5th grades.

In English-speaking countries, long division does not use the division slash or obelus signs but instead constructs a tableau. The divisor is separated from the dividend by a right parenthesis or vertical bar ; the dividend is separated from the quotient by a vinculum (i.e., overbar). The combination of these two symbols is sometimes known as a long division symbol or division bracket. It developed in the 18th century from an earlier single-line notation separating the dividend from the quotient by a left parenthesis.

The process is begun by dividing the left-most digit of the dividend by the divisor. The quotient (rounded down to an integer) becomes the first digit of the result, and the remainder is calculated (this step is notated as a subtraction). This remainder carries forward when the process is repeated on the following digit of the dividend (notated as 'bringing down' the next digit to the remainder). When all digits have been processed and no remainder is left, the process is complete.

An example is shown below, representing the division of 500 by 4 (with a result of 125).
A more detailed breakdown of the steps goes as follows:


If the last remainder when we ran out of dividend digits had been something other than 0, there would have been two possible courses of action:


In this example, the decimal part of the result is calculated by continuing the process beyond the units digit, "bringing down" zeros as being the decimal part of the dividend.

This example also illustrates that, at the beginning of the process, a step that produces a zero can be omitted. Since the first digit 1 is less than the divisor 4, the first step is instead performed on the first two digits 12. Similarly, if the divisor were 13, one would perform the first step on 127 rather than 12 or 1.


A divisor of any number of digits can be used. In this example, 1260257 is to be divided by 37. First the problem is set up as follows:

Digits of the number 1260257 are taken until a number greater than or equal to 37 occurs. So 1 and 12 are less than 37, but 126 is greater. Next, the greatest multiple of 37 less than or equal to 126 is computed. So 3 × 37 = 111 < 126, but 4 × 37 > 126. The multiple 111 is written underneath the 126 and the 3 is written on the top where the solution will appear:

Note carefully which place-value column these digits are written into. The 3 in the quotient goes in the same column (ten-thousands place) as the 6 in the dividend 1260257, which is the same column as the last digit of 111.

The 111 is then subtracted from the line above, ignoring all digits to the right:

Now the digit from the next smaller place value of the dividend is copied down and appended to the result 15:

The process repeats: the greatest multiple of 37 less than or equal to 150 is subtracted. This is 148 = 4 × 37, so a 4 is added to the top as the next quotient digit. Then the result of the subtraction is extended by another digit taken from the dividend:

The greatest multiple of 37 less than or equal to 22 is 0 × 37 = 0. Subtracting 0 from 22 gives 22, we often don't write the subtraction step. Instead, we simply take another digit from the dividend:

The process is repeated until 37 divides the last line exactly:

For non-decimal currencies (such as the British £sd system before 1971) and measures (such as avoirdupois) mixed mode division must be used. Consider dividing 50 miles 600 yards into 37 pieces:

Each of the four columns is worked in turn. Starting with the miles: 50/37 = 1 remainder 13. No further division is
possible, so perform a long multiplication by 1,760 to convert miles to yards, the result is 22,880 yards. Carry this to the top of the yards column and add it to the 600 yards in the dividend giving 23,480. Long division of 23,480 / 37 now proceeds as normal yielding 634 with remainder 22. The remainder is multiplied by 3 to get feet and carried up to the feet column. Long division of the feet gives 1 remainder 29 which is then multiplied by twelve to get 348 inches. Long division continues with the final remainder of 15 inches being shown on the result line.

When the quotient is not an integer and the division process is extended beyond the decimal point, one of two things can happen:


China, Japan, Korea use the same notation as English-speaking nations including India. Elsewhere, the same general principles are used, but the figures are often arranged differently.

In Latin America (except Argentina, Bolivia, Mexico, Colombia, Paraguay, Venezuela, Uruguay and Brazil), the calculation is almost exactly the same, but is written down differently as shown below with the same two examples used above. Usually the quotient is written under a bar drawn under the divisor. A long vertical line is sometimes drawn to the right of the calculations.

and

In Mexico, the English-speaking world notation is used, except that only the result of the subtraction is annotated and the calculation is done mentally, as shown below:

In Bolivia, Brazil, Paraguay, Venezuela, Uruguay, Quebec, Colombia, and Peru, the European notation (see below) is used, except that the quotient is not separated by a vertical line, as shown below:

Same procedure applies in Mexico and Argentina, only the result of the subtraction is annotated and the calculation is done mentally.

In Spain, Italy, France, Portugal, Lithuania, Romania, Turkey, Greece, Belgium, Belarus, Ukraine, and Russia, the divisor is to the right of the dividend, and separated by a vertical bar. The division also occurs in the column, but the quotient (result) is written below the divider, and separated by the horizontal line. The same method is used in Iran and Mongolia.

In Cyprus, as well as in France, a long vertical bar separates the dividend and subsequent subtractions from the quotient and divisor, as in the below of 6359 divided by 17, which is 374 with a remainder of 1.

Decimal numbers are not divided directly, the dividend and divisor are multiplied by a power of ten so that the division involves two whole numbers. Therefore, if one were dividing 12,7 by 0,4 (commas being used instead of decimal points), the dividend and divisor would first be changed to 127 and 4, and then the division would proceed as above.

In Austria, Germany and Switzerland, the notational form of a normal equation is used. <dividend> : <divisor> = <quotient>, with the colon ":" denoting a binary infix symbol for the division operator (analogous to "/" or "÷"). In these regions the decimal separator is written as a comma. (cf. first section of Latin American countries above, where it's done virtually the same way):

The same notation is adopted in Denmark, Norway, Bulgaria, North Macedonia, Poland, Croatia, Slovenia, Hungary, Czech Republic, Slovakia, Vietnam and in Serbia.

In the Netherlands, the following notation is used:

Every natural number n can be uniquely represented in an arbitrary number base formula_1 as a sequence of digits formula_2 where formula_3, where formula_4 is the number of digits in formula_5. The value of n in terms of its digits and the base is
Let formula_5 be the dividend and formula_8 be the divisor, where formula_9 is the number of digits in formula_8. If formula_11, then formula_12 and formula_13. Otherwise, we iterate from formula_14, before stopping.

For each iteration formula_15, let formula_16 be the quotient extracted so far, formula_17 be the intermediate dividend, formula_18 be the intermediate remainder, formula_19 be the next digit of the original dividend, and formula_20 be the next digit of the quotient. By definition of digits in base formula_21, formula_22. All values are natural numbers. We initiate 
the first formula_9 digits of formula_5.

With every iteration, the three equations are true:
There only exists one such formula_20 such that formula_31.
The final quotient is formula_32 and the final remainder is formula_33

In base 10, using the example above with formula_34 and formula_35, the initial values formula_36 and formula_37.

Thus, formula_38 and formula_39.

In base 16, with formula_40 and formula_41, the initial values are formula_36 and formula_43.

Thus, formula_44 and formula_45.

If one doesn't have the addition, subtraction, or multiplication tables for base formula_21 memorised, then this algorithm still works if the numbers are converted to decimal and at the end are converted back to base formula_21. For example, with the above example, 
and 
with formula_50. The initial values are formula_36 and formula_52.
Thus, formula_53 and formula_54.

This algorithm can be done using the same kind of pencil-and-paper notations as shown in above sections.

If the quotient is not constrained to be an integer, then the algorithm does not terminate for formula_55. Instead, if formula_55 then formula_57 by definition. If the remainder formula_18 is equal to zero at any iteration, then the quotient is a formula_21-adic fraction, and is represented as a finite decimal expansion in base formula_21 positional notation. Otherwise, it is still a rational number but not a formula_21-adic rational, and is instead represented as an infinite repeating decimal expansion in base formula_21 positional notation.

Calculation within the binary number system is simpler, because each digit in the course can only be 1 or 0 - no multiplication is needed as multiplication by either either results in the same number or zero.

If this were on a computer, multiplication by 10 can be represented by a bit shift of 1 to the left, and finding formula_20 reduces down to the logical operation formula_64, where true = 1 and false = 0. With every iteration formula_14, the following operations are done:

For example, with formula_81 and formula_82, the initial values are formula_36 and formula_84.

Thus, formula_85 and formula_86.

On each iteration, the most time-consuming task is to select formula_20. We know that there are formula_21 possible values, so we can find formula_20 using formula_90 comparisons. Each comparison will require evaluating formula_91. Let formula_4 be the number of digits in the dividend formula_5 and formula_9 be the number of digits in the divisor formula_8. The number of digits in formula_96. The multiplication of formula_97 is therefore formula_98, and likewise the subtraction of formula_91. Thus it takes formula_100 to select formula_20. The remainder of the algorithm are addition and the digit-shifting of formula_16 and formula_18 to the left one digit, and so takes time formula_104 and formula_98 in base formula_21, so each iteration takes formula_107, or just formula_108. For all formula_109 digits, the algorithm takes time formula_110, or formula_111 in base formula_21.

Long division of integers can easily be extended to include non-integer dividends, as long as they are rational. This is because every rational number has a recurring decimal expansion. The procedure can also be extended to include divisors which have a finite or terminating decimal expansion (i.e. decimal fractions). In this case the procedure involves multiplying the divisor and dividend by the appropriate power of ten so that the new divisor is an integer – taking advantage of the fact that "a" ÷ "b" = ("ca") ÷ ("cb") – and then proceeding as above.

A generalised version of this method called polynomial long division is also used for dividing polynomials (sometimes using a shorthand version called synthetic division).




</doc>
<doc id="4104986" url="https://en.wikipedia.org/wiki?curid=4104986" title="How to Solve it by Computer">
How to Solve it by Computer

How to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.
It is occasionally used as a textbook, especially in India.

It is an introduction to the "why"s of algorithms and data structures.
Features of the book:

The very fundamental algorithms portrayed by this book are mostly presented in Pseudocode and/or Pascal notation.



</doc>
<doc id="54625345" url="https://en.wikipedia.org/wiki?curid=54625345" title="Right to explanation">
Right to explanation

In the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to "an" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be "Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for."

Some such legal rights already exist, while the scope of a general "right to explanation" is a matter of ongoing debate.

Credit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),
Title 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):

The official interpretation of this section details what types of statements are acceptable.

Credit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:

The European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: "[the data subject should have] the right ... to obtain an explanation of the decision reached". In full:

However, the extent to which the regulations themselves provide a "right to explanation" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both "solely" based on automated processing, and have legal or similarly significant effect — which limits their applicability in many of the cases of algorithmic controversy that have been picked up in the media.

A second source of such a right has been pointed to in Article 15, the "right of access by the data subject". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to "meaningful information about the logic involved" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.

In France the 2016 "Loi pour une République numérique" (Digital Republic Act or "loi numérique") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is "a decision taken on the basis of an algorithmic treatment", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:
Scholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions "solely" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a "right to an explanation" has been sought within, find their origins in French law in the late 1970s.

Some argue that a "right to explanation" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.

More fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.

Similarly, human decisions often cannot be easily explained: they may be based on intuition or a "gut feeling" that is hard to put into words. Requiring machines to meet a higher standard than humans is thus arguably unreasonable.




</doc>
<doc id="55206702" url="https://en.wikipedia.org/wiki?curid=55206702" title="Seidel's algorithm">
Seidel's algorithm

Seidel's algorithm is an algorithm designed by Raimund Seidel in 1992 for the all-pairs-shortest-path problem for undirected, unweighted, connected graphs. It solves the problem in formula_1 expected time for a graph with formula_2 vertices, where formula_3 is the exponent in the complexity formula_4 of formula_5 matrix multiplication. If only the distances between each pair of vertices are sought, the same time bound can be achieved in the worst case. Note that even though the algorithm is designed for connected graphs, it can be applied individually to each connected component of a graph with the same running time overall. Note also that there is an exception to the expected running time given above for computing the paths: if formula_6 the expected running time becomes formula_7.

The core of the algorithm is a procedure that computes the length of the shortest-paths between any pair of vertices.
This can be done in formula_1 time in the worst case. Once the lengths are computed, the paths can be reconstructed using a Las Vegas algorithm whose expected running time is formula_1 for formula_10 and formula_7 for formula_6.

The Python code below assumes the input graph is given as a formula_13 formula_14-formula_15 adjacency matrix formula_16 with zeros on the diagonal. It defines the function APD which returns a matrix with entries formula_17 such that formula_17 is the length of the shortest path between the vertices formula_19 and formula_20. The matrix class used can be any matrix class implementation supporting the multiplication, exponentiation, and indexing operators (for example numpy.matrix).
def apd(A, n: int):
The base case tests whether the input adjacency matrix describes a complete graph, in which case all shortest paths have length formula_15.

Algorithms for undirected and directed graphs with weights from a finite universe formula_22 also exist. The best known algorithm for the directed case is in time formula_23 by Zwick in 1998. This algorithm uses rectangular matrix multiplication instead of square matrix multiplication. Better upper bounds can be obtained if one uses the best rectangular matrix multiplication algorithm available instead of achieving rectangular multiplication via multiple square matrix multiplications. The best known algorithm for the undirected case is in time formula_24 by Shoshan and Zwick in 1999. The original implementation of this algorithm was erroneous and has been corrected by Eirinakis, Williamson, and Subramani in 2016.


</doc>
<doc id="1881722" url="https://en.wikipedia.org/wiki?curid=1881722" title="External memory algorithm">
External memory algorithm

In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives, or when memory is on a computer network. External memory algorithms are analyzed in the external memory model.

External memory algorithms are analyzed in an idealized model of computation called the external memory model (or I/O model, or disk access model). The external memory model is an abstract machine similar to the RAM machine model, but with a cache in addition to main memory. The model captures the fact that read and write operations are much faster in a cache than in main memory, and that reading long contiguous blocks is faster than reading randomly using a disk read-and-write head. The running time of an algorithm in the external memory model is defined by the number of reads and writes to memory required. The model was introduced by Alok Aggarwal and Jeffrey Vitter in 1988. The external memory model is related to the cache-oblivious model, but algorithms in the external memory model may know both the block size and the cache size. For this reason, the model is sometimes referred to as the cache-aware model.

The model consists of a processor with an internal memory or cache of size , connected to an unbounded external memory. Both the internal and external memory are divided into blocks of size . One input/output or memory transfer operation consists of moving a block of contiguous elements from external to internal memory, and the running time of an algorithm is determined by the number of these input/output operations.

Algorithms in the external memory model take advantage of the fact that retrieving one object from external memory retrieves an entire block of size formula_1. This property is sometimes referred to as locality.

Searching for an element among formula_2 objects is possible in the external memory model using a B-tree with branching factor formula_1. Using a B-tree, searching, insertion, and deletion can be achieved in formula_4 time (in Big O notation). Information theoretically, this is the minimum running time possible for these operations, so using a B-tree is asymptotically optimal.

External sorting is sorting in an external memory setting. External sorting can be done via distribution sort, which is similar to quicksort, or via a formula_5-way merge sort. Both variants achieve the asymptotically optimal runtime of formula_6 to sort objects. This bound also applies to the Fast Fourier Transform in the external memory model.

The permutation problem is to rearrange formula_2 elements into a specific permutation. This can either be done either by sorting, which requires the above sorting runtime, or inserting each element in order and ignoring the benefit of locality. Thus, permutation can be done in formula_8 time.

The external memory model captures the memory hierarchy, which is not modeled in other common models used in analyzing data structures, such as the random access machine, and is useful for proving lower bounds for data structures. The model is also useful for analyzing algorithms that work on datasets too big to fit in internal memory.

A typical example is geographic information systems, especially digital elevation models, where the full data set easily exceeds several gigabytes or even terabytes of data.

This methodology extends beyond general purpose CPUs and also includes GPU computing as well as classical digital signal processing. In general-purpose computing on graphics processing units (GPGPU), powerful graphics cards (GPUs) with little memory (compared with the more familiar system memory, which is most often referred to simply as RAM) are utilized with relatively slow CPU-to-GPU memory transfer (when compared with computation bandwidth).

An early use of the term "out-of-core" as an adjective is in 1962 in reference to "devices" that are other than the core memory of an IBM 360. An early use of the term "out-of-core" with respect to "algorithms" appears in 1971.




</doc>
<doc id="56036557" url="https://en.wikipedia.org/wiki?curid=56036557" title="Proof of authority">
Proof of authority

Proof of authority (PoA) is an algorithm used with blockchains that delivers comparatively fast transactions through a consensus mechanism based on identity as a stake.

In PoA-based networks, transactions and blocks are validated by approved accounts, known as validators. Validators run software allowing them to put transactions in blocks. The process is automated and does not require validators to be constantly monitoring their computers. It, however, does require maintaining the computer (the authority node) uncompromised. The term was coined by Gavin Wood, co-founder of Ethereum and Parity Technologies.

With PoA, individuals earn the right to become validators, so there is an incentive to retain the position that they have gained. By attaching a reputation to identity, validators are incentivized to uphold the transaction process, as they do not wish to have their identities attached to a negative reputation. This is considered more robust than PoS (proof-of-stake) - PoS, while a stake between two parties may be even, it does not take into account each party’s total holdings. This means that incentives can be unbalanced. 
On the other hand, PoA only allows non-consecutive block approval from any one validator, meaning that the risk of serious damage is centralized to the authority node.

PoA is suited for both private networks and public networks, like POA Network, where trust is distributed.


</doc>
<doc id="47937215" url="https://en.wikipedia.org/wiki?curid=47937215" title="The Master Algorithm">
The Master Algorithm

The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.

The book outlines five tribes of machine learning: inductive reasoning, connectionism, evolutionary computation, Bayes' theorem and analogical modelling. The author explains these tribes to the reader by referring to more understandable processes of logic, connections made in the brain, natural selection, probability and similarity judgements. Throughout the book, it is suggested that each different tribe has the potential to contribute to a unifying "master algorithm".

Towards the end of the book the author pictures a "master algorithm" in the near future, where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work. Although the algorithm doesn't yet exist, he briefly reviews his own invention of the Markov logic network.

In 2016 Bill Gates recommended the book, alongside Nick Bostrom's "Superintelligence", as one of two books everyone should read to understand AI. In 2018 the book was noted to be on Chinese President Xi Jinping's bookshelf.

A computer science educator stated in "Times Higher Education" that the examples are clear and accessible. In contrast, "The Economist" agreed Domingos "does a good job" but complained that he "constantly invents metaphors that grate or confuse". "Kirkus Reviews" praised the book, stating "Readers unfamiliar with logic and computer theory will have a difficult time, but those who persist will discover fascinating insights."

A "New Scientist" review called it "compelling but rather unquestioning".



</doc>
<doc id="56463048" url="https://en.wikipedia.org/wiki?curid=56463048" title="Algorithms of Oppression">
Algorithms of Oppression

Algorithms of Oppression: How Search Engines Reinforce Racism is a 2018 book by Dr. Safiya Umoja Noble in the fields of information science, machine learning, and human-computer interaction.

Dr. Noble is a professor at the University of California, Los Angeles in the Department of Information Studies. She is a Co-Director of the Critical Internet Inquiry Center and also works with African American Studies and Gender Studies. Her best-selling book, Algorithms Of Oppression, has been featured in the Los Angeles Review of Books, New York Public Library 2018 Best Books for Adults, and Bustle’s magazine 10 Books about Race to Read Instead of Asking a Person of Color to Explain Things to You. Her work markets the ways that digital media impacts issues of race, gender, culture, and technology. 

"Algorithms of Oppression" is a text based on over six years of academic research on Google search algorithms. Noble argues that search algorithms become racist because they reflect the biases and values of the people who create them. These algorithms can then have negative biases against women of color and other marginalized populations, while also affecting Internet users in general by leading to "racial and gender profiling, misrepresentation, and even economic redlining." She mentions the issue of technological redlining, that profiles users. 
In Chapter 1 of "Algorithms of Oppression", Safiya Noble explores how Google search’s auto suggestion feature is demoralizing. On September 18, 2011 a mother googled “black girls” attempting to find fun activities to show her stepdaughter and nieces. To her surprise, the results encompassed websites and images of porn. This result encloses the data failures specific to people of color and women which Noble coins algorithmic oppression. Noble also adds that as a society we must have a feminist lens, with racial awareness to understand the “problematic positions about the benign instrumentality of technologies.”

Noble also discusses how Google can remove the human curation from the first page of results to eliminate any potential racial slurs or inappropriate imaging. Another example discussed in this text is a public dispute of the results that were returned when “jew” was searched on Google. The results included a number of anti-Semitic pages and Google claimed little ownership for the way it provided these identities. Google instead encouraged people to use “jews” or “Jewish people” and claimed the actions of White supremacist groups are out of Google’s control. Unless pages are unlawful, Google will allow its algorithm to continue to act without removing pages.

Noble reflects on AdWords which is Google's advertising tool and how this tool can add to the biases on Google. Adwords allows anyone to advertise on Google’s search pages and is highly customizable. First, Google ranks ads on relevance and then displays the ads on pages which is believes are relevant to the search query taking place. An advertiser can also set a maximum amount of money per day to spend on advertising. The more you spend on ads, the higher probability your ad will be closer to the top. Therefore, if an advertiser is passionate about his/her topic but is controversial it may be the first to appear on a Google search. 

Critical reception for "Algorithms of Oppression" has been largely positive. In the "Los Angeles Review of Books", Emily Drabinski writes, "What emerges from these pages is the sense that Google’s algorithms of oppression comprise just one of the hidden infrastructures that govern our daily lives, and that the others are likely just as hard-coded with white supremacy and misogyny as the one that Noble explores." In "PopMatters," Hans Rollman describes writes that "Algorithms of Oppression" "demonstrate[s] that search engines, and in particular Google, are not simply imperfect machines, but systems designed by humans in ways that replicate the power structures of the western countries where they are built, complete with all the sexism and racism that are built into those structures." In "Booklist," reviewer Lesley Williams states, "Noble’s study should prompt some soul-searching about our reliance on commercial search engines and about digital social equity."

In early February 2018, "Algorithms of Oppression" received press attention when the official Twitter account for the Institute of Electrical and Electronics Engineers expressed criticism of the book, citing that the thesis of the text, based on the text of the book's official blurb on commercial sites, could not be reproduced. IEEE's outreach historian, Alexander Magoun, later revealed that he had not read the book, and issued an apology.




</doc>
<doc id="23632960" url="https://en.wikipedia.org/wiki?curid=23632960" title="Sardinas–Patterson algorithm">
Sardinas–Patterson algorithm

In coding theory, the Sardinas–Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 1953. The algorithm carries out a systematic search for a string which admits two different decompositions into codewords. As Knuth reports, the algorithm was rediscovered about ten years later in 1963 by Floyd, despite the fact that it was at the time already well known in coding theory.

Consider the code formula_1. This code, which is based on an example by Berstel, is an example of a code which is not uniquely decodable, since the string

can be interpreted as the sequence of codewords

but also as the sequence of codewords

Two possible decodings of this encoded string are thus given by "cdb" and "babe".

In general, a codeword can be found by the following idea: In the first round, we choose two codewords formula_2 and formula_3 such that formula_2 is a prefix of formula_3, that is,
formula_6 for some "dangling suffix" formula_7. If one tries first formula_8 and formula_9, the dangling suffix is formula_10. If we manage to find two sequences formula_11 and formula_12 of codewords such that
formula_13, then we are finished: For then the string formula_14 can alternatively be decomposed as formula_15, and we have found the desired string having at least two different decompositions into codewords.

In the second round, we try out two different approaches: the first trial is to look for a codeword that has "w" as prefix. Then we obtain a new dangling suffix "w"', with which we can continue our search. If we eventually encounter a dangling suffix that is itself a codeword (or the empty word), then the search will terminate, as we know there exists a string with two decompositions. The second trial is to seek for a codeword that is itself a prefix of "w". In our example, we have formula_10, and the sequence "1" is a codeword. We can thus also continue with "w'=0" as the new dangling suffix.

The algorithm is described most conveniently using quotients of formal languages. In general, for two sets of strings "D" and "N", the (left) quotient formula_17 is defined as the residual words obtained from "D" by removing some prefix in "N". Formally, formula_18. Now let formula_19 denote the (finite) set of codewords in the given code.

The algorithm proceeds in rounds, where we maintain in each round not only one dangling suffix as described above, but the (finite) set of all potential dangling suffixes. Starting with round formula_20, the set of potential dangling suffixes will be denoted by formula_21. The sets formula_21 are defined inductively as follows:

formula_23. Here, the symbol formula_24 denotes the empty word.

formula_25, for all formula_26.

The algorithm computes the sets formula_21 in increasing order of formula_28. As soon as one of the formula_21 contains a word from "C" or the empty word, then the algorithm terminates and answers that the given code is not uniquely decodable. Otherwise, once a set formula_21
equals a previously encountered set formula_31 with <math>j, then the algorithm would enter in principle an endless loop. Instead of continuing endlessly, it answers that the given code is uniquely decodable.

Since all sets formula_21 are sets of suffixes of a finite set of codewords, there are only finitely many different candidates for formula_21. Since visiting one of the sets for the second time will cause the algorithm to stop, the algorithm cannot continue endlessly and thus must always terminate. More precisely, the total number of dangling suffixes that the algorithm considers is at most equal to the total of the lengths of the codewords in the input, so the algorithm runs in polynomial time as a function of this input length. By using a suffix tree to speed the comparison between each dangling suffix and the codewords, the time for the algorithm can be bounded by O("nk"), where "n" is the total length of the codewords and "k" is the number of codewords. The algorithm can be implemented using a pattern matching machine. The algorithm can also be implemented to run on a nondeterministic Turing machine that uses only logarithmic space; the problem of testing unique decipherability is NL-complete, so this space bound is optimal.

A proof that the algorithm is correct, i.e. that it always gives the correct answer, is found in the textbooks by Salomaa and by Berstel et al.






</doc>
<doc id="55213052" url="https://en.wikipedia.org/wiki?curid=55213052" title="Hub labels">
Hub labels

In computer science, hub labels or the hub-labelling algorithm is a method that consumes much fewer resources than the lookup table but is still extremely fast for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.

This method allows at the most with two SELECT statements and the analysis of two strings to compute the shortest path between two vertices of a graph.
For a graph that is oriented like a road graph, this technique requires the prior computation of two tables from structures constructed using the method of the contraction hierarchies. 
In the end, these two computed tables will have as many rows as nodes present within the graph. For each row (each node), a label will be calculated.

A label is a string containing the distance information between the current node (the node of the row) and all the other nodes that can be reached with an ascending search on the relative multi-level structure. The advantage of these distances is that they all represent the shortest paths. 

So, for future queries, the search of a shortest path will start from the source on the first table and the destination on the second table, from which it will be search within the labels for the common nodes with the associated distance information. Only the smallest sum of distances will be kept as the shortest path result.


</doc>
<doc id="54117020" url="https://en.wikipedia.org/wiki?curid=54117020" title="Unrestricted algorithm">
Unrestricted algorithm

An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.

In the problem of developing algorithms for computing, as regards the values of a real-valued function of a real variable (e.g., "g"["x"] in "restricted" algorithms), the error that can be tolerated in the result is specified in advance. An interval on the real line would also be specified for values when the values of a function are to be evaluated. Different algorithms may have to be applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of "x" and also the precision required in "g"("x") quite arbitrarily. The algorithm should then produce an acceptable result without failure.


</doc>
<doc id="632487" url="https://en.wikipedia.org/wiki?curid=632487" title="List of algorithm general topics">
List of algorithm general topics

This is a list of algorithm general topics. 




</doc>
<doc id="18568" url="https://en.wikipedia.org/wiki?curid=18568" title="List of algorithms">
List of algorithms

The following is a list of algorithms along with one-line descriptions for each.
































































</doc>
<doc id="56460428" url="https://en.wikipedia.org/wiki?curid=56460428" title="Reduce (parallel pattern)">
Reduce (parallel pattern)

Reduce is a collective communication primitive used in the context of a parallel programming model to combine multiple vectors into one, using an associative binary operator formula_1. Every vector is present at a distinct processor in the beginning. The goal of the primitive is to apply the operator in the order given by the processor-indices to the vectors until only one is left. The reduction of sets of elements is an integral part of programming models such as Map Reduce, where a function is applied (mapped) to all elements before they are reduced. Other parallel algorithms use reduce as a primary operation to solve more complex problems. The Message Passing Interface implements it in the operations codice_1 and codice_2, with the difference that the result is available at one (root) processing unit or all of them. Closely related to reduce is the broadcast operation, which distributes data to all processors. Many reduce algorithms can be used for broadcasting by reverting them and omitting the operator.

Formally, reduce takes an associative (but not necessarily commutative) operator formula_1, which can be evaluated in constant time and an input set formula_3of formula_4 vectors with formula_5 elements each. The total size of a vector is defined as formula_6. The result formula_7 of the operation is the combination of the elements formula_8 and has to be stored at a specified root processor at the end of the execution. For example, the result of a reduction on the set formula_9, where all vectors have size one is formula_10. If the result formula_7 has to be available at every processor after the computation has finished, it is often called Allreduce. An optimal sequential linear-time algorithm for reduction can apply the operator successively from front to back, always replacing two vectors with the result of the operation applied to all its elements, thus creating an instance that has one vector less. It needs formula_12 steps until only formula_7 is left. Sequential algorithms can not perform better than linear time, but parallel algorithms leave some space left to optimize.

Regarding parallel algorithms, there are two main models of parallel computation, the parallel random access machine as an extension of the RAM with shared memory between processing units and the bulk synchronous parallel computer which takes communication and synchronization into account. Both models have different implications for the time-complexity, therefore two algorithms will be shown.

This algorithm represents a widely spread method to handle inputs where formula_4 is a power of two. The reverse procedure is often used for broadcasting elements.
The binary operator for vectors is defined such that formula_25. The algorithm further assumes that in the beginning formula_26 for all formula_21 and formula_4 is a power of two and uses the processing units formula_29. In every iteration, half of the processing units become inactive and do not contribute to further computations. The figure shows a visualization of the algorithm using addition as the operator. Vertical lines represent the processing units where the computation of the elements on that line take place. The eight input elements are located on the bottom and every animation step corresponds to one parallel step in the execution of the algorithm. An active processor formula_19 evaluates the given operator on the element formula_31 it is currently holding and formula_32 where formula_33 is the minimal index fulfilling formula_34, so that formula_35 is becoming an inactive processor in the current step. formula_31 and formula_32 are not necessarily elements of the input set formula_38 as the fields are overwritten and reused for previously evaluated expressions. To coordinate the roles of the processing units in each step without causing additional communication between them, the fact that the processing units are indexed with numbers from formula_39 to formula_40 is used. Each processor looks at its formula_20-th least significant bit and decides whether to get inactive or compute the operator on its own element and the element with the index where the formula_20-th bit is not set. The underlying communication pattern of the algorithm is a binomial tree, hence the name of the algorithm.

Only formula_43 holds the result in the end, therefore it is the root processor. For an Allreduce-operation the result has to be distributed, which can be done by appending a broadcast from formula_43. Furthermore, the number formula_4 of processors is restricted to be a power of two. This can be lifted by padding the number of processors to the next power of two. There are also algorithms that are more tailored for this use-case.

The main loop is executed formula_46 times, the time needed for the part done in parallel is in formula_47 as a processing unit either combines two vectors or becomes inactive. Thus the parallel time formula_48 for the PRAM is formula_49. The strategy for handling read and write conflicts can be chosen as restrictive as an exclusive read and exclusive write (EREW). The efficiency formula_50 of the algorithm is formula_51 and therefore the efficiency is formula_52. The efficiency suffers because of the fact that half of the active processing units become inactive after each step, so formula_53 units are active in step formula_21.

In contrast to the PRAM-algorithm, in the distributed memory model memory is not shared between processing units and data has to be exchanged explicitly between units, resulting in communication overhead that is accounted for. The following algorithm takes this into consideration.
The only difference between the distributed algorithm and the PRAM version is the inclusion of explicit communication primitives, the operating principle stays the same.

A simple analysis for the algorithm uses the BSP-model and incorporates the time formula_68 needed to initiate communication and formula_69 the time needed to send a byte. Then the resulting runtime is formula_70, as formula_5 elements of a vector are send in each iteration and have size formula_6 in total.

For distributed memory models, it can make sense to use pipelined communication. This is especially the case when formula_68 is small in comparison to formula_69. Usually, linear pipelines split data or a task into smaller pieces and process them in stages. In contrast to the binomial tree algorithms, the pipelined algorithm uses the fact that the vectors are not inseparable, but the operator can be evaluated for single elements:

It is important to note that the send and receive operations have to be executed concurrently for the algorithm to work. The result vector is stored at formula_86 at the end. The associated animation shows an execution of the algorithm on vectors of size four with five processing units. Two steps of the animation visualize one parallel execution step. The number of steps in the parallel execution are formula_87, it takes formula_40 steps until the last processing unit receives its first element and additional formula_89 until all elements are received. Therefore, the runtime in the BSP-model is formula_90, assuming that formula_6 is the total byte-size of a vector.

Although formula_5 has a fixed value, it is possible to logically group elements of a vector together and reduce formula_5. For example, a problem instance with vectors of size four can be handled by splitting the vectors into the first two and last two elements, which are always transmitted and computed together. In this case, double the volume is send each step, but the number of steps has roughly halved. It means that the parameter formula_5 is halved, while the total byte-size formula_6 stays the same. The runtime formula_96 for this approach depends on the value of formula_5, which can be optimized if formula_68 and formula_69 are known. It is optimal for formula_100, assuming that this results in a smaller formula_5 that divides the original one.

The binomial tree and the pipeline both have their advantages and disadvantages, depending on the values of formula_68 and formula_69 for the parallel communication. While the binomial tree algorithm is better suited for small vectors, the pipelined algorithm profits from a distribution of the elements to fewer processing units with more elements contained in one vector. Both approaches can be combined into one algorithm which uses a tree as its underlying communication pattern and splits the computation of the operator into pieces at the same time. Instead of the binomial tree, a Fibonacci tree is used which has the property that the height of the trees rooted at its two children differ by one. It helps to balance the load on all processing units as each unit can only evaluate one operator in one iteration on one of its elements, but it has two child-processors it receives values from.

The animation shows the execution of such an algorithm in a full-duplex communication model. Communication links are represented by black lines between the vectors of elements and build a Fibonacci tree of size seven in this example. If an element is send to another processing unit the link is colored with the color of the corresponding element. An element that is received by a processor is added to the already existing element of same color (at the same index in the vector).

The algorithm itself propagates the partial sums from bottom to top until all elements are contained in the sum at the root processor on top. In the first step of the execution, the processing units which are leafs in the underlying tree send their first elements to their parent. This is similar to the send operations of the binomial tree algorithm with the key difference that the leaf units each have two more elements which have to be send and therefore do not become inactive, but can continue to send elements, which is analogous to the pipelined approach and improves efficiency. Processing units that are not leafs start to send their elements in order of the indices in the vector once they have received an element from a child. In the example they send green, blue and red elements in this order. If two processors compete to send their elements to the same processor, then the element of the right child is received first. Because of the structure of the Fibonacci tree all processors send or receive elements while the "pipeline" is filled. The pipeline is filled from the point where each unit has received an element and until the leaf units have no more elements to send.

Each iteration of the algorithm takes at most time formula_104. The height of the tree factors into the time it needs to fill the pipeline and for Fibonacci trees it is known to be about formula_105 where formula_106 is the golden ratio. Once the pipeline is filled, all processors are active in each step. Because inner nodes have two children, they have to receive formula_107 elements. Therefore, the runtime of the algorithm is formula_108. It is minimal if the number of elements in a vector is chosen such that formula_109.

Reduction is one of the main collective operations implemented in the Message Passing Interface, where performance of the used algorithm is important and evaluated constantly for different use cases.

MapReduce relies heavily on efficient reduction algorithms to process big data sets, even on huge clusters.

Some parallel sorting algorithms use reductions to be able to handle very big data sets.



</doc>
<doc id="57373227" url="https://en.wikipedia.org/wiki?curid=57373227" title="Krauss wildcard-matching algorithm">
Krauss wildcard-matching algorithm

In computer science, the Krauss wildcard-matching algorithm is a pattern matching algorithm. Based on the wildcard syntax in common use, e.g. in the Microsoft Windows command-line interface, the algorithm provides a non-recursive mechanism for matching patterns in software applications, based on syntax simpler than that typically offered by regular expressions.

The algorithm is based on a history of development, correctness and performance testing, and programmer feedback that began with an unsuccessful search for a reliable non-recursive algorithm for matching wildcards. An initial algorithm, implemented in a single while loop, quickly prompted comments from software developers, leading to improvements. Ongoing comments and suggestions culminated in a revised algorithm still implemented in a single while loop but refined based on a collection of test cases and a performance profiler. The experience tuning the single while loop using the profiler prompted development of a two-loop strategy that achieved further performance gains, particularly in situations involving empty input strings or input containing no wildcard characters. The two-loop algorithm is available for use by the open-source software development community, under the terms of the Apache License v. 2.0, and is accompanied by test case code.

The algorithm made available under the Apache license is implemented in both pointer-based C++ and portable C++ (implemented without pointers). The test case code, also available under the Apache license, can be applied to any algorithm that provides the pattern matching operations below. The implementation as coded is unable to handle multibyte character sets and poses problems when the text being searched may contain multiple incompatible character sets.

The algorithm supports three pattern matching operations:


The original algorithm has been ported to the DataFlex programming language by Larry Heiges for use with Data Access Worldwide code library. It has been posted on GitHub in modified form as part of a log file reader. The 2014 algorithm is part of the Unreal Model Viewer built into the Epic Games Unreal Engine game engine.



</doc>
<doc id="57504451" url="https://en.wikipedia.org/wiki?curid=57504451" title="Algorithms and Combinatorics">
Algorithms and Combinatorics

Algorithms and Combinatorics () is a book series in mathematics, and particularly in combinatorics and the design and analysis of algorithms. It is published by Springer Science+Business Media, and was founded in 1987.

, the books published in this series include:


</doc>
<doc id="57506816" url="https://en.wikipedia.org/wiki?curid=57506816" title="Hall circles">
Hall circles

Hall circles (also known as M-circles and N-circles) are a graphical tool in control theory used to obtain values of a closed-loop transfer function from the Nyquist plot (or the Nichols plot) of the associated open-loop transfer function. Hall circles have been introduced in control theory by Albert C. Hall in his thesis.

Consider a closed-loop linear control system with open-loop transfer function given by transfer function formula_1 and with a unit gain in the feedback loop. The closed-loop transfer function is given by formula_2. 

To check the stability of "T"("s"), it is possible to use the Nyquist stability criterion with the Nyquist plot of the open-loop transfer function "G"("s"). Note, however, that only the Nyquist plot of "G"("s") does not give the actual values of "T"("s"). To get this information from the G(s)-plane, Hall proposed to construct the locus of points in the "G"("s")-plane such that "T"("s") has constant magnitude and the also the locus of points in the "G"("s")-plane such that "T"("s") has constant phase angle.

Given a positive real value "M" representing a fixed magnitude, and denoting G(s) by "z", the points satisfying formula_3are given by the points "z" in the "G"("s")-plane such that the ratio of the distance between "z" and 0 and the distance between "z" and -1 is equal to "M". The points "z" satisfying this locus condition are circles of Apollonius, and this locus is known in the context of control systems as "M-circles".

Given a positive real value "N" representing a phase angle, the points satisfying formula_4are given by the points z in the "G"("s")-plane such that the angle between -1 and z and the angle between 0 and z is constant. In other words, the angle opposed to the line segment between -1 and 0 must be constant. This implies that the points z satisfying this locus condition are arcs of circles, and this locus is known in the context of control systems as "N-circles".

To use the Hall circles, a plot of M and N circles is done over the Nyquist plot of the open-loop transfer function. The points of the intersection between these graphics give the corresponding value of the closed-loop transfer function.

Hall circles are also used with the Nichols plot and in this setting, are also known as Nichols chart. Rather than overlaying directly the Hall circles over the Nichols plot, the points of the circles are transferred to a new coordinate system where the ordinate is given by formula_5 and the abscissa is given by formula_6. The advantage of using Nichols chart is that adjusting the gain of the open loop transfer function directly reflects in up and down translation of the Nichols plot in the chart.




</doc>
<doc id="52773150" url="https://en.wikipedia.org/wiki?curid=52773150" title="Algorithmic transparency">
Algorithmic transparency

Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.

The phrases "algorithmic transparency" and "algorithmic accountability" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, "algorithmic transparency" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair. "Algorithmic accountability" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.

Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a "right to explanation" of decisions made by algorithms, though it is unclear what this means.



</doc>
<doc id="58006810" url="https://en.wikipedia.org/wiki?curid=58006810" title="Abramov's algorithm">
Abramov's algorithm

In mathematics, particularly in computer algebra, Abramov's algorithm computes all rational solutions of a linear recurrence equation with polynomial coefficients. The algorithm was published by Sergei A. Abramov in 1989. 

The main concept in Abramov's algorithm is a universal denominator. Let formula_1 be a field of characteristic zero. The "dispersion" formula_2 of two polynomials formula_3 is defined asformula_4where formula_5 denotes the set of non-negative integers. Therefore the dispersion is the maximum formula_6 such that the polynomial formula_7 and the formula_8-times shifted polynomial formula_9 have a common factor. It is formula_10 if such a formula_8 does not exist. The dispersion can be computed as the largest non-negative integer root of the resultant formula_12. Let formula_13 be a recurrence equation of order formula_14 with polynomial coefficients formula_15, polynomial right-hand side formula_16 and rational sequence solution formula_17. It is possible to write formula_18 for two relatively prime polynomials formula_3. Let formula_20 andformula_21where formula_22 denotes the falling factorial of a function. Then formula_23 divides formula_24. So the polynomial formula_24 can be used as a denominator for all rational solutions formula_26 and hence it is called a universal denominator.

Let again formula_13 be a recurrence equation with polynomial coefficients and formula_24 a universal denominator. After substituting formula_29 for an unknown polynomial formula_30 and setting formula_31 the recurrence equation is equivalent toformula_32As the formula_33 cancel this is a linear recurrence equation with polynomial coefficients which can be solved for an unknown polynomial solution formula_34. There are algorithms to find polynomial solutions. The solutions for formula_34 can then be used again to compute the rational solutions formula_36. 

The homogeneous recurrence equation of order formula_46formula_47over formula_48 has a rational solution. It can be computed by considering the dispersionformula_49This yields the following universal denominator:formula_50andformula_51Multiplying the original recurrence equation with formula_52 and substituting formula_53 leads toformula_54This equation has the polynomial solution formula_55 for an arbitrary constant formula_56. Using formula_57 the general rational solution isformula_58for arbitrary formula_56.


</doc>
<doc id="58462412" url="https://en.wikipedia.org/wiki?curid=58462412" title="Join-based tree algorithms">
Join-based tree algorithms

In computer science, join-based tree algorithms are a class of algorithms for self-balancing binary search trees.
The algorithmic framework is based on a single operation "join". Under this framework, the "join" operation captures all balancing criteria of different balancing schemes, and all other functions "join" have generic implementation across different balancing schemes. The "join-based algorithms" can be applied to at least four balancing schemes: AVL trees, red-black trees, weight-balanced trees and treaps.

The "join"formula_1 operation takes as input two binary balanced trees formula_2 and formula_3 of the same balancing scheme, and a key formula_4, and outputs a new balanced binary tree formula_5 whose in-order traversal is the in-order traversal of formula_2, then formula_4 then the in-order traversal of formula_3. In particular, if the trees are search trees, which means that the in-order of the trees maintain a total ordering on keys, it must satisfy the condition that all keys in formula_2 are smaller than formula_4 and all keys in formula_3 are greater than formula_4.

The "join" operation was first defined by Tarjan on red-black trees, which runs in worst-case logarithmic time. Later Sleator and Tarjan described an "join" algorithm for splay trees which runs in amortized logarithmic time. Later Adams extended "join" to weight-balanced trees and used it for fast set-set functions including union, intersection and set difference. In 1998, Blelloch and Reid-Miller extended "join" on treaps, and proved the bound of the set functions to be formula_13 for two trees of size formula_14 and formula_15, which is optimal in the comparison model. They also brought up parallelism in Adams' algorithm by using a divide-and-conquer scheme. In 2016, Blelloch et al. formally proposed the join-based algorithms, and formalized the "join" algorithm for four different balancing schemes: AVL trees, red-black trees, weight-balanced trees and treaps. In the same work they proved that Adams' algorithms on union, intersection and difference are work-optimal on all the four balancing schemes.

The function "join"formula_16 considers rebalancing the tree, and thus depends on the input balancing scheme. If the two trees are balanced, "join" simply creates a new node with left subtree , root and right subtree . Suppose that is heavier (this "heavier" depends on the balancing scheme) than (the other case is symmetric). "Join" follows the right spine of until a node which is balanced with . At this point a new node with left child , root and right child is created to replace c. The new node may invalidate the balancing invariant. This can be fixed with rotations.

The following is the "join" algorithms on different balancing schemes.

The "join" algorithm for AVL trees:

Here formula_17 of a node formula_18 the height of formula_18. expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4, and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4, and right child formula_23.

The "join" algorithm for red-black trees:

Here formula_27 of a node formula_18 means twice the black height of a black node, and the twice the black height of a red node. expose(v)=(l,⟨k,c⟩,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4, the color of the node formula_32 and the right child formula_23. Node(l,⟨k,c⟩,r) means to create a node of left child formula_21, key formula_4, color formula_32 and right child formula_23.

The "join" algorithm for weight-balanced trees:

Here balanceformula_38 means two weights formula_39 and formula_40 are balanced. expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4 and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4 and right child formula_23.

In the following, expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4 and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4 and right child formula_23. right(formula_18) and left(formula_18) extracts the right child and the left child of a tree nodeformula_18, respectively. formula_58 extract the key of a node formula_18. "formula_60" means that two statements formula_61 and formula_62 can run in parallel.

To split a tree into two trees, those smaller than key "x", and those larger than key "x", we first draw a path from the root by inserting "x" into the tree. After this insertion, all values less than "x" will be found on the left of the path, and all values greater than "x" will be found on the right. By applying "Join", all the subtrees on the left side are merged bottom-up using keys on the path as intermediate nodes from bottom to top to form the left tree, and the right part is asymmetric. For some applications, "Split" also returns a boolean value denoting if "x" appears in the tree. The cost of "Split" is formula_63, order of the height of the tree. 

The split algorithm is as follows:

This function is defined similarly as "join" but without the middle key. It first splits out the last key formula_4 of the left tree, and then join the rest part of the left tree with the right tree with formula_4.
The algorithm is as follows:

The cost is formula_63 for a tree of size formula_67.

The insertion and deletion algorithms, when making use of "join" can be independent of balancing schemes. For an insertion, the algorithm compares the key to be inserted with the key in the root, inserts it to the left/right subtree if the key is smaller/greater than the key in the root, and joins the two subtrees back with the root. A deletion compares the key to be deleted with the key in the root. If they are equal, return join2 on the two subtrees. Otherwise, delete the key from the corresponding subtree, and join the two subtrees back with the root.
The algorithms are as follows:

Both insertion and deletion requires formula_63 time if formula_69.

Several set operations have been defined on weight-balanced trees: union, intersection and set difference. The union of two weight-balanced trees and representing sets and , is a tree that represents . The following recursive function computes this union:

Similarly, the algorithms of intersection and set-difference are as follows:

The complexity of each of union, intersection and difference is formula_70 for two weight-balanced trees of sizes formula_14 and formula_15. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth formula_73. When formula_74, the join-based implementation applies the same computation as in a single-element insertion or deletion if the root of the larger tree is used to split the smaller tree.

The algorithm for building a tree can make use of the union algorithm, and use the divide-and-conquer scheme:

This algorithm costs formula_75 work and has formula_76 depth. A more-efficient algorithm makes use of a parallel sorting algorithm.

This algorithm costs formula_75 work and has formula_63 depth assuming the sorting algorithm has formula_75 work and formula_63 depth.

This function selects all entries in a tree satisfying an indicator formula_81, and return a tree containing all selected entries. It recursively filters the two subtrees, and join them with the root if the root satisfies formula_81, otherwise "join2" the two subtrees.

This algorithm costs work formula_83 and depth formula_63 on a tree of size formula_67, assuming formula_81 has constant cost.

The join-based algorithms are applied to support interface for sets, maps, and augmented maps in libarays such as Hackage, SML/NJ, and PAM.



</doc>
<doc id="73415" url="https://en.wikipedia.org/wiki?curid=73415" title="Sieve of Eratosthenes">
Sieve of Eratosthenes

In mathematics, the Sieve of Eratosthenes is a simple, ancient algorithm for finding all prime numbers up to any given limit.

It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, . The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.

The earliest known reference to the sieve (, "kóskinon Eratosthénous") is in Nicomachus of Gerasa's "Introduction to Arithmetic", which describes it and attributes it to Eratosthenes of Cyrene, a Greek mathematician.

One of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in arithmetic progressions.

A prime number is a natural number that has exactly two distinct natural number divisors: 1 and itself.

To find all the prime numbers less than or equal to a given integer by Eratosthenes' method:


The main idea here is that every value given to will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).

As a refinement, it is sufficient to mark the numbers in step 3 starting from , as all the smaller multiples of will have already been marked at that point. This means that the algorithm is allowed to terminate in step 4 when is greater than . 

Another refinement is to initially list odd numbers only, , and count in increments of from in step 3, thus marking only odd multiples of . This actually appears in the original algorithm. This can be generalized with wheel factorization, forming the initial list only from numbers coprime with the first few primes and not just from odds (i.e., numbers coprime with 2), and counting in the correspondingly adjusted increments so that only such multiples of are generated that are coprime with those small primes, in the first place.

To find all the prime numbers less than or equal to 30, proceed as follows.

First, generate a list of integers from 2 to 30:

The first number in the list is 2; cross out every 2nd number in the list after 2 by counting up from 2 in increments of 2 (these will be all the multiples of 2 in the list):

The next number in the list after 2 is 3; cross out every 3rd number in the list after 3 by counting up from 3 in increments of 3 (these will be all the multiples of 3 in the list):

The next number not yet crossed out in the list after 3 is 5; cross out every 5th number in the list after 5 by counting up from 5 in increments of 5 (i.e. all the multiples of 5):

The next number not yet crossed out in the list after 5 is 7; the next step would be to cross out every 7th number in the list after 7, but they are all already crossed out at this point, as these numbers (14, 21, 28) are also multiples of smaller primes because 7 × 7 is greater than 30. The numbers not crossed out at this point in the list are all the prime numbers below 30:

The sieve of Eratosthenes can be expressed in pseudocode, as follows:

This algorithm produces all primes not greater than . It includes a common optimization, which is to start enumerating the multiples of each prime from . The time complexity of this algorithm is , provided the array update is an operation, as is usually the case.

As Sorenson notes, the problem with the sieve of Eratosthenes is not the number of operations it performs but rather its memory requirements. For large , the range of primes may not fit in memory; worse, even for moderate , its cache use is highly suboptimal. The algorithm walks through the entire array , exhibiting almost no locality of reference.

A solution to these problems is offered by "segmented" sieves, where only portions of the range are sieved at a time. These have been known since the 1970s, and work as follows:


If is chosen to be , the space complexity of the algorithm is , while the time complexity is the same as that of the regular sieve.

For ranges with upper limit so large that the sieving primes below as required by the page segmented sieve of Eratosthenes cannot fit in memory, a slower but much more space-efficient sieve like the sieve of Sorenson can be used instead.

An incremental formulation of the sieve generates primes indefinitely (i.e., without an upper bound) by interleaving the generation of primes with the generation of their multiples (so that primes can be found in gaps between the multiples), where the multiples of each prime are generated directly by counting up from the square of the prime in increments of (or for odd primes). The generation must be initiated only when the prime's square is reached, to avoid adverse effects on efficiency. It can be expressed symbolically under the dataflow paradigm as

Primes can also be produced by iteratively sieving out the composites through [[Trial division|divisibility testing]] by sequential primes, one prime at a time. It is not the sieve of Eratosthenes but is often confused with it, even though the sieve of Eratosthenes directly generates the composites instead of testing for them. Trial division has worse theoretical [[Analysis of algorithms|complexity]] than that of the sieve of Eratosthenes in generating ranges of primes.

When testing each prime, the "optimal" trial division algorithm uses all prime numbers not exceeding its square root, whereas the sieve of Eratosthenes produces each composite from its prime factors only, and gets the primes "for free", between the composites. The widely known 1975 [[functional programming|functional]] sieve code by [[David Turner (computer scientist)|David Turner]] is often presented as an example of the sieve of Eratosthenes but is actually a sub-optimal trial division sieve.

The work performed by this algorithm is almost entirely the operations to cull the composite number representations which for the basic non-optimized version is the sum of the range divided by each of the primes up to that range or
where is the sieving range in this and all further analysis.

By rearranging Mertens' second theorem, this is equal to as approaches infinity, where M is the Meissel–Mertens constant of about ...

The optimization of starting at the square of each prime and only culling for primes less than the square root changes the "" in the above expression to (or ) and not culling until the square means that the sum of the base primes each minus two is subtracted from the operations. As the sum of the first primes is and the prime number theorem says that is approximately , then the sum of primes to is , and therefore the sum of base primes to is expressed as a factor of . The extra offset of two per base prime is , where is the prime-counting function in this case, or ; expressing this as a factor of as are the other terms, this is . Combining all of this, the expression for the number of optimized operations without wheel factorization is

For the wheel factorization cases, there is a further offset of the operations not done of
where is the highest wheel prime and a constant factor of the whole expression is applied which is the fraction of remaining prime candidates as compared to the repeating wheel circumference. The wheel circumference is
and it can easily be determined that this wheel factor is
as is the fraction of remaining candidates for the highest wheel prime, , and each succeeding smaller prime leaves its corresponding fraction of the previous combined fraction.

Combining all of the above analysis, the total number of operations for a sieving range up to including wheel factorization for primes up to is approximately

To show that the above expression is a good approximation to the number of composite number cull operations performed by the algorithm, following is a table showing the actually measured number of operations for a practical implementation of the sieve of Eratosthenes as compared to the number of operations predicted from the above expression with both expressed as a fraction of the range (rounded to four decimal places) for different sieve ranges and wheel factorizations (Note that the last column is a maximum practical wheel as to the size of the wheel gaps Look Up Table - almost 10 million values):

The above table shows that the above expression is a very good approximation to the total number of culling operations for sieve ranges of about a hundred thousand (10) and above.

The sieve of Eratosthenes is a popular way to benchmark computer performance. As can be seen from the above by removing all constant offsets and constant factors and ignoring terms that tend to zero as n approaches infinity, the time complexity of calculating all primes below in the random access machine model is operations, a direct consequence of the fact that the prime harmonic series asymptotically approaches . It has an exponential time complexity with regard to input size, though, which makes it a pseudo-polynomial algorithm. The basic algorithm requires of memory.

The bit complexity of the algorithm is bit operations with a memory requirement of .

The normally implemented page segmented version has the same operational complexity of as the non-segmented version but reduces the space requirements to the very minimal size of the segment page plus the memory required to store the base primes less than the square root of the range used to cull composites from successive page segments of size .

A special rarely if ever implemented segmented version of the sieve of Eratosthenes, with basic optimizations, uses operations and bits of memory.

To show that the above approximation in complexity is not very accurate even for about as large as practical a range, the following is a table of the estimated number of operations as a fraction of the range rounded to four places, the calculated ratio for a factor of ten change in range based on this estimate, and the factor based on the estimate for various ranges and wheel factorizations (the combo column uses a frequently practically used pre-cull by the maximum wheel factorization but only the 2/3/5/7 wheel for the wheel factor as the full factorization is difficult to implement efficiently for page segmentation):

The above shows that the estimate is not very accurate even for maximum practical ranges of about 10. One can see why it does not match by looking at the computational analysis above and seeing that within these practical sieving range limits, there are very significant constant offset terms such that the very slowly growing term does not get large enough so as to make these terms insignificant until the sieving range approaches infinity – well beyond any practical sieving range. Within these practical ranges, these significant constant offsets mean that the performance of the Sieve of Eratosthenes is much better than one would expect just using the asymptotic time complexity estimates by a significant amount, but that also means that the slope of the performance with increasing range is steeper than predicted as the benefit of the constant offsets becomes slightly less significant.

One should also note that in using the calculated operation ratios to the sieve range, it must be less than about 0.2587 in order to be faster than the often compared sieve of Atkin if the operations take approximately the same time each in CPU clock cycles, which is a reasonable assumption for the one huge bit array algorithm. Using that assumption, the sieve of Atkin is only faster than the maximally wheel factorized sieve of Eratosthenes for ranges of over 10 at which point the huge sieve buffer array would need about a quarter of a terabyte (about 250 gigabytes) of RAM memory even if bit packing were used. An analysis of the page segmented versions will show that the assumption that the time per operation stays the same between the two algorithms does not hold for page segmentation and that the sieve of Atkin operations get slower much faster than the sieve of Eratosthenes with increasing range. Thus for practical purposes, the maximally wheel factorized Sieve of Eratosthenes is faster than the Sieve of Atkin although the Sieve of Atkin is faster for lesser amounts of wheel factorization.

Using big O notation is also not the correct way to compare practical performance of even variations of the Sieve of Eratosthenes as it ignores constant factors and offsets that may be very significant for practical ranges: The sieve of Eratosthenes variation known as the Pritchard wheel sieve has an performance, but its basic implementation requires either a "one large array" algorithm which limits its usable range to the amount of available memory else it needs to be page segmented to reduce memory use. When implemented with page segmentation in order to save memory, the basic algorithm still requires about bits of memory (much more than the requirement of the basic page segmented sieve of Eratosthenes using bits of memory). Pritchard's work reduced the memory requirement to the limit as described above the table, but the cost is a fairly large constant factor of about three in execution time to about three quarters the sieve range due to the complex computations required to do so. As can be seen from the above table for the basic sieve of Eratosthenes, even though the resulting wheel sieve has performance and an acceptable memory requirement, it will never be faster than a reasonably Wheel Factorized basic sieve of Eratosthenes for any practical sieving range by a factor of about two. Other than that it is quite complex to implement, it is rarely practically implemented because it still uses more memory than the basic Sieve of Eratosthenes implementations described here as well as being slower for practical ranges. It is thus more of an intellectual curiosity than something practical.

Euler's proof of the zeta product formula contains a version of the sieve of Eratosthenes in which each composite number is eliminated exactly once. The same sieve was rediscovered and observed to take linear time by . It, too, starts with a list of numbers from 2 to in order. On each step the first element is identified as the next prime and the results of multiplying this prime with each element of the list are marked in the list for subsequent deletion. The initial element and the marked elements are then removed from the working sequence, and the process is repeated:
Here the example is shown starting from odds, after the first step of the algorithm. Thus, on the th step all the remaining multiples of the th prime are removed from the list, which will thereafter contain only numbers coprime with the first primes (cf. wheel factorization), so that the list will start with the next prime, and all the numbers in it below the square of its first element will be prime too.

Thus, when generating a bounded sequence of primes, when the next identified prime exceeds the square root of the upper limit, all the remaining numbers in the list are prime. In the example given above that is achieved on identifying 11 as next prime, giving a list of all primes less than or equal to 80.

Note that numbers that will be discarded by a step are still used while marking the multiples in that step, e.g., for the multiples of 3 it is , , , , ..., , ..., so care must be taken dealing with this.




</doc>
<doc id="775" url="https://en.wikipedia.org/wiki?curid=775" title="Algorithm">
Algorithm

In mathematics and computer science, an algorithm () is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous specifications for performing calculation, data processing, automated reasoning, and other tasks.

As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as Al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.

The word "algorithm" itself is derived from the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized "Algoritmi". A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.

The word 'algorithm' has its roots in Latinizing the name of Persian mathematician Muhammad ibn Musa al-Khwarizmi in the first steps to "algorismus". Al-Khwārizmī (, , c. 780–850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan.

About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title "Algoritmi de numero Indorum". This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, "algorismus", English 'algorism', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word ἀριθμός 'number' ("cf." 'arithmetic'), the Latin word was altered to "algorithmus", and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.

In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.

Another early use of the word is from 1240, in a manual titled "Carmen de Algorismo" composed by Alexandre de Villedieu. It begins with:
which translates to:
The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.

An informal definition could be "a set of rules that precisely defines a sequence of operations", which would include all computer programs, including programs that do not perform numeric calculations, and (for example) any prescribed bureaucratic procedure.
In general, a program is only an algorithm if it stops eventually.

A prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.

No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give "explicit instructions for determining the nth member of the set", for arbitrary finite "n". Such instructions are to be given quite explicitly, in a form in which "they could be followed by a computing machine", or by a "human who is capable of carrying out only very elementary operations on symbols."

An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an "arbitrary" "input" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as "y = m + n" (i.e., two arbitrary "input variables" "m" and "n" that produce an output "y"), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):

The concept of "algorithm" is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of "algorithm" that suits both concrete (in some sense) and abstract usage of the term.

Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):

Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to of a major theorem of Computability Theory known as the Halting Problem.

Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some of these computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom"—an idea that is described more formally by "flow of control".

So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of "memory" as a scratchpad. An example of such an assignment can be found below.

For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.

Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.

There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see Turing machine for more).

Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:

For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.

Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.

One of the most important aspects of algorithm design lies in the creation of algorithm that has an efficient run-time, also known as its Big O.

Typical steps in the development of algorithms:

Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.

In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended "target" computer(s) to produce "output" from given (perhaps null) "input". An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.

""Elegant" (compact) programs, "good" (fast) programs ": The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant—such a proof would solve the Halting problem (ibid).

"Algorithm versus function computable by an algorithm": For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of "algorithm", i.e. procedure and the notion of "function computable by algorithm", i.e. mapping yielded by procedure. The same function may have several different algorithms".

Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.

"Computers (and computors), models of computation": A computer (or human "computor") is a restricted type of machine, a "discrete deterministic mechanical device" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable "locations", (ii) discrete, indistinguishable "counters" (iii) an agent, and (iv) a list of instructions that are "effective" relative to the capability of the agent.

Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three "assignment" (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general "types" of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction " Z ← 0 "; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.

"Simulation of an algorithm: computer (computor) language": Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can "effectively" execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.

This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).

But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of "simulation" enters". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").

"Structured programming, canonical structures": Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.

"Canonical flowchart symbols": The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.

One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:

"High-level description:"

"(Quasi-)formal description:"
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:

Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his "Elements". Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length "s" successively ("q" times) along longer length "l" until the remaining portion "r" is less than the shorter length "s". In modern words, remainder "r" = "l" − "q"×"s", "q" being the quotient, or remainder "r" is the "modulus", the integer-fractional part left over after the division.

For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).

Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the "greatest". While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.

Only a few instruction "types" are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.

The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length "s" from the remaining length "r" until "r" is less than "s". The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:

INPUT:

E0: [Ensure "r" ≥ "s".]

E1: [Find remainder]: Until the remaining length "r" in R is less than the shorter length "s" in S, repeatedly subtract the measuring number "s" in S from the remaining length "r" in R.

E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.

E3: [Interchange "s" and "r"]: The nut of Euclid's algorithm. Use remainder "r" to measure what was previously smaller number "s"; L serves as a temporary location.

OUTPUT:

DONE:

 The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction is the assignment instruction symbolized by ←.

"How "Elegant" works": In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become "s" (the new measuring length) and the subtrahend can become the new "r" (the length to be measured); in other words the "sense" of the subtraction reverses.

The following version can be used with Object Oriented languages:
Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.

But "exceptional cases" must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if "negative" numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).

"Proof of program correctness by use of mathematical induction": Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.

"Elegance (compactness) versus goodness (speed)": With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is "faster" (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: "Elegant" does "two" conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, "on average" much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.

"Can the algorithms be improved?": Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?

The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.

The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.

It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O("n"), using the big O notation with "n" as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of "O(1)", if the space required to store the input numbers is not counted, or O("n") if it is counted.

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.

The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.

To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.

There are various ways to classify algorithms, each with its own merits.

One way to classify algorithms is by implementation means.


Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:


For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:


Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.

Algorithms can be classified by the amount of time they need to complete compared to their input size:

Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.

The adjective "continuous" when applied to the word "algorithm" can mean:

Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.

Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

The earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.

Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the "Introduction to Arithmetic" by Nicomachus, and the Euclidean algorithm, which was first described in "Euclid's Elements" (c. 300 BC).

Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.

Muhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the "Al-jabr" in the 9th century. The terms "algorism" and "algorithm" are derived from the name al-Khwārizmī, while the term "algebra" is derived from the book "Al-jabr". In Europe, the word "algorithm" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):

The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in "A Manuscript On Deciphering Cryptographic Messages". He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.

"The clock": Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.

"Logical machines 1870 – Stanley Jevons' "logical abacus" and "logical machine"": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a "Logical Machine"" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] ...". With this machine he could analyze a "syllogism or any other simple logical argument".

This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 "Symbolic Logic", turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's "abacus" ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".

"Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay": Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.

"Telephone-switching networks" of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".

Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" "open" and "closed"):

"Symbols and rules": In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's "The principles of arithmetic, presented by a new method" (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".

But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a "lingua characterica", a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).

"The paradoxes": At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.

"Effective calculability": In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine". S.C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis".

Emil Post (1936) described the actions of a "computer" (human being) as follows:

His symbol space would be

Alan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.

Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.

Turing's reduction yields the following:
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:

A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):

Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his "An Unsolvable Problem of Elementary Number Theory" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper "On Formally Undecidable Propositions of Principia Mathematica and Related Systems I" (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.

Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):

A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.




</doc>
<doc id="6770335" url="https://en.wikipedia.org/wiki?curid=6770335" title="Car–Parrinello molecular dynamics">
Car–Parrinello molecular dynamics

Car–Parrinello molecular dynamics or CPMD refers to either a method used in molecular dynamics (also known as the Car–Parrinello method) or the computational chemistry software package used to implement this method.

The CPMD method is related to the more common Born–Oppenheimer molecular dynamics (BOMD) method in that the quantum mechanical effect of the electrons is included in the calculation of energy and forces for the classical motion of the nuclei. However, whereas BOMD treats the electronic structure problem within the time-"independent" Schrödinger equation, CPMD explicitly includes the electrons as active degrees of freedom, via (fictitious) dynamical variables.

The software is a parallelized plane wave / pseudopotential implementation of density functional theory, particularly designed for "ab initio" molecular dynamics.

The Car–Parrinello method is a type of molecular dynamics, usually employing periodic boundary conditions, planewave basis sets, and density functional theory, proposed by Roberto Car and Michele Parrinello in 1985, who were subsequently awarded the Dirac Medal by ICTP in 2009.

In contrast to Born–Oppenheimer molecular dynamics wherein the nuclear (ions) degree of freedom are propagated using ionic forces which are calculated at each iteration by approximately solving the electronic problem with conventional matrix diagonalization methods, the Car–Parrinello method explicitly introduces the electronic degrees of freedom as (fictitious) dynamical variables, writing an extended Lagrangian for the system which leads to a system of coupled equations of motion for both ions and electrons. In this way an explicit electronic minimization at each time step, as done in Born-Oppenheimer MD, is not needed: after an initial standard electronic minimization, the fictitious dynamics of the electrons keeps them on the electronic ground state corresponding to each new ionic configuration visited along the dynamics, thus yielding accurate ionic forces. In order to maintain this adiabaticity condition, it is necessary that the fictitious mass of the electrons is chosen small enough to avoid a significant energy transfer from the ionic to the electronic degrees of freedom. This small fictitious mass in turn requires that the equations of motion are integrated using a smaller time step than the one (1–10 fs) commonly used in Born–Oppenheimer molecular dynamics.

In CPMD the core electrons are usually described by a pseudopotential and the wavefunction of the valence electrons are approximated by a plane wave basis set.

The ground state electronic density (for fixed nuclei) is calculated self-consistently, usually using the density functional theory method. Then, using that density, forces on the nuclei can be computed, to update the trajectories (using, e.g. the Verlet integration algorithm). In addition, however, the coefficients used to obtain the electronic orbital functions can be treated as a set of extra spatial dimensions, and trajectories for the orbitals can be calculated in this context.

CPMD is an approximation of the Born–Oppenheimer MD (BOMD) method. In BOMD, the electrons' wave function must be minimized via matrix diagonalization at every step in the trajectory. CPMD uses fictitious dynamics to keep the electrons close to the ground state, preventing the need for a costly self-consistent iterative minimization at each time step. The fictitious dynamics relies on the use of a fictitious electron mass (usually in the range of 400 – 800 a.u.) to ensure that there is very little energy transfer from nuclei to electrons, i.e. to ensure adiabaticity. Any increase in the fictitious electron mass resulting in energy transfer would cause the system to leave the ground-state BOMD surface.

where "E"[{"ψ"},{R}] is the Kohn–Sham energy density functional, which outputs energy values when given Kohn–Sham orbitals and nuclear positions.

where "δ" is the Kronecker delta.

The equations of motion are obtained by finding the stationary point of the Lagrangian under variations of "ψ" and R, with the orthogonality constraint.

where Λ is a Lagrangian multiplier matrix to comply with the orthonormality constraint.

In the formal limit where "μ" → 0, the equations of motion approach Born–Oppenheimer molecular dynamics.





</doc>
<doc id="59538271" url="https://en.wikipedia.org/wiki?curid=59538271" title="Newest vertex bisection">
Newest vertex bisection

Newest Vertex Bisection is an algorithmic method to locally refine triangulations. It is widely used in computational science, numerical simulation, and computer graphics. The advantage of newest vertex bisection is that it allows local refinement of triangulations without degenerating the shape of the triangles after repeated usage.

In newest vertex bisection, whenever a triangle is to be split into smaller triangles, it will be bisected by drawing a line from the newest vertex to the midpoint of the edge opposite to that vertex. That midpoint becomes the newest vertex of the two newer triangles. One can show that repeating this procedure for a given triangulation leads to triangles that belong to only a finite number of similarity classes.

Generalizations of newest vertex bisection to dimension three and higher are known. Newest vertex bisection is used in local mesh refinement for adaptive finite element methods, where it is an alternative to red-green refinement and uniform mesh refinement.


</doc>
<doc id="59892172" url="https://en.wikipedia.org/wiki?curid=59892172" title="Neural Style Transfer">
Neural Style Transfer

Neural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks in order to perform the image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user supplied photographs. Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma.

NST is an example of image stylization, a problem studied for over two decades within the field of non-photorealistic rendering. Prior to NST, the transfer of image style was performed using machine learning techniques based on image analogy. Given a training pair of images–a photo and an artwork depicting that photo–a transformation could be learned and then applied to create a new artwork from a new photo, by analogy. The drawback of this method is that such a training pair rarely exists in practice. For example, original source material (photos) are rarely available for famous artworks.

NST requires no such pairing; only a single example of artwork is needed for the algorithm to transfer its style.

NST was first published in the paper "A Neural Algorithm of Artistic Style" by Gatys et al., originally released to ArXiv 2015, and subsequently accepted by the peer-reviewed Computer Vision and Pattern Recognition (CVPR) in 2016.

The core innovation of NST is the use of deep learning to disentangle the representation of the content (structure) of an image, from the appearance (style) in which it is depicted. The original paper used a convolutional neural network (CNN) VGG-19 architecture that has been pre-trained to perform object recognition using the ImageNet dataset.

The process of NST assumes an input image formula_1 and an example style image formula_2.

The image formula_1 is fed through the CNN, and network activations are sampled at a late convolution layer of the VGG-19 architecture. Let formula_4 be the resulting output sample, called the 'content' of the input formula_1.

The style image formula_2 is then fed through the same CNN, and network activations are sampled at the early to middle layers of the CNN. These activations are encoded into a Gramian matrix representation, call it formula_7 to denote the 'style' of formula_2.

The goal of NST is to synthesize an output image formula_9 that exhibits the content of formula_1 applied with the style of formula_2, i.e. formula_12 and formula_13.

An iterative optimization (usually gradient descent) then gradually updates formula_9 to minimize the loss function error:

formula_15,

where formula_16 is the L2 distance. The constant formula_17 controls the level of the stylization effect.

Image formula_9 is initially approximated by adding a small amount of white noise to input image formula_1 and feeding it through the CNN. Then we successively backpropagate this loss through the network with the CNN weights fixed in order to update the pixels of formula_9. After several thousand epochs of training, an formula_9 (hopefully) emerges that matches the style of formula_2 and the content of formula_1.

Algorithms are typically implemented for GPUs, so that training takes a few minutes.

NST has also been extended to videos.

Subsequent work improved the speed of NST for images.

In a paper by Fei-Fei Li et al. adopted a different regularized loss metric and accelerated method for training to produce results in real time (three times faster than Gatys). Their idea was to use not the "pixel-based loss" defined above but rather a 'perceptual loss' measuring the differences between higher level layers within the CNN. They used a symmetric encoder-decoder CNN. Training uses a similar loss function to the basic NST method but also regularizes the output for smoothness using a total variation (TV) loss. Once trained, the network may be used to transform an image into the style used during training, using a single feed-forward pass of the network. However the network is restricted to the single style in which it has been trained.

In a work by Chen Dongdong et al. they explored the fusion of optical flow information into feedforward networks in order to improve the temporal coherence of the output.

Most recently, feature transform based NST methods have been explored for fast stylization that are not coupled to single specific style and enable user-controllable "blending" of styles, for example the Whitening and Coloring Transform (WCT).


</doc>
<doc id="59730114" url="https://en.wikipedia.org/wiki?curid=59730114" title="Parallel external memory">
Parallel external memory

In computer science, a parallel external memory (PEM) model is a cache-aware, external-memory abstract machine. It is the parallel-computing analogy to the single-processor external memory (EM) model. In a similar way, it is the cache-aware analogy to the parallel random-access machine (PRAM). The PEM model consists of a number of processors, together with their respective private caches and a shared main memory.

The PEM model is a combination of the EM model and the PRAM model. The PEM model is a computation model which consists of formula_1 processors and a two-level memory hierarchy. This memory hierarchy consists of a large external memory (main memory) of size formula_2 and formula_1 small internal memories (caches). The processors share the main memory. Each cache is exclusive to a single processor. A processor can't access another’s cache. The caches have a size formula_4 which is partitioned in blocks of size formula_5. The processors can only perform operations on data which are in their cache. The data can be transferred between the main memory and the cache in blocks of size formula_5.

The complexity measure of the PEM model is the I/O complexity, which determines the number of parallel blocks transfers between the main memory and the cache. During a parallel block transfer each processor can transfer a block. So if formula_1 processors load parallelly a data block of size formula_5 form the main memory into their caches, it is considered as an I/O complexity of formula_9 not formula_10. A program in the PEM model should minimize the data transfer between main memory and caches and operate as much as possible on the data in the caches.

In the PEM model, there is no direct communication network between the P processors. The processors have to communicate indirectly over the main memory. If multiple processors try to access the same block in main memory concurrently read/write conflicts occur. Like in the PRAM model, three different variations of this problem are considered:
The following two algorithms solve the CREW and EREW problem if formula_11 processors write to the same block simultaneously.
A first approach is to serialize the write operations. Only one processor after the other writes to the block. This results in a total of formula_1 parallel block transfers. A second approach needs formula_13 parallel block transfers and an additional block for each processor. The main idea is to schedule the write operations in a binary tree fashion and gradually combine the data into a single block. In the first round formula_1 processors combine their blocks into formula_15 blocks. Then formula_15 processors combine the formula_15 blocks into formula_18. This procedure is continued until all the data is combined in one block.

Let formula_19 be a vector of d-1 pivots sorted in increasing order. Let formula_20 be an unordered set of N elements. A d-way partition of formula_20 is a set formula_22 , where formula_23 and formula_24 for formula_25. formula_26 is called the i-th bucket. The number of elements in formula_26 is greater than formula_28 and smaller than formula_29. In the following algorithm the input is partitioned into N/P-sized contiguous segments formula_30 in main memory. The processor i primarily works on the segment formula_31. The multiway partitioning algorithm (codice_1) uses a PEM prefix sum algorithm to calculate the prefix sum with the optimal formula_32 I/O complexity. This algorithm simulates an optimal PRAM prefix sum algorithm.

If the vector of formula_43 pivots M and the input set A are located in contiguous memory, then the d-way partitioning problem can be solved in the PEM model with formula_44 I/O complexity. The content of the final buckets have to be located in contiguous memory.

The selection problem is about finding the k-th smallest item in an unordered list formula_20 of size formula_2.
The following code makes use of codice_2 which is a PRAM optimal sorting algorithm which runs in formula_47, and codice_3, which is a cache optimal single-processor selection algorithm.

Under the assumption that the input is stored in contiguous memory, codice_4 has an I/O complexity of:

formula_59

Distribution sort partitions an input list formula_20 of size formula_2 into formula_62 disjoint buckets of similar size. Every bucket is then sorted recursively and the results are combined into a fully sorted list.

If formula_63 the task is delegated to a cache-optimal single-processor sorting algorithm.

Otherwise the following algorithm is used:

The I/O complexity of codice_5 is:

formula_96

where

formula_97

If the number of processors is chosen that formula_98and formula_99 the I/O complexity is then:

formula_100

Where formula_101 is the time it takes to sort formula_2 items with formula_1 processors in the PEM model.



</doc>
<doc id="588615" url="https://en.wikipedia.org/wiki?curid=588615" title="Ant colony optimization algorithms">
Ant colony optimization algorithms

In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. Artificial Ants stand for multi-agent methods inspired by the behavior of real ants. 
The pheromone-based communication of biological ants is often the predominant paradigm used. Combinations of Artificial Ants and local search algorithms have become a method of choice for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as AntOptima.

As an example, Ant colony optimization is a class of optimization algorithms modeled on the actions of an ant colony. Artificial 'ants' (e.g. simulation agents) locate optimal solutions by moving through a parameter space representing all possible solutions. Real ants lay down pheromones directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions. One variation on this approach is the bees algorithm, which is more analogous to the foraging patterns of the honey bee, another social insect.

This algorithm is a member of the ant colony algorithms family, in swarm intelligence methods, and it constitutes some metaheuristic optimizations. Initially proposed by Marco Dorigo in 1992 in his PhD thesis, the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of ants seeking a path between their colony and a source of food. The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants. From a broader perspective, ACO performs a model-based search and shares some similarities with estimation of distribution algorithms.

In the natural world, ants of some species (initially) wander randomly, and upon finding food return to their colony while laying down pheromone trails. If other ants find such a path, they are likely not to keep travelling at random, but instead to follow the trail, returning and reinforcing it if they eventually find food (see Ant communication).

Over time, however, the pheromone trail starts to evaporate, thus reducing its attractive strength. The more time it takes for an ant to travel down the path and back again, the more time the pheromones have to evaporate. A short path, by comparison, gets marched over more frequently, and thus the pheromone density becomes higher on shorter paths than longer ones. Pheromone evaporation also has the advantage of avoiding the convergence to a locally optimal solution. If there were no evaporation at all, the paths chosen by the first ants would tend to be excessively attractive to the following ones. In that case, the exploration of the solution space would be constrained. The influence of pheromone evaporation in real ant systems is unclear, but it is very important in artificial systems.

The overall result is that when one ant finds a good (i.e., short) path from the colony to a food source, other ants are more likely to follow that path, and positive feedback eventually leads to many ants following a single path. The idea of the ant colony algorithm is to mimic this behavior with "simulated ants" walking around the graph representing the problem to solve.

New concepts are required since “intelligence” is no longer centralized but can be found throughout all minuscule objects. Anthropocentric concepts have been known to lead to the production of IT systems in which data processing, control units and calculating forces are centralized. These centralized units have continually increased their performance and can be compared to the human brain. The model of the brain has become the ultimate vision of computers. Ambient networks of intelligent objects and, sooner or later, a new generation of information systems which are even more diffused and based on nanotechnology, will profoundly change this concept. Small devices that can be compared to insects do not dispose of a high intelligence on their own. Indeed, their intelligence can be classed as fairly limited. It is, for example, impossible to integrate a high performance calculator with the power to solve any kind of mathematical problem into a biochip that is implanted into the human body or integrated in an intelligent tag which is designed to trace commercial articles. However, once those objects are interconnected they dispose of a form of intelligence that can be compared to a colony of ants or bees. In the case of certain problems, this type of intelligence can be superior to the reasoning of a centralized system similar to the brain.

Nature offers several examples of how minuscule organisms, if they all follow the same basic rule, can create a form of collective intelligence on the macroscopic level. Colonies of social insects perfectly illustrate this model which greatly differs from human societies. This model is based on the co-operation of independent units with simple and unpredictable behavior. They move through their surrounding area to carry out certain tasks and only possess a very limited amount of information to do so. A colony of ants, for example, represents numerous qualities that can also be applied to a network of ambient objects. Colonies of ants have a very high capacity to adapt themselves to changes in the environment as well as an enormous strength in dealing with situations where one individual fails to carry out a given task. This kind of flexibility would also be very useful for mobile networks of objects which are perpetually developing. Parcels of information that move from a computer to a digital object behave in the same way as ants would do. They move through the network and pass from one knot to the next with the objective of arriving at their final destination as quickly as possible.

Pheromone-based communication is one of the most effective ways of communication which is widely observed in nature. Pheromone is used by social insects such as
bees, ants and termites; both for inter-agent and agent-swarm communications. Due to its feasibility, artificial pheromones have been adopted in multi-robot and swarm robotic systems. Pheromone-based communication was implemented by different means such as chemical or physical (RFID tags, light, sound) ways. However, those implementations were not able to replicate all the aspects of pheromones as seen in nature.

Using projected light was presented in an 2007 IEEE paper by Garnier, Simon, et al. as an experimental setup to study pheromone-based communication with micro autonomous robots. Another study that proposed a novel pheromone communication method, "COSΦ", for a swarm robotic system is based on precise and fast visual localization. 
The system allows simulation of a virtually unlimited number of different pheromones and provides the result of their interaction as a gray-scale image on a horizontal LCD screen that the robots move on. In order to demonstrate the pheromone communication method, Colias autonomous micro robot was deployed as the swarm robotic platform.

In the ant colony optimization algorithms, an artificial ant is a simple computational agent that searches for good solutions to a given optimization problem. To apply an ant colony algorithm, the optimization problem needs to be converted into the problem of finding the shortest path on a weighted graph. In the first step of each iteration, each ant stochastically constructs a solution, i.e. the order in which the edges in the graph should be followed. In the second step, the paths found by the different ants are compared. The last step consists of updating the pheromone levels on each edge.

Each ant needs to construct a solution to move through the graph. To select the next edge in its tour, an ant will consider the length of each edge available from its current position, as well as the corresponding pheromone level. At each step of the algorithm, each ant moves from a state formula_1 to state formula_2, corresponding to a more complete intermediate solution. Thus, each ant formula_3 computes a set formula_4 of feasible expansions to its current state in each iteration, and moves to one of these in probability. For ant formula_3, the probability formula_6 of moving from state formula_1 to state formula_2 depends on the combination of two values, the "attractiveness" formula_9 of the move, as computed by some heuristic indicating the "a priori" desirability of that move and the "trail level" formula_10 of the move, indicating how proficient it has been in the past to make that particular move. The "trail level" represents a posteriori indication of the desirability of that move.

In general, the formula_3th ant moves from state formula_1 to state formula_2 with probability

formula_14

where

formula_10 is the amount of pheromone deposited for transition from state formula_1 to formula_2, 0 ≤ formula_18 is a parameter to control the influence of formula_10, formula_9 is the desirability of state transition formula_21 ("a priori" knowledge, typically formula_22, where formula_23 is the distance) and formula_24 ≥ 1 is a parameter to control the influence of formula_9. formula_26 and formula_27 represent the trail level and attractiveness for the other possible state transitions.

Trails are usually updated when all ants have completed their solution, increasing or decreasing the level of trails corresponding to moves that were part of "good" or "bad" solutions, respectively. An example of a global pheromone updating rule is

formula_28

where formula_10 is the amount of pheromone deposited for a state transition formula_21, formula_31 is the "pheromone evaporation coefficient" and formula_32 is the amount of pheromone deposited by formula_3th ant, typically given for a TSP problem (with moves corresponding to arcs of the graph) by

formula_34

where formula_35 is the cost of the formula_3th ant's tour (typically length) and formula_37 is a constant.

Here are some of the most popular variations of ACO algorithms.

The Ant System is the first ACO algorithm. This algorithm corresponds to the one presented above. It was developed by Dorigo.

In the Ant Colony System algorithm, the original Ant System was modified in three aspects: (i) the edge selection is biased towards exploitation (i.e. favoring the probability of selecting the shortest edges with a large amount of pheromone); (ii) while building a solution, ants change the pheromone level of the edges they are selecting by applying a local pheromone updating rule; (iii) at the end of each iteration, only the best ant is allowed to update the trails by applying a modified global pheromone updating rule. 

In this algorithm, the global best solution deposits pheromone on its trail after every iteration (even if this trial has not been revisited), along with all the other ants.

This algorithm controls the maximum and minimum pheromone amounts on each trail. Only the global best tour or the iteration best tour are allowed to add pheromone to its trail. To avoid stagnation of the search algorithm, the range of possible pheromone amounts on each trail is limited to an interval [τ,τ]. All edges are initialized to τ to force a higher exploration of solutions. The trails are reinitialized to τ when nearing stagnation.

All solutions are ranked according to their length. Only a fixed number of the best ants in this iteration are allowed to update their trials. The amount of pheromone deposited is weighted for each solution, such that solutions with shorter paths deposit more pheromone than the solutions with longer paths.

The pheromone deposit mechanism of COAC is to enable ants to search for solutions collaboratively and effectively. By using an orthogonal design method, ants in the feasible domain can explore their chosen regions rapidly and efficiently, with enhanced global search capability and accuracy. The orthogonal design method and the adaptive radius adjustment method can also be extended to other optimization algorithms for delivering wider advantages in solving practical problems.

It is a recursive form of ant system which divides the whole search domain into several sub-domains and solves the objective on these subdomains. The results from all the subdomains are compared and the best few of them are promoted for the next level. The subdomains corresponding to the selected results are further subdivided and the process is repeated until an output of desired precision is obtained. This method has been tested on ill-posed geophysical inversion problems and works well.

For some versions of the algorithm, it is possible to prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of convergence for an ant colony algorithm was made in 2000, the graph-based ant system algorithm, and later on for the ACS and MMAS algorithms. Like most metaheuristics, it is very difficult to estimate the theoretical speed of convergence. A performance analysis of a continuous ant colony algorithm with respect to its various parameters (edge selection strategy, distance measure metric, and pheromone evaporation rate) showed that its performance and rate of convergence are sensitive to the chosen parameter values, and especially to the value of the pheromone evaporation rate. In 2004, Zlochin and his colleagues showed that COAC-type algorithms could be assimilated methods of stochastic gradient descent, on the cross-entropy and estimation of distribution algorithm. They proposed these metaheuristics as a "research-based model".

Ant colony optimization algorithms have been applied to many combinatorial optimization problems, ranging from quadratic assignment to protein folding or routing vehicles and a lot of derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and parallel implementations.
It has also been used to produce near-optimal solutions to the travelling salesman problem. They have an advantage over simulated annealing and genetic algorithm approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time. This is of interest in network routing and urban transportation systems.

The first ACO algorithm was called the ant system and it was aimed to solve the travelling salesman problem, in which the goal is to find the shortest round-trip to link a series of cities. The general algorithm is relatively simple and based on a set of ants, each making one of the possible round-trips along the cities. At each stage, the ant chooses to move from one city to another according to some rules:






To optimize the form of antennas, ant colony algorithms can be used. As example can be considered antennas RFID-tags based on ant colony algorithms (ACO)., loopback and unloopback vibrators 10×10

The ACO algorithm is used in image processing for image edge detection and edge linking.
The graph here is the 2-D image and the ants traverse from one pixel depositing pheromone.The movement of ants from one pixel to another is directed by the local variation of the image's intensity values. This movement causes the highest density of the pheromone to be deposited at the edges.

The following are the steps involved in edge detection using ACO:

"Step1: Initialization:"Randomly place formula_38 ants on the image formula_39 where formula_40 . Pheromone matrix formula_41 are initialized with a random value. The major challenge in the initialization process is determining the heuristic matrix.

There are various methods to determine the heuristic matrix. For the below example the heuristic matrix was calculated based on the local statistics:
the local statistics at the pixel position (i,j).

formula_42

Where formula_43 is the image of size formula_44
formula_45,which is a normalization factor

formula_46

formula_47 can be calculated using the following functions:formula_48formula_49formula_50formula_51The parameter formula_52 in each of above functions adjusts the functions’ respective shapes."Step 2 Construction process:"The ant's movement is based on 4-connected pixels or 8-connected pixels. The probability with which the ant moves is given by the probability equation formula_53"Step 3 and Step 5 Update process:"The pheromone matrix is updated twice. in step 3 the trail of the ant (given by formula_54 ) is updated where as in step 5 the evaporation rate of the trail is updated which is given by the below equation.formula_55, where formula_56 is the pheromone decay coefficient formula_57

"Step 7 Decision Process:"Once the K ants have moved a fixed distance L for N iteration, the decision whether it is an edge or not is based on the threshold T on the pheromone matrixτ. Threshold for the below example is calculated based on Otsu's method.

Image Edge detected using ACO:The images below are generated using different functions given by the equation (1) to (4).


With an ACO algorithm, the shortest path in a graph, between two points A and B, is built from a combination of several paths. It is not easy to give a precise definition of what algorithm is or is not an ant colony, because the definition may vary according to the authors and uses. Broadly speaking, ant colony algorithms are regarded as populated metaheuristics with each solution represented by an ant moving in the search space. Ants mark the best solutions and take account of previous markings to optimize their search. They can be seen as probabilistic multi-agent algorithms using a probability distribution to make the transition between each iteration. In their versions for combinatorial problems, they use an iterative construction of solutions. According to some authors, the thing which distinguishes ACO algorithms from other relatives (such as algorithms to estimate the distribution or particle swarm optimization) is precisely their constructive aspect. In combinatorial problems, it is possible that the best solution eventually be found, even though no ant would prove effective. Thus, in the example of the Travelling salesman problem, it is not necessary that an ant actually travels the shortest route: the shortest route can be built from the strongest segments of the best solutions. However, this definition can be problematic in the case of problems in real variables, where no structure of 'neighbours' exists. The collective behaviour of social insects remains a source of inspiration for researchers. The wide variety of algorithms (for optimization or not) seeking self-organization in biological systems has led to the concept of "swarm intelligence", which is a very general framework in which ant colony algorithms fit.

There is in practice a large number of algorithms claiming to be "ant colonies", without always sharing the general framework of optimization by canonical ant colonies. In practice, the use of an exchange of information between ants via the environment (a principle called "stigmergy") is deemed enough for an algorithm to belong to the class of ant colony algorithms. This principle has led some authors to create the term "value" to organize methods and behavior based on search of food, sorting larvae, division of labour and cooperative transportation.


The inventors are Frans Moyson and Bernard Manderick. Pioneers of the field include Marco Dorigo, Luca Maria Gambardella.

Chronology of ant colony optimization algorithms.




</doc>
<doc id="60034541" url="https://en.wikipedia.org/wiki?curid=60034541" title="Miller's recurrence algorithm">
Miller's recurrence algorithm

Miller's recurrence algorithm is a procedure for calculating a rapidly decreasing solution of a linear recurrence relation developed by J. C. P. Miller. It was originally developed to compute tables of the modified Bessel function but also applies to Bessel functions of the first kind and has other applications such as computation of the coefficients of Chebyshev expansions of other special functions.

Many families of special functions satisfy a recurrence relation that relates the values of the functions of different orders with common argument formula_1.

The modified Bessel functions of the first kind formula_2 satisfy the recurrence relation
However, the modified Bessel functions of the second kind formula_4 also satisfy the same recurrence relation

The first solution decreases rapidly with formula_6. The second solution increases rapidly with formula_6. Miller's algorithm provides a numerically stable procedure to obtain the decreasing solution.

To compute the terms of a recurrence formula_8 through formula_9 according to Miller's algorithm, one first chooses a value formula_10 much larger than formula_11 and computes a trial solution taking initial conditionformula_12 to an arbitrary non-zero value (such as 1) and taking formula_13 and later terms to be zero. Then the recurrence relation is used to successively compute trial values for formula_14, formula_15 down to formula_8. Noting that a second sequence obtained from the trial sequence by multiplication by a constant normalizing factor will still satisfy the same recurrence relation, one can then apply a separate normalizing relationship to determine the normalizing factor that yields the actual solution.

In the example of the modified Bessel functions, a suitable normalizing relation is a summation involving the even terms of the recurrence: 
where the infinite summation becomes finite due to the approximation that formula_13 and later terms are zero.

Finally, it is confirmed that the approximation error of the procedure is acceptable by repeating the procedure with a second choice of formula_10 larger than the initial choice and confirming that the second set of results for formula_8 through formula_9 agree within the first set within the desired tolerance. Note that to obtain this agreement, the value of formula_10 must be large enough such that the term formula_12 is small compared to the desired tolerance.

In contrast to Miller's algorithm, attempts to apply the recurrence relation in the forward direction starting from known values of formula_24 and formula_25 obtained by other methods will fail as rounding errors introduce components of the rapidly increasing solution.

Olver and Gautschi analyses the error propagation of the algorithm in detail.

For Bessel functions of the first kind, the equivalent recurrence relation and normalizing relationship are :

The algorithm is particularly efficient in applications that require the values of the Bessel functions for all orders formula_28 for each value of formula_1 compared to direct independent computations of formula_30 separate functions.


</doc>
<doc id="60327286" url="https://en.wikipedia.org/wiki?curid=60327286" title="Whitehead's algorithm">
Whitehead's algorithm

Whitehead's algorithm is a mathematical algorithm in group theory for solving the automorphic equivalence problem in the finite rank free group "F". The algorithm is based on a classic 1936 paper of J. H. C. Whitehead. It is still unknown (except for the case "n" = 2) if Whitehead's algorithm has polynomial time complexity.

Let formula_1 be a free group of rank formula_2 with a free basis formula_3. The automorphism problem, or the automorphic equivalence problem for formula_4 asks, given two freely reduced words formula_5 whether there exists an automorphism formula_6 such that formula_7.

Thus the automorphism problem asks, for formula_5 whether formula_9.
For formula_5 one has formula_9 if and only if formula_12, where formula_13 are conjugacy classes in formula_4 of formula_15 accordingly. Therefore, the automorphism problem for formula_4 is often formulated in terms of formula_17-equivalence of conjugacy classes of elements of formula_4.

For an element formula_19, formula_20 denotes the freely reduced length of formula_21 with respect to formula_22, and formula_23 denotes the cyclically reduced length of formula_21 with respect to formula_22. For the automorphism problem, the length of an input formula_21 is measured as formula_20 or as formula_23, depending on whether one views formula_21 as an element of formula_4 or as defining the corresponding conjugacy class formula_31 in formula_4.

The automorphism problem for formula_4 was algorithmically solved by J. H. C. Whitehead in a classic 1936 paper, and his solution came to be known as Whitehead's algorithm. Whitehead used a topological approach in his paper. Namely, consider the 3-manifold formula_34, the connected sum of formula_35 copies of formula_36. Then formula_37, and, moreover, up to a quotient by a finite normal subgroup isomorphic to formula_38, the mapping class group of formula_39 is equal to formula_17; see. Different free bases of formula_4 can be represented by isotopy classes of "sphere systems" in formula_39, and the cyclically reduced form of an element formula_43, as well as the Whitehead graph of formula_31, can be "read-off" from how a loop in general position representing formula_31 intersects the spheres in the system. Whitehead moves can be represented by certain kinds of topological "swapping" moves modifying the sphere system.

Subsequently, Rapaport, and later, based on her work, Higgins and Lyndon, gave a purely combinatorial and algebraic re-interpretation of Whitehead's work and of Whitehead's algorithm. The exposition of Whitehead's algorithm in the book of Lyndon and Schupp is based on this combinatorial approach. Culler and Vogtmann, in their 1986 paper that introduced the Outer space, gave a hybrid approach to Whitehead's algorithm, presented in combinatorial terms but closely following Whitehead's original ideas.

Our exposition regarding Whitehead's algorithm mostly follows Ch.I.4 in the book of Lyndon and Schupp, as well as.

The automorphism group formula_46 has a particularly useful finite generating set formula_47 of Whitehead automorphisms or Whitehead moves. Given formula_48 the first part of Whitehead's algorithm consists of iteratively applying Whitehead moves to formula_49 to take each of them to an ``automorphically minimal" form, where the cyclically reduced length strictly decreases at each step. Once we find automorphically these minimal forms formula_50 of formula_49, we check if formula_52. If formula_53 then formula_49 are not automorphically equivalent in formula_4.

If formula_52, we check if there exists a finite chain of Whitehead moves taking formula_57 to formula_58 so that the cyclically reduced length remains constant throughout this chain. The elements formula_49 are not automorphically equivalent in formula_4 if and only if such a chain exists.

Whitehead's algorithm also solves the "search automorphism problem" for formula_4. Namely, given formula_48, if Whitehead's algorithm concludes that formula_9, the algorithm also outputs an automorphism formula_64 such that formula_65. Such an element formula_64 is produced as the composition of a chain of Whitehead moves arising from the above procedure and taking formula_21 to formula_68.

A Whitehead automorphism, or Whitehead move, of formula_4 is an automorphism formula_70 of formula_4 of one of the following two types:

(i) There is a permutation formula_72 of formula_73 such that for formula_74

(ii) There is an element formula_77, called the multiplier, such that for every formula_78

Often, for a Whitehead automorphism formula_84, the corresponding outer automorphism in formula_17 is also called a Whitehead automorphism or a Whitehead move.
Let formula_86.

Let formula_87 be a homomorphism such that
Then formula_76 is actually an automorphism of formula_90, and, moreover, formula_76 is a Whitehead automorphism of the second kind, with the multiplier formula_92.

Let formula_93 be a homomorphism such that
Then formula_95 is actually an inner automorphism of formula_90 given by conjugation by formula_97, and, moreover, formula_95is a Whitehead automorphism of the second kind, with the multiplier formula_99.

For formula_43, the conjugacy class formula_31 is called automorphically minimal if for every formula_64 we have formula_103. 
Also, a conjugacy class formula_31 is called Whitehead minimal if for every Whitehead move formula_70 we have formula_106.

Thus, by definition, if formula_31 is automorphically minimal then it is also Whitehead minimal. It turns out that the converse is also true.

The following statement is referred to as Whitehead's "Peak Reduction Lemma", see Proposition 4.20 in and Proposition 1.2 in:

Let formula_43. Then the following hold:

(1) If formula_31 is not automorphically minimal, then there exists a Whitehead automorphism formula_70 such that formula_111.

(2) Suppose that formula_31 is automorphically minimal, and that another conjugacy class formula_113 is also automorphically minimal. Then formula_9 if and only if formula_115 and there exists a finite sequence of Whitehead moves formula_116 such that
and

Part (1) of the Peak Reduction Lemma implies that a conjugacy class formula_31 is Whitehead minimal if and only if it is automorphically minimal.

The automorphism graph formula_120 of formula_4 is a graph with the vertex set being the set of conjugacy classes formula_122 of elements formula_123. Two distinct vertices formula_124 are adjacent in formula_120 if formula_126 and there exists a Whitehead automorphism formula_76 such that formula_128. For a vertex formula_122 of formula_120, the connected component of formula_122 in formula_120 is denoted formula_133.

For formula_134 with cyclically reduced form formula_57, the Whitehead graph formula_136 is a labelled graph with the vertex set formula_137, where for formula_138 there is an edge joining formula_139 and formula_140 with the label or "weight" formula_141 which is equal to the number of distinct occurrences of subwords formula_142 read cyclically in formula_57. (In some versions of the Whitehead graph one only includes the edges with formula_144.)

If formula_84 is a Whitehead automorphism, then the length change formula_146 can be expressed as a linear combination, with integer coefficients determined by formula_76, of the weights formula_141 in the Whitehead graph formula_136. See Proposition 4.6 in Ch. I of. This fact plays a key role in the proof of Whitehead's peak reduction result.

Whitehead's minimization algorithm, given a freely reduced word formula_43, finds an automorphically minimal formula_151 such that formula_152

This algorithm proceeds as follows. Given formula_43, put formula_154. If formula_155 is already constructed, check if there exists a Whitehead automorphism formula_84 such that formula_157. (This condition can be checked since the set of Whitehead automorphisms of formula_4 is finite.) If such formula_76 exists, put formula_160 and go to the next step. If no such formula_76 exists, declare that formula_162 is automorphically minimal, with formula_163, and terminate the algorithm.

Part (1) of the Peak Reduction Lemma implies that the Whitehead's minimization algorithm terminates with some formula_164, where formula_165, and that then formula_166 is indeed automorphically minimal and satisfies formula_167.

Whitehead's algorithm for the automorphic equivalence problem, given formula_168 decides whether or not formula_9.

The algorithm proceeds as follows. Given formula_168, first apply the Whitehead minimization algorithm to each of formula_49 to find automorphically minimal formula_172 such that formula_173 and formula_174. If formula_175, declare that formula_176 and terminate the algorithm. Suppose now that formula_177. Then check if there exists a 
finite sequence of Whitehead moves formula_116 such that

and

This condition can be checked since the number of cyclically reduced words of length formula_181 in formula_4 is finite. More specifically, using the breadth-first approach, one constracts the connected components formula_183 of the automorphism graph and checks if formula_184.

If such a sequence exists, declare that formula_9, and terminate the algorithm. If no such sequence exists, declare that formula_9 and terminate the algorithm.

The Peak Reduction Lemma implies that Whitehead's algorithm correctly solves the automorphic equivalence problem in formula_187. Moreover, if formula_9, the algorithm actually produces (as a composition of Whitehead moves) an automorphism formula_64 such that formula_65.





</doc>
<doc id="60378307" url="https://en.wikipedia.org/wiki?curid=60378307" title="Broadcast (parallel pattern)">
Broadcast (parallel pattern)

Broadcast is a collective communication primitive in parallel programming to distribute programming instructions or data to nodes in a cluster it is the reverse operation of reduce. The broadcast operation is widely used in parallel algorithms, such as matrix-vector multiplication, Gaussian elimination and shortest paths.

The Message Passing Interface implements broadcast in codice_1.

A message formula_1of length n should be distributed from one node to all other formula_2 nodes.

formula_3is the time it takes to send one byte.

formula_4is the time it takes for a message to travel to another node, independent of its length.

Therefore, the time to send a package from one node to another is formula_5.

formula_6 is the number of nodes and the number of processors.

With Binomial Tree Broadcast the whole message is sent at once. Each node that has already received the message sends it on further. This grows exponentially as each time step the amount of sending nodes is doubled. The algorithm is ideal for short messages but falls short with longer ones as during the time when the first transfer happens and only one node is busy.

Sending a message to all nodes takes formula_7 time which results in a runtime of formula_8
Message M

id := node number
p := number of nodes

if id > 0 
for (i := ceil(log_2(id)) - 1; i >= 0; i--)

The message is split up into formula_9 packages and send piecewise from node formula_10 to node formula_11. The time needed to distribute the first message piece is formula_12 whereby formula_13 is the time needed to send a package from one processor to another.

Sending a whole message takes formula_14.

Optimal is to choose formula_15 resulting in a runtime of approximately formula_16

The run time is dependent on not only message length but also the number of processors that play roles. This approach shines when the length of the message is much larger than the amount of processors.
Message M := [m_1, m_2, ... ,m_n]
id = node number

for (i := 1; i <= n; i++) in parallel

This algorithm combines Binomial Tree Broadcast and Linear Pipeline Broadcast, which makes the algorithm work well for both short and long messages. The aim is to have as many nodes work as possible while maintaining the ability to send short messages quickly. A good approach is to use Fibonacci trees for splitting up the tree, which are a good choice as a message cannot be send to both children at the same time. This results in a binary tree structure.

We will assume in the following that communication is full-duplex. The Fibonacci tree structure has a depth of about formula_17whereby formula_18the golden ratio.

The resulting runtime is formula_19. Optimal is formula_20.

This results in a runtime of formula_21.
Message M := [m_1, m_2, ... ,m_k]

for i = 1 to k 

This algorithm aims to improve on some disadvantages of tree structure models with pipelines. Normally in tree structure models with pipelines (see above methods), leaves receive just their data and cannot contribute to send and spread data.

The algorithm concurrently uses two binary trees to communicate over. Those trees will be called tree A and B. Structurally in binary trees there are relatively more leave nodes than inner nodes. Basic Idea of this algorithm is to make a leaf node of tree A be an inner node of tree B. It has also the same technical function in opposite side from B to A tree. This means, two packets are sent and received by inner nodes and leaves in different steps.

The number of steps needed to construct construct two parallel-working binary trees is dependent on the amount of processors. Like with other structures one processor can is the root node who sends messages to two trees. It is not necessary to set a root node, because it is not hard to recognize that the direction of sending messages in binary tree is normally top to bottom. There is no limitation on the number of processors to build two binary trees. Let the height of the combined tree be . Tree A and B can have a height of formula_22. Especially, if the number of processors correspond to formula_23, we can make both sides trees and a root node.

To construct this model efficiently and easily with a fully built tree, we can use two methods called "Shifting" and "Mirroring" to get second tree. Let assume tree A is already modelled and tree B is supposed to be constructed based on tree A. We assume that we have formula_24 processors ordered from 0 to formula_25.

The "Shifting" method, first copies tree A and moves every node one position to the left to get tree B. The node, which will be located on -1, becomes a child of processor formula_26.

"Mirroring" is ideal for an even number of processors. With this method tree B can be more easily constructed by tree A, because there are no structural transformations in order to create the new tree. In addition, a symmetric process makes this approach simple. This method can also handle an odd number of processors, in this case, we can set processor formula_25 as root node for both trees. For the remaining processors "Mirroring" can be used.

We need to find a schedule in order to make sure that no processor has to send or receive two messages from two trees in a step. The edge, is a communication connection to connect two nodes, and can be labelled as either 0 or 1 to make sure that every processor can alternate between 0 and 1-labelled edges. The edges of and can be colored with two colors (0 and 1) such that


In every even step the edges with 0 are activated and edges with 1 are activated in every odd step.

In this case the number of packet k is divided in half for each tree. Both trees are working together the total number of packets formula_28 (upper tree + bottom tree)

In each binary tree sending a message to another nodes takes formula_29 steps until a processor has at least a packet in step formula_30. Therefore, we can calculate all steps as formula_31.
The resulting run time is formula_32. (Optimal formula_33)

This results in a run time of formula_34.

In this section, another broadcasting algorithm with an underlying telephone communication model will be introduced. A Hypercube creates network system with formula_35. Every node is represented by binary formula_36 depending on the number of dimensions. Fundamentally ESBT(Edge-disjoint Spanning Binomial Trees) is based on hypercube graphs, pipelining(formula_37 messages are divided by formula_38 packets) and binomial trees. The Processor formula_39 cyclically spreads packets to roots of ESBTs. The roots of ESBTs broadcast data with binomial tree. To leave all of formula_38 from formula_41, formula_38 steps are required, because all packets are distributed by formula_43. It takes another d steps until the last leaf node receives the packet. In total formula_44 steps are necessary to broadcast formula_37 message through ESBT.

The resulting run time is formula_46. formula_47.

This results in a run time of formula_48.



</doc>
<doc id="59742671" url="https://en.wikipedia.org/wiki?curid=59742671" title="DSatur">
DSatur

DSatur is a graph colouring algorithm put forward by Daniel Brélaz in 1979. Similarly to the greedy colouring algorithm, DSatur colours the vertices of a graph one after another, expending a previously unused colour when needed. Once a new vertex has been coloured, the algorithm determines which of the remaining uncoloured vertices has the highest number of colours in its neighbourhood and colours this vertex next. Brélaz defines this number as the "degree of saturation" of a given vertex. The contraction of the degree of saturation forms the name of the algorithm. DSatur is a heuristic graph colouring algorithm, yet produces exact results for bipartite, cycle, and wheel graphs. DSatur has also been referred to as saturation LF in the literature.

Define the degree of saturation of a vertex as the number of different colours in its neighbourhood. Given a simple, undirected graph "G" compromising vertex set "V" and edge set "E":


The worst-case complexity of DSatur is "Ο"("n"), however in practical some additional expenses result from the need for holding the degree of saturation of the uncoloured vertices. DSatur has been proven to be exact for bipartite graphs, as well as for cycle and wheel graphs. In an empirical comparison by Lewis 2015, DSatur produced significantly better vertex colourings than the greedy algorithm on random graphs with edge probability "p" = 0.5 at varying number of vertices, while in turn producing significantly worse colourings than the Recursive Largest First algorithm.


</doc>
<doc id="60842845" url="https://en.wikipedia.org/wiki?curid=60842845" title="Enumeration algorithm">
Enumeration algorithm

In computer science, an enumeration algorithm is an algorithm that enumerates the answers to a computational problem. Formally, such an algorithm applies to problems that take an input and produce a list of solutions, similarly to function problems. For each input, the enumeration algorithm must produce the list of all solutions, without duplicates, and then halt. The performance of an enumeration algorithm is measured in terms of the time required to produce the solutions, either in terms of the total time required to produce all solutions, or in terms of the maximal delay between two consecutive solutions and in terms of a preprocessing time, counted as the time before outputting the first solution. This complexity can be expressed in terms of the size of the input, the size of each individual output, or the total size of the set of all outputs, similarly to what is done with output-sensitive algorithms.

An enumeration problem formula_1 is defined as a relation formula_2 over strings of an arbitrary alphabet formula_3:

formula_4

An algorithm solves formula_1 if for every input formula_6 the algorithm produces the (possibly infinite) sequence formula_7 such that formula_7 has no duplicate and formula_9 if and only if formula_10. The algorithm should halt if the sequence formula_7 is finite.

Enumeration problems have been studied in the context of computational complexity theory, and several complexity classes have been introduced for such problems.

A very general such class is EnumP, the class of problems for which the correctness of a possible output can be checked in polynomial time in the input and output. Formally, for such a problem, there must exist an algorithm A which takes as input the problem input "x", the candidate output "y", and solves the decision problem of whether "y" is a correct output for the input "x", in polynomial time in "x" and "y". For instance, this class contains all problems that amount to enumerating the witnesses of a problem in the class NP.

Other classes that have been defined include the following. In the case of problems that are also in EnumP, these problems are ordered from least to most specific




The notion of enumeration algorithms is also used in the field of computability theory to define some high complexity classes such as RE, the class of all recursively enumerable problems. This is the class of sets for which there exist an enumeration algorithm that will produce all elements of the set: the algorithm may run forever if the set is infinite, but each solution must be produced by the algorithm after a finite time.


</doc>
<doc id="61070746" url="https://en.wikipedia.org/wiki?curid=61070746" title="Snap rounding">
Snap rounding

Snap rounding is a method of approximating line segment locations by creating a grid and placing each point in the centre of a cell (pixel) of the grid. The method preserves certain topological properties of the arrangement of line segments.

Drawbacks include the potential interpolation of additional vertices in line segments (lines become polylines), the arbitrary closeness of a point to a non-incident edge, and arbitrary numbers of intersections between input line-segments. The 3 dimensional case is worse, with a polyhedral subdivision of complexity n becoming complexity "O"(n).

There are more refined algorithms to cope with some of these issues, for example "iterated snap rounding" guarantees a "large" separation between points and non-incident edges.


Conversely there are undesirable properties:




</doc>
<doc id="61176336" url="https://en.wikipedia.org/wiki?curid=61176336" title="Block swap algorithms">
Block swap algorithms

Block swap algorithms is the simple art of swapping two elements of an array in computer algorithms. It is also simple to swap two non-overlapping regions of an array of equal size. However, it is not simple to swap two non-overlapping regions of an array in-place that are next to each other, but are of unequal sizes. Three algorithms are known to accomplish this: Bentley's Juggling, Gries-Mills, and Reversal. All three algorithms are linear time "O"("n") (see Time Complexity).

The reversal algorithm is the simplest to explain, using rotations. A rotation is an in-place reversal of array elements. This method swaps two elements of an array from outside in within a range. The rotation works for an even number of elements or an odd number of array elements. The reversal algorithm uses three in-place rotations to accomplish an in-place block swap:

Gries-Mills and Reversal algorithms perform better than Bentley's Juggling, because of their cache-friendly memory access pattern behavior.

The Reversal algorithm parallelizes well, because rotations can be split into sub-regions, which can be rotated independently of others.


</doc>
<doc id="61186810" url="https://en.wikipedia.org/wiki?curid=61186810" title="Pan–Tompkins algorithm">
Pan–Tompkins algorithm

The Pan–Tompkins algorithm is commonly used to detect QRS complexes in electrocardiographic signals (ECG). The QRS complex represents the ventricular depolarization and the main spike visible in an ECG signal (see figure). This feature makes it particularly suitable for measuring heart rate, the first way to assess the heart health state. In the first derivation of Einthoven of a physiological heart, the QRS complex is composed by a downward deflection (Q wave), a high upward deflection (R wave) and a final downward deflection (S wave).

The Pan–Tompkins algorithm applies a series of filters to highlight the frequency content of this rapid heart depolarization and removes the background noise. Then, it squares the signal to amplify the QRS contribute. Finally, it applies adaptive thresholds to detect the peaks of the filtered signal. The algorithm was proposed by Jiapu Pan and Willis J. Tompkins in 1985, in the journal IEEE Transactions on Biomedical Engineering. The performance of the method was tested on an annotated arrhythmia database (MIT/BIH) and evaluated also in presence of noise. Pan and Tompkins reported that the 99.3 percent of QRS complexes was correctly detected.

As a first step, a band-pass filter is applied to increase the signal-to-noise ratio. A filter bandwidth of 5-15 Hz is suggested to maximize the QRS contribute and reduce muscle noise, baseline wander, powerline interference and the P wave/T wave frequency content. In the original algorithm proposed in 1985, the band-pass filter was obtained with a low-pass filter and a high-pass filter in cascade to reduce the computational cost and allow a real-time detection, while ensuring a 3 dB passband in the 5–12 Hz frequency range, reasonably close to the design goal.

For a signal sampled at a frequency of 200 Hz, Pan and Tompkins suggested the filters with the following transfer functions formula_1 in an updated version of their article:


As a third step, a derivative filter is applied to provide information about the slope of the QRS. For a signal sampled at 200 Hz, Pan and Tompkins suggested the following transfer function:

formula_4for a 5-point derivative filter with gain of 0.1 and a processing delay of 2 samples.

The filtered signal is squared to enhance the dominant peaks (QRSs) and reduce the possibility of erroneously recognizing a T wave as an R peak. Then, a moving average filter is applied to provide information about the duration of the QRS complex. The number of samples to average is chosen in order to average on windows of 150 ms. The signal so obtained is called integrated signal.

In order to detect a QRS complex, the local peaks of the integrated signal are found. A peak is defined as the point in which the signal changes direction (from an increasing direction to a decreasing direction). After each peak, no peak can be detected in the next 200 ms. This is a physiological constraint due to the refractory period during which ventricular depolarization cannot occur even in the presence of a stimulus.

Each fiducial mark is considered as a potential QRS. To reduce the possibility of wrongly selecting a noise peak as a QRS, each peak amplitude is compared to a threshold ("Threshold") that takes into account the available information about already detected QRS and the noise level:

formula_5

where "NoiseLevel" is the running estimate of the noise level in the integrated signal and "SignalLevel" is the running estimate of the signal level in the integrated signal.

The threshold is automatically updated after detecting a new peak, based on its classification as signal or noise peak:

formula_6(if "PEAK" is a signal peak)

formula_7(if "PEAK" is a noise peak)

where "PEAK" is the new peak found in the integrated signal.

At the beginning of the QRS detection, a 2 seconds learning phase is needed to initialize "SignalLevel" and "NoiseLevel" as a percentage of the maximum and average amplitude of the integrated signal, respectively.

If a new "PEAK" is under the "Threshold", the noise level is updated. If "PEAK" is above the "Threshold", the algorithm implements a further check before confirming the peak as a true QRS, taking into consideration the information provided by the bandpass filtered signal.

In the filtered signal the peak corresponding to the one evaluated on the integrated signal is searched and compared with a threshold, calculated in a similar way to the previous step:

formula_8

formula_9(if "PEAK" is a signal peak)

formula_10(if "PEAK" is a noise peak)

where the final F stands for filtered signal.

The algorithm takes into account the possibility of setting too high values of "ThresholdI" and "ThresholdI" A check is performed to continuously assess the RR intervals (namely the temporal interval between two consecutively QRS peaks) to overcome this issue. The average RR is computed in two ways to consider both regular and irregular heart rhythm. In the first method "RRaverage1" is computed as the mean of the last RR intervals. In the second method "RRaverage2" is computed as the mean of the last RR intervals that fell between the limits specified as:

formula_11

formula_12

If no QRS is detected in a window of 166% of the average RR ("RRaverage1" or "RRaverage2", if the heart rhythm is regular or irregular, respectively)"," the algorithm adds the maximal peak in the window as a potential QRS and classify it considering half the values of the thresholds (both "ThresholdI and ThresholdI"). This check is implemented because the temporal distance between two consecutive beats cannot physiologically change more quickly than this.

The algorithm takes particularly into consideration the possibility of a false detection of T waves. If a potential QRS falls up to a 160 ms window after the refractory periody from the last correctly detected QRS complex, the algorithm evaluates if it could be a T wave with particular high amplitude. In this case, its slope is compared to the one of the precedent QRS complex. If the slope is less than half the previous one, the current QRS is recognized as a T wave and discarded, and it also updates the "NoiseLevel" (both in the filtered signal and the integrated signal).

Once the QRS complex is successfully recognized, the heart rate is computed as a function of the distance in seconds between two consecutive QRS complexes (or R peaks):

formula_13

where bpm stands for beats per minute. The HR is often used to compute the heart rate variability (HRV) a measure of the variability of the time interval between heartbeats. HRV is often used in the clinical field to diagnose and monitor pathological conditions and their treatment, but also in the affective computing research to study new methods to assess the emotional state of people.



</doc>
<doc id="61379828" url="https://en.wikipedia.org/wiki?curid=61379828" title="Berlekamp's root finding algorithm">
Berlekamp's root finding algorithm

In number theory, Berlekamp's root finding algorithm, also called the Berlekamp–Rabin algorithm, is the probabilistic method of finding roots of polynomials over a field formula_1. The method was discovered by Elwyn Berlekamp in 1970 as an auxiliary to the algorithm for polynomial factorization over finite fields. The algorithm was later modified by Rabin for arbitrary finite fields in 1979. The method was also independently discovered before Berlekamp by other researchers.

The method was proposed by Elwyn Berlekamp in his 1970 work on polynomial factorization over finite fields. His original work lacked a formal correctness proof and was later refined and modified for arbitrary finite fields by Michael Rabin. In 1986 René Peralta proposed a similar algorithm for finding square roots in formula_1. In 2000 Peralta's method was generalized for cubic equations.

Let formula_3 be an odd prime number. Consider the polynomial formula_4 over the field formula_1 of remainders modulo formula_3. The algorithm should find all formula_7 in formula_1 such that formula_9 in formula_1.

Let formula_11. Finding all roots of this polynomial is equivalent to finding its factorization into linear factors. To find such factorization it is sufficient to split the polynomial into any two non-trivial divisors and factorize them recursively. To do this, consider the polynomial formula_12 where formula_13 is some any element of formula_1. If one can represent this polynomial as the product formula_15 then in terms of the initial polynomial it means that formula_16, which provides needed factorization of formula_17.

Due to Euler's criterion, for every monomial formula_19 exactly one of following properties holds:


Thus if formula_28 is not divisible by formula_20, which may be checked separately, then formula_28 is equal to the product of greatest common divisors formula_31 and formula_32.

The property above leads to the following algorithm:


If formula_17 is divisible by some non-linear primitive polynomial formula_45 over formula_1 then when calculating formula_40 with formula_48 and formula_49 one will obtain a non-trivial factorization of formula_50, thus algorithm allows to find all roots of arbitrary polynomials over formula_1.

Consider equation formula_52 having elements formula_53 and formula_54 as its roots. Solution of this equation is equivalent to factorization of polynomial formula_55 over formula_1. In this particular case problem it is sufficient to calculate only formula_57. For this polynomial exactly one of the following properties will hold:


In the third case GCD is equal to either formula_63 or formula_64. It allows to write the solution as formula_65.

Assume we need to solve the equation formula_66. For this we need to factorize formula_67. Consider some possible values of formula_13:



A manual check shows that, indeed, formula_80 and formula_81.

The algorithm finds factorization of formula_28 in all cases except for ones when all numbers formula_83 are quadratic residues or non-residues simultaneously. According to theory of cyclotomy, the probability of such an event for the case when formula_84 are all residues or non-residues simultaneously (that is, when formula_85 would fail) may be estimated as formula_86 where formula_87 is the number of distinct values in formula_84. In this way even for the worst case of formula_89 and formula_90, the probability of error may be estimated as formula_91 and for modular square root case error probability is at most formula_92.

Let a polynomial have degree formula_93. We derive the algorithm's complexity as follows:


Thus the whole procedure may be done in formula_101. Using the fast Fourier transform and Half-GCD algorithm, the algorithm's complexity may be improved to formula_105. For the modular square root case, the degree is formula_106, thus the whole complexity of algorithm in such case is bounded by formula_107 per iteration.


</doc>
<doc id="58536963" url="https://en.wikipedia.org/wiki?curid=58536963" title="Bartels–Stewart algorithm">
Bartels–Stewart algorithm

In numerical linear algebra, the Bartels–Stewart algorithm is used to numerically solve the Sylvester matrix equation formula_1. Developed by R.H. Bartels and G.W. Stewart in 1971, it was the first numerically stable method that could by systematically applied to solve such equations. The algorithm works by using the real Schur decompositions of formula_2 and formula_3 to transform formula_1 into a triangular system that can then be solved using forward or backward substitution. In 1979, G. Golub, C. Van Loan and S. Nash introduced an improved version of the algorithm, known as the Hessenberg–Schur algorithm. It remains a standard approach for solving Sylvester equations when formula_5 is of small to moderate size.

Let formula_6, and assume that the eigenvalues of formula_2 are distinct from the eigenvalues of formula_3. Then, the matrix equation formula_1 has a unique solution. The Bartels–Stewart algorithm computes formula_5 by applying the following steps: 

1.Compute the real Schur decompositions

The matrices formula_13 and formula_14 are block-upper triangular matrices, with square blocks of size no greater than formula_15.

2. Set formula_16

3. Solve the simplified system formula_17, where formula_18. This can be done using forward substitution on the blocks. Specifically, if formula_19, then

where formula_21is the formula_22th column of formula_23. When formula_24, columns formula_25 should be concatenated and solved for simultaneously. 

4. Set formula_26

Using the QR algorithm, the real Schur decompositions in step 1 require approximately formula_27 flops, so that the overall computational cost is formula_28. 

In the special case where formula_29 and formula_30 is symmetric, the solution formula_5 will also be symmetric. This symmetry can be exploited so that formula_23 is found more efficiently in step 3 of the algorithm. 

The Hessenberg–Schur algorithm replaces the decomposition formula_33 in step 1 with the decomposition formula_34, where formula_35 is an upper-Hessenberg matrix. This leads to a system of the form formula_36 that can be solved using forward substitution. The advantage of this approach is that formula_34 can be found using Householder reflections at a cost of formula_38 flops, compared to the formula_39 flops required to compute the real Schur decomposition of formula_2. 

The subroutines required for the Hessenberg-Schur variant of the Bartels–Stewart algorithm are implemented in the SLICOT library. These are used in the MATLAB control system toolbox.

For large systems, the formula_41 cost of the Bartels–Stewart algorithm can be prohibitive. When formula_2 and formula_3 are sparse or structured, so that linear solves and matrix vector multiplies involving them are efficient, iterative algorithms can potentially perform better. These include projection-based methods, which use Krylov subspace iterations, methods based on the alternating direction implicit (ADI) iteration, and hybridizations that involve both projection and ADI. Iterative methods can also be used to directly construct low rank approximations to formula_5 when solving formula_45. 


</doc>
<doc id="52300160" url="https://en.wikipedia.org/wiki?curid=52300160" title="Shapiro–Senapathy algorithm">
Shapiro–Senapathy algorithm

Gene regulation is the main genetic program through which an organism controls its normal functions. Thus, any error in this program caused by mutations will alter the normal state and lead to disease. RNA splicing is increasingly realized to be at the center of gene regulation in eukaryotic organisms, including all animals and plants. In this context, Dr. Periannan Senapathy has pioneered research in the biology of RNA splicing, and provided tenable solutions for why genes are split, how splice junction sequences originated, and why exons are very short and introns are very long (Split-Gene Theory). Based on these findings, he has provided an algorithm known as the Shapiro & Senapathy algorithm (S&S) for predicting the splice sites, exons and genes in animals and plants. This algorithm has the ability to discover disease-causing mutations in splice junctions in cancerous and non-cancerous diseases that is being used in major research institutions around the world.

The S&S algorithm has been cited in ~3,000 publications in clinical genomics on finding splicing mutations in thousands of diseases including many different forms of cancer and non-cancer diseases. It has been the basis of many leading software tools, such as Human Splicing Finder, Splice-site Analyzer Tool, dbass (Ensembl), Alamut and SROOGLE, which are cited by approx. 1,500 additional citations. The S&S algorithm has thus significantly impacted the field of medicine, and is increasingly applied in today's disease research, pharmacogenomics, and Precision Medicine, as up to 50% of all diseases and ADRs (Adverse Drug Reactions) are now thought to be caused by RNA splicing mutations.

Using the S&S algorithm, scientists have identified mutations and genes that cause numerous cancers, inherited disorders, immune deficiency diseases and neurological disorders. In addition, mutations in various drug metabolizing genes that cause ADRs to different drugs that are used to treat different diseases, including cancer chemotherapeutic drugs, have been identified. S&S is also used in detecting the “cryptic” splice sites that are not authentic sites used in the normal splicing of gene transcripts, and the mutations in which cause numerous diseases. The details are provided in the following sections.

By using the S&S algorithm, mutations and genes that cause many different forms of cancer have been discovered. For example, genes causing commonly occurring cancers including breast cancer, ovarian cancer, colorectal cancer, leukemia, head and neck cancers, prostate cancer, retinoblastoma, squamous cell carcinoma, gastrointestinal cancer, melanoma, liver cancer, Lynch syndrome, skin cancer, and neurofibromatosis have been found.  In addition, splicing mutations in genes causing less commonly known cancers including gastric cancer, gangliogliomas, Li-Fraumeni syndrome, Loeys–Dietz syndrome, Osteochondromas (bone tumor), Nevoid basal cell carcinoma syndrome, and Pheochromocytomas have been identified.

Specific mutations in different splice sites in various genes causing breast cancer (e.g., BRCA1, PALB2), ovarian cancer (e.g.,  SLC9A3R1, COL7A1, HSD17B7), colon cancer (e.g., APC, MLH1, DPYD), colorectal cancer (e.g., COL3A1, APC, HLA-A), skin cancer (e.g., COL17A1, XPA, POLH), and Fanconi anemia (e.g., FANC, FANA) have been uncovered. The mutations in the donor and acceptor splice sites in different genes causing a variety of cancers that have been identified by S&S are shown in Table 1.

Table 1. Mutations in the donor and acceptor splice sites in different genes

Specific mutations in different splice sites in various genes that cause inherited disorders, including, for example, Type 1 diabetes (e.g., PTPN22, TCF1 (HCF-1A)), hypertension (e.g., LDL, LDLR, LPL), marfane syndrome (e.g., FBN1, TGFBR2, FBN2), cardiac diseases (e.g., COL1A2, MYBPC3, ACTC1), eye disorders (e.g., EVC, VSX1) have been uncovered. Few example mutations in the donor and acceptor splice sites in different genes causing a variety of inherited disorders identified using S&S are shown in Table 2.

Table 2. Mutations in the donor and acceptor splice sites in different genes causing inherited disorders

More than 100 immune system disorders affect humans, including inflammatory bowel diseases, multiple sclerosis, systemic lupus erythematosus, bloom syndrome, familial cold autoinflammatory syndrome, and dyskeratosis congenita. The Shapiro–Senapathy algorithm has been used to discover genes and mutations involved in many immune disorder diseases, including Ataxia telangiectasia, B-cell defects, Epidermolysis bullosa, and X-linked agammaglobulinemia.

Xeroderma pigmentosum, an autosomal recessive disorder is caused by faulty proteins formed due to new preferred splice donor site identified using S&S algorithm and resulted in defective nucleotide excision repair.

Type I Bartter syndrome (BS) is caused by mutations in the gene SLC12A1. S&S algorithm helped in disclosing the presence of two novel heterozygous mutations c.724 + 4A > G in intron 5 and c.2095delG in intron 16 leading to complete exon 5 skipping.

Mutations in the MYH gene, which is responsible for removing the oxidatively damaged DNA lesion are cancer-susceptible in the individuals. The IVS1+5C plays a causative role in the activation of a cryptic splice donor site and the alternative splicing in intron 1, S&S algorithm shows, guanine (G) at the position of IVS+5 is well conserved (at the frequency of 84%) among primates. This also supported the fact that the G/C SNP in the conserved splice junction of the MYH gene causes the alternative splicing of intron 1 of the β type transcript.

Splice site scores were calculated according to S&S to find EBV infection in X-linked lymphoproliferative disease. Identification of Familial tumoral calcinosis (FTC) is an autosomal recessive disorder characterized by ectopic calcifications and elevated serum phosphate levels and it is because of aberrant splicing.

Applying the S&S technology platform in modern clinical genomics research hasadvance diagnosis and treatment of human diseases.

In the modern era of Next Generation Sequencing (NGS) technology, S&S is applied in clinical practice extensively. Clinicians and molecular diagnostic laboratories apply S&S using various computational tools including HSF, SSF, and Alamut. It is aiding in the discovery of genes and mutations in patients whose disease are stratified or when the disease in a patient is unknown based on clinical investigations.

In this context, S&S has been applied on cohorts of patients in different ethnic groups with various cancers and inherited disorders. A few examples are given below.


Dr. Senapathy's original objective in developing a method for identifying splice sites was to find complete genes in raw uncharacterized genomic sequence that could be used in the human genome project. In the landmark paper with this objective, he described the basic method for identifying the splice sites within a given sequence based on the Position Weight Matrix (PWM) of the splicing sequences in different eukaryotic organism groups for the first time. He also created the first exon detection method by defining the basic characteristics of an exon as the sequence bounded by an acceptor and a donor splice sites that had S&S scores above a threshold, and by an ORF that was mandatory for an exon. An algorithm for finding complete genes based on the identified exons was also described by Dr. Senapathy for the first time.

Dr. Senapathy demonstrated that only deleterious mutations in the donor or acceptor splice sites that would drastically make the protein defective would reduce the splice site score (later known as the Shapiro–Senapathy score), and other non-deleterious variations would not reduce the score. The S&S method was adapted for researching the cryptic splice sites caused by mutations leading to diseases. This method for detecting deleterious splicing mutations in eukaryotic genes has been used extensively in disease research in the humans, animals and plants over the past three decades, as described above.  

The basic method for splice site identification, and for defining exons and genes was subsequently used by researchers in finding splice sites, exons and eukaryotic genes in a variety of organisms. These methods also formed the basis of all subsequent tools development for discovering genes in uncharacterized genomic sequences. It also was used in a different computational approaches including machine learning and neural network, and in alternative splicing research.

The Shapiro–Senapathy algorithm has been used to determine the various aberrant splicing mechanisms in genes due to deleterious mutations in the splice sites, which cause numerous diseases. Deleterious splice site mutations impair the normal splicing of the gene transcripts, and thereby make the encoded protein defective. A mutant splice site can become “weak” compared to the original site, due to which the mutated splice junction becomes unrecognizable by the spliceosomal machinery. This can lead to the skipping of the exon in the splicing reaction, resulting in the loss of that exon in the spliced mRNA (exon-skipping). On the other hand, a partial or complete intron could be included in the mRNA due to a splice site mutation that makes it unrecognizable (intron inclusion). A partial exon-skipping or intron inclusion can lead to premature termination of the protein from the mRNA, which will become defective leading to diseases. The S&S has thus paved the way to determine the mechanisms by which a deleterious mutation could lead to a defective protein, resulting in different diseases depending on which gene is affected.

An example of splicing aberration (exon skipping) caused by a mutation in the donor splice site in the exon 8 of MLH1 gene that led to colorectal cancer is given below. This example shows that a mutation in a splice site within a gene can lead to a profound effect in the sequence and structure of the mRNA, and the sequence, structure and function of the encoded protein, leading to disease.

The proper identification of splice sites has to be highly precise as the consensus splice sequences are very short and there are many other sequences similar to the authentic splice sites within gene sequences, which are known as cryptic, non-canonical, or pseudo splice sites. When an authentic or real splice site is mutated, any cryptic splice sites present close to the original real splice site could be erroneously used as authentic site, resulting in an aberrant mRNA. The erroneous mRNA may include a partial sequence from the neighboring intron or lose a partial exon, which may result in a premature stop codon. The result may be a truncated protein that would have lost its function completely.

Shapiro–Senapathy algorithm can identify the cryptic splice sites, in addition to the authentic splice sites. Cryptic sites can often be stronger than the authentic sites, with a higher S&S score. However, due to the lack of an accompanying complementary donor or acceptor site, this cryptic site will not be active or used in a splicing reaction. When a neighboring real site is mutated to become weaker than the cryptic site, then the cryptic site may be used instead of the real site, resulting in a cryptic exon and an aberrant transcript.

Numerous diseases have been caused by cryptic splice site mutations or usage of cryptic splice sites due to the mutations in authentic splice sites.

S&S has also been used in RNA splicing research in many animals and plants.

The mRNA splicing plays a fundamental role in gene functional regulation. Very recently, it has been shown that A to G conversions at splice sites can lead to mRNA mis-splicing in Arabidopsis. The splicing and exon–intron junction prediction coincided with the GT/AG rule (S&S) in the Molecular characterization and evolution of carnivorous sundew (Drosera rotundifolia L.) class V b-1,3-glucanase. Unspliced (LSDH) and spliced (SSDH) transcripts of NAD+ dependent sorbitol dehydroge nase (NADSDH) of strawberry (Fragaria ananassa Duch., cv. Nyoho) were investigated for phytohormonal treatments.

Ambra1 is a positive regulator of autophagy, a lysosome-mediated degradative process involved both in physiological and pathological conditions. Nowadays, this function of Ambra1 has been characterized only in mammals and zebrafish. Diminution of "rbm24a" or "rbm24b" gene products by morpholino knockdown resulted in significant disruption of somite formation in mouse and zebrafish. Dr.Senapathy algorithm used extensively to study intron-exon organization of fut8 genes.The intron-exon boundaries of "Sf"9 "fut8" were in agreement with the consensus sequence for the splicing donor and acceptor sites concluded using S&S.

The motivation for Dr. Senapathy to develop a method for the detection of splice junctions came from his split-gene theory. If primordial DNA sequences had a random nucleotide organization, the random distribution of stop codons would allow only very short Open Reading Frames (ORFs), as three stop codons out of 64 codons would result in an average ORF of ~60 bases. When Senapathy tested this in random DNA sequences, not only this was proven to be true, but the longest ORFs even in very long DNA sequences was found to be ~600 bases above which no ORFs existed. If so, a long coding sequence of even 1,200 bases (the average coding sequence length of genes from living organisms), and longer coding sequences of 6,000 bases (many of which occur in living organisms) will not occur in a primordial random sequence. Thus, genes had to occur in pieces in a split form, with short coding sequences (ORFs) that became exons, interrupted by very long random sequences that became introns. When the eukaryotic DNA was tested for ORF length distribution, it exactly matched that from random DNA, with very short ORFs that matched the lengths of exons, and very long introns as predicted, supporting the split gene theory.

If this split gene theory was true, then the ends of these ORFs that had a stop codon by nature would have become the ends of exons that would occur within introns, and that would define the splice junctions. When this hypothesis was tested, the almost all splice junctions in eukaryotic genes were found to contain stop codons exactly at the ends of introns, bordering the exons. In fact, these stop codons were found to form the “canonical” AG:GT splicing sequence, with the three stop codons occurring as part of the strong consensus signals. The Nobel Laureate Dr. Marshall Nirenberg, who deciphered the codons, stated that these findings strongly showed that the split gene theory for the origin of introns and the split structure of genes must be valid, and communicated the paper to the PNAS. New Scientist covered this publication in “A long explanation for introns”.

This basic split gene theory led to the hypothesis that the splice junctions originated from the stop codons.  Besides the codon CAG, only TAG, which is a stop codon, was found at the ends of introns. Surprisingly, all three stop codons (TGA, TAA and TAG) were found after one base (G) at the start of introns. These stop codons are shown in the consensus canonical donor splice junction as AG:GT(A/G)GGT, wherein the TAA and TGA are the stop codons, and the additional TAG is also present at this position. The canonical acceptor splice junction is shown as (C/T)AG:GT, in which TAG is the stop codon. These consensus sequence clearly show the presence of the stop codons at the ends of introns bordering the exons in all eukaryotic genes.  Dr. Marshall Nirenberg again stated that these observations fully supported the split gene theory for the origin of splice junction sequences from stop codons, who was the referee for this paper. New Scientist covered this publication in “Exons, Introns and Evolution”.

Dr. Senapathy wanted to detect the splice junctions in random DNA based on the consensus splice signal sequences, as he found that there were many sequences resembling splice sites that were not the real splice sites within genes. This Position Weight Matrix method turned out to be a highly accurate algorithm to detect the real splice sites and the cryptic sites in genes. He also formulated the first exon detection method, based on the requirement for splice junctions at the ends of exons, and the requirement for an Open Reading Frame that would contain the exon. This exon detection method also turned to be highly accurate, detecting most of the exons with few false positives and false negatives. He extended this approach to define a complete split gene in a eukaryotic genomic sequence. Thus, the PWM based algorithm turned out to be very sensitive to not only detect the real splice sites and cryptic sites, but also to detect mutated splice sites that are deleterious as opposed to non-deleterious splicing mutations.

The stop codons within splice junctions turned out to be the strongest bases in splice junctions of eukaryotic genes, when tested using the PWMs of the consensus sequences. In fact, it was shown that mutations in these bases were the cause of diseases compared to other bases, as these three of the four bases (base 1, 3 and 4) of the canonical AG:GT were part of the stop codons. Senapathy showed that, when these canonical bases were mutated, the splice site score became weak, causing splicing aberrations in the splicing process and translation of the mRNA (as described under the diseases section above). Although the value of the splice site detection method in discovering genes with splicing mutations that caused disease has been realized over the years, its importance in clinical medicine is increasingly realized in the Next Generation Sequencing era over the past five years, with its incorporation in several tools based on the S&S algorithm.

Dr. Senapathy is currently the President and CSO of Genome International Corporation (GIC), a genomics R&D company based in Madison, WI. His team has developed several databases and tools for the analysis of splice junctions, including EuSplice, AspAlt, ExDom and RoBust. AspAlt was commended by Biotechniques, which stated that it solved a difficult problem for scientists in the comparative analysis and visualization of alternative splicing across different genomes. GIC has most recently developed the clinical genomics analysis platform Genome Explorer.



</doc>
<doc id="61982153" url="https://en.wikipedia.org/wiki?curid=61982153" title="Magic state distillation">
Magic state distillation

Magic state distillation is an algorithm for performing fault tolerant quantum computation. Thanks to the Gottesman–Knill theorem, it is known that some quantum operations can be perfectly simulated in polynomial time on a probabilistic classical computer. In order to achieve universal quantum computation, a quantum computer must be able to perform operations outside this set. Magic state distillation achieves this, in principle, by concentrating the usefulness of imperfect resources, represented by mixed states, into states that are conducive for performing operations that are difficult to simulate classically.

The Clifford group consists of a set of formula_1-qubit operations generated by the gates (where H is Hadamard and S is formula_2) called Clifford gates. The Clifford group generates stabilizer states which can be efficiently simulated classically, as shown by the Gottesman–Knill theorem. This set of gates with a non-Clifford operation is universal for quantum computation.

Magic states are purified from formula_1 copies of a mixed state formula_4. These states is usually provided as an ancilla in the circuit. The magic state formula_5 where formula_6; which could be approximated by the Clifford operations given enough formula_4, combined with Clifford gates can be used to make a non-Clifford gate. Since Clifford gates combined with a non-Clifford gate are universal for quantum computation, magic states combined with Clifford gates are also universal.

The first magic state distillation algorithm; invented by Sergey Bravyi and Alexei Kitaev is a follows.


</doc>
<doc id="26978338" url="https://en.wikipedia.org/wiki?curid=26978338" title="Gale–Shapley algorithm">
Gale–Shapley algorithm

In mathematics, economics, and computer science, the Gale–Shapley algorithm (also known as the deferred acceptance algorithm) is an algorithm for finding a solution to the stable matching problem, named for David Gale and Lloyd Shapley.
It takes polynomial time, and the time is linear in the size of the input to the algorithm. Depending on how it is used, it can find either the solution that is optimal for the participants on one side of the matching, or for the participants on the other side. It is a truthful mechanism from the point of view of the participants for whom it provides the optimal solution.

The stable matching problem, in its most basic form, takes as input equal numbers of two types of participants ( men and women, or medical students and internships, for example), and an ordering for each participant giving their preference for who to be matched to among the participants of the other type. A matching is "not" stable if:

In other words, a matching is stable when there does not exist any match ("A", "B") which both prefer each other to their current partner under the matching.
A stable matching always exists, and the algorithmic problem solved by the Gale–Shapley algorithm is to find one.

In 1962, David Gale and Lloyd Shapley proved that, for any equal number of men and women, it is always possible to solve the SMP and make all marriages stable. They presented an algorithm to do so.

The Gale–Shapley algorithm involves a number of "rounds" (or "iterations"):

The runtime complexity of this algorithm is formula_1 where formula_2 is the number of men or women.

This algorithm guarantees that:


 algorithm stable_matching is

The existence of different stable matchings raises the question: which matching is returned by the Gale-Shapley algorithm? Is it the matching better for men, for women, or the intermediate one? In the above example, the algorithm converges in a single round on the men-optimal solution because each woman receives exactly one proposal, and therefore selects that proposal as her best choice, ensuring that each man has an accepted offer, ending the match.

This is a general fact: the Gale-Shapley algorithm in which men propose to women "always" yields a stable matching that is the "best for all men" among all stable matchings. Similarly, if the women propose then the resulting matching is the "best for all women" among all stable matchings. These two matchings are the top and bottom elements of a larger structure on all stable matchings, the lattice of stable matchings.

To see this, consider the illustration at the right. Let A be the matching generated by the men-proposing algorithm, and B an alternative stable matching that is better for at least one man, say "m". Suppose "m" is matched in B to a woman "w", which he prefers to his match in A. This means that in A, "m" has visited "w", but she rejected him (this is denoted by the green arrow from "m" to "w"). "w" rejected him in favor of some man that she prefers, say "m". So in B, "w" is matched to "m" but "yearns" (wants but unmatched) to "m" (this is denoted by the red arrow from "w" to "m").

Since B is a stable matching, "m" must be matched in B to some woman he prefers to "w", say "w". This means that in A, "m" has visited "w" before arriving at "w", which means that "w" has rejected him. By similar considerations, and since the graph is finite, we must eventually have a directed cycle in which each man was rejected in A by the next woman in the cycle, who rejected him in favor of the next man in the cycle. But this is impossible since such "cycle of rejections" cannot start anywhere: suppose by contradiction that it starts at e.g. "m" - the first man rejected by his adjacent woman ("w"). By assumption, this rejection happens only after "m" comes to "w". But this can happen only after "w" rejects "m" - contradiction to "m" being the first.

The GS algorithm is a truthful mechanism from the point of view of men (the proposing side). I.e, no man can get a better matching for himself by misrepresenting his preferences. Moreover, the GS algorithm is even "group-strategy proof" for men, i.e., no coalition of men can coordinate a misrepresentation of their preferences such that all men in the coalition are strictly better-off. However, it is possible for some coalition to misrepresent their preferences such that some men are better-off and the other men retain the same partner.

The GS algorithm is non-truthful for the women (the reviewing side): each woman may be able to misrepresent her preferences and get a better match.





</doc>
