<doc id="12401" url="https://en.wikipedia.org/wiki?curid=12401" title="Graph theory">
Graph theory

In mathematics, graph theory is the study of "graphs", which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of "vertices" (also called "nodes" or "points") which are connected by "edges" (also called "links" or "lines"). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.

Refer to the glossary of graph theory for basic definitions in graph theory.

Definitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.

In one restricted but very common sense of the term, a graph is an ordered pair comprising:


</doc>
<doc id="325802" url="https://en.wikipedia.org/wiki?curid=325802" title="Glossary of graph theory terms">
Glossary of graph theory terms

This is a glossary of graph theory terms. Graph theory is the study of graphs, systems of nodes or vertices connected in pairs by edges.



</doc>
<doc id="392431" url="https://en.wikipedia.org/wiki?curid=392431" title="Adjacency list">

Adjacency list

In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. Each list describes the set of neighbors of a vertex in the graph. This is one of several commonly used representations of graphs for use in computer programs.

An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighboring vertices or edges. There are many variations of this basic idea, differing in the details of how they implement the association between vertices and collections, in how they implement the collections, in whether they include both vertices and edges or only vertices as first class objects, and in what kinds of objects are used to represent the vertices and edges.

The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex. Using any of the implementations detailed above, this can be performed in constant time per neighbor. In other words, the total time to report all of the neighbors of a vertex "v" is proportional to the degree of "v".

It is also possible, but not as efficient, to use adjacency lists to test whether an edge exists or does not exist between two specified vertices. In an adjacency list in which the neighbors of each vertex are unsorted, testing for the existence of an edge may be performed in time proportional to the minimum degree of the two given vertices, by using a sequential search through the neighbors of this vertex. If the neighbors are represented as a sorted array, binary search may be used instead, taking time proportional to the logarithm of the degree.

The main alternative to the adjacency list is the adjacency matrix, a matrix whose rows and columns are indexed by vertices and whose cells contain a Boolean value that indicates whether an edge is present between the vertices corresponding to the row and column of the cell. For a sparse graph (one in which most pairs of vertices are not connected by edges) an adjacency list is significantly more space-efficient than an adjacency matrix (stored as an array): the space usage of the adjacency list is proportional to the number of edges and vertices in the graph, while for an adjacency matrix stored in this way the space is proportional to the square of the number of vertices. However, it is possible to store adjacency matrices more space-efficiently, matching the linear space usage of an adjacency list, by using a hash table indexed by pairs of vertices rather than an array.

The other significant difference between adjacency lists and adjacency matrices is in the efficiency of the operations they perform. In an adjacency list, the neighbors of each vertex may be listed efficiently, in time proportional to the degree of the vertex. In an adjacency matrix, this operation takes time proportional to the number of vertices in the graph, which may be significantly higher than the degree. On the other hand, the adjacency matrix allows testing whether two vertices are adjacent to each other in constant time; the adjacency list is slower to support this operation.

For use as a data structure, the main alternative to the adjacency list is the adjacency matrix. Because each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only bytes of contiguous space, where is the number of vertices of the graph. Besides avoiding wasted space, this compactness encourages locality of reference.

However, for a sparse graph, adjacency lists require less space, because they do not waste any space to represent edges that are not present. Using a naïve array implementation on a 32-bit computer, an adjacency list for an undirected graph requires about bytes of space, where is the number of edges of the graph.

Noting that an undirected simple graph can have at most edges, allowing loops, we can let denote the density of the graph. Then, when , that is the adjacency list representation occupies more space than the adjacency matrix representation when . Thus a graph must be sparse enough to justify an adjacency list representation.

Besides the space trade-off, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list. With an adjacency matrix, an entire row must instead be scanned, which takes time. Whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.



</doc>
<doc id="244463" url="https://en.wikipedia.org/wiki?curid=244463" title="Adjacency matrix">
Adjacency matrix

In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.

In the special case of a finite simple graph, the adjacency matrix is a (0,1)-matrix with zeros on its diagonal. If the graph is undirected, the adjacency matrix is symmetric. 
The relationship between a graph and the eigenvalues and eigenvectors of its adjacency matrix is studied in spectral graph theory.

The adjacency matrix should be distinguished from the incidence matrix for a graph, a different matrix representation whose elements indicate whether vertex–edge pairs are incident or not, and degree matrix which contains information about the degree of each vertex.

For a simple graph with vertex set "V", the adjacency matrix is a square  ×  matrix "A" such that its element "A" is one when there is an edge from vertex "i" to vertex "j", and zero when there is no edge. The diagonal elements of the matrix are all zero, since edges from a vertex to itself (loops) are not allowed in simple graphs. It is also sometimes useful in algebraic graph theory to replace the nonzero elements with algebraic variables.

The same concept can be extended to multigraphs and graphs with loops by storing the number of edges between each two vertices in the corresponding matrix element, and by allowing nonzero diagonal elements. Loops may be counted either once (as a single edge) or twice (as two vertex-edge incidences), as long as a consistent convention is followed. Undirected graphs often use the latter convention of counting loops twice, whereas directed graphs typically use the former convention.

The adjacency matrix "A" of a bipartite graph whose two parts have "r" and "s" vertices can be written in the form 
where "B" is an matrix, and 0 and 0 represent the and zero matrices. In this case, the smaller matrix "B" uniquely represents the graph, and the remaining parts of "A" can be discarded as redundant. "B" is sometimes called the biadjacency matrix.

Formally, let be a bipartite graph with parts } and }. The "biadjacency matrix" is the 0–1 matrix "B" in which if and only if ∈ "E". 

If "G" is a bipartite multigraph or weighted graph then the elements "b" are taken to be the number of edges between the vertices or the weight of the edge , respectively.

An -"adjacency matrix" "A" of a simple graph has "A" = "a" if ("i", "j") is an edge, "b" if it is not, and "c" on the diagonal. The Seidel adjacency matrix is a -"adjacency matrix". This matrix is used in studying strongly regular graphs and two-graphs.

The distance matrix has in position ("i", "j") the distance between vertices "v" and "v". The distance is the length of a shortest path connecting the vertices. Unless lengths of edges are explicitly provided, the length of a path is the number of edges in it. The distance matrix resembles a high power of the adjacency matrix, but instead of telling only whether or not two vertices are connected (i.e., the connection matrix, which contains boolean values), it gives the exact distance between them.

The convention followed here (for undirected graphs) is that each edge adds 1 to the appropriate cell in the matrix, and each loop adds 2. This allows the degree of a vertex to be easily found by taking the sum of the values in either its respective row or column in the adjacency matrix.

In directed graphs, the in-degree of a vertex can be computed by summing the entries of the corresponding column, and the out-degree can be computed by summing the entries of the corresponding row.
The adjacency matrix of a complete graph contains all ones except along the diagonal where there are only zeros. The adjacency matrix of an empty graph is a zero matrix.

The adjacency matrix of an undirected simple graph is symmetric, and therefore has a complete set of real eigenvalues and an orthogonal eigenvector basis. The set of eigenvalues of a graph is the spectrum of the graph. It is common to denote the eigenvalues by formula_2

The greatest eigenvalue formula_3 is bounded above by the maximum degree. This can be seen as result of the Perron–Frobenius theorem, but it can be proved easily. Let "v" be one eigenvector associated to formula_3 and "x" the component in which "v" has maximum absolute value. Without loss of generality assume "v" is positive since otherwise you simply take the eigenvector formula_5, also associated to formula_3. Then

For "d"-regular graphs, "d" is the first eigenvalue of "A" for the vector (it is easy to check that it is an eigenvalue and it is the maximum because of the above bound). The multiplicity of this eigenvalue is the number of connected components of "G", in particular formula_8 for connected graphs. It can be shown that for each eigenvalue formula_9, its opposite formula_10 is also an eigenvalue of "A" if "G" is a bipartite graph. In particular −"d" is an eigenvalue of bipartite graphs.

The difference formula_11 is called the spectral gap and it is related to the expansion of "G". It is also useful to introduce the spectral radius of formula_12 denoted by formula_13. This number is bounded by formula_14. This bound is tight in the Ramanujan graphs, which have applications in many areas.

Suppose two directed or undirected graphs "G" and "G" with adjacency matrices "A" and "A" are given. "G" and "G" are isomorphic if and only if there exists a permutation matrix "P" such that
In particular, "A" and "A" are similar and therefore have the same minimal polynomial, characteristic polynomial, eigenvalues, determinant and trace. These can therefore serve as isomorphism invariants of graphs. However, two graphs may possess the same set of eigenvalues but not be isomorphic. Such linear operators are said to be isospectral.

If "A" is the adjacency matrix of the directed or undirected graph "G", then the matrix "A" (i.e., the matrix product of "n" copies of "A") has an interesting interpretation: the element gives the number of (directed or undirected) walks of length "n" from vertex "i" to vertex "j". If "n" is the smallest nonnegative integer, such that for some "i", "j", the element of "A" is positive, then "n" is the distance between vertex "i" and vertex "j". This implies, for example, that the number of triangles in an undirected graph "G" is exactly the trace of "A" divided by 6. Note that the adjacency matrix can be used to determine whether or not the graph is connected.

The adjacency matrix may be used as a data structure for the representation of graphs in computer programs for manipulating graphs. The main alternative data structure, also in use for this application, is the adjacency list.

Because each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only /8 bytes to represent a directed graph, or (by using a packed triangular format and only storing the lower triangular part of the matrix) approximately /16 bytes to represent an undirected graph. Although slightly more succinct representations are possible, this method gets close to the information-theoretic lower bound for the minimum number of bits needed to represent all -vertex graphs. For storing graphs in text files, fewer bits per byte can be used to ensure that all bytes are text characters, for instance by using a Base64 representation.
Besides avoiding wasted space, this compactness encourages locality of reference.
However, for a large sparse graph, adjacency lists require less storage space, because they do not waste any space to represent edges that are "not" present.

An alternative form of adjacency matrix (which, however, requires a larger amount of space) replaces the numbers in each element of the matrix with pointers to edge objects (when edges are present) or null pointers (when there is no edge).
It is also possible to store edge weights directly in the elements of an adjacency matrix.

Besides the space tradeoff, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list, and takes time proportional to the number of neighbors. With an adjacency matrix, an entire row must instead be scanned, which takes a larger amount of time, proportional to the number of vertices in the whole graph. On the other hand, testing whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.




</doc>
<doc id="24109545" url="https://en.wikipedia.org/wiki?curid=24109545" title="Implicit graph">
Implicit graph

In the study of graph algorithms, an implicit graph representation (or more simply implicit graph) is a graph whose vertices or edges are not represented as explicit objects in a computer's memory, but rather are determined algorithmically from some more concise input.

The notion of an implicit graph is common in various search algorithms which are described in terms of graphs. In this context, an implicit graph may be defined as a set of rules to define all neighbors for any specified vertex. This type of implicit graph representation is analogous to an adjacency list, in that it provides easy access to the neighbors of each vertex. For instance, in searching for a solution to a puzzle such as Rubik's Cube, one may define an implicit graph in which each vertex represents one of the possible states of the cube, and each edge represents a move from one state to another. It is straightforward to generate the neighbors of any vertex by trying all possible moves in the puzzle and determining the states reached by each of these moves; however, an implicit representation is necessary, as the state space of Rubik's Cube is too large to allow an algorithm to list all of its states.

In computational complexity theory, several complexity classes have been defined in connection with implicit graphs, defined as above by a rule or algorithm for listing the neighbors of a vertex. For instance, PPA is the class of problems in which one is given as input an undirected implicit graph (in which vertices are -bit binary strings, with a polynomial time algorithm for listing the neighbors of any vertex) and a vertex of odd degree in the graph, and must find a second vertex of odd degree. By the handshaking lemma, such a vertex exists; finding one is a problem in NP, but the problems that can be defined in this way may not necessarily be NP-complete, as it is unknown whether PPA = NP. PPAD is an analogous class defined on implicit directed graphs that has attracted attention in algorithmic game theory because it contains the problem of computing a Nash equilibrium. The problem of testing reachability of one vertex to another in an implicit graph may also be used to characterize space-bounded nondeterministic complexity classes including NL (the class of problems that may be characterized by reachability in implicit directed graphs whose vertices are -bit bitstrings), SL (the analogous class for undirected graphs), and PSPACE (the class of problems that may be characterized by reachability in implicit graphs with polynomial-length bitstrings). In this complexity-theoretic context, the vertices of an implicit graph may represent the states of a nondeterministic Turing machine, and the edges may represent possible state transitions, but implicit graphs may also be used to represent many other types of combinatorial structure. PLS, another complexity class, captures the complexity of finding local optima in an implicit graph.

Implicit graph models have also been used as a form of relativization in order to prove separations between complexity classes that are stronger than the known separations for non-relativized models. For instance, Childs et al. used neighborhood representations of implicit graphs to define a graph traversal problem that can be solved in polynomial time on a quantum computer but that requires exponential time to solve on any classical computer.

In the context of efficient representations of graphs, J. H. Muller defined a "local structure" or "adjacency labeling scheme" for a graph in a given family of graphs to be an assignment of an -bit identifier to each vertex of , together with an algorithm (that may depend on but is independent of the individual graph ) that takes as input two vertex identifiers and determines whether or not they are the endpoints of an edge in . That is, this type of implicit representation is analogous to an adjacency matrix: it is straightforward to check whether two vertices are adjacent but finding the neighbors of any vertex may involve looping through all vertices and testing which ones are neighbors.

Graph families with adjacency labeling schemes include:

Not all graph families have local structures. For some families, a simple counting argument proves that adjacency labeling schemes do not exist: only bits may be used to represent an entire graph, so a representation of this type can only exist when the number of -vertex graphs in the given family is at most . Graph families that have larger numbers of graphs than this, such as the bipartite graphs or the triangle-free graphs, do not have adjacency labeling schemes. However, even families of graphs in which the number of graphs in the family is small might not have an adjacency labeling scheme; for instance, the family of graphs with fewer edges than vertices has -vertex graphs but does not have an adjacency labeling scheme, because one could transform any given graph into a larger graph in this family by adding a new isolated vertex for each edge, without changing its labelability. Kannan et al. asked whether having a forbidden subgraph characterization and having at most -vertex graphs are together enough to guarantee the existence of an adjacency labeling scheme; this question, which Spinrad restated as a conjecture, remains open.
Among the families of graphs which satisfy the conditions of the conjecture and for which there is no known adjacency labeling scheme are the family of disk graphs and line segment intersection graphs.

If a graph family has an adjacency labeling scheme, then the -vertex graphs in may be represented as induced subgraphs of a common induced universal graph of polynomial size, the graph consisting of all possible vertex identifiers. Conversely, if an induced universal graph of this type can be constructed, then the identities of its vertices may be used as labels in an adjacency labeling scheme. For this application of implicit graph representations, it is important that the labels use as few bits as possible, because the number of bits in the labels translates directly into the number of vertices in the induced universal graph. Alstrup and Rauhe showed that any tree has an adjacency labeling scheme with bits per label, from which it follows that any graph with arboricity "k" has a scheme with bits per label and a universal graph with vertices. In particular, planar graphs have arboricity at most three, so they have universal graphs with a nearly-cubic number of vertices.
This bound was improved by Gavoille and Labourel who showed that planar graphs and minor-closed graph families have a labeling scheme with bits per label, and that graphs of bounded treewidth have a labeling scheme with bits per label.

The Aanderaa–Karp–Rosenberg conjecture concerns implicit graphs given as a set of labeled vertices with a black-box rule for determining whether any two vertices are adjacent. This definition differs from an adjacency labeling scheme in that the rule may be specific to a particular graph rather than being a generic rule that applies to all graphs in a family. Because of this difference, every graph has an implicit representation. For instance, the rule could be to look up the pair of vertices in a separate adjacency matrix. However, an algorithm that is given as input an implicit graph of this type must operate on it only through the implicit adjacency test, without reference to how the test is implemented.

A "graph property" is the question of whether a graph belongs to a given family of graphs; the answer must remain invariant under any relabeling of the vertices. In this context, the question to be determined is how many pairs of vertices must be tested for adjacency, in the worst case, before the property of interest can be determined to be true or false for a given implicit graph. Rivest and Vuillemin proved that any deterministic algorithm for any nontrivial graph property must test a quadratic number of pairs of vertices. The full Aanderaa–Karp–Rosenberg conjecture is that any deterministic algorithm for a monotonic graph property (one that remains true if more edges are added to a graph with the property) must in some cases test every possible pair of vertices. Several cases of the conjecture have been proven to be true—for instance, it is known to be true for graphs with a prime number of vertices—but the full conjecture remains open. Variants of the problem for randomized algorithms and quantum algorithms have also been studied.

Bender and Ron have shown that, in the same model used for the evasiveness conjecture, it is possible in only constant time to distinguish directed acyclic graphs from graphs that are very far from being acyclic. In contrast, such a fast time is not possible in neighborhood-based implicit graph models,



</doc>
<doc id="97034" url="https://en.wikipedia.org/wiki?curid=97034" title="Depth-first search">
Depth-first search

Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.

A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.

The time and space analysis of DFS differs according to its application area. In theoretical computer science, DFS is typically used to traverse an entire graph, and takes time formula_1, linear in the size of the graph. In these applications it also uses space formula_2 in the worst case to store the stack of vertices on the current search path as well as the set of already-visited vertices. Thus, in this setting, the time and space bounds are the same as for breadth-first search and the choice of which of these two algorithms to use depends less on their complexity and more on the different properties of the vertex orderings the two algorithms produce.

For applications of DFS in relation to specific domains, such as searching for solutions in artificial intelligence or web-crawling, the graph to be traversed is often either too large to visit in its entirety or infinite (DFS may suffer from non-termination). In such cases, search is only performed to a limited depth; due to limited resources, such as memory or disk space, one typically does not use data structures to keep track of the set of all previously visited vertices. When search is performed to a limited depth, the time is still linear in terms of the number of expanded vertices and edges (although this number is not the same as the size of the entire graph because some vertices may be searched more than once and others not at all) but the space complexity of this variant of DFS is only proportional to the depth limit, and as a result, is much smaller than the space needed for searching to the same depth using breadth-first search. For such applications, DFS also lends itself much better to heuristic methods for choosing a likely-looking branch. When an appropriate depth limit is not known a priori, iterative deepening depth-first search applies DFS repeatedly with a sequence of increasing limits. In the artificial intelligence mode of analysis, with a branching factor greater than one, iterative deepening increases the running time by only a constant factor over the case in which the correct depth limit is known due to the geometric growth of the number of nodes per level.

DFS may also be used to collect a sample of graph nodes. However, incomplete DFS, similarly to incomplete BFS, is biased towards nodes of high degree.

For the following graph:

a depth-first search starting at A, assuming that the left edges in the shown graph are chosen before right edges, and assuming the search remembers previously visited nodes and will not repeat them (since this is a small graph), will visit the nodes in the following order: A, B, D, F, E, C, G. The edges traversed in this search form a Trémaux tree, a structure with important applications in graph theory.
Performing the same search without remembering previously visited nodes results in visiting nodes in the order A, B, D, F, E, A, B, D, F, E, etc. forever, caught in the A, B, D, F, E cycle and never reaching C or G.

Iterative deepening is one technique to avoid this infinite loop and would reach all nodes.

A convenient description of a depth-first search of a graph is in terms of a spanning tree of the vertices reached during the search. Based on this spanning tree, the edges of the original graph can be divided into three classes: forward edges, which point from a node of the tree to one of its descendants, back edges, which point from a node to one of its ancestors, and cross edges, which do neither. Sometimes tree edges, edges which belong to the spanning tree itself, are classified separately from forward edges. If the original graph is undirected then all of its edges are tree edges or back edges.

An enumeration of the vertices of a graph is said to be a DFS ordering if it is the possible output of the application of DFS to this graph.

Let formula_3 be a graph with formula_4 vertices. 
For formula_5 be a list of distinct elements of formula_6, for formula_7, let formula_8 be the greatest formula_9 such that formula_10 is a neighbor of formula_11, if such a formula_9 exists, and be formula_13 otherwise.

Let formula_14 be an enumeration of the vertices of formula_6.
The enumeration formula_16 is said to be a DFS ordering (with source formula_17) if, for all <math>1, formula_10 is the vertex formula_19 such that formula_20 is maximal.
Recall that formula_21 is the set of neighbors of formula_11. Equivalently, formula_16 is a DFS ordering if, for all formula_24 with formula_25, there exists a neighbor formula_26 of formula_27 such that formula_28.

It is also possible to use depth-first search to linearly order the vertices of a graph or tree. There are three common ways of doing this:


For example, when searching the directed graph below beginning at node A, the sequence of traversals is either A B D B A C A or A C D C A B A (choosing to first visit B or C from A is up to the algorithm). Note that repeat visits in the form of backtracking to a node, to check if it has still unvisited neighbors, are included here (even if it is found to have none). Thus the possible preorderings are A B D C and A C D B, while the possible postorderings are D B C A and D C B A, and the possible reverse postorderings are A C B D and A B C D.

Reverse postordering produces a topological sorting of any directed acyclic graph. This ordering is also useful in control flow analysis as it often represents a natural linearization of the control flows. The graph above might represent the flow of control in the code fragment below, and it is natural to consider this code in the order A B C D or A C B D but not natural to use the order A B D C or A C D B.

Input: A graph "G" and a vertex "v" of G

Output: All vertices reachable from "v" labeled as discovered

A recursive implementation of DFS:
The order in which the vertices are discovered by this algorithm is called the lexicographic order.

A non-recursive implementation of DFS with worst-case space complexity formula_29:

These two variations of DFS visit the neighbors of each vertex in the opposite order from each other: the first neighbor of "v" visited by the recursive variation is the first one in the list of adjacent edges, while in the iterative variation the first visited neighbor is the last one in the list of adjacent edges. The recursive implementation will visit the nodes from the example graph in the following order: A, B, D, F, E, C, G. The non-recursive implementation will visit the nodes as: A, E, F, B, D, C, G.

The non-recursive implementation is similar to breadth-first search but differs from it in two ways: 

Algorithms that use depth-first search as a building block include:

The computational complexity of DFS was investigated by John Reif. More precisely, given a graph formula_30, let formula_31 be the ordering computed by the standard recursive DFS algorithm. This ordering is called the lexicographic depth-first search ordering. John Reif considered the complexity of computing the lexicographic depth-first search ordering, given a graph and a source. A decision version of the problem (testing whether some vertex occurs before some vertex in this order) is P-complete, meaning that it is "a nightmare for parallel processing".

A depth-first search ordering (not necessarily the lexicographic one), can be computed by a randomized parallel algorithm in the complexity class RNC. As of 1997, it remained unknown whether a depth-first traversal could be constructed by a deterministic parallel algorithm, in the complexity class NC.





</doc>
<doc id="97026" url="https://en.wikipedia.org/wiki?curid=97026" title="Breadth-first search">
Breadth-first search

Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.

It uses the opposite strategy as depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes.

BFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, in his (rejected) Ph.D. thesis on the Plankalkül programming language, but this was not published until 1972.
It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze, and later developed by C. Y. Lee into a wire routing algorithm (published 1961).

Input: A graph and a "starting vertex" of 

Output: Goal state. The "parent" links trace the shortest path back to 

This non-recursive implementation is similar to the non-recursive implementation of depth-first search, but differs from it in two ways:


The "Q" queue contains the frontier along which the algorithm is currently searching.

Nodes can be labelled as discovered by storing them in a set, or by an attribute on each node, depending on the implementation.

Note that the word "node" is usually interchangeable with the word "vertex".

The "parent" attribute of each node is useful for accessing the nodes in a shortest path, for example by backtracking from the destination node up to the starting node, once the BFS has been run, and the predecessors nodes have been set.

Breadth-first search produces a so-called "breadth first tree". You can see how a "breadth first tree" looks in the following example.

The following is an example of the breadth-first tree obtained by running a BFS on German cities starting from "Frankfurt":

The time complexity can be expressed as formula_1, since every vertex and every edge will be explored in the worst case. formula_2 is the number of vertices and formula_3 is the number of edges in the graph.
Note that formula_4 may vary between formula_5 and formula_6, depending on how sparse the input graph is.

When the number of vertices in the graph is known ahead of time, and additional data structures are used to determine which vertices have already been added to the queue, the space complexity can be expressed as formula_7, where formula_2 is the cardinality of the set of vertices. This is in addition to the space
required for the graph itself, which may vary depending on the graph representation used by an implementation of the algorithm.

When working with graphs that are too large to store explicitly (or infinite), it is more practical to describe the complexity of breadth-first search in different terms: to find the nodes that are at distance from the start node (measured in number of edge traversals), BFS takes time and memory, where is the "branching factor" of the graph (the average out-degree).

In the analysis of algorithms, the input to breadth-first search is assumed to be a finite graph, represented explicitly as an adjacency list or similar representation. However, in the application of graph traversal methods in artificial intelligence the input may be an implicit representation of an infinite graph. In this context, a search method is described as being complete if it is guaranteed to find a goal state if one exists. Breadth-first search is complete, but depth-first search is not. When applied to infinite graphs represented implicitly, breadth-first search will eventually find the goal state, but depth-first search may get lost in parts of the graph that have no goal state and never return.

An enumeration of the vertices of a graph is said to be a BFS ordering if it is the possible output of the application of BFS to this graph.

Let formula_9 be a graph with formula_10 vertices. Recall that formula_11 is the set of neighbors of formula_12.
For formula_13 be a list of distinct elements of formula_14, for formula_15, let formula_16 be the least formula_17 such that formula_18 is a neighbor of formula_12, if such a formula_17 exists, and be formula_21 otherwise.

Let formula_22 be an enumeration of the vertices of formula_14.
The enumeration formula_24 is said to be a BFS ordering (with source formula_25) if, for all <math>1, formula_18 is the vertex formula_27 such that formula_28 is minimal. Equivalently, formula_24 is a BFS ordering if, for all formula_30 with formula_31, there exists a neighbor formula_32 of formula_33 such that <math>m. 

Breadth-first search can be used to solve many problems in graph theory, for example:




</doc>
<doc id="22336498" url="https://en.wikipedia.org/wiki?curid=22336498" title="Lexicographic breadth-first search">
Lexicographic breadth-first search

In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. The algorithm is different from a breadth-first search, but it produces an ordering that is consistent with breadth-first search.

The lexicographic breadth-first search algorithm is based on the idea of partition refinement and was first developed by . A more detailed survey of the topic is presented by .
It has been used as a subroutine in other graph algorithms including the recognition of chordal graphs, and optimal coloring of distance-hereditary graphs.

The breadth-first search algorithm is commonly defined by the following process:
However, rather than defining the vertex to choose at each step in an imperative way as the one produced by the dequeue operation of a queue, one can define the same sequence of vertices declaratively by the properties of these vertices. That is, a standard breadth-first search is just the result of repeatedly applying this rule:
In some cases, this ordering of vertices by the output positions of their predecessors may have ties — two different vertices have the same earliest predecessor. In this case, the order in which those two vertices are chosen may be arbitrary. The output of lexicographic breadth-first search differs from a standard breadth-first search in having a consistent rule for breaking such ties. In lexicographic breadth-first search, the output ordering is the order that would be produced by the rule:
So, when two vertices and have the same earliest predecessor, earlier than any other unchosen vertices,
the standard breadth-first search algorithm will order them arbitrarily. Instead, in this case, the LexBFS algorithm would choose between and by the output ordering of their second-earliest predecessors.
If only one of them has a second-earliest predecessor that has already been output, that one is chosen.
If both and have the same second-earliest predecessor, then the tie is broken by considering their third-earliest predecessors, and so on.

Applying this rule directly by comparing vertices according to this rule would lead to an inefficient algorithm. Instead, the lexicographic breadth-first search uses a set partitioning data structure in order to produce the same ordering more efficiently, just as a standard breadth-first search uses a queue data structure to produce its ordering efficiently.

The lexicographic breadth-first search algorithm replaces the queue of vertices of a standard breadth-first search with an ordered sequence of sets of vertices. The sets in the sequence form a partition of the remaining vertices. At each step, a vertex "v" from the first set in the sequence is removed from that set, and if that removal causes the set to become empty then the set is removed from the sequence. Then, each set in the sequence is replaced by two subsets: the neighbors of "v" and the non-neighbors of "v". The subset of neighbors is placed earlier in the sequence than the subset of non-neighbors. In pseudocode, the algorithm can be expressed as follows:


Each vertex is processed once, each edge is examined only when its two endpoints are processed, and (with an appropriate representation for the sets in Σ that allows items to be moved from one set to another in constant time) each iteration of the inner loop takes only constant time. Therefore, like simpler graph search algorithms such as breadth-first search and depth first search, this algorithm takes linear time.

The algorithm is called lexicographic breadth-first search because the order it produces is an ordering that could also have been produced by a breadth-first search, and because if the ordering is used to index the rows and columns of an adjacency matrix of a graph then the algorithm sorts the rows and columns into lexicographical order.

A graph "G" is defined to be chordal if its vertices have a "perfect elimination ordering", an ordering such that for any vertex "v" the neighbors that occur later in the ordering form a clique. In a chordal graph, the reverse of a lexicographic ordering is always a perfect elimination ordering. Therefore, one can test whether a graph is chordal in linear time by the following algorithm:

This application was the original motivation that led to develop the lexicographic breadth first search algorithm.

A graph "G" is said to be "perfectly orderable" if there is a sequence of its vertices with the property that, for any induced subgraph of "G", a greedy coloring algorithm that colors the vertices in the induced sequence ordering is guaranteed to produce an optimal coloring.

For a chordal graph, a perfect elimination ordering is a perfect ordering: the number of the color used for any vertex is the size of the clique formed by it and its earlier neighbors, so the maximum number of colors used is equal to the size of the largest clique in the graph, and no coloring can use fewer colors. An induced subgraph of a chordal graph is chordal and the induced subsequence of its perfect elimination ordering is a perfect elimination ordering on the subgraph, so chordal graphs are perfectly orderable, and lexicographic breadth-first search can be used to optimally color them.

The same property is true for a larger class of graphs, the distance-hereditary graphs: distance-hereditary graphs are perfectly orderable, with a perfect ordering given by the reverse of a lexicographic ordering, so lexicographic breadth-first search can be used in conjunction with greedy coloring algorithms to color them optimally in linear time.

 describe an extension of lexicographic breadth-first search that breaks any additional ties using the complement graph of the input graph. As they show, this can be used to recognize cographs in linear time. describe additional applications of lexicographic breadth-first search including the recognition of comparability graphs and interval graphs.

An enumeration of the vertices of a graph is said to be a LexBFS ordering if it is the possible output of the application of LexBFS to this graph.

Let formula_1 be a graph with formula_2 vertices. Recall that formula_3 is the set of neighbors of formula_4.
Let formula_5 be an enumeration of the vertices of formula_6.
The enumeration formula_7 is a LexBFS ordering (with source formula_8) if, for all formula_9 with formula_10, there exists formula_11.



</doc>
<doc id="433326" url="https://en.wikipedia.org/wiki?curid=433326" title="Iterative deepening depth-first search">
Iterative deepening depth-first search

In computer science, iterative deepening search or more specifically iterative deepening depth-first search (IDS or IDDFS) is a state space/graph search strategy in which a depth-limited version of depth-first search is run repeatedly with increasing depth limits until the goal is found. IDDFS is optimal like breadth-first search, but uses much less memory; at each iteration, it visits the nodes in the search tree in the same order as depth-first search, but the cumulative order in which nodes are first visited is effectively breadth-first.

The following pseudocode shows IDDFS implemented in terms of a recursive depth-limited DFS (called DLS) for directed graphs. This implementation of IDDFS does not account for already-visited nodes and therefore does not work for undirected graphs.

If the goal node is found, then DLS unwinds the recursion returning with no further iterations. Otherwise, if at least one node exists at that level of depth, the "remaining" flag will let IDDFS continue.

2-tuples are useful as return value to signal IDDFS to continue deepening or stop, in case tree depth and goal membership are unknown "a priori". Another solution could use sentinel values instead to represent "not found" or "remaining level" results.

IDDFS combines depth-first search's space-efficiency and breadth-first search's completeness (when the branching factor is finite). If a solution exists, it will find a solution path with the fewest arcs.

Since iterative deepening visits states multiple times, it may seem wasteful, but it turns out to be not so costly, since in a tree most of the nodes are in the bottom level, so it does not matter much if the upper levels are visited multiple times.

The main advantage of IDDFS in game tree searching is that the earlier searches tend to improve the commonly used heuristics, such as the killer heuristic and alpha-beta pruning, so that a more accurate estimate of the score of various nodes at the final depth search can occur, and the search completes more quickly since it is done in a better order. For example, alpha-beta pruning is most efficient if it searches the best moves first.

A second advantage is the responsiveness of the algorithm. Because early iterations use small values for formula_1, they execute extremely quickly. This allows the algorithm to supply early indications of the result almost immediately, followed by refinements as formula_1 increases. When used in an interactive setting, such as in a chess-playing program, this facility allows the program to play at any time with the current best move found in the search it has completed so far. This can be phrased as each depth of the search "co"recursively producing a better approximation of the solution, though the work done at each step is recursive. This is not possible with a traditional depth-first search, which does not produce intermediate results.

The time complexity of IDDFS in a (well-balanced) tree works out to be the same as breadth-first search, i.e. formula_3, where formula_4 is the branching factor and formula_1 is the depth of the goal.

In an iterative deepening search, the nodes at depth formula_1 are expanded once, those at depth formula_7 are expanded twice, and so on up to the root of the search tree, which is
expanded formula_8 times. So the total number of expansions in an iterative deepening search is

where formula_10 is the number of expansions at depth formula_1, formula_12 is the number of expansions at depth formula_7, and so on. Factoring out formula_10 gives

Now let formula_16. Then we have

This is less than the infinite series

which converges to

That is, we have

formula_21, for formula_20

Since formula_23 or formula_24 is a constant independent of formula_1 (the depth), if formula_26 (i.e., if the branching factor is greater than 1), the running time of the depth-first iterative deepening search is formula_27.

For formula_28 and formula_29 the number is

All together, an iterative deepening search from depth formula_31 all the way down to depth formula_1 expands only about formula_33 more nodes than a single breadth-first or depth-limited search to depth formula_1, when formula_35.

The higher the branching factor, the lower the overhead of repeatedly expanded states, but even when the branching factor is 2, iterative deepening search only takes about twice as long as a complete breadth-first search. This means that the time complexity of iterative deepening is still formula_27.

The space complexity of IDDFS is formula_37, where formula_1 is the depth of the goal.

Since IDDFS, at any point, is engaged in a depth-first search, it need only store a stack of nodes which represents the branch of the tree it is expanding. Since it finds a solution of optimal length, the maximum depth of this stack is formula_1, and hence the maximum amount of space is formula_37.

In general, iterative deepening is the preferred search method when there is a large search space and the depth of the solution is not known.

For the following graph:

a depth-first search starting at A, assuming that the left edges in the shown graph are chosen before right edges, and assuming the search remembers previously-visited nodes and will not repeat them (since this is a small graph), will visit the nodes in the following order: A, B, D, F, E, C, G. The edges traversed in this search form a Trémaux tree, a structure with important applications in graph theory.

Performing the same search without remembering previously visited nodes results in visiting nodes in the order A, B, D, F, E, A, B, D, F, E, etc. forever, caught in the A, B, D, F, E cycle and never reaching C or G.

Iterative deepening prevents this loop and will reach the following nodes on the following depths, assuming it proceeds left-to-right as above:

(Iterative deepening has now seen C, when a conventional depth-first search did not.)
(It still sees C, but that it came later. Also it sees E via a different path, and loops back to F twice.)

For this graph, as more depth is added, the two cycles "ABFE" and "AEFB" will simply get longer before the algorithm gives up and tries another branch.

Similar to iterative deepening is a search strategy called iterative lengthening search that works with increasing path-cost limits instead of depth-limits. It expands nodes in the order of increasing path cost; therefore the first goal it encounters is the one with the cheapest path cost. But iterative lengthening incurs substantial overhead that makes it less useful than iterative deepening.

Iterative deepening A* is a best-first search that performs iterative deepening based on ""-values similar to the ones computed in the A* algorithm.

IDDFS has a bidirectional counterpart, which alternates two searches: one starting from the source node and moving along the directed arcs, and another one starting from the target node and proceeding along the directed arcs in opposite direction (from the arc's head node to the arc's tail node). The search process first checks that the source node and the target node are same, and if so, returns the trivial path consisting of a single source/target node. Otherwise, the forward search process expands the child nodes of the source node (set formula_41), the backward search process expands the parent nodes of the target node (set formula_42), and it is checked whether formula_41 and formula_42 intersect. If so, a shortest path is found. Otherwise, the search depth is incremented and the same computation takes place.

One limitation of the algorithm is that the shortest path consisting of an odd number of arcs will not be detected. Suppose we have a shortest path formula_45 When the depth will reach two hops along the arcs, the forward search will proceed to formula_46 from formula_47, and the backward search will proceed from formula_46 to formula_47. Pictorially, the search frontiers will go through each other, and instead a suboptimal path consisting of an even number of arcs will be returned. This is illustrated in the below diagrams:

What comes to space complexity, the algorithm colors the deepest nodes in the forward search process in order to detect existing of the middle node where the two search processes meet.

Additional difficulty of applying bidirectional IDDFS is that if the source and the target nodes are in different strongly connected components, say, formula_50, if there is no arc leaving formula_51 and entering formula_52, the search will never terminate.

The running time of bidirectional IDDFS is given by
and the space complexity is given by
where formula_55 is the number of nodes in the shortest formula_56-path. Since the running time complexity of iterative deepening depth-first search is formula_57, the speedup is roughly

 function Build-Path(s, μ, B)


</doc>
<doc id="897064" url="https://en.wikipedia.org/wiki?curid=897064" title="Topological sorting">
Topological sorting

In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge "uv" from vertex "u" to vertex "v", "u" comes before "v" in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.

The canonical application of topological sorting is in scheduling a sequence of jobs or tasks based on their dependencies. The jobs are represented by vertices, and there is an edge from "x" to "y" if job "x" must be completed before job "y" can be started (for example, when washing clothes, the washing machine must finish before we put the clothes in the dryer). Then, a topological sort gives an order in which to perform the jobs. A closely related application of topological sorting algorithms was first studied in the early 1960s in the context of the PERT technique for scheduling in project management ; in this application, the vertices of a graph represent the milestones of a project, and the edges represent tasks that must be performed between one milestone and another. Topological sorting forms the basis of linear-time algorithms for finding the critical path of the project, a sequence of milestones and tasks that controls the length of the overall project schedule.

In computer science, applications of this type arise in instruction scheduling, ordering of formula cell evaluation when recomputing formula values in spreadsheets, logic synthesis, determining the order of compilation tasks to perform in makefiles, data serialization, and resolving symbol dependencies in linkers. It is also used to decide in which order to load tables with foreign keys in databases.

The usual algorithms for topological sorting have running time linear in the number of nodes plus the number of edges, asymptotically, formula_1

One of these algorithms, first described by , works by choosing vertices in the same order as the eventual topological sort. First, find a list of "start nodes" which have no incoming edges and insert them into a set S; at least one such node must exist in a non-empty acyclic graph. Then:

If the graph is a DAG, a solution will be contained in the list L (the solution is not necessarily unique). Otherwise, the graph must have at least one cycle and therefore a topological sort is impossible.

Reflecting the non-uniqueness of the resulting sort, the structure S can be simply a set or a queue or a stack. Depending on the order that nodes n are removed from set S, a different solution is created. A variation of Kahn's algorithm that breaks ties lexicographically forms a key component of the Coffman–Graham algorithm for parallel scheduling and layered graph drawing.

An alternative algorithm for topological sorting is based on depth-first search. The algorithm loops through each node of the graph, in an arbitrary order, initiating a depth-first search that terminates when it hits any node that has already been visited since the beginning of the topological sort or the node has no outgoing edges (i.e. a leaf node):

Each node "n" gets "prepended" to the output list L only after considering all other nodes which depend on "n" (all descendants of "n" in the graph). Specifically, when the algorithm adds node "n", we are guaranteed that all nodes which depend on "n" are already in the output list L: they were added to L either by the recursive call to visit() which ended before the call to visit "n", or by a call to visit() which started even before the call to visit "n". Since each edge and node is visited once, the algorithm runs in linear time. This depth-first-search-based algorithm is the one described by ; it seems to have been first described in print by .

On a parallel random-access machine, a topological ordering can be constructed in "O"(log "n") time using a polynomial number of processors, putting the problem into the complexity class NC .
One method for doing this is to repeatedly square the adjacency matrix of the given graph, logarithmically many times, using min-plus matrix multiplication with maximization in place of minimization. The resulting matrix describes the longest path distances in the graph. Sorting the vertices by the lengths of their longest incoming paths produces a topological ordering .

The topological ordering can also be used to quickly compute shortest paths through a weighted directed acyclic graph. Let be the list of vertices in such a graph, in topological order. Then the following algorithm computes the shortest path from some source vertex to all other vertices:

On a graph of vertices and edges, this algorithm takes , i.e., linear, time.

If a topological sort has the property that all pairs of consecutive vertices in the sorted order are connected by edges, then these edges form a directed Hamiltonian path in the DAG. If a Hamiltonian path exists, the topological sort order is unique; no other order respects the edges of the path. Conversely, if a topological sort does not form a Hamiltonian path, the DAG will have two or more valid topological orderings, for in this case it is always possible to form a second valid ordering by swapping two consecutive vertices that are not connected by an edge to each other. Therefore, it is possible to test in linear time whether a unique ordering exists, and whether a Hamiltonian path exists, despite the NP-hardness of the Hamiltonian path problem for more general directed graphs .

Topological orderings are also closely related to the concept of a linear extension of a partial order in mathematics. In high-level terms, there is an adjunction between directed graphs and partial orders.

A partially ordered set is just a set of objects together with a definition of the "≤" inequality relation, satisfying the axioms of reflexivity ("x" ≤ "x"), antisymmetry (if "x" ≤ "y" and "y" ≤ "x" then "x" = "y") and transitivity (if "x" ≤ "y" and "y" ≤ "z", then "x" ≤ "z"). A total order is a partial order in which, for every two objects "x" and "y" in the set, either "x" ≤ "y" or "y" ≤ "x". Total orders are familiar in computer science as the comparison operators needed to perform comparison sorting algorithms. For finite sets, total orders may be identified with linear sequences of objects, where the "≤" relation is true whenever the first object precedes the second object in the order; a comparison sorting algorithm may be used to convert a total order into a sequence in this way. A linear extension of a partial order is a total order that is compatible with it, in the sense that, if "x" ≤ "y" in the partial order, then "x" ≤ "y" in the total order as well.

One can define a partial ordering from any DAG by letting the set of objects be the vertices of the DAG, and defining "x" ≤ "y" to be true, for any two vertices "x" and "y", whenever there exists a directed path from "x" to "y"; that is, whenever "y" is reachable from "x". With these definitions, a topological ordering of the DAG is the same thing as a linear extension of this partial order. Conversely, any partial ordering on a finite set may be defined as the reachability relation in a DAG. One way of doing this is to define a DAG that has a vertex for every object in the partially ordered set, and an edge "xy" for every pair of objects for which "x" ≤ "y". An alternative way of doing this is to use the transitive reduction of the partial ordering; in general, this produces DAGs with fewer edges, but the reachability relation in these DAGs is still the same partial order. By using these constructions, one can use topological ordering algorithms to find linear extensions of partial orders.






</doc>
<doc id="32183016" url="https://en.wikipedia.org/wiki?curid=32183016" title="Ear decomposition">
Ear decomposition

In graph theory, an ear of an undirected graph "G" is a path "P" where the two endpoints of the path may coincide, but where otherwise no repetition of edges or vertices is allowed, so every internal vertex of "P" has degree two in "P". An ear decomposition of an undirected graph "G" is a partition of its set of edges into a sequence of ears, such that the one or two endpoints of each ear belong to earlier ears in the sequence and such that the internal vertices of each ear do not belong to any earlier ear. Additionally, in most cases the first ear in the sequence must be a cycle. An open ear decomposition or a proper ear decomposition is an ear decomposition in which the two endpoints of each ear after the first are distinct from each other.

Ear decompositions may be used to characterize several important graph classes, and as part of efficient graph algorithms. They may also be generalized from graphs to matroids.

Several important classes of graphs may be characterized as the graphs having certain types of ear decompositions.

A graph is "k"-vertex-connected if the removal of any ("k" − 1) vertices leaves a connected subgraph, and "k"-edge-connected if the removal of any ("k" − 1) edges leaves a connected subgraph.

The following result is due to :

The following result is due to :

In both cases the number of ears is necessarily equal to the circuit rank of the given graph. Robbins introduced the ear decomposition of 2-edge-connected graphs as a tool for proving the Robbins theorem, that these are exactly the graphs that may be given a strongly connected orientation. Because of the pioneering work of Whitney and Robbins on ear decompositions, an ear decomposition is sometimes also called a Whitney–Robbins synthesis .

A non-separating ear decomposition is an open ear decomposition such that, for each vertex "v" with only one exception, "v" has a neighbor whose first appearance in the decomposition is in a later ear than the first appearance of "v". This type of ear decomposition may be used to generalize Whitney's result:
If such a decomposition exists, it can be chosen with respect to a particular edge "uv" of "G" in such a way that "u" is in the first ear, "v" is the new vertex in the last ear with more than one edge, and "uv" is a single-edge ear.
This result was first stated explicitly by , but as describes, it is equivalent to a result in the 1971 Ph.D. thesis of Lee Mondshein. Structures closely related to non-separating ear decompositions of maximal planar graphs, called canonical orderings, are also a standard tool in graph drawing.

The above definitions can also be applied to directed graphs. An ear would then be a directed path where all internal vertices have indegree and outdegree equal to 1. A directed graph is strongly connected if it contains a directed path from every vertex to every other vertex. Then we have the following theorem:
Similarly, a directed graph is biconnected if, for every two vertices, there exists a simple cycle in the graph containing both of them. Then

An ear decomposition is "odd" if each of its ears uses an odd number of edges. A factor-critical graph is a graph with an odd number of vertices, such that for each vertex "v", if "v" is removed from the graph then the remaining vertices have a perfect matching. found that:
More generally, a result of makes it possible to find in any graph "G" the ear decomposition with the fewest even ears.

A "tree" ear decomposition is a proper ear decomposition in which the first ear is a single edge and for each subsequent ear formula_5, there is a single ear formula_6, formula_7, such that both endpoints of formula_5 lie on formula_6 . A "nested" ear decomposition is a tree ear decomposition such that, within each ear formula_6, the set of pairs of endpoints of other ears formula_5 that lie within formula_6 form a set of nested intervals. A series-parallel graph is a graph with two designated terminals "s" and "t" that can be formed recursively by combining smaller series-parallel graphs in one of two ways: series composition (identifying one terminal from one smaller graph with one terminal from the other smaller graph, and keeping the other two terminals as the terminals of the combined graph) and parallel composition (identifying both pairs of terminals from the two smaller graphs).

The following result is due to :
Moreover, any open ear decomposition of a 2-vertex-connected series-parallel graph must be nested. The result may be extended to series-parallel graphs that are not 2-vertex-connected by using open ear decompositions that start with a path between the two terminals.

The concept of an ear decomposition can be extended from graphs to matroids. An ear decomposition of a matroid is defined to be a sequence of circuits of the matroid, with two properties:
When applied to the graphic matroid of a graph "G", this definition of an ear decomposition coincides with the definition of a proper ear decomposition of "G": improper decompositions are excluded by the requirement that each circuit include at least one edge that also belongs to previous circuits. Using this definition, a matroid may be defined as factor-critical when it has an ear decomposition in which each circuit in the sequence has an odd number of new elements .

On classical computers, ear decompositions of 2-edge-connected graphs and open ear decompositions of 2-vertex-connected graphs may be found by greedy algorithms that find each ear one at a time. A simple greedy approach that computes at the same time ear decompositions, open ear decompositions, st-numberings and -orientations in linear time (if exist) is given in . The approach is based on computing a special ear decomposition named chain decomposition by one path-generating rule. shows that non-separating ear decompositions may also be constructed in linear time.

, , and provided efficient parallel algorithms for constructing ear decompositions of various types. For instance, to find an ear decomposition of a 2-edge-connected graph, the algorithm of proceeds according to the following steps:
These algorithms may be used as subroutines for other problems including testing connectivity, recognizing series-parallel graphs, and constructing "st"-numberings of graphs (an important subroutine in planarity testing).

An ear decomposition of a given matroid, with the additional constraint that every ear contains the same fixed element of the matroid, may be found in polynomial time given access to an independence oracle for the matroid .



</doc>
<doc id="8244667" url="https://en.wikipedia.org/wiki?curid=8244667" title="Tarjan's strongly connected components algorithm">
Tarjan's strongly connected components algorithm

Tarjan's algorithm is an algorithm in graph theory for finding the strongly connected components of a directed graph. It runs in linear time, matching the time bound for alternative methods including Kosaraju's algorithm and the path-based strong component algorithm. Tarjan's algorithm is named for its inventor, Robert Tarjan.

The algorithm takes a directed graph as input, and produces a partition of the graph's vertices into the graph's strongly connected components. Each vertex of the graph appears in exactly one of the strongly connected components. Any vertex that is not on a directed cycle forms a strongly connected component all by itself: for example, a vertex whose in-degree or out-degree is 0, or any vertex of an acyclic graph.

The basic idea of the algorithm is this: a depth-first search begins from an arbitrary start node (and subsequent depth-first searches are conducted on any nodes that have not yet been found). As usual with depth-first search, the search visits every node of the graph exactly once, declining to revisit any node that has already been visited. Thus, the collection of search trees is a spanning forest of the graph. The strongly connected components will be recovered as certain subtrees of this forest. The roots of these subtrees are called the "roots" of the strongly connected components. Any node of a strongly connected component might serve as a root, if it happens to be the first node of a component that is discovered by search.

Nodes are placed on a stack in the order in which they are visited. When the depth-first search recursively visits a node codice_1 and its descendants, those nodes are not all necessarily popped from the stack when this recursive call returns. The crucial invariant property is that a node remains on the stack after it has been visited if and only if there exists a path in the input graph from it to some node earlier on the stack.

At the end of the call that visits codice_1 and its descendants, we know whether codice_1 itself has a path to any node earlier on the stack. If so, the call returns, leaving codice_1 on the stack to preserve the invariant. If not, then codice_1 must be the root of its strongly connected component, which consists of codice_1 together with any nodes later on the stack than codice_1 (such nodes all have paths back to codice_1 but not to any earlier node, because if they had paths to earlier nodes then codice_1 would also have paths to earlier nodes which is false). The connected component rooted at codice_1 is then popped from the stack and returned, again preserving the invariant.

Each node codice_1 is assigned a unique integer codice_12, which numbers the nodes consecutively in the order in which they are discovered. It also maintains a value codice_13 that represents the smallest index of any node known to be reachable from codice_1 through codice_1's DFS subtree, including codice_1 itself. Therefore codice_1 must be left on the stack if codice_18, whereas v must be removed as the root of a strongly connected component if codice_19. The value codice_13 is computed during the depth-first search from codice_1, as this finds the nodes that are reachable from codice_1.

 algorithm tarjan is

The codice_23 variable is the depth-first search node number counter. codice_24 is the node stack, which starts out empty and stores the history of nodes explored but not yet committed to a strongly connected component. Note that this is not the normal depth-first search stack, as nodes are not popped as the search returns up the tree; they are only popped when an entire strongly connected component has been found.

The outermost loop searches each node that has not yet been visited, ensuring that nodes which are not reachable from the first node are still eventually traversed. The function codice_25 performs a single depth-first search of the graph, finding all successors from the node codice_1, and reporting all strongly connected components of that subgraph.

When each node finishes recursing, if its lowlink is still set to its index, then it is the root node of a strongly connected component, formed by all of the nodes above it on the stack. The algorithm pops the stack up to and including the current node, and presents all of these nodes as a strongly connected component.

Note that codice_27 is the correct way to update codice_28 if codice_29 is on stack. Because codice_29 is on the stack already, codice_31 is a back-edge in the DFS tree and therefore codice_29 is not in the subtree of codice_33. Because codice_28 takes into account nodes reachable only through the nodes in the subtree of codice_33 we must stop at codice_29 and use codice_37 instead of codice_38.

"Time Complexity": The Tarjan procedure is called once for each node; the forall statement considers each edge at most once. The algorithm's running time is therefore linear in the number of edges and nodes in G, i.e. formula_1.

In order to achieve this complexity, the test for whether codice_39 is on the stack should be done in constant time.
This may be done, for example, by storing a flag on each node that indicates whether it is on the stack, and performing this test by examining the flag.

"Space Complexity": The Tarjan procedure requires two words of supplementary data per vertex for the codice_23 and codice_41 fields, along with one bit for codice_42 and another for determining when codice_23 is undefined. In addition, one word is required on each stack frame to hold codice_1 and another for the current position in the edge list. Finally, the worst-case size of the stack codice_24 must be formula_2 (i.e. when the graph is one giant component). This gives a final analysis of formula_3 where formula_4 is the machine word size. The variation of Nuutila and Soisalon-Soininen reduced this to formula_5 and, subsequently, that of Pearce requires only formula_6.

While there is nothing special about the order of the nodes within each strongly connected component, one useful property of the algorithm is that no strongly connected component will be identified before any of its successors. Therefore, the order in which the strongly connected components are identified constitutes a reverse topological sort of the DAG formed by the strongly connected components.

Donald Knuth described Tarjan's algorithm as one of his favorite implementations in the book "The Stanford GraphBase".
He also wrote: 


</doc>
<doc id="12377419" url="https://en.wikipedia.org/wiki?curid=12377419" title="Path-based strong component algorithm">
Path-based strong component algorithm

In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by , , , , and ; of these, Dijkstra's version was the first to achieve linear time.

The algorithm performs a depth-first search of the given graph "G", maintaining as it does two stacks "S" and "P" (in addition to the normal call stack for a recursive function).
Stack "S" contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices.
Stack "P" contains vertices that have not yet been determined to belong to different strongly connected components from each other. It also uses a counter "C" of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.

When the depth-first search reaches a vertex "v", the algorithm performs the following steps:

The overall algorithm consists of a loop through the vertices of the graph, calling this recursive search on each vertex that does not yet have a preorder number assigned to it.

Like this algorithm, Tarjan's strongly connected components algorithm also uses depth first search together with a stack to keep track of vertices that have not yet been assigned to a component, and moves these vertices into a new component when it finishes expanding the final vertex of its component. However, in place of the stack "P", Tarjan's algorithm uses a vertex-indexed array of preorder numbers, assigned in the order that vertices are first visited in the depth-first search. The preorder array is used to keep track of when to form a new component.



</doc>
<doc id="2833097" url="https://en.wikipedia.org/wiki?curid=2833097" title="Reachability">
Reachability

In graph theory, reachability refers to the ability to get from one vertex to another within a graph. A vertex formula_1 can reach a vertex formula_2 (and formula_2 is reachable from formula_1) if there exists a sequence of adjacent vertices (i.e. a path) which starts with formula_1 and ends with formula_2.

In an undirected graph, reachability between all pairs of vertices can be determined by identifying the connected components of the graph. Any pair of vertices in such a graph can reach each other if and only if they belong to the same connected component; therefore, in such a graph, reachability is symmetric (formula_1 reaches formula_2 iff formula_2 reaches formula_1). The connected components of an undirected graph can be identified in linear time. The remainder of this article focuses on the more difficult problem of determining pairwise reachability in a directed graph (which, incidentally, need not be symmetric).

For a directed graph formula_11, with vertex set formula_12 and edge set formula_13, the reachability relation of formula_14 is the transitive closure of formula_13, which is to say the set of all ordered pairs formula_16 of vertices in formula_12 for which there exists a sequence of vertices formula_18 such that the edge formula_19 is in formula_13 for all formula_21.

If formula_14 is acyclic, then its reachability relation is a partial order; any partial order may be defined in this way, for instance as the reachability relation of its transitive reduction. A noteworthy consequence of this is that since partial orders are anti-symmetric, if formula_1 can reach formula_2, then we know that formula_2 "cannot" reach formula_1. Intuitively, if we could travel from formula_1 to formula_2 and back to formula_1, then formula_14 would contain a cycle, contradicting that it is acyclic.
If formula_14 is directed but "not" acyclic (i.e. it contains at least one cycle), then its reachability relation will correspond to a preorder instead of a partial order.

Algorithms for determining reachability fall into two classes: those that require preprocessing and those that do not.

If you have only one (or a few) queries to make, it may be more efficient to forgo the use of more complex data structures and compute the reachability of the desired pair directly. This can be accomplished in linear time using algorithms such as breadth first search or iterative deepening depth-first search.

If you will be making many queries, then a more sophisticated method may be used; the exact choice of method depends on the nature of the graph being analysed. In exchange for preprocessing time and some extra storage space, we can create a data structure which can then answer reachability queries on any pair of vertices in as low as formula_32 time. Three different algorithms and data structures for three different, increasingly specialized situations are outlined below.

The Floyd–Warshall algorithm can be used to compute the transitive closure of any directed graph, which gives rise to the reachability relation as in the definition, above.

The algorithm requires formula_33 time and formula_34 space in the worst case. This algorithm is not solely interested in reachability as it also computes the shortest path distance between all pairs of vertices. For graphs containing negative cycles, shortest paths may be undefined, but reachability between pairs can still be noted.

For planar digraphs, a much faster method is available, as described by Mikkel Thorup in 2004. This method can answer reachability queries on a planar graph in formula_32 time after spending formula_36 preprocessing time to create a data structure of formula_36 size. This algorithm can also supply approximate shortest path distances, as well as route information.

The overall approach is to associate with each vertex a relatively small set of so-called separator paths such that any path from a vertex formula_38 to any other vertex formula_39 must go through at least one of the separators associated with formula_38 or formula_39. An outline of the reachability related sections follows.

Given a graph formula_14, the algorithm begins by organizing the vertices into layers starting from an arbitrary vertex formula_43. The layers are built in alternating steps by first considering all vertices reachable "from" the previous step (starting with just formula_43) and then all vertices which reach "to" the previous step until all vertices have been assigned to a layer. By construction of the layers, every vertex appears at most two layers, and every directed path, or dipath, in formula_14 is contained within two adjacent layers formula_46 and formula_47. Let formula_48 be the last layer created, that is, the lowest value for formula_48 such that formula_50.

The graph is then re-expressed as a series of digraphs formula_51 where each formula_52 and where formula_53 is the contraction of all previous levels formula_54 into a single vertex. Because every dipath appears in at most two consecutive layers, and because
each formula_55 is formed by two consecutive layers, every dipath in formula_14 appears in its entirety in at least one formula_55 (and no more than 2 consecutive such graphs)

For each formula_55, three separators are identified which, when removed, break the graph into three components which each contain at most formula_59 the vertices of the original. As formula_55 is built from two layers of opposed dipaths, each separator may consist of up to 2 dipaths, for a total of up to 6 dipaths over all of the separators. Let formula_61 be this set of dipaths. The proof that such separators can always be found is related to the Planar Separator Theorem of Lipton and Tarjan, and these separators can be located in linear time.

For each formula_62, the directed nature of formula_63 provides for a natural indexing of its vertices from the start to the end of the path. For each vertex formula_38 in formula_55, we locate the first vertex in formula_63 reachable by formula_38, and the last vertex in formula_63 that reaches to formula_38. That is, we are looking at how early into formula_63 we can get from formula_38, and how far
we can stay in formula_63 and still get back to formula_38. This information is stored with
each formula_38. Then for any pair of vertices formula_75 and formula_39, formula_75 can reach formula_39 "via" formula_63 if formula_75 connects to formula_63 earlier than formula_39 connects from formula_63.

Every vertex is labelled as above for each step of the recursion which builds
formula_84. As this recursion has logarithmic depth, a total of
formula_85 extra information is stored per vertex. From this point, a
logarithmic time query for reachability is as simple as looking over each pair
of labels for a common, suitable formula_63. The original paper then works to tune the
query time down to formula_32.

In summarizing the analysis of this method, first consider that the layering
approach partitions the vertices so that each vertex is considered only formula_32
times. The separator phase of the algorithm breaks the graph into components
which are at most formula_59 the size of the original graph, resulting in a
logarithmic recursion depth. At each level of the recursion, only linear work
is needed to identify the separators as well as the connections possible between
vertices. The overall result is formula_90 preprocessing time with only
formula_85 additional information stored for each vertex.

An even faster method for pre-processing, due to T. Kameda in 1975,
can be used if the graph is planar, acyclic, and also exhibits the following additional properties: all 0-indegree and all 0-outdegree vertices appear on the same face (often assumed to be the outer face), and it is possible to partition the boundary of that face into two parts such that all 0-indegree vertices appear on one part, and all
0-outdegree vertices appear on the other (i.e. the two types of vertices do not alternate).

If formula_14 exhibits these properties, then we can preprocess the graph in only
formula_93 time, and store only formula_85 extra bits per vertex, answering
reachability queries for any pair of vertices in formula_32 time with a simple
comparison.

Preprocessing performs the following steps. We add a new vertex formula_1 which has an edge to each 0-indegree vertex, and another new vertex formula_2 with edges from each 0-outdegree vertex. Note that the properties of formula_14 allow us to do so while maintaining planarity, that is, there will still be no edge crossings after these additions. For each vertex we store the list of adjacencies (out-edges) in order of the planarity of the graph (for example, clockwise with respect to the graph's embedding). We then initialize a counter formula_99 and begin a Depth-First Traversal from formula_1. During this traversal, the adjacency list of each vertex is visited from left-to-right as needed. As vertices are popped from the traversal's stack, they are labelled with the value formula_101, and formula_101 is then decremented. Note that formula_2 is always labelled with the value formula_104 and formula_1 is always labelled with formula_106. The depth-first traversal is then repeated, but this time the adjacency list of each vertex is visited from right-to-left.

When completed, formula_1 and formula_2, and their incident edges, are removed. Each
remaining vertex stores a 2-dimensional label with values from formula_109 to formula_110.
Given two vertices formula_75 and formula_38, and their labels formula_113 and formula_114, we say that formula_115 if and only if formula_116, formula_117, and there exists at least one component formula_118 or formula_119 which is strictly
less than formula_120 or formula_121, respectively.

The main result of this method then states that formula_38 is reachable from formula_75 if and
only if formula_115, which is easily calculated in formula_32 time.

A related problem is to solve reachability queries with some number formula_48 of vertex failures. For example: "Can vertex formula_75 still reach vertex formula_38 even though vertices formula_129 have failed and can no longer be used?" A similar problem may consider edge failures rather than vertex failures, or a mix of the two. The breadth-first search technique works just as well on such queries, but constructing an efficient oracle is more challenging.

Another problem related to reachability queries is in quickly recalculating changes to reachability relationships when some portion of the graph is changed. For example, this is a relevant concern to garbage collection which needs to balance the reclamation of memory (so that it may be reallocated) with the performance concerns of the running application.



</doc>
<doc id="292239" url="https://en.wikipedia.org/wiki?curid=292239" title="Transitive closure">
Transitive closure

In mathematics, the transitive closure of a binary relation "R" on a set "X" is the smallest relation on "X" that contains "R" and is transitive.

For example, if "X" is a set of airports and "xRy" means "there is a direct flight from airport "x" to airport "y"" (for "x" and "y" in "X"), then the transitive closure of "R" on "X" is the relation "R" such that "x R y" means "it is possible to fly from "x" to "y" in one or more flights". Informally, the "transitive closure" gives you the set of all places you can get to from any starting place.

More formally, the transitive closure of a binary relation "R" on a set "X" is the transitive relation "R" on set "X" such that "R" contains "R" and "R" is minimal . If the binary relation itself is transitive, then the transitive closure is that same binary relation; otherwise, the transitive closure is a different relation.

A relation "R" on a set "X" is transitive if, for all "x", "y", "z" in "X", whenever and then . Examples of transitive relations include the equality relation on any set, the "less than or equal" relation on any linearly ordered set, and the relation ""x" was born before "y"" on the set of all people. Symbolically, this can be denoted as: if and then .

One example of a non-transitive relation is "city "x" can be reached via a direct flight from city "y"" on the set of all cities. Simply because there is a direct flight from one city to a second city, and a direct flight from the second city to the third, does not imply there is a direct flight from the first city to the third. The transitive closure of this relation is a different relation, namely "there is a sequence of direct flights that begins at city "x" and ends at city "y". Every relation can be extended in a similar way to a transitive relation.

An example of a non-transitive relation with a less meaningful transitive closure is "x" is the day of the week after "y"". The transitive closure of this relation is "some day "x" comes after a day "y" on the calendar", which is trivially true for all days of the week "x" and "y" (and thus equivalent to the Cartesian square, which is ""x" and "y" are both days of the week").

For any relation "R", the transitive closure of "R" always exists. To see this, note that the intersection of any family of transitive relations is again transitive. Furthermore, there exists at least one transitive relation containing "R", namely the trivial one: "X" × "X". The transitive closure of "R" is then given by the intersection of all transitive relations containing "R".

For finite sets, we can construct the transitive closure step by step, starting from "R" and adding transitive edges.
This gives the intuition for a general construction. For any set "X", we
can prove that transitive closure is given by the following expression
where formula_2 is the "i"-th power of "R", defined inductively by
and, for formula_4,
where formula_6 denotes composition of relations.

To show that the above definition of "R" is the least transitive relation containing "R", we show that it contains "R", that it is transitive, and that it is the smallest set with both of those characteristics.


The intersection of two transitive relations is transitive.

The union of two transitive relations need not be transitive. To preserve transitivity, one must take the transitive closure. This occurs, for example, when taking the union of two equivalence relations or two preorders. To obtain a new equivalence relation or preorder one must take the transitive closure (reflexivity and symmetry—in the case of equivalence relations—are automatic).

In computer science, the concept of transitive closure can be thought of as constructing a data structure that makes it possible to answer reachability questions. That is, can one get from node "a" to node "d" in one or more hops? A binary relation tells you only that node a is connected to node "b", and that node "b" is connected to node "c", etc. After the transitive closure is constructed, as depicted in the following figure, in an O(1) operation one may determine that node "d" is reachable from node "a". The data structure is typically stored as a matrix, so if matrix[1][4] = 1, then it is the case that node 1 can reach node 4 through one or more hops.

The transitive closure of the adjacency relation of a directed acyclic graph (DAG) is the reachability relation of the DAG and a strict partial order.

The transitive closure of a binary relation cannot, in general, be expressed in first-order logic (FO).
This means that one cannot write a formula using predicate symbols "R" and "T" that will be satisfied in
any model if and only if "T" is the transitive closure of "R".
In finite model theory, first-order logic (FO) extended with a transitive closure operator is usually called transitive closure logic, and abbreviated FO(TC) or just TC. TC is a sub-type of fixpoint logics. The fact that FO(TC) is strictly more expressive than FO was discovered by Ronald Fagin in 1974; the result was then rediscovered by Alfred Aho and Jeffrey Ullman in 1979, who proposed to use fixpoint logic as a database query language (Libkin 2004:vii). With more recent concepts of finite model theory, proof that FO(TC) is strictly more expressive than FO follows immediately from the fact that FO(TC) is not Gaifman-local (Libkin 2004:49).

In computational complexity theory, the complexity class NL corresponds precisely to the set of logical sentences expressible in TC. This is because the transitive closure property has a close relationship with the NL-complete problem STCON for finding directed paths in a graph. Similarly, the class L is first-order logic with the commutative, transitive closure. When transitive closure is added to second-order logic instead, we obtain PSPACE.

Since the 1980s Oracle Database has implemented a proprietary SQL extension CONNECT BY... START WITH that allows the computation of a transitive closure as part of a declarative query. The SQL 3 (1999) standard added a more general WITH RECURSIVE construct also allowing transitive closures to be computed inside the query processor; as of 2011 the latter is implemented in IBM DB2, Microsoft SQL Server, Oracle, and PostgreSQL, although not in MySQL (Benedikt and Senellart 2011:189).

Datalog also implements transitive closure computations (Silberschatz et al. 2010:C.3.6).

Efficient algorithms for computing the transitive closure of the adjacency relation of a graph can be found in Nuutila (1995). The fastest worst-case methods, which are not practical, reduce the problem to matrix multiplication. The problem can also be solved by the Floyd–Warshall algorithm, or by repeated breadth-first search or depth-first search starting from each node of the graph.

More recent research has explored efficient ways of computing transitive closure on distributed systems based on the MapReduce paradigm (Afrati et al. 2011).





</doc>
<doc id="3757117" url="https://en.wikipedia.org/wiki?curid=3757117" title="Transitive reduction">
Transitive reduction

In mathematics, a transitive reduction of a directed graph "D" is another directed graph with the same vertices and as few edges as possible, such that if there is a (directed) path from vertex "v" to vertex "w" in "D", then there is also such a path in the reduction. Transitive reductions were introduced by , who provided tight bounds on the computational complexity of constructing them.

More technically, the reduction is a directed graph that has the same reachability relation as "D". Equivalently, "D" and its transitive reduction should have the same transitive closure as each other, and its transitive reduction should have as few edges as possible among all graphs with this property. 

The transitive reduction of a finite directed acyclic graph (a directed graph without directed cycles) is unique and is a subgraph of the given graph. However, uniqueness fails for graphs with (directed) cycles, and for infinite graphs not even existence is guaranteed. 

The closely related concept of a minimum equivalent graph is a subgraph of "D" that has the same reachability relation and as few edges as possible. The difference is that a transitive reduction does not have to be a subgraph of "D". For finite directed acyclic graphs, the minimum equivalent graph is the same as the transitive reduction. However, for graphs that may contain cycles, minimum equivalent graphs are NP-hard to construct, while transitive reductions can be constructed in polynomial time. 

Transitive reduction can be defined for an abstract binary relation on a set, by interpreting the pairs of the relation as arcs in a directed graph.

The transitive reduction of a finite directed graph "G" is a graph with the fewest possible edges that has the same reachability relation as the original graph. That is, if there is a path from a vertex "x" to a vertex "y" in graph "G", there must also be a path from "x" to "y" in the transitive reduction of "G", and vice versa. The following image displays drawings of graphs corresponding to a non-transitive binary relation (on the left) and its transitive reduction (on the right).

The transitive reduction of a finite directed acyclic graph "G" is unique, and consists of the edges of "G" that form the only path between their endpoints. In particular, it is always a subgraph of the given graph. For this reason, the transitive reduction coincides with the minimum equivalent graph in this case.

In the mathematical theory of binary relations, any relation "R" on a set "X" may be thought of as a directed graph that has the set "X" as its vertex set and that has an arc "xy" for every ordered pair of elements that are related in "R". In particular, this method lets partially ordered sets be reinterpreted as directed acyclic graphs, in which there is an arc "xy" in the graph whenever there is an order relation "x" < "y" between the given pair of elements of the partial order. When the transitive reduction operation is applied to a directed acyclic graph that has been constructed in this way, it generates the covering relation of the partial order, which is frequently given visual expression by means of a Hasse diagram.

Transitive reduction has been used on networks which can be represented as directed acyclic graphs (e.g. citation graphs or citation networks) to reveal structural differences between networks.

In a finite graph that may have cycles, the transitive reduction is not unique: there may be more than one graph on the same vertex set that has a minimum number of edges and has the same reachability relation as the given graph. Additionally, it may be the case that none of these minimum graphs is a subgraph of the given graph. Nevertheless, it is straightforward to characterize the minimum graphs with the same reachability relation as the given graph "G". If "G" is an arbitrary directed graph, and "H" is a graph with the minimum possible number of edges having the same reachability relation as "G", then "H" consists of
The total number of edges in this type of transitive reduction is then equal to the number of edges in the transitive reduction of the condensation, plus the number of vertices in nontrivial strongly connected components (components with more than one vertex).

The edges of the transitive reduction that correspond to condensation edges can always be chosen to be a subgraph of the given graph "G". However, the cycle within each strongly connected component can only be chosen to be a subgraph of "G" if that component has a Hamiltonian cycle, something that is not always true and is difficult to check. Because of this difficulty, it is NP-hard to find the smallest subgraph of a given graph "G" with the same reachability (its minimum equivalent graph).

As Aho et al. show, when the time complexity of graph algorithms is measured only as a function of the number "n" of vertices in the graph, and not as a function of the number of edges, transitive closure and transitive reduction of directed acyclic graphs have the same complexity. It had already been shown that transitive closure and multiplication of Boolean matrices of size "n" × "n" had the same complexity as each other, so this result put transitive reduction into the same class. The fastest known exact algorithms for matrix multiplication, as of 2015, take time O("n"), and this gives the fastest known worst-case time bound for transitive reduction in dense graphs.

To prove that transitive reduction is as easy as transitive closure, Aho et al. rely on the already-known equivalence with Boolean matrix multiplication. They let "A" be the adjacency matrix of the given directed acyclic graph, and "B" be the adjacency matrix of its transitive closure (computed using any standard transitive closure algorithm). Then an edge "uv" belongs to the transitive reduction if and only if there is a nonzero entry in row "u" and column "v" of matrix "A", and there is a zero entry in the same position of the matrix product "AB". In this construction, the nonzero elements of the matrix "AB" represent pairs of vertices connected by paths of length two or more.

To prove that transitive reduction is as hard as transitive closure, Aho et al. construct from a given directed acyclic graph "G" another graph "H", in which each vertex of "G" is replaced by a path of three vertices, and each edge of "G" corresponds to an edge in "H" connecting the corresponding middle vertices of these paths. In addition, in the graph "H", Aho et al. add an edge from every path start to every path end. In the transitive reduction of "H", there is an edge from the path start for "u" to the path end for "v", if and only if edge "uv" does not belong to the transitive closure of "G". Therefore, if the transitive reduction of "H" can be computed efficiently, the transitive closure of "G" can be read off directly from it.

When measured both in terms of the number "n" of vertices and the number "m" of edges in a directed acyclic graph, transitive reductions can also be found in time O("nm"), a bound that may be faster than the matrix multiplication methods for sparse graphs. To do so, collect edges ("u","v") such that the longest-path distance from "u" to "v" is one, calculating those distances by linear-time search from each possible starting vertex, "u". This O("nm") time bound matches the complexity of constructing transitive closures by using depth first search or breadth first search to find the vertices reachable from every choice of starting vertex, so again with these assumptions transitive closures and transitive reductions can be found in the same amount of time.



</doc>
<doc id="41985" url="https://en.wikipedia.org/wiki?curid=41985" title="Shortest path problem">
Shortest path problem

In graph theory, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized.

The problem of finding the shortest path between two intersections on a road map may be modeled as a special case of the shortest path problem in graphs, where the vertices correspond to intersections and the edges correspond to road segments, each weighted by the length of the segment.

The shortest path problem can be defined for graphs whether undirected, directed, or mixed.
It is defined here for undirected graphs; for directed graphs the definition of path
requires that consecutive vertices be connected by an appropriate directed edge.

Two vertices are adjacent when they are both incident to a common edge.
A path in an undirected graph is a sequence of vertices formula_1
such that formula_2 is adjacent to formula_3 for formula_4.
Such a path formula_5 is called a path of length formula_6
from formula_7 to formula_8.

Let formula_10 be the edge incident to both formula_2 and formula_12. Given a real-valued weight function formula_13, and an undirected (simple) graph formula_14, the shortest path from formula_15 to formula_16 is the path formula_17 (where formula_18 and formula_19) that over all possible formula_20 minimizes the sum formula_21 When each edge in the graph has unit weight or formula_22, this is equivalent to finding the path with fewest edges.

The problem is also sometimes called the single-pair shortest path problem, to distinguish it from the following variations:

These generalizations have significantly more efficient algorithms than the simplistic approach of running a single-pair shortest path algorithm on all relevant pairs of vertices.

The most important algorithms for solving this problem are:

Additional algorithms and associated evaluations may be found in .

An algorithm using topological sorting can solve the single-source shortest path problem in linear time, , in weighted DAGs.

The following table is taken from , with some corrections and additions.
A green background indicates an asymptotically best bound in the table; "L" is the maximum length (or weight) among all edges, assuming integer edge weights.

The all-pairs shortest path problem finds the shortest paths between every pair of vertices , in the graph. The all-pairs shortest paths problem for unweighted directed graphs was introduced by , who observed that it could be solved by a linear number of matrix multiplications that takes a total time of .

Shortest path algorithms are applied to automatically find directions between physical locations, such as driving directions on web mapping websites like MapQuest or Google Maps. For this application fast specialized algorithms are available.

If one represents a nondeterministic abstract machine as a graph where vertices describe states and edges describe possible transitions, shortest path algorithms can be used to find an optimal sequence of choices to reach a certain goal state, or to establish lower bounds on the time needed to reach a given state. For example, if vertices represent the states of a puzzle like a Rubik's Cube and each directed edge corresponds to a single move or turn, shortest path algorithms can be used to find a solution that uses the minimum possible number of moves.

In a networking or telecommunications mindset, this shortest path problem is sometimes called the min-delay path problem and usually tied with a widest path problem. For example, the algorithm may seek the shortest (min-delay) widest path, or widest shortest (min-delay) path.

A more lighthearted application is the games of "six degrees of separation" that try to find the shortest path in graphs like movie stars appearing in the same film.

Other applications, often studied in operations research, include plant and facility layout, robotics, transportation, and VLSI design.

A road network can be considered as a graph with positive weights. The nodes represent road junctions and each edge of the graph is associated with a road segment between two junctions. The weight of an edge may correspond to the length of the associated road segment, the time needed to traverse the segment, or the cost of traversing the segment. Using directed edges it is also possible to model one-way streets. Such graphs are special in the sense that some edges are more important than others for long distance travel (e.g. highways). This property has been formalized using the notion of highway dimension. There are a great number of algorithms that exploit this property and are therefore able to compute the shortest path a lot quicker than would be possible on general graphs.

All of these algorithms work in two phases. In the first phase, the graph is preprocessed without knowing the source or target node. The second phase is the query phase. In this phase, source and target node are known. The idea is that the road network is static, so the preprocessing phase can be done once and used for a large number of queries on the same road network.

The algorithm with the fastest known query time is called hub labeling and is able to compute shortest path on the road networks of Europe or the USA in a fraction of a microsecond. Other techniques that have been used are:


For shortest path problems in computational geometry, see Euclidean shortest path.

The travelling salesman problem is the problem of finding the shortest path that goes through every vertex exactly once, and returns to the start. Unlike the shortest path problem, which can be solved in polynomial time in graphs without negative cycles, the travelling salesman problem is NP-complete and, as such, is believed not to be efficiently solvable for large sets of data (see P = NP problem). The problem of finding the longest path in a graph is also NP-complete.

The Canadian traveller problem and the stochastic shortest path problem are generalizations where either the graph isn't completely known to the mover, changes over time, or where actions (traversals) are probabilistic.

The shortest multiple disconnected path is a representation of the primitive path network within the framework of Reptation theory.

The widest path problem seeks a path so that the minimum label of any edge is as large as possible.

Sometimes, the edges in a graph have personalities: each edge has its own selfish interest. An example is a communication network, in which each edge is a computer that possibly belongs to a different person. Different computers have different transmission speeds, so every edge in the network has a numeric weight equal to the number of milliseconds it takes to transmit a message. Our goal is to send a message between two points in the network in the shortest time possible. If we know the transmission-time of each computer (the weight of each edge), then we can use a standard shortest-paths algorithm. If we do not know the transmission times, then we have to ask each computer to tell us its transmission-time. But, the computers may be selfish: a computer might tell us that its transmission time is very long, so that we will not bother it with our messages. A possible solution to this problem is to use a variant of the VCG mechanism, which gives the computers an incentive to reveal their true weights.

There is a natural linear programming formulation for the shortest path problem, given below. It is very simple compared to most other uses of linear programs in discrete optimization, however it illustrates connections to other concepts.

Given a directed graph ("V", "A") with source node "s", target node "t", and cost "w" for each edge ("i", "j") in "A", consider the program with variables "x"

The intuition behind this is that formula_26 is an indicator variable for whether edge ("i", "j") is part of the shortest path: 1 when it is, and 0 if it is not. We wish to select the set of edges with minimal weight, subject to the constraint that this set forms a path from "s" to "t" (represented by the equality constraint: for all vertices except "s" and "t" the number of incoming and outcoming edges that are part of the path must be the same (i.e., that it should be a path from s to t).

This LP has the special property that it is integral; more specifically, every basic optimal solution (when one exists) has all variables equal to 0 or 1, and the set of edges whose variables equal 1 form an "s"-"t" dipath. See Ahuja et al. for one proof, although the origin of this approach dates back to mid-20th century.

The dual for this linear program is
and feasible duals correspond to the concept of a consistent heuristic for the A* algorithm for shortest paths. For any feasible dual "y" the reduced costs formula_27 are nonnegative and A* essentially runs Dijkstra's algorithm on these reduced costs.

Many problems can be framed as a form of the shortest path for some suitably substituted notions of addition along a path and taking the minimum. The general approach to these is to consider the two operations to be those of a semiring. Semiring multiplication is done along the path, and the addition is between paths. This general framework is known as the algebraic path problem.

Most of the classic shortest-path algorithms (and new ones) can be formulated as solving linear systems over such algebraic structures.

More recently, an even more general framework for solving these (and much less obviously related problems) has been developed under the banner of valuation algebras.

In real-life situations, the transportation network is usually stochastic and time-dependent. In fact, a traveler traversing a link daily may experiences different travel times on that link due not only to the fluctuations in travel demand (origin-destination matrix) but also due to such incidents as work zones, bad weather conditions, accidents and vehicle breakdowns. As a result, a stochastic time-dependent (STD) network is a more realistic representation of an actual road network compared with the deterministic one.

Despite considerable progress during the course of the past decade, it remains a controversial question how an optimal path should be defined and identified in stochastic road networks. In other words, there is no unique definition of an optimal path under uncertainty. One possible and common answer to this question is to find a path with the minimum expected travel time. The main advantage of using this approach is that efficient shortest path algorithms introduced for the deterministic networks can be readily employed to identify the path with the minimum expected travel time in a stochastic network. However, the resulting optimal path identified by this approach may not be reliable, because this approach fails to address travel time variability. To tackle this issue some researchers use distribution of travel time instead of expected value of it so they find the probability distribution of total travelling time using different optimization methods such as dynamic programming and Dijkstra's algorithm . These methods use stochastic optimization, specifically stochastic dynamic programming to find the shortest path in networks with probabilistic arc length. The concept of travel time reliability is used interchangeably with travel time variability in the transportation research literature, so that, in general, one can say that the higher the variability in travel time, the lower the reliability would be, and vice versa.

In order to account for travel time reliability more accurately, two common alternative definitions for an optimal path under uncertainty have been suggested. Some have introduced the concept of the most reliable path, aiming to maximize the probability of arriving on time or earlier than a given travel time budget. Others, alternatively, have put forward the concept of an α-reliable path based on which they intended to minimize the travel time budget required to ensure a pre-specified on-time arrival probability.




</doc>
<doc id="3157516" url="https://en.wikipedia.org/wiki?curid=3157516" title="Bidirectional search">
Bidirectional search

Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet. The reason for this approach is that in many cases it is faster: for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor "b", and the distance from start to goal is "d", each of the two searches has complexity "O"("b") (in Big O notation), and the sum of these two search times is much less than the "O"("b") complexity that would result from a single search from the beginning to the goal.

Andrew Goldberg and others explained the correct termination conditions for the bidirectional version of Dijkstra’s Algorithm.

As in A* search, bi-directional search can be guided by a heuristic estimate of the remaining distance to the goal (in the forward tree) or from the start (in the backward tree).

A solution found by the uni-directional A* algorithm using an admissible heuristic has a shortest path length; the same property holds for the BHFFA2 bidirectional heuristic version described in de Champeaux (1983). BHFFA2 has, among others, more careful termination conditions than BHFFA.

A Bidirectional Heuristic Search is a state space search from some state formula_1 to another state formula_2, searching from formula_1 to formula_2 and from formula_2 to formula_1 simultaneously. It returns a valid list of operators that if applied to formula_1 will give us formula_2.

While it may seem as though the operators have to be invertible for the reverse search, it is only necessary to be able to find, given any node formula_9, the set of parent nodes of formula_9 such that there exists some valid operator from each of the parent nodes to formula_9. This has often been likened to a one-way street in the route-finding domain: it is not necessary to be able to travel down both directions, but it is necessary when standing at the end of the street to determine the beginning of the street as a possible route.

Similarly, for those edges that have inverse arcs (i.e. arcs going in both directions) it is not necessary that each direction be of equal cost. The reverse search will always use the inverse cost (i.e. the cost of the arc in the forward direction). More formally, if formula_9 is a node with parent formula_13, then formula_14, defined as being the cost from formula_13 to formula_9.(Auer Kaindl 2004)


Bidirectional algorithms can be broadly split into three categories: Front-to-Front, Front-to-Back (or Front-to-End), and Perimeter Search (Kaindl Kainz 1997). These differ by the function used to calculate the heuristic.

Front-to-Back algorithms calculate the formula_42 value of a node formula_19 by using the heuristic estimate between formula_19 and the root of the opposite search tree, formula_25 or formula_26.

Front-to-Back is the most actively researched of the three categories. The current best algorithm (at least in the Fifteen puzzle domain) is the BiMAX-BS*F algorithm, created by Auer and Kaindl (Auer, Kaindl 2004).

Front-to-Front algorithms calculate the value of a node by using the heuristic estimate between and some subset of formula_47. The canonical example is that of the BHFFA (Bidirectional Heuristic Front-to-Front Algorithm), where the function is defined as the minimum of all heuristic estimates between the current node and the nodes on the opposing front. Or, formally:

where formula_49 returns an admissible (i.e. not overestimating) heuristic estimate of the distance between nodes and .

Front-to-Front suffers from being excessively computationally demanding. Every time a node is put into the open list, its formula_50 value must be calculated. This involves calculating a heuristic estimate from to every node in the opposing set, as described above. The sets increase in size exponentially for all domains with .



</doc>
<doc id="100558" url="https://en.wikipedia.org/wiki?curid=100558" title="A* search algorithm">
A* search algorithm

A* (pronounced "A-star") is a graph traversal and path search algorithm, which is often used in computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is its formula_1 space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.

Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Edsger Dijkstra's 1959 algorithm. A* achieves better performance by using heuristics to guide its search.

A* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions.  Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey's path planning. Graph Traverser is guided by a heuristic function formula_2 the estimated distance from node formula_3 to the goal node: it entirely ignores formula_4 the distance from the start node to formula_5 Bertram Raphael suggested using the sum, formula_6. Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions.  A* was originally designed for finding least-cost paths when the cost of a path is the sum of its edge costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.

The original 1968 A* paper contained a theorem that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*’s tie-breaking rule is suitably chosen. A ″correction″ was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl’s definitive study of A*'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.

A* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.). It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.

At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizes

where is the next node on the path, is the cost of the path from the start node to , and is a heuristic function that estimates the cost of the cheapest path from to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.

Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the "open set" or "fringe". At each step of the algorithm, the node with the lowest value is removed from the queue, the and values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a goal node has a lower value than any node in the queue (or until the queue is empty). The value of the goal is then the cost of the shortest path, since at the goal is zero in an admissible heuristic.

The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node's predecessor is the start node.

As an example, when searching for the shortest route on a map, might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points.

If the heuristic satisfies the additional condition for every edge of the graph (where denotes the length of that edge), then is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra's algorithm with the reduced cost .

The following pseudocode describes the algorithm:

Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not consistent. If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again.

An example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the straight-line distance to target point:

Key: green: start; blue: goal; orange: visited

The A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.

There are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation. The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations. If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).

When a path is required at the end of the search, it is common to keep with each node a reference to that node's parent. At the end of the search these references can be used to recover the optimal path. If these references are being kept then it can be important that the same node doesn't appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost). A standard approach here is to check if a node about to be added already appears in the priority queue. If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.

Dijkstra's algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* where for all "x". General depth-first search can be implemented using A* by considering that there is a global counter "C" initialized with a very large value. Every time we process a node we assign "C" to all of its newly discovered neighbors. After each single assignment, we decrease the counter "C" by one. Thus the earlier a node is discovered, the higher its value. Both Dijkstra's algorithm and depth-first search can be implemented more efficiently without including an value at each node.

On finite graphs with non-negative edge weights A* is guaranteed to terminate and is "complete", i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (formula_8 for some fixed formula_9), A* is guaranteed to terminate only if there exists a solution.

A search algorithm is said to be "admissible" if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive ″proof″ of this is as follows:

When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node's value). When the heuristic is admissible, those estimates are optimistic (not quite—see the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.

The actual proof is a bit more involved because the values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because the values of open nodes are not guaranteed to be optimal, so the sum is not guaranteed to be optimistic.

Algorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A′ in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.
They considered a variety of definitions of Alts and P in combination with A*'s heuristic being merely admissible or being both consistent and admissible. The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all ″non-pathological″ search problems. Roughly speaking, their notion of non-pathological problem is what we now mean by ″up to tie-breaking″. This result does not hold if A*'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.

Optimal efficiency is about the "set" of nodes expanded, not the "number" of node expansions (the number of iterations of A*'s main loop). When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.
In such circumstances Dijkstra's algorithm could outperform A* by a large margin.

While the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + "ε") times the optimal solution path. This new guarantee is referred to as "ε"-admissible.

There are a number of "ε"-admissible algorithms:







The time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) : , where is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.

The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the nodes that an uninformed search would expand. Its quality can be expressed in terms of the "effective" branching factor , which can be determined empirically for a problem instance by measuring the number of nodes expanded, , and the depth of the solution, then solving

Good heuristics are those with low effective branching factor (the optimal being ).

The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function "h" meets the following condition:

where is the optimal heuristic, the exact cost to get from to the goal. In other words, the error of will not grow faster than the logarithm of the "perfect heuristic" that returns the true distance from to the goal.

The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.

A* is commonly used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.
It finds applications to diverse problems, including the problem of parsing using stochastic grammars in NLP.
Other cases include an Informational search with online learning.

What sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, , into account.

Some common variants of Dijkstra's algorithm can be viewed as a special case of A* where the heuristic formula_17 for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.
A* itself is a special case of a generalization of branch and bound.


A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.




</doc>
<doc id="18757567" url="https://en.wikipedia.org/wiki?curid=18757567" title="Longest path problem">
Longest path problem

In graph theory and theoretical computer science, the longest path problem is the problem of finding a simple path of maximum length in a given graph. A path is called simple if it does not have any repeated vertices; the length of a path may either be measured by its number of edges, or (in weighted graphs) by the sum of the weights of its edges. In contrast to the shortest path problem, which can be solved in polynomial time in graphs without negative-weight cycles, the longest path problem is NP-hard and the decision version of the problem, which asks whether a path exists of at least some given length, is NP-complete. This means that the decision problem cannot be solved in polynomial time for arbitrary graphs unless P = NP. Stronger hardness results are also known showing that it is difficult to approximate. However, it has a linear time solution for directed acyclic graphs, which has important applications in finding the critical path in scheduling problems.

The NP-hardness of the unweighted longest path problem can be shown using a reduction from the Hamiltonian path problem: a graph "G" has a Hamiltonian path if and only if its longest path has length "n" − 1, where "n" is the number of vertices in "G". Because the Hamiltonian path problem is NP-complete, this reduction shows that the decision version of the longest path problem is also NP-complete. In this decision problem, the input is a graph "G" and a number "k"; the desired output is "yes" if "G" contains a path of "k" or more edges, and "no" otherwise.

If the longest path problem could be solved in polynomial time, it could be used to solve this decision problem, by finding a longest path and then comparing its length to the number "k". Therefore, the longest path problem is NP-hard. The question "does there exist a simple path in a given graph with at least "k" edges" is NP-complete.

In weighted complete graphs with non-negative edge weights, the weighted longest path problem is the same as the Travelling salesman path problem, because the longest path always includes all vertices.

A longest path between two given vertices "s" and "t" in a weighted graph "G" is the same thing as a shortest path in a graph −"G" derived from "G" by changing every weight to its negation. Therefore, if shortest paths can be found in −"G", then longest paths can also be found in "G".

For most graphs, this transformation is not useful because it creates cycles of negative length in −"G". But if "G" is a directed acyclic graph, then no negative cycles can be created, and a longest path in "G" can be found in linear time by applying a linear time algorithm for shortest paths in −"G", which is also a directed acyclic graph. For instance, for each vertex "v" in a given DAG, the length of the longest path ending at "v" may be obtained by the following steps:
Once this has been done, the longest path in the whole DAG may be obtained by starting at the vertex "v" with the largest recorded value, then repeatedly stepping backwards to its incoming neighbor with the largest recorded value, and reversing the sequence of vertices found in this way.

The critical path method for scheduling a set of activities involves the construction of a directed acyclic graph in which the vertices represent project milestones and the edges represent activities that must be performed after one milestone and before another; each edge is weighted by an estimate of the amount of time the corresponding activity will take to complete. In such a graph, the longest path from the first milestone to the last one is the critical path, which describes the total time for completing the project.

Longest paths of directed acyclic graphs may also be applied in layered graph drawing: assigning each vertex "v" of a directed acyclic graph "G" to the layer whose number is the length of the longest path ending at "v" results in a layer assignment for "G" with the minimum possible number of layers.

 write that the longest path problem in unweighted undirected graphs "is notorious for the difficulty of understanding its approximation hardness".
The best polynomial time approximation algorithm known for this case achieves only a very weak approximation ratio, formula_1. For all formula_2, it is not possible to approximate the longest path to within a factor of formula_3 unless NP is contained within quasi-polynomial deterministic time; however, there is a big gap between this inapproximability result and the known approximation algorithms for this problem.

In the case of unweighted but directed graphs, strong inapproximability results are known. For every formula_2 the problem cannot be approximated to within a factor of formula_5 unless P = NP, and with stronger complexity-theoretic assumptions it cannot be approximated to within a factor of formula_6. The color-coding technique can be used to find paths of logarithmic length, if they exist, but this gives an approximation ratio of only formula_7.

The longest path problem is fixed-parameter tractable when parameterized by the length of the path. For instance, it can be solved in time linear in the size of the input graph (but exponential in the length of the path), by an algorithm that performs the following steps:
Since the output path has length at least as large as formula_8, the running time is also bounded by formula_13, where formula_14 is the length of the longest path. Using color-coding, the dependence on path length can be reduced to singly exponential. A similar dynamic programming technique shows that the longest path problem is also fixed-parameter tractable when parameterized by the treewidth of the graph.

For graphs of bounded clique-width, the longest path can also be solved by a polynomial time dynamic programming algorithm. However, the exponent of the polynomial depends on the clique-width of the graph, so this algorithms is not fixed-parameter tractable. The longest path problem, parameterized by clique-width, is hard for the parameterized complexity class formula_15, showing that a fixed-parameter tractable algorithm is unlikely to exist.

A linear-time algorithm for finding a longest path in a tree was proposed by Dijkstra in 1960's, while a formal proof of this algorithm was published in 2002.
Furthermore, a longest path can be computed in polynomial time on weighted trees, on block graphs, on cacti,
on bipartite permutation graphs,
and on Ptolemaic graphs.

For the class of interval graphs, an formula_16-time algorithm is known, which uses a dynamic programming approach.
This dynamic programming approach has been exploited to obtain polynomial-time algorithms on the greater classes of circular-arc graphs
and of co-comparability graphs (i.e. of the complements of comparability graphs, which also contain permutation graphs),
both having the same running time formula_16. The latter algorithm is based on special properties of the Lexicographic Depth First Search (LDFS) vertex ordering
of co-comparability graphs. For co-comparability graphs also an alternative polynomial-time algorithm with higher running time formula_18 is known, which is based on the Hasse diagram of the partially ordered set defined by the complement of the input co-comparability graph.

Furthermore, the longest path problem is solvable in polynomial time on any class of graphs with bounded treewidth or bounded clique-width, such as the distance-hereditary graphs. Finally, it is clearly NP-hard on all graph classes on which the Hamiltonian path problem is NP-hard, such as on split graphs, circle graphs, and planar graphs.




</doc>
<doc id="31567349" url="https://en.wikipedia.org/wiki?curid=31567349" title="Widest path problem">
Widest path problem

In graph algorithms, the widest path problem is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path. The widest path problem is also known as the bottleneck shortest path problem or the maximum capacity path problem. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.

For instance, in a graph that represents connections between routers in the Internet, where the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth. The smallest edge weight on this path is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic pathway analysis, and the computation of maximum flows.

A closely related problem, the minimax path problem, asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.

In an undirected graph, a widest path may be found as the path between the two vertices in the maximum spanning tree of the graph, and a minimax path may be found as the path between the two vertices in the minimum spanning tree.

In any graph, directed or undirected, there is a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using breadth first search or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.

A solution to the minimax path problem between the two opposite corners of a grid graph can be used to find the weak Fréchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Fréchet distance needed to pass from one pair of segments to another.

If all edge weights of an undirected graph are positive, then the minimax distances between pairs of points (the maximum edge weights of minimax paths) form an ultrametric; conversely every finite ultrametric space comes from minimax distances in this way. A data structure constructed from the minimum spanning tree allows the minimax distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, and the children of the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the minimax distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.

In directed graphs, the maximum spanning tree solution cannot be used. Instead, several different algorithms are known; the choice of which algorithm to use depends on whether a start or destination vertex for the path is fixed, or whether paths for many start or destination vertices must be found simultaneously.

The all-pairs widest path problem has applications in the Schulze method for choosing a winner in multiway elections in which voters rank the candidates in preference order. The Schulze method constructs a complete directed graph in which the vertices represent the candidates and every two vertices are connected by an edge. Each edge is directed from the winner to the loser of a pairwise contest between the two candidates it connects, and is labeled with the margin of victory of that contest. Then the method computes widest paths between all pairs of vertices, and the winner is the candidate whose vertex has wider paths to each opponent than vice versa. The results of an election using this method are consistent with the Condorcet method – a candidate who wins all pairwise contests automatically wins the whole election – but it generally allows a winner to be selected, even in situations where the Concorcet method itself fails. The Schulze method has been used by several organizations including the Wikimedia Foundation.

To compute the widest path widths for all pairs of nodes in a dense directed graph, such as the ones that arise in the voting application, the asymptotically fastest known approach takes time where ω is the exponent for fast matrix multiplication. Using the best known algorithms for matrix multiplication, this time bound becomes . Instead, the reference implementation for the Schulze method uses a modified version of the simpler Floyd–Warshall algorithm, which takes time. For sparse graphs, it may be more efficient to repeatedly apply a single-source widest path algorithm.

If the edges are sorted by their weights, then a modified version of Dijkstra's algorithm can compute the bottlenecks between a designated start vertex and every other vertex in the graph, in linear time. The key idea behind the speedup over a conventional version of Dijkstra's algorithm is that the sequence of bottleneck distances to each vertex, in the order that the vertices are considered by this algorithm, is a monotonic subsequence of the sorted sequence of edge weights; therefore, the priority queue of Dijkstra's algorithm can be implemented as a bucket queue: an array indexed by the numbers from 1 to (the number of edges in the graph), where array cell contains the vertices whose bottleneck distance is the weight of the edge with position in the sorted order. This method allows the widest path problem to be solved as quickly as sorting; for instance, if the edge weights are represented as integers, then the time bounds for integer sorting a list of integers would apply also to this problem.

 suggest that service vehicles and emergency vehicles should use minimax paths when returning from a service call to their base. In this application, the time to return is less important than the response time if another service call occurs while the vehicle is in the process of returning. By using a minimax path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.

Another application of widest paths arises in the Ford–Fulkerson algorithm for the maximum flow problem. Repeatedly augmenting a flow along a maximum capacity path in the residual network of the flow leads to a small bound, , on the number of augmentations needed to find a maximum flow; here, the edge capacities are assumed to be integers that are at most . However, this analysis does not depend on finding a path that has the exact maximum of capacity; any path whose capacity is within a constant factor of the maximum suffices. Combining this approximation idea with the shortest path augmentation method of the Edmonds–Karp algorithm leads to a maximum flow algorithm with running time .

It is possible to find maximum-capacity paths and minimax paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set of edges that are known to contain the bottleneck edge of the optimal path; initially, is just the set of all edges of the graph. At each iteration of the algorithm, it splits into an ordered sequence of subsets of approximately equal size; the number of subsets in this partition is chosen in such a way that all of the split points between subsets can be found by repeated median-finding in time . The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces by the subset that it has determined to contain the bottleneck weight, and starts the next iteration with this new set . The number of subsets into which can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, , and the total time is . In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of , allowing to be split into smaller sets in a single step and leading to a linear overall time bound.

A variant of the minimax path problem has also been considered for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean minimax path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a minimax path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.

In number theory, the unsolved Gaussian moat problem asks whether or not minimax paths in the Gaussian prime numbers have bounded or unbounded minimax length. That is, does there exist a constant such that, for every pair of points and in the infinite Euclidean point set defined by the Gaussian primes, the minimax path in the Gaussian primes between and has minimax edge length at most ?


</doc>
<doc id="18210373" url="https://en.wikipedia.org/wiki?curid=18210373" title="Canadian traveller problem">
Canadian traveller problem

In computer science and graph theory, the Canadian traveller problem (CTP) is a generalization of the shortest path problem to graphs that are "partially observable". In other words, the graph is revealed while it is being explored, and explorative edges are charged even if they do not contribute to the final path.

This optimization problem was introduced by Christos Papadimitriou and Mihalis Yannakakis in 1989 and a number of variants of the problem have been studied since. The name supposedly originates from conversations of the authors who learned of a difficulty Canadian drivers had: traveling a network of cities with snowfall randomly blocking roads. The stochastic version, where each edge is associated with a probability of independently being in the graph, has been given considerable attention in operations research under the name "the Stochastic Shortest Path Problem with Recourse" (SSPPR).

For a given instance, there are a number of possibilities, or "realizations", of how the hidden graph may look. Given an instance, a description of how to follow the instance in the best way is called a "policy". The CTP task is to compute the expected cost of the optimal policies. To compute an actual description of an optimal policy may be a harder problem.

Given an instance and policy for the instance, every realization produces its own (deterministic) walk in the graph. Note that the walk is not necessarily a path since the best strategy may be to, e.g., visit every vertex of a cycle and return to the start. This differs from the shortest path problem (with strictly positive weights), where repetitions in a walk implies that a better solution exists.

There are primarily five parameters distinguishing the number of variants of the Canadian Traveller Problem. The first parameter is how to value the walk produced by a policy for a given instance and realization. In the Stochastic Shortest Path Problem with Recourse, the goal is simply to minimize the cost of the walk (defined as the sum over all edges of the cost of the edge times the number of times that edge was taken). For the Canadian Traveller Problem, the task is to minimize the competitive ratio of the walk; i.e., to minimize the number of times longer the produced walk is to the shortest path in the realization.

The second parameter is how to evaluate a policy with respect to different realizations consistent with the instance under consideration. In the Canadian Traveller Problem, one wishes to study the worst case and in SSPPR, the average case. For average case analysis, one must furthermore specify an a priori distribution over the realizations.

The third parameter is restricted to the stochastic versions and is about what assumptions we can make about the distribution of the realizations and how the distribution is represented in the input. In the Stochastic Canadian Traveller Problem and in the Edge-independent Stochastic Shortest Path Problem (i-SSPPR), each uncertain edge (or cost) has an associated probability of being in the realization and the event that an edge is in the graph is independent of which other edges are in the realization. Even though this is a considerable simplification, the problem is still #P-hard. Another variant is to make no assumption on the distribution but require that each realization with non-zero probability be explicitly stated (such as “Probability 0.1 of edge set { {3,4},{1,2} }, probability 0.2 of...”). This is called the Distribution Stochastic Shortest Path Problem (d-SSPPR or R-SSPPR) and is NP-complete. The first variant is harder than the second because the former can represent in logarithmic space some distributions that the latter represents in linear space.

The fourth and final parameter is how the graph changes over time. In CTP and SSPPR, the realization is fixed but not known. In the Stochastic Shortest Path Problem with Recourse and Resets or the Expected Shortest Path problem, a new realization is chosen from the distribution after each step taken by the policy. This problem can be solved in polynomial time by reducing it to a Markov decision process with polynomial horizon. The Markov generalization, where the realization of the graph may influence the next realization, is known to be much harder.

An additional parameter is how new knowledge is being discovered on the realization. In traditional variants of CTP, the agent uncovers the exact weight (or status) of an edge upon reaching an adjacent vertex. A new variant was recently suggested where an agent also has the ability to perform remote sensing from any location on the realization. In this variant, the task is to minimize the travel cost plus the cost of sensing operations.

We define the variant studied in the paper from 1989. That is, the goal is to minimize the competitive ratio in the worst case. It is necessary that we begin by introducing certain terms.

Consider a given graph and the family of undirected graphs that can be constructed by adding one or more edges from a given set. Formally, let formula_1 where we think of "E" as the edges that must be in the graph and of "F" as the edges that may be in the graph. We say that formula_2 is a "realization" of the graph family. Furthermore, let W be an associated cost matrix where formula_3 is the cost of going from vertex "i" to vertex "j", assuming that this edge is in the realization.

For any vertex "v" in "V", we call formula_4 its incident edges with respect to the edge set "B" on "V". Furthermore, for a realization formula_2, let formula_6 be the cost of the shortest path in the graph from "s" to "t". This is called the off-line problem because an algorithm for such a problem would have complete information of the graph.

We say that a strategy formula_7 to navigate such a graph is a mapping from formula_8 to formula_9, where formula_10 denotes the powerset of "X". We define the cost formula_11 of a strategy formula_7 with respect to a particular realization formula_13 as follows.

In other words, we evaluate the policy based on the edges we currently know are in the graph (formula_23) and the edges we know might be in the graph (formula_24). When we take a step in the graph, the edges incident to our new location become known to us. Those edges that are in the graph are added to formula_23, and regardless of whether the edges are in the graph or not, they are removed from the set of unknown edges, formula_24. If the goal is never reached, we say that we have an infinite cost. If the goal is reached, we define the cost of the walk as the sum of the costs of all of the edges traversed, with cardinality.

Finally, we define the Canadian traveller problem.

Papadimitriou and Yannakakis noted that this defines a two-player game, where the players compete over the cost of their respective paths and the edge set is chosen by the second player (nature).

The original paper analysed the complexity of the problem and reported it to be PSPACE-complete. It was also shown that finding an optimal path in the case where each edge has an associated probability of being in the graph (i-SSPPR) is a PSPACE-easy but ♯P-hard problem. It was an open problem to bridge this gap, but since then both the directed and undirected versions were shown to be PSPACE-hard.

The directed version of the stochastic problem is known in operations research as the Stochastic Shortest Path Problem with Recourse.

The problem is said to have applications in operations research, transportation planning, artificial intelligence, machine learning, communication networks, and routing. A variant of the problem has been studied for robot navigation with probabilistic landmark recognition.

Despite the age of the problem and its many potential applications, many natural questions still remain open. Is there a constant-factor approximation or is the problem APX-hard? Is i-SSPPR #P-complete? An even more fundamental question has been left unanswered: is there a polynomial-size "description" of an optimal policy, setting aside for a moment the time necessary to compute the description?




</doc>
<doc id="37804593" url="https://en.wikipedia.org/wiki?curid=37804593" title="K shortest path routing">
K shortest path routing

The "k" shortest path routing problem is a generalization of the shortest path routing problem in a given network. It asks not only about a shortest path but also about next "k−1" shortest paths (which may be longer than the shortest path). A variation of the problem is the loopless "k" shortest paths.

Finding "k" shortest paths is possible by extending Dijkstra algorithm or Bellman-Ford algorithm and extend them to find more than one path.

Since 1957 many papers were published on the "k" shortest path routing problem. Most of the fundamental works were done between 1960s and 2001. Since then, most of the research has been on the problem's applications and its variants. In 2010, Michael Günther et al. published a book on "Symbolic calculation of "k"-shortest paths and related measures with the stochastic process algebra tool CASPA".

The Dijkstra algorithm can be generalized to find the "k" shortest paths.

There are two main variations of the "k" shortest path routing problem. In on variation, paths are allowed to visit the same node more than once, thus creating loops. In another variation, paths are required to be simple and loopless. The loopy version is solvable using Eppstein's algorithm and the loopless variation is solvable by Yen's algorithm.

In this variant, the problem is simplified by not requiring paths to be loopless. A solution was given by B. L. Fox in 1975 in which the "k"-shortest paths are determined in asymptotic time complexity (using big "O" notation. In 1998, David Eppstein reported an approach that maintains an asymptotic complexity of by computing an implicit representation of the paths, each of which can be output in "O"("n") extra time. In 2007, John Hershberger and Subhash Suri proposed a replacement paths algorithm, a more efficient implementation of Eppstein's algorithm with "O"("n") improvement in time. In 2015, Akuba "et al." devised an indexing method as a significantly faster alternative for Eppstein's algorithm, in which a data structure called an index is constructed from a graph and then top-"k" distances between arbitrary pairs of vertices can be rapidly obtained.

In the loopless variant, the paths are forbidden to contain loops which adds an additional level of complexity. It can be solved using Yen's algorithm to find the lengths of all shortest paths from a fixed node to all other nodes in an "n"-node non negative-distance network, a technique requiring only 2"n" additions and "n" comparison, fewer than other available shortest path algorithms need. The running time complexity is pseudo-polynomial, being (where "m" and "n" represent the number of edges and vertices, respectively).

The following example makes use of Yen’s model to find "k" shortest paths between communicating end nodes. That is, it finds a shortest path, second shortest path, etc. up to the K shortest path. More details can be found here.
The code provided in this example attempts to solve the "k" shortest path routing problem for a 15-nodes network containing a combination of unidirectional and bidirectional links:

Another example is the use of "k" shortest paths algorithm to track multiple objects. The technique implements a multiple object tracker based on the "k" shortest paths routing algorithm. A set of probabilistic occupancy maps is used as input. An object detector provides the input.

The complete details can be found at "Computer Vision Laboratory – CVLAB" .

Another use of "k" shortest paths algorithms is to design a transit network that enhances passengers' experience in public transportation systems. Such an example of a transit network can be constructed by putting traveling time under consideration. In addition to traveling time, other conditions may be taken depending upon economical and geographical limitations. Despite variations in parameters, the "k" shortest path algorithms finds the most optimal solutions that satisfies almost all user needs. Such applications of "k" shortest path algorithms are becoming common, recently Xu, He, Song, and Chaudry (2012) studied the "k" shortest path problems in transit network systems. 

The "k" shortest path routing is a good alternative for:

Cherkassky et al. provide more algorithms and associated evaluations.




</doc>
<doc id="41795" url="https://en.wikipedia.org/wiki?curid=41795" title="Minimum spanning tree">
Minimum spanning tree

A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. That is, it is a spanning tree whose sum of edge weights is as small as possible. More generally, any edge-weighted undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of the minimum spanning trees for its connected components.

There are quite a few use cases for minimum spanning trees. One example would be a telecommunications company trying to lay cable in a new neighborhood. If it is constrained to bury the cable only along certain paths (e.g. roads), then there would be a graph containing the points (e.g. houses) connected by those paths. Some of the paths might be more expensive, because they are longer, or require the cable to be buried deeper; these paths would be represented by edges with larger weights. Currency is an acceptable unit for edge weight – there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality. A "spanning tree" for that graph would be a subset of those paths that has no cycles but still connects every house; there might be several spanning trees possible. A "minimum spanning tree" would be one with the lowest total cost, representing the least expensive path for laying the cable.

If there are vertices in the graph, then each spanning tree has edges.
There may be several minimum spanning trees of the same weight; in particular, if all the edge weights of a given graph are the same, then every spanning tree of that graph is minimum.

"If each edge has a distinct weight then there will be only one, unique minimum spanning tree". This is true in many realistic situations, such as the telecommunications company example above, where it's unlikely any two paths have "exactly" the same cost. This generalizes to spanning forests as well.

Proof:

More generally, if the edge weights are not all distinct then only the (multi-)set of weights in minimum spanning trees is certain to be unique; it is the same for all minimum spanning trees.

If the weights are "positive", then a minimum spanning tree is in fact a minimum-cost subgraph connecting all vertices, since subgraphs containing cycles necessarily have more total weight.

"For any cycle in the graph, if the weight of an edge of is larger than the individual weights of all other edges of , then this edge cannot belong to an MST."

Proof: Assume the contrary, i.e. that belongs to an MST . Then deleting will break into two subtrees with the two ends of in different subtrees. The remainder of reconnects the subtrees, hence there is an edge of with ends in different subtrees, i.e., it reconnects the subtrees into a tree with weight less than that of , because the weight of is less than the weight of .

"For any cut "C" of the graph, if the weight of an edge "e" in the cut-set of "C" is strictly smaller than the weights of all other edges of the cut-set of "C", then this edge belongs to all MSTs of the graph."

Proof: Assume that there is an MST "T" that does not contain "e". Adding "e" to "T" will produce a cycle, that crosses the cut once at "e" and crosses back at another edge "e' ". Deleting "e' " we get a spanning tree "T∖{e'}∪{e}" of strictly smaller weight than "T". This contradicts the assumption that "T" was a MST.

By a similar argument, if more than one edge is of minimum weight across a cut, then each such edge is contained in some minimum spanning tree.

"If the minimum cost edge "e" of a graph is unique, then this edge is included in any MST."

Proof: if "e" was not included in the MST, removing any of the (larger cost) edges in the cycle formed after adding "e" to the MST, would yield a spanning tree of smaller weight.

"If T is a tree of MST edges, then we can "contract" T into a single vertex while maintaining the invariant that the MST of the contracted graph plus T gives the MST for the graph before contraction.

In all of the algorithms below, "m" is the number of edges in the graph and "n" is the number of vertices.

The first algorithm for finding a minimum spanning tree was developed by Czech scientist Otakar Borůvka in 1926 (see Borůvka's algorithm). Its purpose was an efficient electrical coverage of Moravia. The algorithm proceeds in a sequence of stages. In each stage, called "Boruvka step", it identifies a forest "F" consisting of the minimum-weight edge incident to each vertex in the graph "G", then forms the graph as the input to the next step. Here denotes the graph derived from "G" by contracting edges in "F" (by the Cut property, these edges belong to the MST). Each Boruvka step takes linear time. Since the number of vertices is reduced by at least half in each step, Boruvka's algorithm takes O("m" log "n") time.

A second algorithm is Prim's algorithm, which was invented by Vojtěch Jarník in 1930 and rediscovered by Prim in 1957 and Dijkstra in 1959. Basically, it grows the MST ("T") one edge at a time. Initially, "T" contains an arbitrary vertex. In each step, "T" is augmented with a least-weight edge ("x","y") such that "x" is in "T" and "y" is not yet in "T". By the Cut property, all edges added to "T" are in the MST. Its run-time is either O("m" log "n") or O("m" + "n" log "n"), depending on the data-structures used.

A third algorithm commonly in use is Kruskal's algorithm, which also takes O("m" log "n") time.

A fourth algorithm, not as commonly used, is the reverse-delete algorithm, which is the reverse of Kruskal's algorithm. Its runtime is O("m" log "n" (log log "n")).

All these four are greedy algorithms. Since they run in polynomial time, the problem of finding such trees is in FP, and related decision problems such as determining whether a particular edge is in the MST or determining if the minimum total weight exceeds a certain value are in P.

Several researchers have tried to find more computationally-efficient algorithms.

In a comparison model, in which the only allowed operations on edge weights are pairwise comparisons, found a linear time randomized algorithm based on a combination of Borůvka's algorithm and the reverse-delete algorithm.

The fastest non-randomized comparison-based algorithm with known complexity, by Bernard Chazelle, is based on the soft heap, an approximate priority queue. Its running time is "O"("m" α("m","n")), where α is the classical functional inverse of the Ackermann function. The function α grows extremely slowly, so that for all practical purposes it may be considered a constant no greater than 4; thus Chazelle's algorithm takes very close to linear time.

If the graph is dense (i.e. "m"/"n" ≥ log log log "n"), then a deterministic algorithm by Fredman and Tarjan finds the MST in time O("m"). The algorithm executes a number of phases. Each phase executes Prim's algorithm many times, each for a limited number of steps. The run-time of each phase is O("m"+"n"). If the number of vertices before a phase is formula_2, the number of vertices remaining after a phase is at most formula_3. Hence, at most formula_4 phases are needed, which gives a linear run-time for dense graphs.

There are other algorithms that work in linear time on dense graphs.

If the edge weights are integers represented in binary, then deterministic algorithms are known that solve the problem in "O"("m" + "n") integer operations.
Whether the problem can be solved "deterministically" for a "general graph" in "linear time" by a comparison-based algorithm remains an open question.

Given graph "G" where the nodes and edges are fixed but the weights are unknown, it is possible to construct a binary decision tree (DT) for calculating the MST for any permutation of weights. Each internal node of the DT contains a comparison between two edges, e.g. "Is the weight of the edge between "x" and "y" larger than the weight of the edge between "w" and "z"?". The two children of the node correspond to the two possible answers "yes" or "no". In each leaf of the DT, there is a list of edges from "G" that correspond to an MST. The runtime complexity of a DT is the largest number of queries required to find the MST, which is just the depth of the DT. A DT for a graph "G" is called "optimal" if it has the smallest depth of all correct DTs for "G".

For every integer "r", it is possible to find optimal decision trees for all graphs on "r" vertices by brute-force search. This search proceeds in two steps.

A. Generating all potential DTs

B. Identifying the correct DTs
To check if a DT is correct, it should be checked on all possible permutations of the edge weights.

Hence, the total time required for finding an optimal DT for "all" graphs with "r" vertices is: formula_14, which is less than: formula_15.

Seth Pettie and Vijaya Ramachandran have found a provably optimal deterministic comparison-based minimum spanning tree algorithm. The following is a simplified description of the algorithm.


The runtime of all steps in the algorithm is O("m"), "except for the step of using the decision trees". We don't know the runtime of this step, but we know that it is optimal - no algorithm can do better than the optimal decision tree.

Thus, this algorithm has the peculiar property that it is "provably optimal" although its runtime complexity is "unknown".

Research has also considered parallel algorithms for the minimum spanning tree problem.
With a linear number of processors it is possible to solve the problem in formula_17 time.

Other specialized algorithms have been designed for computing minimum spanning trees of a graph so large that most of it must be stored on disk at all times. These "external storage" algorithms, for example as described in "Engineering an External Memory Minimum Spanning Tree Algorithm" by Roman, Dementiev et al., can operate, by authors' claims, as little as 2 to 5 times slower than a traditional in-memory algorithm. They rely on efficient external storage sorting algorithms and on graph contraction techniques for reducing the graph's size efficiently.

The problem can also be approached in a distributed manner. If each node is considered a computer and no node knows anything except its own connected links, one can still calculate the distributed minimum spanning tree.

Alan M. Frieze showed that given a complete graph on "n" vertices, with edge weights that are independent identically distributed random variables with distribution function formula_18 satisfying formula_19, then as "n" approaches +∞ the expected weight of the MST approaches formula_20, where formula_21 is the Riemann zeta function. Frieze and Steele also proved convergence in probability. Svante Janson proved a central limit theorem for weight of the MST.

For uniform random weights in formula_22, the exact expected size of the minimum spanning tree has been computed for small complete graphs.

Minimum spanning trees have direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids (which they were first invented for, as mentioned above). They are invoked as subroutines in algorithms for other problems, including the Christofides algorithm for approximating the traveling salesman problem, approximating the multi-terminal minimum cut problem (which is equivalent in the single-terminal case to the maximum flow problem),
and approximating the minimum-cost weighted perfect matching.

Other practical applications based on minimal spanning trees include:

The problem of finding the Steiner tree of a subset of the vertices, that is, minimum tree that spans the given subset, is known to be NP-Complete.

A related problem is the "k"-minimum spanning tree ("k"-MST), which is the tree that spans some subset of "k" vertices in the graph with minimum weight.

A set of "k-smallest spanning trees" is a subset of "k" spanning trees (out of all possible spanning trees) such that no spanning tree outside the subset has smaller weight. (Note that this problem is unrelated to the "k"-minimum spanning tree.)

The Euclidean minimum spanning tree is a spanning tree of a graph with edge weights corresponding to the Euclidean distance between vertices which are points in the plane (or space).

The rectilinear minimum spanning tree is a spanning tree of a graph with edge weights corresponding to the rectilinear distance between vertices which are points in the plane (or space).

In the distributed model, where each node is considered a computer and no node knows anything except its own connected links, one can consider distributed minimum spanning tree. The mathematical definition of the problem is the same but there are different approaches for a solution.

The capacitated minimum spanning tree is a tree that has a marked node (origin, or root) and each of the subtrees attached to the node contains no more than a "c" nodes. "c" is called a tree capacity. Solving CMST optimally is NP-hard, but good heuristics such as Esau-Williams and Sharma produce solutions close to optimal in polynomial time.

The degree constrained minimum spanning tree is a minimum spanning tree in which each vertex is connected to no more than "d" other vertices, for some given number "d". The case "d" = 2 is a special case of the traveling salesman problem, so the degree constrained minimum spanning tree is NP-hard in general.

For directed graphs, the minimum spanning tree problem is called the Arborescence problem and can be solved in quadratic time using the Chu–Liu/Edmonds algorithm.

A maximum spanning tree is a spanning tree with weight greater than or equal to the weight of every other spanning tree.
Such a tree can be found with algorithms such as Prim's or Kruskal's after multiplying the edge weights by -1 and solving
the MST problem on the new graph. A path in the maximum spanning tree is the widest path in the graph between its two endpoints: among all possible paths, it maximizes the weight of the minimum-weight edge.
Maximum spanning trees find applications in parsing algorithms for natural languages
and in training algorithms for conditional random fields.

The dynamic MST problem concerns the update of a previously computed MST after an edge weight change in the original graph or the insertion/deletion of a vertex.

The minimum labeling spanning tree problem is to find a spanning tree with least types of labels if each edge in a graph is associated with a label from a finite label set instead of a weight.

A bottleneck edge is the highest weighted edge in a spanning tree. A spanning tree is a minimum bottleneck spanning tree (or MBST) if the graph does not contain a spanning tree with a smaller bottleneck edge weight. A MST is necessarily a MBST (provable by the cut property), but a MBST is not necessarily a MST.




</doc>
<doc id="197253" url="https://en.wikipedia.org/wiki?curid=197253" title="Borůvka's algorithm">
Borůvka's algorithm

Borůvka's algorithm is a greedy algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct,
or a minimum spanning forest in the case of a graph that is not connected.

It was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia.
The algorithm was rediscovered by Choquet in 1938; again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Georges Sollin in 1965. This algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.

The algorithm begins by finding the minimum-weight edge incident to each vertex of the graph, and adding all of those edges to the forest.
Then, it repeats a similar process of finding the minimum-weight edge from each tree constructed so far to a different tree, and adding all of those edges to the forest.
Each repetition of this process reduces the number of trees, within each connected component of the graph, to at most half of this former value,
so after logarithmically many repetitions the process finishes. When it does, the set of edges it has added forms the minimum spanning forest.

Designating each vertex or set of connected vertices a "component", pseudocode for Borůvka's algorithm is:

If edges do not have distinct weights, then a consistent tie-breaking rule (e.g. breaking ties by the object identifiers of the edges) can be used.
An optimization (not necessary for the analysis) is to remove from "G" each edge that is found to connect two vertices in the same component as each other.

Borůvka's algorithm can be shown to take O(log "V") iterations of the outer loop until it terminates, and therefore to run in time O("E" log "V"), where "E" is the number of edges, and "V" is the number of vertices in "G". In planar graphs, and more generally in families of graphs closed under graph minor operations, it can be made to run in linear time, by removing all but the cheapest edge between each pair of components after each stage of the algorithm.

Other algorithms for this problem include Prim's algorithm and Kruskal's algorithm. Fast parallel algorithms can be obtained by combining Prim's algorithm with Borůvka's.

A faster randomized minimum spanning tree algorithm based in part on Borůvka's algorithm due to Karger, Klein, and Tarjan runs in expected time. The best known (deterministic) minimum spanning tree algorithm by Bernard Chazelle is also based in part on Borůvka's and runs in time, where α is the inverse of the Ackermann function. These randomized and deterministic algorithms combine steps of Borůvka's algorithm, reducing the number of components that remain to be connected, with steps of a different type that reduce the number of edges between pairs of components.


</doc>
<doc id="53776" url="https://en.wikipedia.org/wiki?curid=53776" title="Kruskal's algorithm">
Kruskal's algorithm

This algorithm first appeared in "Proceedings of the American Mathematical Society", pp. 48–50 in 1956, and was written by Joseph Kruskal.

Other algorithms for this problem include Prim's algorithm, Reverse-delete algorithm, and Borůvka's algorithm.


At the termination of the algorithm, the forest forms a minimum spanning forest of the graph. If the graph is connected, the forest has a single component and forms a minimum spanning tree

The following code is implemented with disjoint-set data structure:

Kruskal's algorithm can be shown to run in "O"("E" log "E") time, or equivalently, "O"("E" log "V") time, where "E" is the number of edges in the graph and "V" is the number of vertices, all with simple data structures. These running times are equivalent because:

We can achieve this bound as follows: first sort the edges by weight using a comparison sort in "O"("E" log "E") time; this allows the step "remove an edge with minimum weight from "S"" to operate in constant time. Next, we use a disjoint-set data structure to keep track of which vertices are in which components. We need to perform O("V") operations, as in each iteration we connect a vertex to the spanning tree, two 'find' operations and possibly one union for each edge. Even a simple disjoint-set data structure such as disjoint-set forests with union by rank can perform O("V") operations in "O"("V" log "V") time. Thus the total time is "O"("E" log "E") = "O"("E" log "V").

Provided that the edges are either already sorted or can be sorted in linear time (for example with counting sort or radix sort), the algorithm can use a more sophisticated disjoint-set data structure to run in "O"("E" α("V")) time, where α is the extremely slowly growing inverse of the single-valued Ackermann function.

The proof consists of two parts. First, it is proved that the algorithm produces a spanning tree. Second, it is proved that the constructed spanning tree is of minimal weight.

Let formula_5 be a connected, weighted graph and let formula_6 be the subgraph of formula_5 produced by the algorithm. formula_6 cannot have a cycle, being within one subtree and not between two different trees. formula_6 cannot be disconnected, since the first encountered edge that joins two components of formula_6 would have been added by the algorithm. Thus, formula_6 is a spanning tree of formula_5.

We show that the following proposition P is true by induction: If "F" is the set of edges chosen at any stage of the algorithm, then there is some minimum spanning tree that contains "F".

Kruskal's algorithm is inherently sequential and hard to parallelize. It is, however, possible to perform the initial sorting of the edges in parallel or, alternatively, to use a parallel implementation of a binary heap to extract the minimum-weight edge in every iteration.
As parallel sorting is possible in time formula_14 on formula_15 processors, the runtime of Kruskal's algorithm can be reduced to "O"("E" α("V")), where α again is the inverse of the single-valued Ackermann function.

A variant of Kruskal's algorithm, named Filter-Kruskal, has been described by Osipov et al. and is better suited for parallelization. The basic idea behind Filter-Kruskal is to partition the edges in a similar way to quicksort and filter out edges that connect vertices of the same tree to reduce the cost of sorting. The following Pseudocode demonstrates this.

Filter-Kruskal lends itself better for parallelization as sorting, filtering, and partitioning can easily be performed in parallel by distributing the edges between the processors.

Finally, other variants of a parallel implementation of Kruskal's algorithm have been explored. Examples include a scheme that uses helper threads to remove edges that are definitely not part of the MST in the background, and a variant which runs the sequential algorithm on "p" subgraphs, then merges those subgraphs until only one, the final MST, remains.





</doc>
<doc id="53783" url="https://en.wikipedia.org/wiki?curid=53783" title="Prim's algorithm">
Prim's algorithm

In computer science, Prim's (also known as Jarník's) algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.

The algorithm was developed in 1930 by Czech mathematician Vojtěch Jarník and later rediscovered and republished by computer scientists Robert C. Prim in 1957 and Edsger W. Dijkstra in 1959. Therefore, it is also sometimes called the Jarník's algorithm, Prim–Jarník algorithm, Prim–Dijkstra algorithm
or the DJP algorithm.

Other well-known algorithms for this problem include Kruskal's algorithm and Borůvka's algorithm. These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest. In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms.
However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.

The algorithm may informally be described as performing the following steps:
In more detail, it may be implemented following the pseudocode below.
As described above, the starting vertex for the algorithm will be chosen arbitrarily, because the first iteration of the main loop of the algorithm will have a set of vertices in "Q" that all have equal weights, and the algorithm will automatically start a new tree in "F" when it completes a spanning tree of each connected component of the input graph. The algorithm may be modified to start with any particular vertex "s" by setting "C"["s"] to be a number smaller than the other values of "C" (for instance, zero), and it may be modified to only find a single spanning tree rather than an entire spanning forest (matching more closely the informal description) by stopping whenever it encounters another vertex flagged as having no associated edge.

Different variations of the algorithm differ from each other in how the set "Q" is implemented: as a simple linked list or array of vertices, or as a more complicated priority queue data structure. This choice leads to differences in the time complexity of the algorithm. In general, a priority queue will be quicker at finding the vertex "v" with minimum cost, but will entail more expensive updates when the value of "C"["w"] changes.

The time complexity of Prim's algorithm depends on the data structures used for the graph and for ordering the edges by weight, which can be done using a priority queue. The following table shows the typical choices:

A simple implementation of Prim's, using an adjacency matrix or an adjacency list graph representation and linearly searching an array of weights to find the minimum weight edge to add, requires O(<nowiki>|</nowiki>V<nowiki>|</nowiki>) running time. However, this running time can be greatly improved further by using heaps to implement finding minimum weight edges in the algorithm's inner loop.

A first improved version uses a heap to store all edges of the input graph, ordered by their weight. This leads to an O(|E| log |E|) worst-case running time. But storing vertices instead of edges can improve it still further. The heap should order the vertices by the smallest edge-weight that connects them to any vertex in the partially constructed minimum spanning tree (MST) (or infinity if no such edge exists). Every time a vertex "v" is chosen and added to the MST, a decrease-key operation is performed on all vertices "w" outside the partial MST such that "v" is connected to "w", setting the key to the minimum of its previous value and the edge cost of ("v","w").

Using a simple binary heap data structure, Prim's algorithm can now be shown to run in time O(<nowiki>|</nowiki>E<nowiki>|</nowiki> log <nowiki>|</nowiki>V<nowiki>|</nowiki>) where <nowiki>|</nowiki>E<nowiki>|</nowiki> is the number of edges and <nowiki>|</nowiki>V<nowiki>|</nowiki> is the number of vertices. Using a more sophisticated Fibonacci heap, this can be brought down to O(<nowiki>|</nowiki>E<nowiki>|</nowiki> + <nowiki>|</nowiki>V<nowiki>|</nowiki> log <nowiki>|</nowiki>V<nowiki>|</nowiki>), which is asymptotically faster when the graph is dense enough that <nowiki>|</nowiki>E<nowiki>|</nowiki> is ω(<nowiki>|</nowiki>V<nowiki>|</nowiki>), and linear time when |E| is at least |V| log |V|. For graphs of even greater density (having at least |V| edges for some "c" > 1), Prim's algorithm can be made to run in linear time even more simply, by using a "d"-ary heap in place of a Fibonacci heap.

Let "P" be a connected, weighted graph. At every iteration of Prim's algorithm, an edge must be found that connects a vertex in a subgraph to a vertex outside the subgraph. Since "P" is connected, there will always be a path to every vertex. The output "Y" of Prim's algorithm is a tree, because the edge and vertex added to tree "Y" are connected. Let "Y" be a minimum spanning tree of graph P. If "Y"="Y" then "Y" is a minimum spanning tree. Otherwise, let "e" be the first edge added during the construction of tree "Y" that is not in tree "Y", and "V" be the set of vertices connected by the edges added before edge "e". Then one endpoint of edge "e" is in set "V" and the other is not. Since tree "Y" is a spanning tree of graph "P", there is a path in tree "Y" joining the two endpoints. As one travels along the path, one must encounter an edge "f" joining a vertex in set "V" to one that is not in set "V". Now, at the iteration when edge "e" was added to tree "Y", edge "f" could also have been added and it would be added instead of edge "e" if its weight was less than "e", and since edge "f" was not added, we conclude that

Let tree "Y" be the graph obtained by removing edge "f" from and adding edge "e" to tree "Y". It is easy to show that tree "Y" is connected, has the same number of edges as tree "Y", and the total weights of its edges is not larger than that of tree "Y", therefore it is also a minimum spanning tree of graph "P" and it contains edge "e" and all the edges added before it during the construction of set "V". Repeat the steps above and we will eventually obtain a minimum spanning tree of graph "P" that is identical to tree "Y". This shows "Y" is a minimum spanning tree. The minimum spanning tree allows for the first subset of the sub-region to be expanded into a smaller subset "X", which we assume to be the minimum.

The main loop of Prim's algorithm is inherently sequential and thus not parallelizable. However, the inner loop, which determines the next edge of minimum weight that does not form a cycle, can be parallelized by dividing the vertices and edges between the available processors. The following pseudocode demonstrates this.

This algorithm can generally be implemented on distributed machines as well as on shared memory machines. It has also been implemented on graphical processing units (GPUs). The running time is formula_2, assuming that the "reduce" and "broadcast" operations can be performed in formula_3. A variant of Prim's algorithm for shared memory machines, in which Prim's sequential algorithm is being run in parallel, starting from different vertices, has also been explored. It should, however, be noted that more sophisticated algorithms exist to solve the distributed minimum spanning tree problem in a more efficient manner.




</doc>
<doc id="3037035" url="https://en.wikipedia.org/wiki?curid=3037035" title="Degree-constrained spanning tree">
Degree-constrained spanning tree

In graph theory, a degree-constrained spanning tree is a spanning tree where the maximum vertex degree is limited to a certain constant "k". The degree-constrained spanning tree problem is to determine whether a particular graph has such a spanning tree for a particular "k".

Input: "n"-node undirected graph G(V,E); positive integer "k" ≤ "n".

Question: Does G have a spanning tree in which no node has degree greater than "k"?

This problem is NP-complete . This can be shown by a reduction from the Hamiltonian path problem. It remains NP-complete even if "k" is fixed to a value ≥ 2. If the problem is defined as the degree must be ≤ "k", the "k" = 2 case of degree-confined spanning tree is the Hamiltonian path problem.

On a weighted graph, a Degree-constrained minimum spanning tree (DCMST) is a degree-constrained spanning tree in which the sum of its edges has the minimum possible sum. Finding a DCMST is an NP-Hard problem.

Heuristic algorithms that can solve the problem in polynomial time have been proposed, including Genetic and Ant-Based Algorithms.

 give an iterative polynomial time algorithm which, given a graph formula_1, returns a spanning tree with maximum degree no larger than formula_2, where formula_3 is the minimum possible maximum degree over all spanning trees. Thus, if formula_4, such an algorithm will either return a spanning tree of maximum degree formula_5 or formula_6.



</doc>
<doc id="1152079" url="https://en.wikipedia.org/wiki?curid=1152079" title="K-minimum spanning tree">
K-minimum spanning tree

The -minimum spanning tree problem, studied in theoretical computer science, asks for a tree of minimum cost that has exactly vertices and forms a subgraph of a larger graph. It is also called the -MST or edge-weighted -cardinality tree. Finding this tree is NP-hard, but it can be approximated to within a constant approximation ratio in polynomial time.

The input to the problem consists of an undirected graph with weights on its edges, and a The output is a tree with vertices and edges, with all of the edges of the output tree belonging to the input graph. The cost of the output is the sum of the weights of its edges, and the goal is to find the tree that has minimum cost. The problem was formulated by and by .

Ravi et al. also considered a geometric version of the problem, which can be seen as a special case of the graph problem.
In the geometric -minimum spanning tree problem, the input is a set of points in the plane. Again, the output should be a tree with of the points as its vertices, minimizing the total Euclidean length of its edges. That is, it is a graph -minimum spanning tree on a complete graph with Euclidean distances as weights.

When is a fixed constant, the -minimum spanning tree problem can be solved in polynomial time by a brute-force search algorithm that tries all -tuples of vertices.
However, for variable , the -minimum spanning tree problem has been shown to be NP-hard by a reduction from the Steiner tree problem.

The reduction takes as input an instance of the Steiner tree problem: a weighted graph, with a subset of its vertices selected as terminals. The goal of the Steiner tree problem is to connect these terminals by a tree whose weight is as small as possible. To transform this problem into an instance of the -minimum spanning tree problem, attach to each terminal a tree of zero-weight edges with a large number of vertices per tree. (For a graph with vertices and terminals, they use added vertices per tree.) Then, they ask for the -minimum spanning tree in this augmented graph with . The only way to include this many vertices in a -spanning tree is to use at least one vertex from each added tree, for there are not enough vertices remaining if even one of the added trees is missed. However, for this choice of , it is possible for -spanning tree to include only as few edges of the original graph as are needed to connect all the terminals. Therefore, the -minimum spanning tree must be formed by combining the optimal Steiner tree with enough of the zero-weight edges of the added trees to make the total tree size large enough.

Even for a graph whose edge weights belong to the set }, testing whether the optimal solution value is less than a given threshold is NP-complete. It remains NP-complete for planar graphs. The geometric version of the problem is also NP-hard, but not known to belong to NP, because of the difficulty of comparing sums of square roots; instead it lies in the class of problems reducible to the existential theory of the reals.

The -minimum spanning tree may be found in polynomial time for graphs of bounded treewidth, and for graphs with only two distinct edge weights.

Because of the high computational complexity of finding an optimal solution to the -minimum spanning tree, much of the research on the problem has instead concentrated on approximation algorithms for the problem. The goal of such algorithms is to find an approximate solution in polynomial time with a small approximation ratio. The approximation ratio is defined as the ratio of the computed solution length to the optimal length for a worst-case instance, one that maximizes this ratio. Because the NP-hardness reduction for the -minimum spanning tree problem preserves the weight of all solutions, it also preserves the hardness of approximation of the problem. In particular, because the Steiner tree problem is NP-hard to approximate to an approximation ratio better than 96/95, the same is true for the -minimum spanning tree problem.

The best approximation known for the general problem achieves an approximation ratio of 2, and is by . This approximation relies heavily on the primal-dual schema of .
When the input consists of points in the Euclidean plane (any two of which can be connected in the tree with cost equal to their distance) there exists a polynomial time approximation scheme devised by .



</doc>
<doc id="23015254" url="https://en.wikipedia.org/wiki?curid=23015254" title="Capacitated minimum spanning tree">
Capacitated minimum spanning tree

Capacitated minimum spanning tree is a minimal cost spanning tree of a graph that has a designated root node formula_1 and satisfies the capacity constraint formula_2. The capacity constraint ensures that all subtrees (maximal subgraphs connected to the root by a single edge) incident on the root node formula_1 have no more than formula_2 nodes. If the tree nodes have weights, then the capacity constraint may be interpreted as follows: the sum of weights in any subtree should be no greater than formula_2. The edges connecting the subgraphs to the root node are called "gates". Finding the optimal solution is NP-hard.

Suppose we have a graph formula_6, formula_7 with a root formula_8. Let formula_9 be all other nodes in formula_10. Let formula_11 be the edge cost between ver formula_9 and formula_13 which form a cost matrix formula_14.

Esau-Williams heuristic finds suboptimal CMST that are very close to the exact solutions, but on average EW produces better results than many other heuristics.

Initially, all nodes are connected to the root formula_1 (star graph) and the network's cost is formula_16; each of these edges is a gate. At each iteration, we seek the closest neighbor formula_13 for every node in formula_18 and evaluate the tradeoff function: formula_19. We look for the greatest formula_20 among the positive tradeoffs and, if the resulting subtree does not violate the capacity constraints, remove the gate formula_21 connecting the formula_22-th subtree to formula_13 by an edge formula_11. We repeat the iterations until we can not make any further improvements to the tree.

Esau-Williams heuristics for computing a suboptimal CMST:

It is easy to see that EW finds a solution in polynomial time.

Sharma's heuristic.

CMST problem is important in network design: when many terminal computers have to be connected to the central hub, the star configuration is usually not the minimum cost design. Finding a CMST that organizes the terminals into subnetworks can lower the cost of implementing a network.


</doc>
<doc id="249254" url="https://en.wikipedia.org/wiki?curid=249254" title="Clique problem">
Clique problem

In computer science, the clique problem is the computational problem of finding cliques (subsets of vertices, all adjacent to each other, also called complete subgraphs) in a graph. It has several different formulations depending on which cliques, and what information about the cliques, should be found. Common formulations of the clique problem include finding a maximum clique (a clique with the largest possible number of vertices), finding a maximum weight clique in a weighted graph, listing all maximal cliques (cliques that cannot be enlarged), and solving the decision problem of testing whether a graph contains a clique larger than a given size.

The clique problem arises in the following real-world setting. Consider a social network, where the graph's vertices represent people, and the graph's edges represent mutual acquaintance. Then a clique represents a subset of people who all know each other, and algorithms for finding cliques can be used to discover these groups of mutual friends. Along with its applications in social networks, the clique problem also has many applications in bioinformatics, computational chemistry and motion segmentation.

Most versions of the clique problem are hard. The clique decision problem is NP-complete (one of Karp's 21 NP-complete problems). The problem of finding the maximum clique is both fixed-parameter intractable and hard to approximate. And, listing all maximal cliques may require exponential time as there exist graphs with exponentially many maximal cliques. Therefore, much of the theory about the clique problem is devoted to identifying special types of graph that admit more efficient algorithms, or to establishing the computational difficulty of the general problem in various models of computation.

To find a maximum clique, one can systematically inspect all subsets, but this sort of brute-force search is too time-consuming to be practical for networks comprising more than a few dozen vertices.
Although no polynomial time algorithm is known for this problem, more efficient algorithms than the brute-force search are known. For instance, the Bron–Kerbosch algorithm can be used to list all maximal cliques in worst-case optimal time, and it is also possible to list them in polynomial time per clique.

The study of complete subgraphs in mathematics predates the "clique" terminology. For instance, complete subgraphs make an early appearance in the mathematical literature in the graph-theoretic reformulation of Ramsey theory by . But the term "clique" and the problem of algorithmically listing cliques both come from the social sciences, where complete subgraphs are used to model social cliques, groups of people who all know each other. used graphs to model social networks, and adapted the social science terminology to graph theory. They were the first to call complete subgraphs "cliques". The first algorithm for solving the clique problem is that of , who were motivated by the sociological application.
Social science researchers have also defined various other types of cliques and maximal cliques in social network, "cohesive subgroups" of people or actors in the network all of whom share one of several different kinds of connectivity relation. Many of these generalized notions of cliques can also be found by constructing an undirected graph whose edges represent related pairs of actors from the social network, and then applying an algorithm for the clique problem to this graph.

Since the work of Harary and Ross, many others have devised algorithms for various versions of the clique problem. In the 1970s, researchers began studying these algorithms from the point of view of worst-case analysis. See, for instance, , an early work on the worst-case complexity of the maximum clique problem. Also in the 1970s, beginning with the work of and , researchers began using the theory of NP-completeness and related intractability results to provide a mathematical explanation for the perceived difficulty of the clique problem. In the 1990s, a breakthrough series of papers beginning with and reported in "The New York Times", showed that (assuming P ≠ NP) it is not even possible to approximate the problem accurately and efficiently.

Clique-finding algorithms have been used in chemistry, to find chemicals that match a target structure and to model molecular docking and the binding sites of chemical reactions. They can also be used to find similar structures within different molecules.
In these applications, one forms a graph in which each vertex represents a matched pair of atoms, one from each of two molecules. Two vertices are connected by an edge if the matches that they represent are compatible with each other. Being compatible may mean, for instance, that the distances between the atoms within the two molecules are approximately equal, to within some given tolerance. A clique in this graph represents a set of matched pairs of atoms in which all the matches are compatible with each other. A special case of this method is the use of the modular product of graphs to reduce the problem of finding the maximum common induced subgraph of two graphs to the problem of finding a maximum clique in their product.

In automatic test pattern generation, finding cliques can help to bound the size of a test set. In bioinformatics, clique-finding algorithms have been used to infer evolutionary trees, predict protein structures, and find closely interacting clusters of proteins. Listing the cliques in a dependency graph is an important step in the analysis of certain random processes. In mathematics, Keller's conjecture on face-to-face tiling of hypercubes was disproved by , who used a clique-finding algorithm on an associated graph to find a counterexample.

An undirected graph is formed by a finite set of vertices and a set of unordered pairs of vertices, which are called edges. By convention, in algorithm analysis, the number of vertices in the graph is denoted by and the number of edges is denoted by . A clique in a graph is a complete subgraph of . That is, it is a subset of the vertices such that every two vertices in are the two endpoints of an edge in . A maximal clique is a clique to which no more vertices can be added. For each vertex that is not part of a maximal clique, there must be another vertex that is in the clique and non-adjacent to , preventing from being added to the clique. A maximum clique is a clique that includes the largest possible number of vertices. The clique number is the number of vertices in a maximum clique of .

Several closely related clique-finding problems have been studied.
The first four of these problems are all important in practical applications. The clique decision problem is not of practical importance; it is formulated in this way in order to apply the theory of NP-completeness to clique-finding problems.

The clique problem and the independent set problem are complementary: a clique in is an independent set in the complement graph of and vice versa. Therefore, many computational results may be applied equally well to either problem, and some research papers do not clearly distinguish between the two problems. However, the two problems have different properties when applied to restricted families of graphs. For instance, the clique problem may be solved in polynomial time for planar graphs while the independent set problem remains NP-hard on planar graphs.

A maximal clique, sometimes called inclusion-maximal, is a clique that is not included in a larger clique. Therefore, every clique is contained in a maximal clique.
Maximal cliques can be very small. A graph may contain a non-maximal clique with many vertices and a separate clique of size 2 which is maximal. While a maximum (i.e., largest) clique is necessarily maximal, the converse does not hold. There are some types of graphs in which every maximal clique is maximum; these are the complements of the well-covered graphs, in which every maximal independent set is maximum.
However, other graphs have maximal cliques that are not maximum.

A single maximal clique can be found by a straightforward greedy algorithm. Starting with an arbitrary clique (for instance, any single vertex or even the empty set), grow the current clique one vertex at a time by looping through the graph's remaining vertices. For each vertex that this loop examines, add to the clique if it is adjacent to every vertex that is already in the clique, and discard otherwise. This algorithm runs in linear time. 
Because of the ease of finding maximal cliques, and their potential small size, more attention has been given to the much harder algorithmic problem of finding a maximum or otherwise large clique than has been given to the problem of finding a single maximal clique.
However, some research in parallel algorithms has studied the problem of finding a maximal clique. In particular, the problem of finding the lexicographically first maximal clique (the one found by the algorithm above) has been shown to be complete for the class of polynomial-time functions. This result implies that the problem is unlikely to be solvable within the parallel complexity class NC.

One can test whether a graph contains a -vertex clique, and find any such clique that it contains, using a brute force algorithm. This algorithm examines each subgraph with vertices and checks to see whether it forms a clique. It takes time , as expressed using big O notation.
This is because there are subgraphs to check, each of which has edges whose presence in needs to be checked. Thus, the problem may be solved in polynomial time whenever is a fixed constant. However, when does not have a fixed value, but instead may vary as part of the input to the problem, the time is exponential.

The simplest nontrivial case of the clique-finding problem is finding a triangle in a graph, or equivalently determining whether the graph is triangle-free.
In a graph with edges, there may be at most triangles (using big theta notation to indicate that this bound is tight). The worst case for this formula occurs when is itself a clique. Therefore, algorithms for listing all triangles must take at least time in the worst case (using big omega notation), and algorithms are known that match this time bound. For instance, describe an algorithm that sorts the vertices in order from highest degree to lowest and then iterates through each vertex in the sorted list, looking for triangles that include and do not include any previous vertex in the list. To do so the algorithm marks all neighbors of , searches through all edges incident to a neighbor of outputting a triangle for every edge that has two marked endpoints, and then removes the marks and deletes from the graph. As the authors show, the time for this algorithm is proportional to the arboricity of the graph (denoted ) multiplied by the number of edges, which is . Since the arboricity is at most , this algorithm runs in time . More generally, all -vertex cliques can be listed by a similar algorithm that takes time proportional to the number of edges multiplied by the arboricity to the power . For graphs of constant arboricity, such as planar graphs (or in general graphs from any non-trivial minor-closed graph family), this algorithm takes time, which is optimal since it is linear in the size of the input.

If one desires only a single triangle, or an assurance that the graph is triangle-free, faster algorithms are possible. As observe, the graph contains a triangle if and only if its adjacency matrix and the square of the adjacency matrix contain nonzero entries in the same cell. Therefore, fast matrix multiplication techniques such as the Coppersmith–Winograd algorithm can be applied to find triangles in time . used fast matrix multiplication to improve the algorithm for finding triangles to . These algorithms based on fast matrix multiplication have also been extended to problems of finding -cliques for larger values of .

By a result of , every -vertex graph has at most maximal cliques. They can be listed by the Bron–Kerbosch algorithm, a recursive backtracking procedure of . The main recursive subroutine of this procedure has three arguments: a partially constructed (non-maximal) clique, a set of candidate vertices that could be added to the clique, and another set of vertices that should not be added (because doing so would lead to a clique that has already been found). The algorithm tries adding the candidate vertices one by one to the partial clique, making a recursive call for each one. After trying each of these vertices, it moves it to the set of vertices that should not be added again. Variants of this algorithm can be shown to have worst-case running time , matching the number of cliques that might need to be listed. Therefore, this provides a worst-case-optimal solution to the problem of listing all maximal cliques. Further, the Bron–Kerbosch algorithm has been widely reported as being faster in practice than its alternatives.

However, when the number of cliques is significantly smaller than its worst case, other algorithms might be preferable. As showed, it is also possible to list all maximal cliques in a graph in an amount of time that is polynomial per generated clique. An algorithm such as theirs in which the running time depends on the output size is known as an output-sensitive algorithm. Their algorithm is based on the following two observations, relating the maximal cliques of the given graph to the maximal cliques of a graph formed by removing an arbitrary vertex from :
Using these observations they can generate all maximal cliques in by a recursive algorithm that chooses a vertex arbitrarily and then, for each maximal clique in , outputs both and the clique formed by adding to and removing the non-neighbors of . However, some cliques of may be generated in this way from more than one parent clique of , so they eliminate duplicates by outputting a clique in only when its parent in is lexicographically maximum among all possible parent cliques. On the basis of this principle, they show that all maximal cliques in may be generated in time per clique, where is the number of edges in and is the number of vertices. improve this to per clique, where is the arboricity of the given graph. provide an alternative output-sensitive algorithm based on fast matrix multiplication. show that it is even possible to list all maximal cliques in lexicographic order with polynomial delay per clique. However, the choice of ordering is important for the efficiency of this algorithm: for the reverse of this order,
there is no polynomial-delay algorithm unless P = NP.

On the basis of this result, it is possible to list all maximal cliques in polynomial time, for families of graphs in which the number of cliques is polynomially bounded. These families include chordal graphs, complete graphs, triangle-free graphs, interval graphs, graphs of bounded boxicity, and planar graphs. In particular, the planar graphs have cliques, of at most constant size, that can be listed in linear time. The same is true for any family of graphs that is both sparse (having a number of edges at most a constant times the number of vertices) and closed under the operation of taking subgraphs.

It is possible to find the maximum clique, or the clique number, of an arbitrary "n"-vertex graph in time by using one of the algorithms described above to list all maximal cliques in the graph and returning the largest one. However, for this variant of the clique problem better worst-case time bounds are possible. The algorithm of solves this problem in time . It is a recursive backtracking scheme similar to that of the Bron–Kerbosch algorithm, but is able to eliminate some recursive calls when it can be shown that the cliques found within the call will be suboptimal. improved the time to , and improved it to time, at the expense of greater space usage. Robson's algorithm combines a similar backtracking scheme (with a more complicated case analysis) and a dynamic programming technique in which the optimal solution is precomputed for all small connected subgraphs of the complement graph. These partial solutions are used to shortcut the backtracking recursion. The fastest algorithm known today is a refined version of this method by which runs in time .

There has also been extensive research on heuristic algorithms for solving maximum clique problems without worst-case runtime guarantees, based on methods including branch and bound, local search, greedy algorithms, and constraint programming. Non-standard computing methodologies that have been suggested for finding cliques include DNA computing and adiabatic quantum computation. The maximum clique problem was the subject of an implementation challenge sponsored by DIMACS in 1992–1993, and a collection of graphs used as benchmarks for the challenge, which is publicly available.

Planar graphs, and other families of sparse graphs, have been discussed above: they have linearly many maximal cliques, of bounded size, that can be listed in linear time. In particular, for planar graphs, any clique can have at most four vertices, by Kuratowski's theorem.

Perfect graphs are defined by the properties that their clique number equals their chromatic number, and that this equality holds also in each of their induced subgraphs. For perfect graphs, it is possible to find a maximum clique in polynomial time, using an algorithm based on semidefinite programming.
However, this method is complex and non-combinatorial, and specialized clique-finding algorithms have been developed for many subclasses of perfect graphs. In the complement graphs of bipartite graphs, Kőnig's theorem allows the maximum clique problem to be solved using techniques for matching. In another class of perfect graphs, the permutation graphs, a maximum clique is a longest decreasing subsequence of the permutation defining the graph and can be found using known algorithms for the longest decreasing subsequence problem. Conversely, every instance of the longest decreasing subsequence problem can be described equivalently as a problem of finding a maximum clique in a permutation graph. provide an alternative quadratic-time algorithm for maximum cliques in comparability graphs, a broader class of perfect graphs that includes the permutation graphs as a special case. In chordal graphs, the maximal cliques can be found by listing the vertices in an elimination ordering, and checking the clique neighborhoods of each vertex in this ordering.

In some cases, these algorithms can be extended to other, non-perfect, classes of graphs as well. For instance, in a circle graph, the neighborhood of each vertex is a permutation graph, so a maximum clique in a circle graph can be found by applying the permutation graph algorithm to each neighborhood. Similarly, in a unit disk graph (with a known geometric representation), there is a polynomial time algorithm for maximum cliques based on applying the algorithm for complements of bipartite graphs to shared neighborhoods of pairs of vertices.

The algorithmic problem of finding a maximum clique in a random graph drawn from the Erdős–Rényi model (in which each edge appears with probability , independently from the other edges) was suggested by . Because the maximum clique in a random graph has logarithmic size with high probability, it
can be found by a brute force search in expected time . This is a quasi-polynomial time bound. Although the clique number of such graphs is usually very close to , simple greedy algorithms as well as more sophisticated randomized approximation techniques only find cliques with size , half as big. The number of maximal cliques in such graphs is with high probability exponential in , which prevents methods that list all maximal cliques from running in polynomial time. Because of the difficulty of this problem, several authors have investigated the planted clique problem, the clique problem on random graphs that have been augmented by adding large cliques. While spectral methods and semidefinite programming can detect hidden cliques of size , no polynomial-time algorithms are currently known to detect those of size (expressed using little-o notation).

Several authors have considered approximation algorithms that attempt to find a clique or independent set that, although not maximum, has size as close to the maximum as can be found in polynomial time.
Although much of this work has focused on independent sets in sparse graphs, a case that does not make sense for the complementary clique problem, there has also been work on approximation algorithms that do not use such sparsity assumptions.

The clique decision problem is NP-complete. It was one of Richard Karp's original 21 problems shown NP-complete in his 1972 paper "Reducibility Among Combinatorial Problems". This problem was also mentioned in Stephen Cook's paper introducing the theory of NP-complete problems. Because of the hardness of the decision problem, the problem of finding a maximum clique is also NP-hard. If one could solve it, one could also solve the decision problem, by comparing the size of the maximum clique to the size parameter given as input in the decision problem.

Karp's NP-completeness proof is a many-one reduction from the Boolean satisfiability problem.
It describes how to translate Boolean formulas in conjunctive normal form (CNF) into equivalent instances of the maximum clique problem.
Satisfiability, in turn, was proved NP-complete in the Cook–Levin theorem. From a given CNF formula, Karp forms a graph that has a vertex for every pair , where is a variable or its negation and is a clause in the formula that contains . Two of these vertices are connected by an edge if they represent compatible variable assignments for different clauses. That is, there is an edge from to whenever and and are not each other's negations. If denotes the number of clauses in the CNF formula, then the -vertex cliques in this graph represent consistent ways of assigning truth values to some of its variables in order to satisfy the formula. Therefore, the formula is satisfiable if and only if a -vertex clique exists.

Some NP-complete problems (such as the travelling salesman problem in planar graphs) may be solved in time that is exponential in a sublinear function of the input size parameter ,
significantly faster than a brute-force search.
However, it is unlikely that such a subexponential time bound is possible for the clique problem in arbitrary graphs, as it would imply similarly subexponential bounds for many other standard NP-complete problems.

The computational difficulty of the clique problem has led it to be used to prove several lower bounds in circuit complexity. The existence of a clique of a given size is a monotone graph property, meaning that, if a clique exists in a given graph, it will exist in any supergraph. Because this property is monotone, there must exist a monotone circuit, using only and gates and or gates, to solve the clique decision problem for a given fixed clique size. However, the size of these circuits can be proven to be a super-polynomial function of the number of vertices and the clique size, exponential in the cube root of the number of vertices. Even if a small number of NOT gates are allowed, the complexity remains superpolynomial. Additionally, the depth of a monotone circuit for the clique problem using gates of bounded fan-in must be at least a polynomial in the clique size.

The (deterministic) decision tree complexity of determining a graph property is the number of questions of the form "Is there an edge between vertex and vertex ?" that have to be answered in the worst case to determine whether a graph has a particular property. That is, it is the minimum height of a boolean decision tree for the problem. There are possible questions to be asked. Therefore, any graph property can be determined with at most questions. It is also possible to define random and quantum decision tree complexity of a property, the expected number of questions (for a worst case input) that a randomized or quantum algorithm needs to have answered in order to correctly determine whether the given graph has the property.

Because the property of containing a clique is monotone, it is covered by the Aanderaa–Karp–Rosenberg conjecture, which states that the deterministic decision tree complexity of determining any non-trivial monotone graph property is exactly . For arbitrary monotone graph properties, this conjecture remains unproven. However, for deterministic decision trees, and for any in the range , the property of containing a -clique was shown to have decision tree complexity exactly by . Deterministic decision trees also require exponential size to detect cliques, or large polynomial size to detect cliques of bounded size.

The Aanderaa–Karp–Rosenberg conjecture also states that the randomized decision tree complexity of non-trivial monotone functions is . The conjecture again remains unproven, but has been resolved for the property of containing a clique for . This property is known to have randomized decision tree complexity . For quantum decision trees, the best known lower bound is , but no matching algorithm is known for the case of .

Parameterized complexity is the complexity-theoretic study of problems that are naturally equipped with a small integer parameter and for which the problem becomes more difficult as increases, such as finding -cliques in graphs. A problem is said to be fixed-parameter tractable if there is an algorithm for solving it on inputs of size , and a function , such that the algorithm runs in time . That is, it is fixed-parameter tractable if it can be solved in polynomial time for any fixed value of and moreover if the exponent of the polynomial does not depend on .

For finding -vertex cliques, the brute force search algorithm has running time . Because the exponent of depends on , this algorithm is not fixed-parameter tractable.
Although it can be improved by fast matrix multiplication the running time still has an exponent that is linear in Thus, although the running time of known algorithms for the clique problem is polynomial for any fixed these algorithms do not suffice for fixed-parameter tractability. defined a hierarchy of parametrized problems, the W hierarchy, that they conjectured did not have fixed-parameter tractable algorithms. They proved that independent set (or, equivalently, clique) is hard for the first level of this hierarchy, W[1]. Thus, according to their conjecture, clique has no fixed-parameter tractable algorithm. Moreover, this result provides the basis for proofs of W[1]-hardness of many other problems, and thus serves as an analogue of the Cook–Levin theorem for parameterized complexity.

Although the problems of listing maximal cliques or finding maximum cliques are unlikely to be fixed-parameter tractable with the parameter , they may be fixed-parameter tractable for other parameters of instance complexity. For instance, both problems are known to be fixed-parameter tractable when parametrized by the degeneracy of the input graph.

Weak results hinting that the clique problem might be hard to approximate have been known for a long time. observed that, because of the fact that the clique number takes on small integer values and is NP-hard to compute, it cannot have a fully polynomial-time approximation scheme. If too accurate an approximation were available, rounding its value to an integer would give the exact clique number. However, little more was known until the early 1990s, when several authors began to make connections between the approximation of maximum cliques and probabilistically checkable proofs. They used these connections to prove hardness of approximation results for the maximum clique problem.
After many improvements to these results it is now known that, for every real number , there can be no polynomial time algorithm that approximates the maximum clique to within a factor better than , unless P = NP.

The rough idea of these inapproximability results is to form a graph that represents a probabilistically checkable proof system for an NP-complete problem such as the Boolean satisfiability problem. In a probabilistically checkable proof system, a proof is represented as a sequence of bits. An instance of the satisfiability problem should have a valid proof if and only if it is satisfiable. The proof is checked by an algorithm that, after a polynomial-time computation on the input to the satisfiability problem, chooses to examine a small number of randomly chosen positions of the proof string. Depending on what values are found at that sample of bits, the checker will either accept or reject the proof, without looking at the rest of the bits. False negatives are not allowed: a valid proof must always be accepted. However, an invalid proof may sometimes mistakenly be accepted. For every invalid proof, the probability that the checker will accept it must be low.

To transform a probabilistically checkable proof system of this type into a clique problem, one forms a graph with a vertex for each possible accepting run of the proof checker. That is, a vertex is defined by one of the possible random choices of sets of positions to examine, and by bit values for those positions that would cause the checker to accept the proof. Two vertices are adjacent, in this graph, if the corresponding two accepting runs see the same bit values at every position they both examine. Each (valid or invalid) proof string corresponds to a clique, the set of accepting runs that see that proof string, and all maximal cliques arise in this way. One of these cliques is large if and only if it corresponds to a proof string that many proof checkers accept. If the original satisfiability instance is satisfiable, it will have a valid proof string, one that is accepted by all runs of the checker, and this string will correspond to a large clique in the graph. However, if the original instance is not satisfiable, then all proof strings are invalid, each proof string has only a small number of checker runs that mistakenly accept it, and all cliques are small. Therefore, if one could distinguish in polynomial time between graphs that have large cliques and graphs in which all cliques are small, or if one could accurately approximate the clique problem, then applying this approximation to the graphs generated from satisfiability instances would allow satisfiable instances to be distinguished from unsatisfiable instances. However, this is not possible unless P = NP.



</doc>
<doc id="1793590" url="https://en.wikipedia.org/wiki?curid=1793590" title="Maximal independent set">
Maximal independent set

In graph theory, a maximal independent set (MIS) or maximal stable set is an independent set that is not a subset of any other independent set. In other words, there is no vertex outside the independent set that may join it because it is maximal with respect to the independent set property.

For example, in the graph formula_1, a path with three vertices formula_2, formula_3, and formula_4, and two edges formula_5 and formula_6, the sets formula_7 and formula_8 are both maximally independent. The set formula_9 is independent, but is not maximal independent, because it is a subset of the larger independent set formula_8. In this same graph, the maximal cliques are the sets formula_11 and formula_12.

A MIS is also a dominating set in the graph, and every dominating set that is independent must be maximal independent, so MISs are also called independent dominating sets. 

A graph may have many MISs of widely varying sizes; the largest, or possibly several equally large, MISs of a graph is called a maximum independent set. The graphs in which all maximal independent sets have the same size are called well-covered graphs.

The phrase "maximal independent set" is also used to describe maximal subsets of independent elements in mathematical structures other than graphs, and in particular in vector spaces and matroids.

Two algorithmic problems are associated with MISs: finding a "single" MIS in a given graph and listing "all" MISs in a given graph.

For a graph formula_13, an independent set formula_14 is a maximal independent set if for formula_15, one of the following is true:
The above can be restated as a vertex either belongs to the independent set or has at least one neighbor vertex that belongs to the independent set. As a result, every edge of the graph has at least one endpoint not in formula_14. However, it is not true that every edge of the graph has at least one, or even one endpoint in formula_14

Notice that any neighbor to a vertex in the independent set formula_14 cannot be in formula_14 because these vertices are disjoint by the independent set definition.

If "S" is a maximal independent set in some graph, it is a maximal clique or maximal complete subgraph in the complementary graph. A maximal clique is a set of vertices that induces a complete subgraph, and that is not a subset of the vertices of any larger complete subgraph. That is, it is a set "S" such that every pair of vertices in "S" is connected by an edge and every vertex not in "S" is missing an edge to at least one vertex in "S". A graph may have many maximal cliques, of varying sizes; finding the largest of these is the maximum clique problem.

Some authors include maximality as part of the definition of a clique, and refer to maximal cliques simply as cliques.

The complement of a maximal independent set, that is, the set of vertices not belonging to the independent set, forms a minimal vertex cover. That is, the complement is a vertex cover, a set of vertices that includes at least one endpoint of each edge, and is minimal in the sense that none of its vertices can be removed while preserving the property that it is a cover. Minimal vertex covers have been studied in statistical mechanics in connection with the hard-sphere lattice gas model, a mathematical abstraction of fluid-solid state transitions.

Every maximal independent set is a dominating set, a set of vertices such that every vertex in the graph either belongs to the set or is adjacent to the set. A set of vertices is a maximal independent set if and only if it is an independent dominating set.

Certain graph families have also been characterized in terms of their maximal cliques or maximal independent sets. Examples include the maximal-clique irreducible and hereditary maximal-clique irreducible graphs. A graph is said to be "maximal-clique irreducible" if every maximal clique has an edge that belongs to no other maximal clique, and "hereditary maximal-clique irreducible" if the same property is true for every induced subgraph. Hereditary maximal-clique irreducible graphs include triangle-free graphs, bipartite graphs, and interval graphs.

Cographs can be characterized as graphs in which every maximal clique intersects every maximal independent set, and in which the same property is true in all induced subgraphs.

 showed that any graph with "n" vertices has at most 3 maximal cliques. Complementarily, any graph with "n" vertices also has at most 3 maximal independent sets. A graph with exactly 3 maximal independent sets is easy to construct: simply take the disjoint union of "n"/3 triangle graphs. Any maximal independent set in this graph is formed by choosing one vertex from each triangle. The complementary graph, with exactly 3 maximal cliques, is a special type of Turán graph; because of their connection with Moon and Moser's bound, these graphs are also sometimes called Moon-Moser graphs. Tighter bounds are possible if one limits the size of the maximal independent sets: the number of maximal independent sets of size "k" in any "n"-vertex graph is at most
The graphs achieving this bound are again Turán graphs.

Certain families of graphs may, however, have much more restrictive bounds on the numbers of maximal independent sets or maximal cliques. If all "n"-vertex graphs in a family of graphs have O("n") edges, and if every subgraph of a graph in the family also belongs to the family, then each graph in the family can have at most O("n") maximal cliques, all of which have size O(1). For instance, these conditions are true for the planar graphs: every "n"-vertex planar graph has at most 3"n" − 6 edges, and a subgraph of a planar graph is always planar, from which it follows that each planar graph has O("n") maximal cliques (of size at most four). Interval graphs and chordal graphs also have at most "n" maximal cliques, even though they are not always sparse graphs.

The number of maximal independent sets in "n"-vertex cycle graphs is given by the Perrin numbers, and the number of maximal independent sets in "n"-vertex path graphs is given by the Padovan sequence. Therefore, both numbers are proportional to powers of 1.324718, the plastic number.

Given a Graph G(V,E), it is easy to find a single MIS using the following algorithm:


The runtime is O("m") since in the worst case as we have to check all edges.

O(m) is obviously the best possible runtime for a serial algorithm. But a parallel algorithm can solve the problem much faster.

The following algorithm finds a MIS in time O(log "n").


ANALYSIS: For each node v, divide its neighbours to "lower neighbours" (whose degree is lower than the degree of v) and "higher neighbours" (whose degree is higher than the degree of v), breaking ties as in the algorithm.

Call a node v "bad" if more than 2/3 of its neighbors are higher neighbours. Call an edge "bad" if both its endpoints are bad; otherwise the edge is "good".

A worst-case graph, in which the average number of steps is formula_26, is a graph made of "n"/2 connected components, each with 2 nodes. The degree of all nodes is 1, so each node is selected with probability 1/2, and with probability 1/4 both nodes in a component are not chosen. Hence, the number of nodes drops by a factor of 4 each step, and the expected number of steps is formula_27.

The following algorithm is better than the previous one in that at least one new node is always added in each connected component:


Note that in every step, the node with the smallest number in each connected component always enters I, so there is always some progress. In particular, in the worst-case of the previous algorithm ("n"/2 connected components with 2 nodes each), a MIS will be found in a single step.

ANALYSIS: 


</doc>
<doc id="426743" url="https://en.wikipedia.org/wiki?curid=426743" title="Graph coloring">
Graph coloring

In graph theory, graph coloring is a special case of graph labeling; it is an assignment of labels traditionally called "colors" to elements of a graph subject to certain constraints. In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices are of the same color; this is called a vertex coloring. Similarly, an edge coloring assigns a color to each edge so that no two adjacent edges are of the same color, and a face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color.

Vertex coloring is usually used to introduce graph coloring problems since other coloring problems can be transformed into a vertex coloring instance. For example, an edge coloring of a graph is just a vertex coloring of its line graph, and a face coloring of a plane graph is just a vertex coloring of its dual. However, non-vertex coloring problems are often stated and studied "as is". This is partly pedagogical, and partly because some problems are best studied in their non-vertex form, as in the case of edge coloring.

The convention of using colors originates from coloring the countries of a map, where each face is literally colored. This was generalized to coloring the faces of a graph embedded in the plane. By planar duality it became coloring the vertices, and in this form it generalizes to all graphs. In mathematical and computer representations, it is typical to use the first few positive or non-negative integers as the "colors". In general, one can use any finite set as the "color set". The nature of the coloring problem depends on the number of colors but not on what they are.

Graph coloring enjoys many practical applications as well as theoretical challenges. Beside the classical types of problems, different limitations can also be set on the graph, or on the way a color is assigned, or even on the color itself. It has even reached popularity with the general public in the form of the popular number puzzle Sudoku. Graph coloring is still a very active field of research.

"Note: Many terms used in this article are defined in Glossary of graph theory."

The first results about graph coloring deal almost exclusively with planar graphs in the form of the coloring of "maps".
While trying to color a map of the counties of England, Francis Guthrie postulated the four color conjecture, noting that four colors were sufficient to color the map so that no regions sharing a common border received the same color. Guthrie’s brother passed on the question to his mathematics teacher Augustus de Morgan at University College, who mentioned it in a letter to William Hamilton in 1852. Arthur Cayley raised the problem at a meeting of the London Mathematical Society in 1879. The same year, Alfred Kempe published a paper that claimed to establish the result, and for a decade the four color problem was considered solved. For his accomplishment Kempe was elected a Fellow of the Royal Society and later President of the London Mathematical Society.

In 1890, Heawood pointed out that Kempe’s argument was wrong. However, in that paper he proved the five color theorem, saying that every planar map can be colored with no more than "five" colors, using ideas of Kempe. In the following century, a vast amount of work and theories were developed to reduce the number of colors to four, until the four color theorem was finally proved in 1976 by Kenneth Appel and Wolfgang Haken. The proof went back to the ideas of Heawood and Kempe and largely disregarded the intervening developments. The proof of the four color theorem is also noteworthy for being the first major computer-aided proof.

In 1912, George David Birkhoff introduced the chromatic polynomial to study the coloring problems, which was generalised to the Tutte polynomial by Tutte, important structures in algebraic graph theory. Kempe had already drawn attention to the general, non-planar case in 1879, and many results on generalisations of planar graph coloring to surfaces of higher order followed in the early 20th century.

In 1960, Claude Berge formulated another conjecture about graph coloring, the "strong perfect graph conjecture", originally motivated by an information-theoretic concept called the zero-error capacity of a graph introduced by Shannon. The conjecture remained unresolved for 40 years, until it was established as the celebrated strong perfect graph theorem by Chudnovsky, Robertson, Seymour, and Thomas in 2002.

Graph coloring has been studied as an algorithmic problem since the early 1970s: the chromatic number problem is one of Karp’s 21 NP-complete problems from 1972, and at approximately the same time various exponential-time algorithms were developed based on backtracking and on the deletion-contraction recurrence of . One of the major applications of graph coloring, register allocation in compilers, was introduced in 1981.

When used without any qualification, a coloring of a graph is almost always a "proper vertex coloring", namely a labeling of the graph’s vertices with colors such that no two vertices sharing the same edge have the same color. Since a vertex with a loop (i.e. a connection directly back to itself) could never be properly colored, it is understood that graphs in this context are loopless.

The terminology of using "colors" for vertex labels goes back to map coloring. Labels like "red" and "blue" are only used when the number of colors is small, and normally it is understood that the labels are drawn from the integers {1, 2, 3, ...}.
A coloring using at most "k" colors is called a (proper) "k"-coloring.
The smallest number of colors needed to color a graph "G" is called its chromatic number, and is often denoted χ("G"). Sometimes γ("G") is used, since χ("G") is also used to denote the Euler characteristic of a graph.
A graph that can be assigned a (proper) "k"-coloring is "k"-colorable, and it is "k"-chromatic if its chromatic number is exactly "k".
A subset of vertices assigned to the same color is called a "color class", every such class forms an independent set. Thus, a "k"-coloring is the same as a partition of the vertex set into "k" independent sets, and the terms "k-partite" and "k-colorable" have the same meaning.

The chromatic polynomial counts the number of ways a graph can be colored using no more than a given number of colors. For example, using three colors, the graph in the adjacent image can be colored in 12 ways. With only two colors, it cannot be colored at all. With four colors, it can be colored in 24 + 4⋅12 = 72 ways: using all four colors, there are 4! = 24 valid colorings ("every" assignment of four colors to "any" 4-vertex graph is a proper coloring); and for every choice of three of the four colors, there are 12 valid 3-colorings. So, for the graph in the example, a table of the number of valid colorings would start like this:
The chromatic polynomial is a function "P"("G", "t") that counts the number of "t"-colorings of "G". As the name indicates, for a given "G" the function is indeed a polynomial in "t". For the example graph, "P"("G", "t") = "t"("t" − 1)("t" − 2), and indeed "P"("G", 4) = 72.

The chromatic polynomial includes at least as much information about the colorability of "G" as does the chromatic number. Indeed, χ is the smallest positive integer that is not a root of the chromatic polynomial

An edge coloring of a graph is a proper coloring of the "edges", meaning an assignment of colors to edges so that no vertex is incident to two edges of the same color. An edge coloring with "k" colors is called a "k"-edge-coloring and is equivalent to the problem of partitioning the edge set into "k" matchings. The smallest number of colors needed for an edge coloring of a graph "G" is the chromatic index, or edge chromatic number, "χ"′("G"). A Tait coloring is a 3-edge coloring of a cubic graph. The four color theorem is equivalent to the assertion that every planar cubic bridgeless graph admits a Tait coloring.

Total coloring is a type of coloring on the vertices "and" edges of a graph. When used without any qualification, a total coloring is always assumed to be proper in the sense that no adjacent vertices, no adjacent edges, and no edge and its end-vertices are assigned the same color. The total chromatic number "χ″(G)" of a graph G is the fewest colors needed in any total coloring of G.
An unlabeled coloring of a graph is an orbit of a coloring under the action of the automorphism group of the graph. If we interpret a coloring of a graph on formula_2 vertices as a vector in formula_3, the action of an automorphism is a permutation of the coefficients of the coloring.
There are analogues of the chromatic polynomials which count the number of unlabeled colorings of a graph from a given finite color set.

Assigning distinct colors to distinct vertices always yields a proper coloring, so

The only graphs that can be 1-colored are edgeless graphs. A complete graph formula_5 of "n" vertices requires formula_6 colors. In an optimal coloring there must be at least one of the graph’s "m" edges between every pair of color classes, so

If "G" contains a clique of size "k", then at least "k" colors are needed to color that clique; in other words, the chromatic number is at least the clique number:

For perfect graphs this bound is tight. Finding cliques is known as clique problem.

The 2-colorable graphs are exactly the bipartite graphs, including trees and forests.
By the four color theorem, every planar graph can be 4-colored.

A greedy coloring shows that every graph can be colored with one more color than the maximum vertex degree,

Complete graphs have formula_10 and formula_11, and odd cycles have formula_12 and formula_13, so for these graphs this bound is best possible. In all other cases, the bound can be slightly improved; Brooks’ theorem states that

Several lower bounds for the chromatic bounds have been discovered over the years:

Hoffman's bound: Let formula_15 be a real symmetric matrix such that formula_16 whenever formula_17 is not an edge in formula_18. Define formula_19, where formula_20 are the largest and smallest eigenvalues of formula_15. Define formula_22, with formula_15 as above. Then:

Lovász number: The Lovász number of a complementary graph, is also a lower bound on the chromatic number:

Fractional chromatic number: The Fractional chromatic number of a graph, is a lower bound on the chromatic number as well:

These bounds are ordered as follows:

Graphs with large cliques have a high chromatic number, but the opposite is not true. The Grötzsch graph is an example of a 4-chromatic graph without a triangle, and the example can be generalised to the Mycielskians.

From Brooks’s theorem, graphs with high chromatic number must have high maximum degree. Another local property that leads to high chromatic number is the presence of a large clique. But colorability is not an entirely local phenomenon: A graph with high girth looks locally like a tree, because all cycles are long, but its chromatic number need not be 2:

An edge coloring of "G" is a vertex coloring of its line graph formula_35, and vice versa. Thus,

There is a strong relationship between edge colorability and the graph’s maximum degree formula_37. Since all edges incident to the same vertex need their own color, we have

Moreover,

In general, the relationship is even stronger than what Brooks’s theorem gives for vertex coloring:

A graph has a "k"-coloring if and only if it has an acyclic orientation for which the longest path has length at most "k"; this is the Gallai–Hasse–Roy–Vitaver theorem .

For planar graphs, vertex colorings are essentially dual to nowhere-zero flows.

About infinite graphs, much less is known.
The following are two of the few results about infinite graph coloring:

As stated above, formula_43 A conjecture of Reed from 1998 is that the value is essentially closer to the lower bound, 
formula_44

The chromatic number of the plane, where two points are adjacent if they have unit distance, is unknown, although it is one of 5, 6, or 7. Other open problems concerning the chromatic number of graphs include the Hadwiger conjecture stating that every graph with chromatic number "k" has a complete graph on "k" vertices as a minor, the Erdős–Faber–Lovász conjecture bounding the chromatic number of unions of complete graphs that have at exactly one vertex in common to each pair, and the Albertson conjecture that among "k"-chromatic graphs the complete graphs are the ones with smallest crossing number.

When Birkhoff and Lewis introduced the chromatic polynomial in their attack on the four-color theorem, they conjectured that for planar graphs "G", the polynomial formula_45 has no zeros in the region formula_46. Although it is known that such a chromatic polynomial has no zeros in the region formula_47 and that formula_48, their conjecture is still unresolved. It also remains an unsolved problem to characterize graphs which have the same chromatic polynomial and to determine which polynomials are chromatic.

Determining if a graph can be colored with 2 colors is equivalent to determining whether or not the graph is bipartite, and thus computable in linear time using breadth-first search or depth-first search. More generally, the chromatic number and a corresponding coloring of perfect graphs can be computed in polynomial time using semidefinite programming. Closed formulas for chromatic polynomial are known for many classes of graphs, such as forests, chordal graphs, cycles, wheels, and ladders, so these can be evaluated in polynomial time.

If the graph is planar and has low branch-width (or is nonplanar but with a known branch decomposition), then it can be solved in polynomial time using dynamic programming. In general, the time required is polynomial in the graph size, but exponential in the branch-width.

Brute-force search for a "k"-coloring considers each of the formula_49 assignments of "k" colors to "n" vertices and checks for each if it is legal. To compute the chromatic number and the chromatic polynomial, this procedure is used for every formula_50, impractical for all but the smallest input graphs.

Using dynamic programming and a bound on the number of maximal independent sets, "k"-colorability can be decided in time and space formula_51. Using the principle of inclusion–exclusion and Yates’s algorithm for the fast zeta transform,
"k"-colorability can be decided in time formula_52 for any "k". Faster algorithms are known for 3- and 4-colorability, which can be decided in time formula_53 and formula_54, respectively.

The contraction formula_55 of a graph "G" is the graph obtained by identifying the vertices "u" and "v", and removing any edges between them. The remaining edges originally incident to "u" or "v" are now incident to their identification. This operation plays a major role in the analysis of graph coloring.

The chromatic number satisfies the recurrence relation:
due to ,
where "u" and "v" are non-adjacent vertices, and formula_57 is the graph with the edge added. Several algorithms are based on evaluating this recurrence and the resulting computation tree is sometimes called a Zykov tree. The running time is based on a heuristic for choosing the vertices "u" and "v".

The chromatic polynomial satisfies the following recurrence relation
where "u" and "v" are adjacent vertices, and formula_59 is the graph with the edge removed. formula_60 represents the number of possible proper colorings of the graph, where the vertices may have the same or different colors. Then the proper colorings arise from two different graphs. To explain, if the vertices "u" and "v" have different colors, then we might as well consider a graph where "u" and "v" are adjacent. If "u" and "v" have the same colors, we might as well consider a graph where "u" and "v" are contracted. Tutte’s curiosity about which other graph properties satisfied this recurrence led him to discover a bivariate generalization of the chromatic polynomial, the Tutte polynomial.

These expressions give rise to a recursive procedure called the "deletion–contraction algorithm", which forms the basis of many algorithms for graph coloring. The running time satisfies the same recurrence relation as the Fibonacci numbers, so in the worst case the algorithm runs in time within a polynomial factor of formula_61 for "n" vertices and "m" edges. The analysis can be improved to within a polynomial factor of the number formula_62 of spanning trees of the input graph.
In practice, branch and bound strategies and graph isomorphism rejection are employed to avoid some recursive calls. The running time depends on the heuristic used to pick the vertex pair.

The greedy algorithm considers the vertices in a specific order formula_63,…,formula_64 and assigns to formula_65 the smallest available color not used by formula_65’s neighbours among formula_63,…,formula_68, adding a fresh color if needed. The quality of the resulting coloring depends on the chosen ordering. There exists an ordering that leads to a greedy coloring with the optimal number of formula_69 colors. On the other hand, greedy colorings can be arbitrarily bad; for example, the crown graph on "n" vertices can be 2-colored, but has an ordering that leads to a greedy coloring with formula_70 colors.

For chordal graphs, and for special cases of chordal graphs such as interval graphs and indifference graphs, the greedy coloring algorithm can be used to find optimal colorings in polynomial time, by choosing the vertex ordering to be the reverse of a perfect elimination ordering for the graph. The perfectly orderable graphs generalize this property, but it is NP-hard to find a perfect ordering of these graphs.

If the vertices are ordered according to their degrees, the resulting greedy coloring uses at most formula_71 colors, at most one more than the graph’s maximum degree. This heuristic is sometimes called the Welsh–Powell algorithm. Another heuristic due to Brélaz establishes the ordering dynamically while the algorithm proceeds, choosing next the vertex adjacent to the largest number of different colors. Many other graph coloring heuristics are similarly based on greedy coloring for a specific static or dynamic strategy of ordering the vertices, these algorithms are sometimes called sequential coloring algorithms.

The maximum (worst) number of colors that can be obtained by the greedy algorithm, by using a vertex ordering chosen to maximize this number, is called the Grundy number of a graph.

In the field of distributed algorithms, graph coloring is closely related to the problem of symmetry breaking. The current state-of-the-art randomized algorithms are faster for sufficiently large maximum degree Δ than deterministic algorithms. The fastest randomized algorithms employ the multi-trials technique by Schneider et al.

In a symmetric graph, a deterministic distributed algorithm cannot find a proper vertex coloring. Some auxiliary information is needed in order to break symmetry. A standard assumption is that initially each node has a "unique identifier", for example, from the set {1, 2, ..., "n"}. Put otherwise, we assume that we are given an "n"-coloring. The challenge is to "reduce" the number of colors from "n" to, e.g., Δ + 1. The more colors are employed, e.g. O(Δ) instead of Δ + 1, the fewer communication rounds are required.

A straightforward distributed version of the greedy algorithm for (Δ + 1)-coloring requires Θ("n") communication rounds in the worst case − information may need to be propagated from one side of the network to another side.

The simplest interesting case is an "n"-cycle. Richard Cole and Uzi Vishkin show that there is a distributed algorithm that reduces the number of colors from "n" to "O"(log "n") in one synchronous communication step. By iterating the same procedure, it is possible to obtain a 3-coloring of an "n"-cycle in "O"( "n") communication steps (assuming that we have unique node identifiers).

The function , iterated logarithm, is an extremely slowly growing function, "almost constant". Hence the result by Cole and Vishkin raised the question of whether there is a "constant-time" distributed algorithm for 3-coloring an "n"-cycle. showed that this is not possible: any deterministic distributed algorithm requires Ω( "n") communication steps to reduce an "n"-coloring to a 3-coloring in an "n"-cycle.

The technique by Cole and Vishkin can be applied in arbitrary bounded-degree graphs as well; the running time is poly(Δ) + "O"( "n"). The technique was extended to unit disk graphs by Schneider et al. The fastest deterministic algorithms for (Δ + 1)-coloring for small Δ are due to Leonid Barenboim, Michael Elkin and Fabian Kuhn. The algorithm by Barenboim et al. runs in time "O"(Δ) + ("n")/2, which is optimal in terms of "n" since the constant factor 1/2 cannot be improved due to Linial's lower bound. use network decompositions to compute a Δ+1 coloring in time formula_72.

The problem of edge coloring has also been studied in the distributed model. achieve a (2Δ − 1)-coloring in "O"(Δ +  "n") time in this model. The lower bound for distributed vertex coloring due to applies to the distributed edge coloring problem as well.

Decentralized algorithms are ones where no message passing is allowed (in contrast to distributed algorithms where local message passing takes places), and efficient decentralized algorithms exist that will color a graph if a proper coloring exists. These assume that a vertex is able to sense whether any of its neighbors are using the same color as the vertex i.e., whether a local conflict exists. This is a mild assumption in many applications e.g. in wireless channel allocation it is usually reasonable to assume that a station will be able to detect whether other interfering transmitters are using the same channel (e.g. by measuring the SINR). This sensing information is sufficient to allow algorithms based on learning automata to find a proper graph coloring with probability one.

Graph coloring is computationally hard. It is NP-complete to decide if a given graph admits a "k"-coloring for a given "k" except for the cases "k" ∈ {0,1,2} . In particular, it is NP-hard to compute the chromatic number.
The 3-coloring problem remains NP-complete even on 4-regular planar graphs. However, for every "k" > 3, a "k"-coloring of a planar graph exists by the four color theorem, and it is possible to find such a coloring in polynomial time.

The best known approximation algorithm computes a coloring of size at most within a factor O("n"(log log "n")(log n)) of the chromatic number. For all "ε" > 0, approximating the chromatic number within "n" is NP-hard.

It is also NP-hard to color a 3-colorable graph with 4 colors and a "k"-colorable graph with "k" colors for sufficiently large constant "k".

Computing the coefficients of the chromatic polynomial is #P-hard. In fact, even computing the value of formula_73 is #P-hard at any rational point "k" except for "k" = 1 and "k" = 2. There is no FPRAS for evaluating the chromatic polynomial at any rational point "k" ≥ 1.5 except for "k" = 2 unless NP = RP.

For edge coloring, the proof of Vizing’s result gives an algorithm that uses at most Δ+1 colors. However, deciding between the two candidate values for the edge chromatic number is NP-complete. In terms of approximation algorithms, Vizing’s algorithm shows that the edge chromatic number can be approximated to within 4/3,
and the hardness result shows that no (4/3 − "ε" )-algorithm exists for any "ε > 0" unless P = NP. These are among the oldest results in the literature of approximation algorithms, even though neither paper makes explicit use of that notion.

Vertex coloring models to a number of scheduling problems. In the cleanest form, a given set of jobs need to be assigned to time slots, each job requires one such slot. Jobs can be scheduled in any order, but pairs of jobs may be in "conflict" in the sense that they may not be assigned to the same time slot, for example because they both rely on a shared resource. The corresponding graph contains a vertex for every job and an edge for every conflicting pair of jobs. The chromatic number of the graph is exactly the minimum "makespan", the optimal time to finish all jobs without conflicts.

Details of the scheduling problem define the structure of the graph. For example, when assigning aircraft to flights, the resulting conflict graph is an interval graph, so the coloring problem can be solved efficiently. In bandwidth allocation to radio stations, the resulting conflict graph is a unit disk graph, so the coloring problem is 3-approximable.

A compiler is a computer program that translates one computer language into another. To improve the execution time of the resulting code, one of the techniques of compiler optimization is register allocation, where the most frequently used values of the compiled program are kept in the fast processor registers. Ideally, values are assigned to registers so that they can all reside in the registers when they are used.

The textbook approach to this problem is to model it as a graph coloring problem. The compiler constructs an "interference graph", where vertices are variables and an edge connects two vertices if they are needed at the same time. If the graph can be colored with "k" colors then any set of variables needed at the same time can be stored in at most "k" registers.

The problem of coloring a graph arises in many practical areas such as pattern matching, sports scheduling, designing seating plans, exam timetabling, the scheduling of taxis, and solving Sudoku puzzles.

An important class of "improper" coloring problems is studied in Ramsey theory, where the graph’s edges are assigned to colors, and there is no restriction on the colors of incident edges. A simple example is the friendship theorem, which states that in any coloring of the edges of formula_74, the complete graph of six vertices, there will be a monochromatic triangle; often illustrated by saying that any group of six people either has three mutual strangers or three mutual acquaintances. Ramsey theory is concerned with generalisations of this idea to seek regularity amid disorder, finding general conditions for the existence of monochromatic subgraphs with given structure.



Coloring can also be considered for signed graphs and gain graphs.






</doc>
<doc id="244431" url="https://en.wikipedia.org/wiki?curid=244431" title="Bipartite graph">
Bipartite graph

In the mathematical field of graph theory, a bipartite graph (or bigraph) is a graph whose vertices can be divided into two disjoint and independent sets formula_1 and formula_2 such that every edge connects a vertex in formula_1 to one in formula_2. Vertex sets formula_1 and formula_2 are usually called the "parts" of the graph. Equivalently, a bipartite graph is a graph that does not contain any odd-length cycles.

The two sets formula_1 and formula_2 may be thought of as a coloring of the graph with two colors: if one colors all nodes in formula_1 blue, and all nodes in formula_2 green, each edge has endpoints of differing colors, as is required in the graph coloring problem. In contrast, such a coloring is impossible in the case of a non-bipartite graph, such as a triangle: after one node is colored blue and another green, the third vertex of the triangle is connected to vertices of both colors, preventing it from being assigned either color.

One often writes formula_11 to denote a bipartite graph whose partition has the parts formula_1 and formula_2, with formula_14 denoting the edges of the graph. If a bipartite graph is not connected, it may have more than one bipartition; in this case, the formula_15 notation is helpful in specifying one particular bipartition that may be of importance in an application. If formula_16, that is, if the two subsets have equal cardinality, then formula_17 is called a "balanced" bipartite graph. If all vertices on the same side of the bipartition have the same degree, then formula_17 is called biregular.

When modelling relations between two different classes of objects, bipartite graphs very often arise naturally. For instance, a graph of football players and clubs, with an edge between a player and a club if the player has played for that club, is a natural example of an "affiliation network", a type of bipartite graph used in social network analysis.

Another example where bipartite graphs appear naturally is in the (NP-complete) railway optimization problem, in which the input is a schedule of trains and their stops, and the goal is to find a set of train stations as small as possible such that every train visits at least one of the chosen stations. This problem can be modeled as a dominating set problem in a bipartite graph that has a vertex for each train and each station and an edge for
each pair of a station and a train that stops at that station.

A third example is in the academic field of numismatics. Ancient coins are made using two positive impressions of the design (the obverse and reverse). The charts numismatists produce to represent the production of coins are bipartite graphs.

More abstract examples include the following:

Bipartite graphs may be characterized in several different ways:

In bipartite graphs, the size of minimum vertex cover is equal to the size of the maximum matching; this is Kőnig's theorem. An alternative and equivalent form of this theorem is that the size of the maximum independent set plus the size of the maximum matching is equal to the number of vertices.
In any graph without isolated vertices the size of the minimum edge cover plus the size of a maximum matching equals the number of vertices. Combining this equality with Kőnig's theorem leads to the facts that, in bipartite graphs, the size of the minimum edge cover is equal to the size of the maximum independent set, and the size of the minimum edge cover plus the size of the minimum vertex cover is equal to the number of vertices.

Another class of related results concerns perfect graphs: every bipartite graph, the complement of every bipartite graph, the line graph of every bipartite graph, and the complement of the line graph of every bipartite graph, are all perfect. Perfection of bipartite graphs is easy to see (their chromatic number is two and their maximum clique size is also two) but perfection of the complements of bipartite graphs is less trivial, and is another restatement of Kőnig's theorem. This was one of the results that motivated the initial definition of perfect graphs. Perfection of the complements of line graphs of perfect graphs is yet another restatement of Kőnig's theorem, and perfection of the line graphs themselves is a restatement of an earlier theorem of Kőnig, that every bipartite graph has an edge coloring using a number of colors equal to its maximum degree.

According to the strong perfect graph theorem, the perfect graphs have a forbidden graph characterization resembling that of bipartite graphs: a graph is bipartite if and only if it has no odd cycle as a subgraph, and a graph is perfect if and only if it has no odd cycle or its complement as an induced subgraph. The bipartite graphs, line graphs of bipartite graphs, and their complements form four out of the five basic classes of perfect graphs used in the proof of the strong perfect graph theorem.

For a vertex, the number of adjacent vertices is called the degree of the vertex and is denoted formula_20.
The "degree sum formula" for a bipartite graph states that

The degree sequence of a bipartite graph is the pair of lists each containing the degrees of the two parts formula_1 and formula_2. For example, the complete bipartite graph "K" has degree sequence formula_24. Isomorphic bipartite graphs have the same degree sequence. However, the degree sequence does not, in general, uniquely identify a bipartite graph; in some cases, non-isomorphic bipartite graphs may have the same degree sequence.

The bipartite realization problem is the problem of finding a simple bipartite graph with the degree sequence being two given lists of natural numbers. (Trailing zeros may be ignored since they are trivially realized by adding an appropriate number of isolated vertices to the digraph.)

The biadjacency matrix of a bipartite graph formula_15 is a (0,1) matrix of size formula_26 that has a one for each pair of adjacent vertices and a zero for nonadjacent vertices. Biadjacency matrices may be used to describe equivalences between bipartite graphs, hypergraphs, and directed graphs.

A hypergraph is a combinatorial structure that, like an undirected graph, has vertices and edges, but in which the edges may be arbitrary sets of vertices rather than having to have exactly two endpoints. A bipartite graph formula_15 may be used to model a hypergraph in which is the set of vertices of the hypergraph, is the set of hyperedges, and contains an edge from a hypergraph vertex to a hypergraph edge exactly when is one of the endpoints of . Under this correspondence, the biadjacency matrices of bipartite graphs are exactly the incidence matrices of the corresponding hypergraphs. As a special case of this correspondence between bipartite graphs and hypergraphs, any multigraph (a graph in which there may be two or more edges between the same two vertices) may be interpreted as a hypergraph in which some hyperedges have equal sets of endpoints, and represented by a bipartite graph that does not have multiple adjacencies and in which the vertices on one side of the bipartition all have degree two.

A similar reinterpretation of adjacency matrices may be used to show a one-to-one correspondence between directed graphs (on a given number of labeled vertices, allowing self-loops) and balanced bipartite graphs, with the same number of vertices on both sides of the bipartition. For, the adjacency matrix of a directed graph with vertices can be any (0,1) matrix of size formula_28, which can then be reinterpreted as the adjacency matrix of a bipartite graph with vertices on each side of its bipartition. In this construction, the bipartite graph is the bipartite double cover of the directed graph.

It is possible to test whether a graph is bipartite, and to return either a two-coloring (if it is bipartite) or an odd cycle (if it is not) in linear time, using depth-first search. The main idea is to assign to each vertex the color that differs from the color of its parent in the depth-first search forest, assigning colors in a preorder traversal of the depth-first-search forest. This will necessarily provide a two-coloring of the spanning forest consisting of the edges connecting vertices to their parents, but it may not properly color some of the non-forest edges. In a depth-first search forest, one of the two endpoints of every non-forest edge is an ancestor of the other endpoint, and when the depth first search discovers an edge of this type it should check that these two vertices have different colors. If they do not, then the path in the forest from ancestor to descendant, together with the miscolored edge, form an odd cycle, which is returned from the algorithm together with the result that the graph is not bipartite. However, if the algorithm terminates without detecting an odd cycle of this type, then every edge must be properly colored, and the algorithm returns the coloring together with the result that the graph is bipartite.

Alternatively, a similar procedure may be used with breadth-first search in place of depth-first search. Again, each node is given the opposite color to its parent in the search forest, in breadth-first order. If, when a vertex is colored, there exists an edge connecting it to a previously-colored vertex with the same color, then this edge together with the paths in the breadth-first search forest connecting its two endpoints to their lowest common ancestor forms an odd cycle. If the algorithm terminates without finding an odd cycle in this way, then it must have found a proper coloring, and can safely conclude that the graph is bipartite.

For the intersection graphs of formula_29 line segments or other simple shapes in the Euclidean plane, it is possible to test whether the graph is bipartite and return either a two-coloring or an odd cycle in time formula_30, even though the graph itself may have up to formula_31 edges.

Odd cycle transversal is an NP-complete algorithmic problem that asks, given a graph "G" = ("V","E") and a number "k", whether there exists a set of "k" vertices whose removal from "G" would cause the resulting graph to be bipartite. The problem is fixed-parameter tractable, meaning that there is an algorithm whose running time can be bounded by a polynomial function of the size of the graph multiplied by a larger function of "k". The name "odd cycle transversal" comes from the fact that a graph is bipartite if and only if it has no odd cycles. Hence, to delete vertices from a graph in order to obtain a bipartite graph, one needs to "hit all odd cycle", or find a so-called odd cycle transversal set. In the illustration, every odd cycle in the graph contains the blue (the bottommost) vertices, so removing those vertices kills all odd cycles and leaves a bipartite graph.

The "edge bipartization" problem is the algorithmic problem of deleting as few edges as possible to make a graph bipartite and is also an important problem in graph modification algorithmics. This problem is also fixed-parameter tractable, and can be solved in time "O"(2 "m"), where "k" is the number of edges to delete and "m" is the number of edges in the input graph.

A matching in a graph is a subset of its edges, no two of which share an endpoint. Polynomial time algorithms are known for many algorithmic problems on matchings, including maximum matching (finding a matching that uses as many edges as possible), maximum weight matching, and stable marriage. In many cases, matching problems are simpler to solve on bipartite graphs than on non-bipartite graphs, and many matching algorithms such as the Hopcroft–Karp algorithm for maximum cardinality matching work correctly only on bipartite inputs.

As a simple example, suppose that a set formula_32 of people are all seeking jobs from among a set of formula_33 jobs, with not all people suitable for all jobs. This situation can be modeled as a bipartite graph formula_34 where an edge connects each job-seeker with each suitable job. A perfect matching describes a way of simultaneously satisfying all job-seekers and filling all jobs; Hall's marriage theorem provides a characterization of the bipartite graphs which allow perfect matchings. The National Resident Matching Program applies graph matching methods to solve this problem for U.S. medical student job-seekers and hospital residency jobs.

The Dulmage–Mendelsohn decomposition is a structural decomposition of bipartite graphs that is useful in finding maximum matchings.

Bipartite graphs are extensively used in modern coding theory, especially to decode codewords received from the channel. Factor graphs and Tanner graphs are examples of this. A Tanner graph is a bipartite graph in which the vertices on one side of the bipartition represent digits of a codeword, and the vertices on the other side represent combinations of digits that are expected to sum to zero in a codeword without errors. A factor graph is a closely related belief network used for probabilistic decoding of LDPC and turbo codes.

In computer science, a Petri net is a mathematical modeling tool used in analysis and simulations of concurrent systems. A system is modeled as a bipartite directed graph with two sets of nodes: A set of "place" nodes that contain resources, and a set of "event" nodes which generate and/or consume resources. There are additional constraints on the nodes and edges that constrain the behavior of the system. Petri nets utilize the properties of bipartite directed graphs and other properties to allow mathematical proofs of the behavior of systems while also allowing easy implementation of simulations of the system.

In projective geometry, Levi graphs are a form of bipartite graph used to model the incidences between points and lines in a configuration. Corresponding to the geometric property of points and lines that every two lines meet in at most one point and every two points be connected with a single line, Levi graphs necessarily do not contain any cycles of length four, so their girth must be six or more.




</doc>
<doc id="21051195" url="https://en.wikipedia.org/wiki?curid=21051195" title="Greedy coloring">
Greedy coloring

In the study of graph coloring problems in mathematics and computer science, a greedy coloring or sequential coloring is a coloring of the vertices of a graph formed by a greedy algorithm that considers the vertices of the graph in sequence and assigns each vertex its first available color. Greedy colorings can be found in linear time, but they do not in general use the minimum number of colors possible.

Different choices of the sequence of vertices will typically produce different colorings of the given graph, so much of the study of greedy colorings has concerned how to find a good ordering. There always exists an ordering that produces an optimal coloring, but although such orderings can be found for many special classes of graphs, they are hard to find in general. Commonly used strategies for vertex ordering involve placing higher-degree vertices earlier than lower-degree vertices, or choosing vertices with fewer available colors in preference to vertices that are less constrained.

Variations of greedy coloring choose the colors in an online manner, without any knowledge of the structure of the uncolored part of the graph, or choose other colors than the first available in order to reduce the total number of colors. Greedy coloring algorithms have been applied to scheduling and register allocation problems, the analysis of combinatorial games, and the proofs of other mathematical results including Brooks' theorem on the relation between coloring and degree.
Other concepts in graph theory derived from greedy colorings include the Grundy number of a graph (the largest number of colors that can be found by a greedy coloring), and the well-colored graphs, graphs for which all greedy colorings use the same number of colors.

The greedy coloring for a given vertex ordering can be computed by an algorithm that runs in linear time. The algorithm processes the vertices in the given ordering, assigning a color to each one as it is processed. The colors may be represented by the numbers formula_1 and each vertex is given the color with the smallest number that is not already used by one of its neighbors.
To find the smallest available color, one may use an array to count the number of neighbors of each color (or alternatively, to represent the set of colors of neighbors), and then scan the array to find the index of its first zero.

In Python, the algorithm can be expressed as:
The codice_1 subroutine takes time proportional to the length of its argument list, because it performs two loops, one over the list itself and one over a list of counts that has the same length. The time for the overall coloring algorithm is dominated by the calls to this subroutine. Each edge in the graph contributes to only one of these calls, the one for the endpoint of the edge that is later in the vertex ordering. Therefore the sum of the lengths of the argument lists to codice_1, and the total time for the algorithm, are proportional to the number of edges in the graph.

An alternative algorithm, producing the same coloring, is to choose the sets of vertices with each color, one color at a time. In this method, each color class formula_2 is chosen by scanning through the vertices in the given ordering. When this scan encounters an uncolored vertex formula_3 that has no neighbor in formula_2, it adds formula_3 to formula_2. In this way, formula_2 becomes a maximal independent set among the vertices that were not already assigned smaller colors. The algorithm repeatedly finds color classes in this way until all vertices are colored. However, it involves making multiple scans of the graph, one scan for each color class, instead of the method outlined above which uses only a single scan.

Different orderings of the vertices of a graph may cause the greedy coloring to use different numbers of colors, ranging from the optimal number of colors to, in some cases, a number of colors that is proportional to the number of vertices in the graph.
For instance, a crown graph (a complete bipartite graph , with the edges of a perfect matching removed) can be a particularly bad case for greedy coloring. If the vertex ordering places two vertices consecutively whenever they belong to one of the pairs of the removed matching, then a greedy coloring will use colors, one color for each of these pairs of vertices. However, the optimal number of colors for this graph is two. There also exist graphs such that with high probability a randomly chosen vertex ordering leads to a number of colors much larger than the minimum. Therefore, it is of some importance in greedy coloring to choose the vertex ordering carefully.

The vertices of any graph may always be ordered in such a way that the greedy algorithm produces an optimal coloring. For, given any optimal coloring, one may order the vertices by their colors. Then when one uses a greedy algorithm with this order, the resulting coloring is automatically optimal. However, because optimal graph coloring is NP-complete, any subproblem that would allow this problem to be solved quickly, including finding an optimal ordering for greedy coloring, is NP-hard.

In interval graphs and chordal graphs, if the vertices are ordered in the reverse of a perfect elimination ordering,
then the earlier neighbors of every vertex will form a clique. This property causes the greedy coloring to produce an optimal coloring, because it never uses more colors than are required for each of these cliques. An elimination ordering can be found in linear time, when it exists.

More strongly, any perfect elimination ordering is "hereditarily optimal", meaning that it is optimal both for the graph itself and for all of its induced subgraphs. The perfectly orderable graphs (which include chordal graphs, comparability graphs, and distance-hereditary graphs) are defined as the graphs that have a hereditarily optimal ordering. Recognizing perfectly orderable graphs is also NP-complete.

The number of colors produced by the greedy coloring for the worst ordering of a given graph is called its Grundy number.
Just as finding a good vertex ordering for greedy coloring is difficult, so is finding a bad vertex ordering.
It is NP-complete to determine, for a given graph and number , whether there exists an ordering of the vertices of that causes the greedy algorithm to use or more colors. In particular, this means that it is difficult to find the worst ordering for .

The well-colored graphs are the graphs for which all vertex colorings produce the same number of colors. This number of colors, in these graphs, equals both the chromatic number and the Grundy number. They include the cographs, which are exactly the graphs in which all induced subgraphs are well-colored. However, it is co-NP-complete to determine whether a graph is well-colored.

If a random graphs is drawn from the Erdős–Rényi model with constant probability of including each edge, then any vertex ordering that is chosen independently of the graph edges leads to a coloring whose number of colors is close to twice the optimal value, with high probability.
It remains unknown whether there is any polynomial time method for finding significantly better colorings of these graphs.

Because optimal vertex orderings are hard to find, heuristics have been used that attempt to reduce the number of colors while not guaranteeing an optimal number of colors. A commonly used ordering for greedy coloring is to choose a vertex of minimum degree, order the subgraph with removed recursively, and then place last in the ordering. The largest number encountered during this algorithm as the degree of a removed vertex is called the degeneracy of the graph. In the context of greedy coloring, the same ordering strategy is also called the smallest last ordering. This vertex ordering, and the degeneracy, may be computed in linear time.
It can be viewed as an improved version of an earlier vertex ordering method, the largest-first ordering, which sorts the vertices in descending order by their degrees.

With the degeneracy ordering, the greedy coloring will use at most colors. This is because, when colored, each vertex will have at most already-colored neighbors, so one of the first colors will be free for it to use. Greedy coloring with the degeneracy ordering can find optimal colorings for certain classes of graphs, including trees, pseudoforests, and crown graphs. define a graph formula_8 to be formula_9-perfect if, for formula_8 and every induced subgraph of formula_8, the chromatic number equals the degeneracy plus one. For these graphs, the greedy algorithm with the degeneracy ordering is always optimal.
Every formula_9-perfect must be an even-hole-free graph, because even cycles have chromatic number two and degeneracy two, not matching the equality in the definition of formula_9-perfect graphs. If a graph and its complement graph are both even-hole-free, they are both formula_9-perfect. The graphs that are both perfect graphs and formula_9-perfect graphs are exactly the chordal graphs. On even-hole-free graphs more generally, the degeneracy ordering produces a 2-approximation to the optimal coloring. On unit disk graphs it produces a 3-approximation. The triangular prism is the smallest graph for which one of its degeneracy orderings leads to a non-optimal coloring, and the square antiprism is the smallest graph that cannot be optimally colored using any of its degeneracy orderings.

 proposes a strategy, called DSatur, for vertex ordering in greedy coloring that interleaves the construction of the ordering with the coloring process.
In his version of the greedy coloring algorithm, the next vertex to color at each step is chosen as the one with the largest number of distinct colors in its neighborhood. In case of ties, a vertex of maximal degree in the subgraph of uncolored vertices is chosen from the tied vertices. By keeping track of the sets of neighboring colors and their cardinalities at each step, it is possible to implement this method in linear time.

This method can find the optimal colorings for bipartite graphs, all cactus graphs, all wheel graphs, all graphs on at most six vertices, and almost every formula_16-colorable graph. Although originally claimed that this method finds optimal colorings for the Meyniel graphs, they later found a counterexample to this claim.

It is possible to define variations of the greedy coloring algorithm in which the vertices of the given graph are colored in a given sequence but in which the color chosen for each vertex is not necessarily the first available color. These include methods in which the uncolored part of the graph is unknown to the algorithm, or in which the algorithm is given some freedom to make better coloring choices than the basic greedy algorithm would.

Alternative color selection strategies have been studied within the framework of online algorithms. In the online graph-coloring problem, vertices of a graph are presented one at a time in an arbitrary order to a coloring algorithm; the algorithm must choose a color for each vertex, based only on the colors of and adjacencies among already-processed vertices. In this context, one measures the quality of a color selection strategy by its competitive ratio, the ratio between the number of colors it uses and the optimal number of colors for the given graph.

If no additional restrictions on the graph are given, the optimal competitive ratio is only slightly sublinear. However, for interval graphs, a constant competitive ratio is possible, while for bipartite graphs and sparse graphs a logarithmic ratio can be achieved. Indeed, for sparse graphs, the standard greedy coloring strategy of choosing the first available color achieves this competitive ratio, and it is possible to prove a matching lower bound on the competitive ratio of any online coloring algorithm.

A "parsimonious coloring", for a given graph and vertex ordering, has been defined to be a coloring produced by a greedy algorithm that colors the vertices in the given order, and only introduces a new color when all previous colors are adjacent to the given vertex, but can choose which color to use (instead of always choosing the smallest) when it is able to re-use an existing color. The "ordered chromatic number" is the smallest number of colors that can be obtained for the given ordering in this way, and the "ochromatic number" is the largest ordered chromatic number among all vertex colorings of a given graph. Despite its different definition, the ochromatic number always equals the Grundy number.

Because it is fast and in many cases can use few colors, greedy coloring can be used in applications where a good but not optimal graph coloring is needed. One of the early applications of the greedy algorithm was to problems such as course scheduling, in which a collection of tasks must be assigned to a given set of time slots, avoiding incompatible tasks being assigned to the same time slot.
It can also be used in compilers for register allocation, by applying it to a graph whose vertices represent values to be assigned to registers and whose edges represent conflicts between two values that cannot be assigned to the same register. In many cases, these interference graphs are chordal graphs, allowing greedy coloring to produce an optimal register assignment.

In combinatorial game theory, for an impartial game given in explicit form as a directed acyclic graph whose vertices represent game positions and whose edges represent valid moves from one position to another, the greedy coloring algorithm (using the reverse of a topological ordering of the graph) calculates the nim-value of each position. These values can be used to determine optimal play in any single game or any disjunctive sum of games.

For a graph of maximum degree , any greedy coloring will use at most colors. Brooks' theorem states that with two exceptions (cliques and odd cycles) at most colors are needed. One proof of Brooks' theorem involves finding a vertex ordering in which the first two vertices are adjacent to the final vertex but not adjacent to each other, and each vertex other than the last one has at least one later neighbor. For an ordering with this property, the greedy coloring algorithm uses at most colors.



</doc>
<doc id="562782" url="https://en.wikipedia.org/wiki?curid=562782" title="Vertex cover">
Vertex cover

In the mathematical discipline of graph theory, a vertex cover (sometimes node cover) of a graph is a set of vertices such that each edge of the graph is incident to at least one vertex of the set.
The problem of finding a minimum vertex cover is a classical optimization problem in computer science and is a typical example of an NP-hard optimization problem that has an approximation algorithm. Its decision version, the vertex cover problem, was one of Karp's 21 NP-complete problems and is therefore a classical NP-complete problem in computational complexity theory. Furthermore, the vertex cover problem is fixed-parameter tractable and a central problem in parameterized complexity theory.

The minimum vertex cover problem can be formulated as a half-integral linear program whose dual linear program is the maximum matching problem.

Formally, a vertex cover formula_1 of an undirected graph formula_2 is a subset of formula_3 such that formula_4, that is to say it is a set of vertices formula_1 where every edge has at least one endpoint in the vertex cover formula_1. Such a set is said to "cover" the edges of formula_7. The following figure shows two examples of vertex covers, with some vertex cover formula_1 marked in red.

A "minimum vertex cover" is a vertex cover of smallest possible size. The vertex cover number formula_9 is the size of a minimum vertex cover, i.e. formula_10. The following figure shows examples of minimum vertex covers in the previous graphs.



The minimum vertex cover problem is the optimization problem of finding a smallest vertex cover in a given graph.
If the problem is stated as a decision problem, it is called the vertex cover problem:

The vertex cover problem is an NP-complete problem: it was one of Karp's 21 NP-complete problems. It is often used in computational complexity theory as a starting point for NP-hardness proofs.

Assume that every vertex has an associated cost of formula_21.
The (weighted) minimum vertex cover problem can be formulated as the following integer linear program (ILP).
This ILP belongs to the more general class of ILPs for covering problems.
The integrality gap of this ILP is formula_22, so its relaxation gives a factor-formula_22 approximation algorithm for the minimum vertex cover problem.
Furthermore, the linear programming relaxation of that ILP is "half-integral", that is, there exists an optimal solution for which each entry formula_24 is either 0, 1/2, or 1.

The decision variant of the vertex cover problem is NP-complete, which means it is unlikely that there is an efficient algorithm to solve it exactly for arbitrary graphs. NP-completeness can be proven by reduction from 3-satisfiability or, as Karp did, by reduction from the clique problem. Vertex cover remains NP-complete even in cubic graphs and even in planar graphs of degree at most 3.

For bipartite graphs, the equivalence between vertex cover and maximum matching described by Kőnig's theorem allows the bipartite vertex cover problem to be solved in polynomial time.

For tree graphs, an algorithm finds a minimal vertex cover in polynomial time by finding the first leaf in the tree and adding its parent to the minimal vertex cover, then deleting the leaf and parent and all associated edges and continuing repeatedly until no edges remain in the tree.

An exhaustive search algorithm can solve the problem in time 2"n", where "k" is the size of the vertex cover. Vertex cover is therefore fixed-parameter tractable, and if we are only interested in small "k", we can solve the problem in polynomial time. One algorithmic technique that works here is called "bounded search tree algorithm", and its idea is to repeatedly choose some vertex and recursively branch, with two cases at each step: place either the current vertex or all its neighbours into the vertex cover.
The algorithm for solving vertex cover that achieves the best asymptotic dependence on the parameter runs in time formula_25. The klam value of this time bound (an estimate for the largest parameter value that could be solved in a reasonable amount of time) is approximately 190. That is, unless additional algorithmic improvements can be found, this algorithm is suitable only for instances whose vertex cover number is 190 or less. Under reasonable complexity-theoretic assumptions, namely the exponential time hypothesis, the running time cannot be improved to 2, even when formula_26 is formula_27.

However, for planar graphs, and more generally, for graphs excluding some fixed graph as a minor, a vertex cover of size "k" can be found in time "formula_28", i.e., the problem is subexponential fixed-parameter tractable. This algorithm is again optimal, in the sense that, under the exponential time hypothesis, no algorithm can solve vertex cover on planar graphs in time "formula_29".

One can find a factor-2 approximation by repeatedly taking "both" endpoints of an edge into the vertex cover, then removing them from the graph. Put otherwise, we find a maximal matching "M" with a greedy algorithm and construct a vertex cover "C" that consists of all endpoints of the edges in "M". In the following figure, a maximal matching "M" is marked with red, and the vertex cover "C" is marked with blue.

The set "C" constructed this way is a vertex cover: suppose that an edge "e" is not covered by "C"; then "M" ∪ {"e"} is a matching and "e" ∉ "M", which is a contradiction with the assumption that "M" is maximal. Furthermore, if "e" = {"u", "v"} ∈ "M", then any vertex cover – including an optimal vertex cover – must contain "u" or "v" (or both); otherwise the edge "e" is not covered. That is, an optimal cover contains at least "one" endpoint of each edge in "M"; in total, the set "C" is at most 2 times as large as the optimal vertex cover.

This simple algorithm was discovered independently by Fanica Gavril and Mihalis Yannakakis.

More involved techniques show that there are approximation algorithms with a slightly better approximation factor. For example, an approximation algorithm with an approximation factor of formula_30 is known. The problem can be approximated with an approximation factor formula_31 in formula_32 - dense graphs.

No better constant-factor approximation algorithm than the above one is known.
The minimum vertex cover problem is APX-complete, that is, it cannot be approximated arbitrarily well unless P = NP.
Using techniques from the PCP theorem, Dinur and Safra proved in 2005 that minimum vertex cover cannot be approximated within a factor of 1.3606 for any sufficiently large vertex degree unless P = NP. Moreover, if the unique games conjecture is true then minimum vertex cover cannot be approximated within any constant factor better than 2.

Although finding the minimum-size vertex cover is equivalent to finding the maximum-size independent set, as described above, the two problems are not equivalent in an approximation-preserving way: The Independent Set problem has "no" constant-factor approximation unless P = NP.

Given a collection of sets, a set which intersects all sets in the collection in at least one element is called a hitting set, and a simple NP-hard problem is to find a hitting set of smallest size or minimum hitting set. By mapping the sets in the collection onto hyperedges, this can be understood as a natural generalization of the vertex cover problem to hypergraphs which is often just called vertex cover for hypergraphs and, in a more combinatorial context, transversal.
The notions of hitting set and set cover are equivalent.

Formally, let "H" = ("V", "E") be a hypergraph with vertex set "V" and hyperedge set "E". Then a set "S" ⊆ "V" is called "hitting set" of "H" if, for all edges "e" ∈ "E", it holds "S" ∩ "e" ≠ ∅.
The computational problems "minimum hitting set" and "hitting set" are defined as in the case of graphs. Note that we get back the case of vertex covers for simple graphs if the maximum size of the hyperedges is 2.

If that size is restricted to "d", the problem of finding a minimum "d"-hitting set permits a "d"-approximation algorithm. Assuming the unique games conjecture, this is the best constant-factor algorithm that is possible and otherwise there is the possibility of improving the approximation to "d" − 1.

For the hitting set problem, different parametrizations make sense.
The hitting set problem is "W"[2]-complete for the parameter OPT, that is, it is unlikely that there is an algorithm that runs in time "f"(OPT)"n" where OPT is the cardinality of the smallest hitting set.
The hitting set problem is fixed-parameter tractable for the parameter OPT + "d", where "d" is the size of the largest edge of the hypergraph. More specifically, there is an algorithm for hitting set that runs in time "d""n".

The hitting set problem is equivalent to the set cover problem:
An instance of set cover can be viewed as an arbitrary bipartite graph, with sets represented by vertices on the left, elements of the universe represented by vertices on the right, and edges representing the inclusion of elements in sets. The task is then to find a minimum cardinality subset of left-vertices which covers all of the right-vertices. In the hitting set problem, the objective is to cover the left-vertices using a minimum subset of the right vertices. Converting from one problem to the other is therefore achieved by interchanging the two sets of vertices.

An example of a practical application involving the hitting set problem arises in efficient dynamic detection of race conditions. In this case, each time global memory is written, the current thread and set of locks held by that thread are stored. Under lockset-based detection, if later another thread writes to that location and there is "not" a race, it must be because it holds at least one lock in common with each of the previous writes. Thus the size of the hitting set represents the minimum lock set size to be race-free. This is useful in eliminating redundant write events, since large lock sets are considered unlikely in practice.




</doc>
<doc id="1747972" url="https://en.wikipedia.org/wiki?curid=1747972" title="Dominating set">
Dominating set

In graph theory, a dominating set for a graph "G" = ("V", "E") is a subset "D" of "V" such that every vertex not in "D" is adjacent to at least one member of "D". The domination number γ("G") is the number of vertices in a smallest dominating set for "G".

The dominating set problem concerns testing whether γ("G") ≤ "K" for a given graph "G" and input "K"; it is a classical NP-complete decision problem in computational complexity theory. Therefore it is believed that there may be no efficient algorithm that finds a smallest dominating set for all graphs, although there are efficient approximation algorithms, as well as both efficient and exact algorithms for certain graph classes.

Figures (a)–(c) on the right show three examples of dominating sets for a graph. In each example, each white vertex is adjacent to at least one red vertex, and it is said that the white vertex is "dominated" by the red vertex. The domination number of this graph is 2: the examples (b) and (c) show that there is a dominating set with 2 vertices, and it can be checked that there is no dominating set with only 1 vertex for this graph.

The domination problem was studied from the 1950s onwards, but the rate of research on domination significantly increased in the mid-1970s. In 1972, Richard Karp proved the set cover problem to be NP-complete. This had immediate implications for the dominating set problem, as there are straightforward vertex to set and edge to non-disjoint-intersection bijections between the two problems. This proved the dominating set problem to be NP-complete as well.

Dominating sets are of practical interest in several areas. In wireless networking, dominating sets are used to find efficient routes within ad-hoc mobile networks. They have also been used in document summarization, and in designing secure systems for electrical grids.

Dominating sets are closely related to independent sets: an independent set is also a dominating set if and only if it is a maximal independent set, so any maximal independent set in a graph is necessarily also a minimal dominating set. Thus, the smallest maximal independent set is greater or equal in size than the smallest independent dominating set. The independent domination number "i"("G") of a graph "G" is the size of the smallest independent dominating set (or, equivalently, the size of the smallest maximal independent set).

The minimum dominating set in a graph will not necessarily be independent, but the size of a minimum dominating set is always less than or equal to the size of a minimum maximal independent set, that is, γ("G") ≤ "i"("G").

There are graph families in which a minimum maximal independent set is a minimum dominating set. For example, γ("G") = "i"("G") if "G" is a claw-free graph.

A graph "G" is called a domination-perfect graph if γ("H") = "i"("H") in every induced subgraph "H" of "G". Since an induced subgraph of a claw-free graph is claw-free, it follows that every claw-free graphs is also domination-perfect.

Figures (a) and (b) are independent dominating sets, while figure (c) illustrates a dominating set that is not an independent set.

For any graph "G", its line graph "L"("G") is claw-free, and hence a minimum maximal independent set in "L"("G") is also a minimum dominating set in "L"("G"). An independent set in "L"("G") corresponds to a matching in "G", and a dominating set in "L"("G") corresponds to an edge dominating set in "G". Therefore a minimum maximal matching has the same size as a minimum edge dominating set.

The following example illustrates that strict inequality can hold for γ("G") ≤ "i"("G"). Let "G"
be the double star graph consisting of vertices "x_1", ..., "x_p", "a", "b", "y_1", ..., "y_q", where "p", "q" > 1. The edges of "G" are defined as follows: each "x_i" is adjacent to "a", "a" is adjacent to "b", and "b" is adjacent to each "b_j". Then γ("G") = 2 since {"a", "b"} is a minimal vertex domination set. If "p" ≤ "q", then "i"("G") = "p" + 1 since {"x_1", ..., "x_p", b} a smallest maximal independent set.

The set cover problem is a well-known NP-hard problem – the decision version of set covering was one of Karp's 21 NP-complete problems. There exist a pair of polynomial-time L-reductions between the minimum dominating set problem and the set cover problem. These reductions (see below) show that an efficient algorithm for the minimum dominating set problem would provide an efficient algorithm for the set cover problem, and vice versa. Moreover, the reductions preserve the approximation ratio: for any α, a polynomial-time α-approximation algorithm for minimum dominating sets would provide a polynomial-time α-approximation algorithm for the set cover problem and vice versa. Both problems are in fact Log-APX-complete.

The approximability of set covering is also well understood: a logarithmic approximation factor can be found by using a simple greedy algorithm, and finding a sublogarithmic approximation factor is NP-hard. More specifically, the greedy algorithm provides a factor 1 + log |"V"| approximation of a minimum dominating set, and no polynomial time algorithm can achieve an approximation factor better than "c" log |"V"| for some "c" > 0 unless P = NP.

The following two reductions show that the minimum dominating set problem and the set cover problem are equivalent under L-reductions: given an instance of one problem, we can construct an equivalent instance of the other problem.

From dominating set to set covering.
Given a graph "G" = ("V", "E") with "V" = {1, 2, ..., "n"}, construct a set cover instance ("U", "S") as follows: the universe "U" is "V", and the family of subsets is "S" = {"S", "S", ..., "S"} such that "S" consists of the vertex "v" and all vertices adjacent to "v" in "G".

Now if "D" is a dominating set for "G", then "C" = {"S" : "v" ∈ "D"} is a feasible solution of the set cover problem, with |"C"| = |"D"|. Conversely, if "C" = {"S" : "v" ∈ "D"} is a feasible solution of the set cover problem, then "D" is a dominating set for "G", with |"D"| = |"C"|.

Hence the size of a minimum dominating set for "G" equals the size of a minimum set cover for ("U", "S"). Furthermore, there is a simple algorithm that maps a dominating set to a set cover of the same size and vice versa. In particular, an efficient α-approximation algorithm for set covering provides an efficient α-approximation algorithm for minimum dominating sets.

From set covering to dominating set.
Let ("S", "U") be an instance of the set cover problem with the universe "U" and the family of subsets "S" = {"S" : "i" ∈ "I"}; we assume that "U" and the index set "I" are disjoint. Construct a graph "G" = ("V", "E") as follows: the set of vertices is "V" = "I" ∪ "U", there is an edge {"i", "j"} ∈ "E" between each pair "i", "j" ∈ "I", and there is also an edge {"i", "u"} for each "i" ∈ "I" and "u" ∈ "S". That is, "G" is a split graph: "I" is a clique and "U" is an independent set.

Now if "C" = {"S" : "i" ∈ "D"} is a feasible solution of the set cover problem for some subset "D" ⊆ "I", then "D" is a dominating set for "G", with |"D"| = |"C"|: First, for each "u" ∈ "U" there is an "i" ∈ "D" such that "u" ∈ "S", and by construction, "u" and "i" are adjacent in "G"; hence "u" is dominated by "i". Second, since "D" must be nonempty, each "i" ∈ "I" is adjacent to a vertex in "D".

Conversely, let "D" be a dominating set for "G". Then it is possible to construct another dominating set "X" such that |"X"| ≤ |"D"| and "X" ⊆ "I": simply replace each "u" ∈ "D" ∩ "U" by a neighbour "i" ∈ "I" of "u". Then "C" = {"S" : "i" ∈ "X"} is a feasible solution of the set cover problem, with |"C"| = |"X"| ≤ |"D"|.

If the graph has maximum degree Δ, then the greedy approximation algorithm finds an "O"(log Δ)-approximation of a minimum dominating set. Also, let "d" be the cardinality of dominating set obtained using greedy approximation then following relation holds, formula_1, where "N" is number of nodes and "M" is number of edges in given undirected graph. For fixed Δ, this qualifies as a dominating set for APX membership; in fact, it is APX-complete.

The problem admits a polynomial-time approximation scheme (PTAS) for special cases such as unit disk graphs and planar graphs. A minimum dominating set can be found in linear time in series-parallel graphs.

A minimum dominating set of an "n"-vertex graph can be found in time "O"(2"n") by inspecting all vertex subsets. show how to find a minimum dominating set in time "O"(1.5137) and exponential space, and in time "O"(1.5264) and polynomial space. A faster algorithm, using "O"(1.5048) time was found by , who also show that the number of minimum dominating sets can be computed in this time. The number of minimal dominating sets is at most 1.7159 and all such sets can be listed in time "O"(1.7159).

Finding a dominating set of size "k" plays a central role in the theory of parameterized complexity. It is the most well-known problem complete for the class W[2] and used in many reductions to show intractability of other problems. In particular, the problem is not fixed-parameter tractable in the sense that no algorithm with running time "f"("k")"n" for any function "f" exists unless the W-hierarchy collapses to FPT=W[2].

On the other hand, if the input graph is planar, the problem remains NP-hard, but a fixed-parameter algorithm is known. In fact, the problem has a kernel of size linear in "k", and running times that are exponential in and cubic in "n" may be obtained by applying dynamic programming to a branch-decomposition of the kernel. More generally, the dominating set problem and many variants of the problem are fixed-parameter tractable when parameterized by both the size of the dominating set and the size of the smallest forbidden complete bipartite subgraph; that is, the problem is FPT on biclique-free graphs, a very general class of sparse graphs that includes the planar graphs.

The complementary set to a dominating set, a nonblocker, can be found by a fixed-parameter algorithm on any graph.

Vizing's conjecture relates the domination number of a cartesian product of graphs to the domination number of its factors.

There has been much work on connected dominating sets. If "S" is a connected dominating set, one can form a spanning tree of "G" in which "S" forms the set of non-leaf vertices of the tree; conversely, if "T" is any spanning tree in a graph with more than two vertices, the non-leaf vertices of "T" form a connected dominating set. Therefore, finding minimum connected dominating sets is equivalent to finding spanning trees with the maximum possible number of leaves.

A "total dominating set" is a set of vertices such that all vertices in the graph (including the vertices in the dominating set themselves) have a neighbor in the dominating set. Figure (c) above shows a dominating set that is a connected dominating set and a total dominating set; the examples in figures (a) and (b) are neither.

A "k-tuple dominating set" is a set of vertices such that each vertex in the graph has at least "k" neighbors in the set. An (1+log n)-approximation of a minimum "k"-tuple dominating set can be found in polynomial time. Similarly, a "k-dominating set" is a set of vertices such that each vertex not in the set has at least "k" neighbors in the set. While every graph admits a "k"-dominating set, only graphs with minimum degree "k" − 1 admit a "k"-tuple dominating set. However, even if the graph admits k-tuple dominating set, a minimum "k"-tuple dominating set can be nearly "k" times as large as a minimum "k"-dominating set for the same graph; An (1.7 + log Δ)-approximation of a minimum "k"-dominating set can be found in polynomial time as well.

A domatic partition is a partition of the vertices into disjoint dominating sets. The domatic number is the maximum size of a domatic partition.

An eternal dominating set is a dynamic version of domination in which a vertex "v" in dominating set "D" is chosen and replaced with a neighbor "u" ("u" is not in "D") such that the modified "D" is also a dominating set and this process can be repeated over any infinite sequence of choices of vertices "v".





</doc>
<doc id="1860368" url="https://en.wikipedia.org/wiki?curid=1860368" title="Feedback vertex set">
Feedback vertex set

In the mathematical discipline of graph theory, a feedback vertex set of a graph is a set of vertices whose removal leaves a graph without cycles. In other words, each feedback vertex set contains at least one vertex of any cycle in the graph.
The feedback vertex set problem is an NP-complete problem in computational complexity theory. It was among the first problems shown to be NP-complete. It has wide applications in operating systems, database systems, and VLSI chip design.

The decision problem is as follows:

The graph formula_7 that remains after removing formula_6 from formula_5 is an induced forest (resp. an induced directed acyclic graph in the case of directed graphs). Thus, finding a minimum feedback vertex set in a graph is equivalent to finding a maximum induced forest (resp. maximum induced directed acyclic graph in the case of directed graphs).

 showed that the feedback vertex set problem for directed graphs is NP-complete. The problem remains NP-complete on directed graphs with maximum in-degree and out-degree two, and on directed planar graphs with maximum in-degree and out-degree three. Karp's reduction also implies the NP-completeness of the feedback vertex set problem on undirected graphs, where the problem stays NP-hard on graphs of maximum degree four. The feedback vertex set problem can be solved in polynomial time on graphs of maximum degree at most three.

Note that the problem of deleting as few "edges" as possible to make the graph cycle-free is equivalent to finding a spanning tree, which can be done in polynomial time. In contrast, the problem of deleting edges from a directed graph to make it acyclic, the feedback arc set problem, is NP-complete.

The corresponding NP optimization problem of finding the size of a minimum feedback vertex set can be solved in time "O"(1.7347), where "n" is the number of vertices in the graph. This algorithm actually computes a maximum induced forest, and when such a forest is obtained, its complement is a minimum feedback vertex set. The number of minimal feedback vertex sets in a graph is bounded by "O"(1.8638). The directed feedback vertex set problem can still be solved in time "O*"(1.9977), where "n" is the number of vertices in the given directed graph. The parameterized versions of the directed and undirected problems are both fixed-parameter tractable.

In undirected graphs of maximum degree three, the feedback vertex set problem can be solved in polynomial time, by transforming it into an instance of the matroid parity problem for linear matroids.

The undirected problem is APX-complete, which directly follows from the APX-completeness of the vertex cover problem, the existence of an approximation preserving L-reduction from the vertex cover problem to it and existing approximation algorithms. The best known approximation algorithm on undirected graphs is by a factor of two. Whether the directed version is polynomial time approximable within constant ratio and thereby APX-complete is an open question.

According to the Erdős–Pósa theorem, the size of a minimum feedback vertex set is within a logarithmic factor of the maximum number of vertex-disjoint cycles in the given graph.

In operating systems, feedback vertex sets play a prominent role in the study of deadlock recovery. In the wait-for graph of an operating system, each directed cycle corresponds to a deadlock situation. In order to resolve all deadlocks, some blocked processes have to be aborted. A minimum feedback vertex set in this graph corresponds to a minimum number of processes that one needs to abort.

Furthermore, the feedback vertex set problem has applications in VLSI chip design.



</doc>
<doc id="1860374" url="https://en.wikipedia.org/wiki?curid=1860374" title="Feedback arc set">
Feedback arc set

In graph theory, a directed graph may contain directed cycles, a one-way loop of edges. In some applications, such cycles are undesirable, and we wish to eliminate them and obtain a directed acyclic graph (DAG). One way to do this is simply to drop edges from the graph to break the cycles. A feedback arc set (FAS) or feedback edge set is a set of edges which, when removed from the graph, leave a DAG. Put another way, it's a set containing at least one edge of every cycle in the graph.

Closely related are the feedback vertex set, which is a set of vertices containing at least one vertex from every cycle in the directed graph, and the minimum spanning tree, which is the undirected variant of the feedback arc set problem.

A minimal feedback arc set (one that can not be reduced in size by removing any edges) has the additional property that, if the edges in it are reversed rather than removed, then the graph remains acyclic. Finding a small edge set with this property is a key step in layered graph drawing.

Sometimes it is desirable to drop as few edges as possible, obtaining a minimum feedback arc set (MFAS), or dually a maximum acyclic subgraph. This is a hard computational problem, for which several approximate solutions have been devised.

As a simple example, consider the following hypothetical situation, where in order to achieve something, certain things must be achieved before other things:


We can express this as a graph problem. Let each vertex represent an item, and add an edge from A to B if you must have A to obtain B. Unfortunately, you don't have any of the three items, and because this graph is cyclic, you can't get any of them either.

However, suppose you offer George $100 for his piano. If he accepts, this effectively removes the edge from the lawnmower to the piano, because you no longer need the lawnmower to get the piano. Consequently, the cycle is broken, and you can trade twice to get the lawnmower. This one edge constitutes a feedback arc set.

As in the above example, there is usually some cost associated with removing an edge. For this reason, we'd like to remove as few edges as possible. Removing one edge suffices in a simple cycle, but in general figuring out the minimum number of edges to remove is an NP-hard problem called the minimum feedback arc set or maximum acyclic subgraph problem.

This problem is particularly difficult in "k"-edge-connected graphs for large "k", where each edge falls in many different cycles. The decision version of the problem, which is NP-complete, asks whether all cycles can be broken by removing at most "k" edges; this was one of Richard M. Karp's 21 NP-complete problems, shown by reducing from the vertex cover problem.

Although NP-complete, the feedback arc set problem is fixed-parameter tractable: there exists an algorithm for solving it whose running time is a fixed polynomial in the size of the input graph (independent of the number of edges in the set) but exponential in the number of edges in the feedback arc set.
Alternatively, a fixed-parameter tractable algorithm is given by a dynamic programming technique that depends only exponentially on the 
dimension of the cycle space of the graph.

The minimum feedback arc set problem is APX-hard, which means that (assuming P ≠ NP) there is a hard limit on its approximation quality, a constant "c" > 1 such that every polynomial-time approximation algorithm will sometimes return an edge set larger than "c" times the optimal size. The proof involves approximation-preserving reductions from vertex cover to feedback vertex set, and from feedback vertex set to feedback arc set. More specifically, because vertex cover has no approximation better than 1.3606 unless P ≠ NP, the same is true for feedback arc set. That is, it is possible to take . If the unique games conjecture is true, this inapproximability threshold could be increased to arbitrarily close to 2.

On the other hand, the best known approximation algorithm has the non-constant approximation ratio "O"(log "n" log log "n"). For the dual problem, of approximating the maximum number of edges in an acyclic subgraph, an approximation somewhat better than 1/2 is possible. Determining whether feedback arc set has a constant-ratio approximation algorithm, or whether a non-constant ratio is necessary, remains an open problem.

If the input digraphs are restricted to be tournaments, the resulting problem is known as the "minimum feedback arc set problem on tournaments" (FAST). This restricted problem does admit a polynomial-time approximation scheme, and this still holds for a restricted weighted version of the problem. A subexponential fixed parameter algorithm for the weighted FAST was given by .

On the other hand, if the edges are undirected, the problem of deleting edges to make the graph cycle-free is equivalent to finding a minimum spanning tree, which can be done easily in polynomial time.

Several approximation algorithms for the problem have been developed - including Monte Carlo randomized algorithm that solves the problem in polynomial time with arbitrary probability. A particularly simple algorithm is the following:


Now both and are acyclic subgraphs of , and at least one of them is at least half the size of the maximum acyclic subgraph.


</doc>
<doc id="333219" url="https://en.wikipedia.org/wiki?curid=333219" title="Eulerian path">
Eulerian path

In graph theory, an Eulerian trail (or Eulerian path) is a trail in a finite graph that visits every edge exactly once (allowing for revisiting vertices). Similarly, an Eulerian circuit or Eulerian cycle is an Eulerian trail that starts and ends on the same vertex. They were first discussed by Leonhard Euler while solving the famous Seven Bridges of Königsberg problem in 1736. The problem can be stated mathematically like this:

Euler proved that a necessary condition for the existence of Eulerian circuits is that all vertices in the graph have an even degree, and stated without proof that connected graphs with all vertices of even degree have an Eulerian circuit. The first complete proof of this latter claim was published posthumously in 1873 by Carl Hierholzer. This is known as Euler's Theorem:

The term Eulerian graph has two common meanings in graph theory. One meaning is a graph with an Eulerian circuit, and the other is a graph with every vertex of even degree. These definitions coincide for connected graphs.

For the existence of Eulerian trails it is necessary that zero or two vertices have an odd degree; this means the Königsberg graph is "not" Eulerian. If there are no vertices of odd degree, all Eulerian trails are circuits. If there are exactly two vertices of odd degree, all Eulerian trails start at one of them and end at the other. A graph that has an Eulerian trail but not an Eulerian circuit is called semi-Eulerian.

An Eulerian trail, or Euler walk in an undirected graph is a walk that uses each edge exactly once. If such a walk exists, the graph is called traversable or semi-eulerian.

An Eulerian cycle, Eulerian circuit or Euler tour in an undirected graph is a cycle that uses each edge exactly once. If such a cycle exists, the graph is called Eulerian or unicursal. The term "Eulerian graph" is also sometimes used in a weaker sense to denote a graph where every vertex has even degree. For finite connected graphs the two definitions are equivalent, while a possibly unconnected graph is Eulerian in the weaker sense if and only if each connected component has an Eulerian cycle.

For directed graphs, "path" has to be replaced with "directed path" and "cycle" with "directed cycle".

The definition and properties of Eulerian trails, cycles and graphs are valid for multigraphs as well.

An Eulerian orientation of an undirected graph "G" is an assignment of a direction to each edge of "G" such that, at each vertex "v", the indegree of "v" equals the outdegree of "v". Such an orientation exists for any undirected graph in which every vertex has even degree, and may be found by constructing an Euler tour in each connected component of "G" and then orienting the edges according to the tour. Every Eulerian orientation of a connected graph is a strong orientation, an orientation that makes the resulting directed graph strongly connected.


Fleury's algorithm is an elegant but inefficient algorithm that dates to 1883. Consider a graph known to have all edges in the same component and at most two vertices of odd degree. The algorithm starts at a vertex of odd degree, or, if the graph has none, it starts with an arbitrarily chosen vertex. At each step it chooses the next edge in the path to be one whose deletion would not disconnect the graph, unless there is no such edge, in which case it picks the remaining edge left at the current vertex. It then moves to the other endpoint of that edge and deletes the edge. At the end of the algorithm there are no edges left, and the sequence from which the edges were chosen forms an Eulerian cycle if the graph has no vertices of odd degree, or an Eulerian trail if there are exactly two vertices of odd degree.

While the "graph traversal" in Fleury's algorithm is linear in the number of edges, i.e. "O"(|"E"|), we also need to factor in the complexity of detecting bridges. If we are to re-run Tarjan's linear time bridge-finding algorithm after the removal of every edge, Fleury's algorithm will have a time complexity of "O"(|"E"|). A dynamic bridge-finding algorithm of allows this to be improved to formula_1 but this is still significantly slower than alternative algorithms.

Hierholzer's 1873 paper provides a different method for finding Euler cycles that is more efficient than Fleury's algorithm:
By using a data structure such as a doubly linked list to maintain the set of unused edges incident to each vertex, to maintain the list of vertices on the current tour that have unused edges, and to maintain the tour itself, the individual operations of the algorithm (finding unused edges exiting each vertex, finding a new starting vertex for a tour, and connecting two tours that share a vertex) may be performed in constant time each, so the overall algorithm takes linear time, formula_2.

This algorithm may also be implemented with a queue. Because it is only possible to get stuck when the queue represents a closed tour, one should rotate the queue (remove an element from the head and add it to the tail) until unstuck, and continue until all edges are accounted for. This also takes linear time, as the number of rotations performed is never larger than formula_3.

The number of Eulerian circuits in "digraphs" can be calculated using the so-called BEST theorem, named after de Bruijn, van Aardenne-Ehrenfest, Smith and Tutte. The formula states that the number of Eulerian circuits in a digraph is the product of certain degree factorials and the number of rooted arborescences. The latter can be computed as a determinant, by the matrix tree theorem, giving a polynomial time algorithm.

BEST theorem is first stated in this form in a "note added in proof" to the Aardenne-Ehrenfest and de Bruijn paper (1951). The original proof was bijective and generalized the de Bruijn sequences. It is a variation on an earlier result by Smith and Tutte (1941).

Counting the number of Eulerian circuits on "undirected" graphs is much more difficult. This problem is known to be #P-complete. In a positive direction, a Markov chain Monte Carlo approach, via the "Kotzig transformations" (introduced by Anton Kotzig in 1968) is believed to give a sharp approximation for the number of Eulerian circuits in a graph, though as yet there is no proof of this fact (even for graphs of bounded degree).

The asymptotic formula for the number of Eulerian circuits in the complete graphs was determined by McKay and Robinson (1995):

A similar formula was later obtained by M.I. Isaev (2009) for complete bipartite graphs:

Eulerian trails are used in bioinformatics to reconstruct the DNA sequence from its fragments. They are also used in CMOS circuit design to find an optimal logic gate ordering. There are some algorithms for processing trees that rely on an Euler tour of the tree (where each edge is treated as a pair of arcs). The Gray code used for error detection and correction can be constructed as an Eulerian trail of de Bruijn graphs.

In an infinite graph, the corresponding concept to an Eulerian trail or Eulerian cycle is an Eulerian line, a doubly-infinite trail that covers all of the edges of the graph. It is not sufficient for the existence of such a trail that the graph be connected and that all vertex degrees be even; for instance, the infinite Cayley graph shown, with all vertex degrees equal to four, has no Eulerian line.
The infinite graphs that contain Eulerian lines were characterized by . For an infinite graph or multigraph to have an Eulerian line, it is necessary and sufficient that all of the following conditions be met:





</doc>
<doc id="244437" url="https://en.wikipedia.org/wiki?curid=244437" title="Hamiltonian path">
Hamiltonian path

In the mathematical field of graph theory, a Hamiltonian path (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once. A Hamiltonian cycle (or Hamiltonian circuit) is a Hamiltonian path that is a cycle. Determining whether such paths and cycles exist in graphs is the Hamiltonian path problem, which is NP-complete.

Hamiltonian paths and cycles are named after William Rowan Hamilton who invented the icosian game, now also known as "Hamilton's puzzle", which involves finding a Hamiltonian cycle in the edge graph of the dodecahedron. Hamilton solved this problem using the icosian calculus, an algebraic structure based on roots of unity with many similarities to the quaternions (also invented by Hamilton). This solution does not generalize to arbitrary graphs.

Despite being named after Hamilton, Hamiltonian cycles in polyhedra had also been studied a year earlier by Thomas Kirkman, who, in particular, gave an example of a polyhedron without Hamiltonian cycles. Even earlier, Hamiltonian cycles and paths in the knight's graph of the chessboard, the knight's tour, had been studied in the 9th century in Indian mathematics by Rudrata, and around the same time in Islamic mathematics by . In 18th century Europe, knight's tours were published by Abraham de Moivre and Leonhard Euler.

A "Hamiltonian path" or "traceable path" is a path that visits each vertex of the graph exactly once. A graph that contains a Hamiltonian path is called a traceable graph. A graph is Hamiltonian-connected if for every pair of vertices there is a Hamiltonian path between the two vertices.

A "Hamiltonian cycle", "Hamiltonian circuit", "vertex tour" or "graph cycle" is a cycle that visits each vertex exactly once. A graph that contains a Hamiltonian cycle is called a Hamiltonian graph.

Similar notions may be defined for "directed graphs", where each edge (arc) of a path or cycle can only be traced in a single direction (i.e., the vertices are connected with arrows and the edges traced "tail-to-head").

A Hamiltonian decomposition is an edge decomposition of a graph into Hamiltonian circuits.

A "Hamilton maze" is a type of logic puzzle in which the goal is to find the unique Hamiltonian cycle in a given graph.


Any Hamiltonian cycle can be converted to a Hamiltonian path by removing one of its edges, but a Hamiltonian path can be extended to Hamiltonian cycle only if its endpoints are adjacent.

All Hamiltonian graphs are biconnected, but a biconnected graph need not be Hamiltonian (see, for example, the Petersen graph).

An Eulerian graph "G" (a connected graph in which every vertex has even degree) necessarily has an Euler tour, a closed walk passing through each edge of "G" exactly once.
This tour corresponds to a Hamiltonian cycle in the line graph "L"("G"), so the line graph of every Eulerian graph is Hamiltonian. Line graphs may have other Hamiltonian cycles that do not correspond to Euler tours, and in particular the line graph "L"("G") of every Hamiltonian graph "G" is itself Hamiltonian, regardless of whether the graph "G" is Eulerian.

A tournament (with more than two vertices) is Hamiltonian if and only if it is strongly connected.

The number of different Hamiltonian cycles in a complete undirected graph on "n" vertices is and in a complete directed graph on "n" vertices is . These counts assume that cycles that are the same apart from their starting point are not counted separately.

The best vertex degree characterization of Hamiltonian graphs was provided in 1972 by the Bondy–Chvátal theorem, which generalizes earlier results by G. A. Dirac (1952) and Øystein Ore. Both Dirac's and Ore's theorems can also be derived from Pósa's theorem (1962). Hamiltonicity has been widely studied with relation to various parameters such as graph density, toughness, forbidden subgraphs and distance among other parameters. Dirac and Ore's theorems basically state that a graph is Hamiltonian if it has "enough edges".

The Bondy–Chvátal theorem operates on the closure cl("G") of a graph "G" with "n" vertices, obtained by repeatedly adding a new edge "uv" connecting a nonadjacent pair of vertices "u" and "v" with until no more pairs with this property can be found.

Bondy–Chvátal theorem (1976)

As complete graphs are Hamiltonian, all graphs whose closure is complete are Hamiltonian, which is the content of the following earlier theorems by Dirac and Ore.

Dirac (1952)

Ore (1960)

The following theorems can be regarded as directed versions:

Ghouila-Houiri (1960)

Meyniel (1973)

The number of vertices must be doubled because each undirected edge corresponds to two directed arcs and thus the degree of a vertex in the directed graph is twice the degree in the undirected graph.

Rahman-Kaykobad (2005)

The above theorem can only recognize the existence of a Hamiltonian path in a graph and not a Hamiltonian Cycle.

Many of these results have analogues for balanced bipartite graphs, in which the vertex degrees are compared to the number of vertices on a single side of the bipartition rather than the number of vertices in the whole graph.



An algebraic representation of the Hamiltonian cycles of a given weighted digraph (whose arcs are assigned weights from a certain ground field) is the Hamiltonian cycle polynomial of its weighted adjacency matrix defined as the sum of the products of the arc weights of the digraph's Hamiltonian cycles. This polynomial is not identically zero as a function in the arc weights if and only if the digraph is Hamiltonian. The relationship between the computational complexities of computing it and computing the permanent was shown in .




</doc>
<doc id="149646" url="https://en.wikipedia.org/wiki?curid=149646" title="Hamiltonian path problem">
Hamiltonian path problem

In the mathematical field of graph theory the Hamiltonian path problem and the Hamiltonian cycle problem are problems of determining whether a Hamiltonian path (a path in an undirected or directed graph that visits each vertex exactly once) or a Hamiltonian cycle exists in a given graph (whether directed or undirected). Both problems are NP-complete. 

The Hamiltonian cycle problem is a special case of the travelling salesman problem, obtained by setting the distance between two cities to one if they are adjacent and two otherwise, and verifying that the total distance travelled is equal to "n" (if so, the route is a Hamiltonian circuit; if there is no Hamiltonian circuit then the shortest route will be longer).

There is a simple relation between the problems of finding a Hamiltonian path and a Hamiltonian cycle:


There are "n"! different sequences of vertices that "might" be Hamiltonian paths in a given "n"-vertex graph (and are, in a complete graph), so a brute force search algorithm that tests all possible sequences would be very slow.
An early exact algorithm for finding a Hamiltonian cycle on a directed graph was the enumerative algorithm of Martello. A search procedure by Frank Rubin divides the edges of the graph into three classes: those that must be in the path, those that cannot be in the path, and undecided. As the search proceeds, a set of decision rules classifies the undecided edges, and determines whether to halt or continue the search. The algorithm divides the graph into components that can be solved separately. Also, a dynamic programming algorithm of Bellman, Held, and Karp can be used to solve the problem in time O("n" 2). In this method, one determines, for each set "S" of vertices and each vertex "v" in "S", whether there is a path that covers exactly the vertices in "S" and ends at "v". For each choice of "S" and "v", a path exists for ("S","v") if and only if "v" has a neighbor "w" such that a path exists for ("S" − "v","w"), which can be looked up from already-computed information in the dynamic program.

Andreas Björklund provided an alternative approach using the inclusion–exclusion principle to reduce the problem of counting the number of Hamiltonian cycles to a simpler counting problem, of counting cycle covers, which can be solved by computing certain matrix determinants. Using this method, he showed how to solve the Hamiltonian cycle problem in arbitrary "n"-vertex graphs by a Monte Carlo algorithm in time O(1.657); for bipartite graphs this algorithm can be further improved to time o(1.415).

For graphs of maximum degree three, a careful backtracking search can find a Hamiltonian cycle (if one exists) in time O(1.251).

Hamiltonian paths and cycles can be found using a SAT solver.

Because of the difficulty of solving the Hamiltonian path and cycle problems on conventional computers, they have also been studied in unconventional models of computing. For instance, Leonard Adleman showed that the Hamiltonian path problem may be solved using a DNA computer. Exploiting the parallelism inherent in chemical reactions, the problem may be solved using a number of chemical reaction steps linear in the number of vertices of the graph; however, it requires a factorial number of DNA molecules to participate in the reaction. 

An optical solution to the Hamiltonian problem has been proposed as well. The idea is to create a graph-like structure made from optical cables and beam splitters which are traversed by light in order to construct a solution for the problem. The weak point of this approach is the required amount of energy which is exponential in the number of nodes.

The problem of finding a Hamiltonian cycle or path is in FNP; the analogous decision problem is to test whether a Hamiltonian cycle or path exists. The directed and undirected Hamiltonian cycle problems were two of Karp's 21 NP-complete problems. They remain NP-complete even for special kinds of graphs, such as:


However, for some special classes of graphs, the problem can be solved in polynomial time:


Putting all of these conditions together, it remains open whether 3-connected 3-regular bipartite planar graphs must always contain a Hamiltonian cycle, in which case the problem restricted to those graphs could not be NP-complete; see Barnette's conjecture.

In graphs in which all vertices have odd degree, an argument related to the handshaking lemma shows that the number of Hamiltonian cycles through any fixed edge is always even, so if one Hamiltonian cycle is given, then a second one must also exist. However, finding this second cycle does not seem to be an easy computational task. Papadimitriou defined the complexity class PPA to encapsulate problems such as this one.


</doc>
<doc id="31248" url="https://en.wikipedia.org/wiki?curid=31248" title="Travelling salesman problem">
Travelling salesman problem

The travelling salesman problem (also called the travelling salesperson problem or TSP) asks the following question: "Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?" It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.

The travelling purchaser problem and the vehicle routing problem are both generalizations of TSP.

In the theory of computational complexity, the decision version of the TSP (where, given a length "L", the task is to decide whether the graph has any tour shorter than "L") belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (but no more than exponentially) with the number of cities.

The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact algorithms are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.

The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept "city" represents, for example, customers, soldering points, or DNA fragments, and the concept "distance" represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimize the time spent moving the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.

The origins of the travelling salesman problem are unclear. A handbook for travelling salesmen from 1832 mentions the problem and includes example tours through Germany and Switzerland, but contains no mathematical treatment.

The travelling salesman problem was mathematically formulated in the 1800s by the Irish mathematician W.R. Hamilton and by the British mathematician Thomas Kirkman. Hamilton’s Icosian Game was a recreational puzzle based on finding a Hamiltonian cycle. The general form of the TSP appears to have been first studied by mathematicians during the 1930s in Vienna and at Harvard, notably by Karl Menger, who defines the problem, considers the obvious brute-force algorithm, and observes the non-optimality of the nearest neighbour heuristic:
It was first considered mathematically in the 1930s by Merrill M. Flood who was looking to solve a school bus routing problem.
Hassler Whitney at Princeton University introduced the name "travelling salesman problem" soon afterward.

In the 1950s and 1960s, the problem became increasingly popular in scientific circles in Europe and the USA after the RAND Corporation in Santa Monica offered prizes for steps in solving the problem. Notable contributions were made by George Dantzig, Delbert Ray Fulkerson and Selmer M. Johnson from the RAND Corporation, who expressed the problem as an integer linear program and developed the cutting plane method for its solution. They wrote what is considered the seminal paper on the subject in which with these new methods they solved an instance with 49 cities to optimality by constructing a tour and proving that no other tour could be shorter. Dantzig, Fulkerson and Johnson, however, speculated that given a near optimal solution we may be able to find optimality or prove optimality by adding a small number of extra inequalities (cuts). They used this idea to solve their initial 49 city problem using a string model. They found they only needed 26 cuts to come to a solution for their 49 city problem. While this paper did not give an algorithmic approach to TSP problems, the ideas that lay within it were indispensable to later creating exact solution methods for the TSP, though it would take 15 years to find an algorithmic approach in creating these cuts. As well as cutting plane methods, Dantzig, Fulkerson and Johnson used branch and bound algorithms perhaps for the first time.

In the following decades, the problem was studied by many researchers from mathematics, computer science, chemistry, physics, and other sciences. In the 1960s however a new approach was created, that instead of seeking optimal solutions, one would produce a solution whose length is provably bounded by a multiple of the optimal length, and in doing so create lower bounds for the problem; these may then be used with branch and bound approaches. One method of doing this was to create a minimum spanning tree of the graph and then double all its edges, which produces the bound that the length of an optimal tour is at most twice the weight of a minimum spanning tree.

Christofides made a big advance in this approach of giving an approach for which we know the worst-case scenario. Christofides algorithm given in 1976, at worst is 1.5 times longer than the optimal solution. As the algorithm was so simple and quick, many hoped it would give way to a near optimal solution method. This remains the method with the best worst-case scenario. However, for a fairly general special case of the problem it was beaten by a tiny margin in 2011.

Richard M. Karp showed in 1972 that the Hamiltonian cycle problem was NP-complete, which implies the NP-hardness of TSP. This supplied a mathematical explanation for the apparent computational difficulty of finding optimal tours.

Great progress was made in the late 1970s and 1980, when Grötschel, Padberg, Rinaldi and others managed to exactly solve instances with up to 2,392 cities, using cutting planes and branch and bound.

In the 1990s, Applegate, Bixby, Chvátal, and Cook developed the program "Concorde" that has been used in many recent record solutions. Gerhard Reinelt published the TSPLIB in 1991, a collection of benchmark instances of varying difficulty, which has been used by many research groups for comparing results. In 2006, Cook and others computed an optimal tour through an 85,900-city instance given by a microchip layout problem, currently the largest solved TSPLIB instance. For many other instances with millions of cities, solutions can be found that are guaranteed to be within 2-3% of an optimal tour.

TSP can be modelled as an undirected weighted graph, such that cities are the graph's vertices, paths are the graph's edges, and a path's distance is the edge's weight. It is a minimization problem starting and finishing at a specified vertex after having visited each other vertex exactly once. Often, the model is a complete graph ("i.e." each pair of vertices is connected by an edge). If no path exists between two cities, adding an arbitrarily long edge will complete the graph without affecting the optimal tour.

In the "symmetric TSP", the distance between two cities is the same in each opposite direction, forming an undirected graph. This symmetry halves the number of possible solutions. In the "asymmetric TSP", paths may not exist in both directions or the distances might be different, forming a directed graph. Traffic collisions, one-way streets, and airfares for cities with different departure and arrival fees are examples of how this symmetry could break down.


The TSP can be formulated as an integer linear program. Several formulations are known. Two notable formulations are the Miller-Tucker-Zemlin (MTZ) formulation and the Dantzig-Fulkerson-Johnson (DFJ) formulation. The DFJ formulation is stronger, though the MTZ formulation is still useful in certain settings.

Label the cities with the numbers 1, …, "n" and define:

For "i" = 1, …, "n", let formula_2 be a dummy variable, and finally take formula_3 to be the distance from city "i" to city "j". Then TSP can be written as the following integer linear programming problem:

The first set of equalities requires that each city is arrived at from exactly one other city, and the second set of equalities requires that from each city there is a departure to exactly one other city. The last constraints enforce that there is only a single tour covering all cities, and not two or more disjointed tours that only collectively cover all cities. To prove this, it is shown below (1) that every feasible solution contains only one closed sequence of cities, and (2) that for every single tour covering all cities, there are values for the dummy variables formula_2 that satisfy the constraints.

To prove that every feasible solution contains only one closed sequence of cities, it suffices to show that every subtour in a feasible solution passes through city 1 (noting that the equalities ensure there can only be one such tour). For if we sum all the inequalities corresponding to formula_6 for any subtour of "k" steps not passing through city 1, we obtain:

which is a contradiction.

It now must be shown that for every single tour covering all cities, there are values for the dummy variables formula_2 that satisfy the constraints.

Without loss of generality, define the tour as originating (and ending) at city 1. Choose formula_9 if city "i" is visited in step "t" ("i", "t" = 1, 2, ..., n). Then

since formula_2 can be no greater than "n" and formula_12 can be no less than 1; hence the constraints are satisfied whenever formula_13 For formula_6, we have:

satisfying the constraint.

Label the cities with the numbers 1, …, "n" and define:

Take formula_3 to be the distance from city "i" to city "j". Then TSP can be written as the following integer linear programming problem:

The last constraint of the DFJ formulation ensures that there are no sub-tours among the non-starting vertices, so the solution returned is a single tour and not the union of smaller tours. Because this leads to an exponential number of possible constraints, in practice it is solved with delayed column generation.

The traditional lines of attack for the NP-hard problems are the following:

The most direct solution would be to try all permutations (ordered combinations) and see which one is cheapest (using brute-force search). The running time for this approach lies within a polynomial factor of formula_19, the factorial of the number of cities, so this solution becomes impractical even for only 20 cities.

One of the earliest applications of dynamic programming is the Held–Karp algorithm that solves the problem in time formula_20. This bound has also been reached by Exclusion-Inclusion in an attempt preceding the dynamic programming approach.
Improving these time bounds seems to be difficult. For example, it has not been determined whether an exact algorithm for TSP that runs in time formula_21 exists.

Other approaches include:


An exact solution for 15,112 German towns from TSPLIB was found in 2001 using the cutting-plane method proposed by George Dantzig, Ray Fulkerson, and Selmer M. Johnson in 1954, based on linear programming. The computations were performed on a network of 110 processors located at Rice University and Princeton University. The total computation time was equivalent to 22.6 years on a single 500 MHz Alpha processor. In May 2004, the travelling salesman problem of visiting all 24,978 towns in Sweden was solved: a tour of length approximately 72,500 kilometres was found and it was proven that no shorter tour exists. In March 2005, the travelling salesman problem of visiting all 33,810 points in a circuit board was solved using "Concorde TSP Solver": a tour of length 66,048,945 units was found and it was proven that no shorter tour exists. The computation took approximately 15.7 CPU-years (Cook et al. 2006). In April 2006 an instance with 85,900 points was solved using "Concorde TSP Solver", taking over 136 CPU-years, see .

Various heuristics and approximation algorithms, which quickly yield good solutions, have been devised. These include the Multi-fragment algorithm. Modern methods can find solutions for extremely large problems (millions of cities) within a reasonable time which are with a high probability just 2–3% away from the optimal solution.

Several categories of heuristics are recognized.

The nearest neighbour (NN) algorithm (a greedy algorithm) lets the salesman choose the nearest unvisited city as his next move. This algorithm quickly yields an effectively short route. For N cities randomly distributed on a plane, the algorithm on average yields a path 25% longer than the shortest possible path. However, there exist many specially arranged city distributions which make the NN algorithm give the worst route. This is true for both asymmetric and symmetric TSPs. Rosenkrantz et al. showed that the NN algorithm has the approximation factor formula_22 for instances satisfying the triangle inequality. A variation of NN algorithm, called Nearest Fragment (NF) operator, which connects a group (fragment) of nearest unvisited cities, can find shorter route with successive iterations. The NF operator can also be applied on an initial solution obtained by NN algorithm for further improvement in an elitist model, where only better solutions are accepted.

The bitonic tour of a set of points is the minimum-perimeter monotone polygon that has the points as its vertices; it can be computed efficiently by dynamic programming.

Another constructive heuristic, Match Twice and Stitch (MTS), performs two sequential matchings, where the second matching is executed after deleting all the edges of the first matching, to yield a set of cycles. The cycles are then stitched to produce the final tour.

The Christofides algorithm follows a similar outline but combines the minimum spanning tree with a solution of another problem, minimum-weight perfect matching. This gives a TSP tour which is at most 1.5 times the optimal. The Christofides algorithm was one of the first approximation algorithms, and was in part responsible for drawing attention to approximation algorithms as a practical approach to intractable problems. As a matter of fact, the term "algorithm" was not commonly extended to approximation algorithms until later; the Christofides algorithm was initially referred to as the Christofides heuristic.

This algorithm looks at things differently by using a result from graph theory which helps improve on the LB of the TSP which originated from doubling the cost of the minimum spanning tree. Given an Eulerian graph we can find an Eulerian tour in time. So if we had an Eulerian graph with cities from a TSP as vertices then we can easily see that we could use such a method for finding an Eulerian tour to find a TSP solution. By triangular inequality we know that the TSP tour can be no longer than the Eulerian tour and as such we have a LB for the TSP. Such a method is described below.


To improve the lower bound, a better way of creating an Eulerian graph is needed. By triangular inequality, the best Eulerian graph must have the same cost as the best travelling salesman tour, hence finding optimal Eulerian graphs is at least as hard as TSP. One way of doing this is by minimum weight matching using algorithms of formula_23.
Making a graph into an Eulerian graph starts with the minimum spanning tree. Then all the vertices of odd order must be made even. So a matching for the odd degree vertices must be added which increases the order of every odd degree vertex by one. This leaves us with a graph where every vertex is of even order which is thus Eulerian. Adapting the above method gives Christofides' algorithm,


The pairwise exchange or "2-opt" technique involves iteratively removing two edges and replacing these with two different edges that reconnect the fragments created by edge removal into a new and shorter tour. Similarly, the 3-opt technique removes 3 edges and reconnects them to form a shorter tour. These are special cases of the "k"-opt method. The label "Lin–Kernighan" is an often heard misnomer for 2-opt. Lin–Kernighan is actually the more general k-opt method.

For Euclidean instances, 2-opt heuristics give on average solutions that are about 5% better than Christofides' algorithm. If we start with an initial solution made with a greedy algorithm, the average number of moves greatly decreases again and is . For random starts however, the average number of moves is . However whilst in order this is a small increase in size, the initial number of moves for small problems is 10 times as big for a random start compared to one made from a greedy heuristic. This is because such 2-opt heuristics exploit `bad' parts of a solution such as crossings. These types of heuristics are often used within Vehicle routing problem heuristics to reoptimize route solutions.

The Lin–Kernighan heuristic is a special case of the "V"-opt or variable-opt technique. It involves the following steps:


The most popular of the "k"-opt methods are 3-opt, as introduced by Shen Lin of Bell Labs in 1965. A special case of 3-opt is where the edges are not disjoint (two of the edges are adjacent to one another). In practice, it is often possible to achieve substantial improvement over 2-opt without the combinatorial cost of the general 3-opt by restricting the 3-changes to this special subset where two of the removed edges are adjacent. This so-called two-and-a-half-opt typically falls roughly midway between 2-opt and 3-opt, both in terms of the quality of tours achieved and the time required to achieve those tours.

The variable-opt method is related to, and a generalization of the "k"-opt method. Whereas the "k"-opt methods remove a fixed number ("k") of edges from the original tour, the variable-opt methods do not fix the size of the edge set to remove. Instead they grow the set as the search process continues. The best known method in this family is the Lin–Kernighan method (mentioned above as a misnomer for 2-opt). Shen Lin and Brian Kernighan first published their method in 1972, and it was the most reliable heuristic for solving travelling salesman problems for nearly two decades. More advanced variable-opt methods were developed at Bell Labs in the late 1980s by David Johnson and his research team. These methods (sometimes called Lin–Kernighan–Johnson) build on the Lin–Kernighan method, adding ideas from tabu search and evolutionary computing. The basic Lin–Kernighan technique gives results that are guaranteed to be at least 3-opt. The Lin–Kernighan–Johnson methods compute a Lin–Kernighan tour, and then perturb the tour by what has been described as a mutation that removes at least four edges and reconnecting the tour in a different way, then "V"-opting the new tour. The mutation is often enough to move the tour from the local minimum identified by Lin–Kernighan. "V"-opt methods are widely considered the most powerful heuristics for the problem, and are able to address special cases, such as the Hamilton Cycle Problem and other non-metric TSPs that other heuristics fail on. For many years Lin–Kernighan–Johnson had identified optimal solutions for all TSPs where an optimal solution was known and had identified the best known solutions for all other TSPs on which the method had been tried.

Optimized Markov chain algorithms which use local searching heuristic sub-algorithms can find a route extremely close to the optimal route for 700 to 800 cities.

TSP is a touchstone for many general heuristics devised for combinatorial optimization such as genetic algorithms, simulated annealing, tabu search, ant colony optimization, river formation dynamics (see swarm intelligence) and the cross entropy method.

Artificial intelligence researcher Marco Dorigo described in 1993 a method of heuristically generating "good solutions" to the TSP using a simulation of an ant colony called "ACS" ("ant colony system"). It models behaviour observed in real ants to find short paths between food sources and their nest, an emergent behaviour resulting from each ant's preference to follow trail pheromones deposited by other ants.

ACS sends out a large number of virtual ant agents to explore many possible routes on the map. Each ant probabilistically chooses the next city to visit based on a heuristic combining the distance to the city and the amount of virtual pheromone deposited on the edge to the city. The ants explore, depositing pheromone on each edge that they cross, until they have all completed a tour. At this point the ant which completed the shortest tour deposits virtual pheromone along its complete tour route ("global trail updating"). The amount of pheromone deposited is inversely proportional to the tour length: the shorter the tour, the more it deposits.

In the "metric TSP", also known as "delta-TSP" or Δ-TSP, the intercity distances satisfy the triangle inequality.

A very natural restriction of the TSP is to require that the distances between cities form a metric to satisfy the triangle inequality; that is the direct connection from "A" to "B" is never farther than the route via intermediate "C":

The edge spans then build a metric on the set of vertices. When the cities are viewed as points in the plane, many natural distance functions are metrics, and so many natural instances of TSP satisfy this constraint.

The following are some examples of metric TSPs for various metrics.

The last two metrics appear, for example, in routing a machine that drills a given set of holes in a printed circuit board. The Manhattan metric corresponds to a machine that adjusts first one co-ordinate, and then the other, so the time to move to a new point is the sum of both movements. The maximum metric corresponds to a machine that adjusts both co-ordinates simultaneously, so the time to move to a new point is the slower of the two movements.

In its definition, the TSP does not allow cities to be visited twice, but many applications do not need this constraint. In such cases, a symmetric, non-metric instance can be reduced to a metric one. This replaces the original graph with a complete graph in which the inter-city distance formula_25 is replaced by the shortest path between "A" and "B" in the original graph.

When the input numbers can be arbitrary real numbers, Euclidean TSP is a particular case of metric TSP, since distances in a plane obey the triangle inequality. When the input numbers must be integers, comparing lengths of tours involves comparing sums of square-roots.

Like the general TSP, Euclidean TSP is NP-hard in either case. With rational coordinates and discretized metric (distances rounded up to an integer), the problem is NP-complete. With rational coordinates and the actual Euclidean metric, Euclidean TSP is known to be in the Counting Hierarchy, a subclass of PSPACE. With arbitrary real coordinates, Euclidean TSP cannot be in such classes, since there are uncountably many possible inputs. However, Euclidean TSP is probably the easiest version for approximation. For example, the minimum spanning tree of the graph associated with an instance of the Euclidean TSP is a Euclidean minimum spanning tree, and so can be computed in expected O ("n" log "n") time for "n" points (considerably less than the number of edges). This enables the simple 2-approximation algorithm for TSP with triangle inequality above to operate more quickly.

In general, for any "c" > 0, where "d" is the number of dimensions in the Euclidean space, there is a polynomial-time algorithm that finds a tour of length at most (1 + 1/"c") times the optimal for geometric instances of TSP in

time; this is called a polynomial-time approximation scheme (PTAS). Sanjeev Arora and Joseph S. B. Mitchell were awarded the Gödel Prize in 2010 for their concurrent discovery of a PTAS for the Euclidean TSP.

In practice, simpler heuristics with weaker guarantees continue to be used.

In most cases, the distance between two nodes in the TSP network is the same in both directions. The case where the distance from "A" to "B" is not equal to the distance from "B" to "A" is called asymmetric TSP. A practical application of an asymmetric TSP is route optimization using street-level routing (which is made asymmetric by one-way streets, slip-roads, motorways, etc.).

Solving an asymmetric TSP graph can be somewhat complex. The following is a 3×3 matrix containing all possible path weights between the nodes "A", "B" and "C". One option is to turn an asymmetric matrix of size "N" into a symmetric matrix of size 2"N".

To double the size, each of the nodes in the graph is duplicated, creating a second "ghost node", linked to the original node with a "ghost" edge of very low (possibly negative) weight, here denoted −"w". (Alternatively, the ghost edges have weight 0, and weight w is added to all other edges.) The original 3×3 matrix shown above is visible in the bottom left and the transpose of the original in the top-right. Both copies of the matrix have had their diagonals replaced by the low-cost hop paths, represented by −"w". In the new graph, no edge directly links original nodes and no edge directly links ghost nodes.

The weight −"w" of the "ghost" edges linking the ghost nodes to the corresponding original nodes must be low enough to ensure that all ghost edges must belong to any optimal symmetric TSP solution on the new graph (w=0 is not always low enough). As a consequence, in the optimal symmetric tour, each original node appears next to its ghost node (e.g. a possible path is formula_27) and by merging the original and ghost nodes again we get an (optimal) solution of the original asymmetric problem (in our example, formula_28).

There is an analogous problem in geometric measure theory which asks the following: under what conditions may a subset "E" of Euclidean space be contained in a rectifiable curve (that is, when is there a curve with finite length that visits every point in "E")? This problem is known as the analyst's travelling salesman problem

Suppose formula_29 are formula_30 independent random variables with uniform distribution in the square formula_31, and let formula_32 be the shortest path length (i.e. TSP solution) for this set of points, according to the usual Euclidean distance. It is known that, almost surely,

where formula_34 is a positive constant that is not known explicitly. Since formula_35 (see below), it follows from bounded convergence theorem that formula_36, hence lower and upper bounds on formula_34 follow from bounds on formula_38.

The almost sure limit formula_39 as formula_40 may not exist 
if the independent locations formula_29 are replaced with observations from a stationary ergodic process with uniform marginals.






where 0.522 comes from the points near square boundary which have fewer neighbours,
and Christine L. Valenzuela and Antonia J. Jones obtained the following other numerical lower bound:

The problem has been shown to be NP-hard (more precisely, it is complete for the complexity class FP; see function problem), and the decision problem version ("given the costs and a number "x", decide whether there is a round-trip route cheaper than "x"") is NP-complete. The bottleneck traveling salesman problem is also NP-hard. The problem remains NP-hard even for the case when the cities are in the plane with Euclidean distances, as well as in a number of other restrictive cases. Removing the condition of visiting each city "only once" does not remove the NP-hardness, since it is easily seen that in the planar case there is an optimal tour that visits each city only once (otherwise, by the triangle inequality, a shortcut that skips a repeated visit would not increase the tour length).

In the general case, finding a shortest travelling salesman tour is NPO-complete. If the distance measure is a metric (and thus symmetric), the problem becomes APX-complete and Christofides’s algorithm approximates it within 1.5.
The best known inapproximability bound is 123/122 .

If the distances are restricted to 1 and 2 (but still are a metric) the approximation ratio becomes 8/7. In the asymmetric case with triangle inequality, only logarithmic performance guarantees are known, the best current algorithm achieves performance ratio 0.814 log("n"); it is an open question if a constant factor approximation exists.
The best known inapproximability bound is 75/74 .

The corresponding maximization problem of finding the "longest" travelling salesman tour is approximable within 63/38. If the distance function is symmetric, the longest tour can be approximated within 4/3 by a deterministic algorithm and within formula_66 by a randomized algorithm.

The TSP, in particular the Euclidean variant of the problem, has attracted the attention of researchers in cognitive psychology. It has been observed that humans are able to produce near-optimal solutions quickly, in a close-to-linear fashion, with performance that ranges from 1% less efficient for graphs with 10-20 nodes, and 11% more efficient for graphs with 120 nodes. The apparent ease with which humans accurately generate near-optimal solutions to the problem has led researchers to hypothesize that humans use one or more heuristics, with the two most popular theories arguably being the convex-hull hypothesis and the crossing-avoidance heuristic. However, additional evidence suggests that human performance is quite varied, and individual differences as well as graph geometry appear to affect performance in the task. Nevertheless, results suggest that computer performance on the TSP may be improved by understanding and emulating the methods used by humans for these problems, and have also led to new insights into the mechanisms of human thought. The first issue of the "Journal of Problem Solving" was devoted to the topic of human performance on TSP, and a 2011 review listed dozens of papers on the subject.

A 2011 study in animal cognition entitled “Let the Pigeon Drive the Bus,” named after the children's book "Don't Let the Pigeon Drive the Bus!", examined spatial cognition in pigeons by studying their flight patterns between multiple feeders in a laboratory in relation to the travelling salesman problem. In the first experiment, pigeons were placed in the corner of a lab room and allowed to fly to nearby feeders containing peas. The researchers found that pigeons largely used proximity to determine which feeder they would select next. In the second experiment, the feeders were arranged in such a way that flying to the nearest feeder at every opportunity would be largely inefficient if the pigeons needed to visit every feeder. The results of the second experiment indicate that pigeons, while still favoring proximity-based solutions, “can plan several steps ahead along the route when the differences in travel costs between efficient and less efficient routes based on proximity become larger.” These results are consistent with other experiments done with non-primates, which have proven that some non-primates were able to plan complex travel routes. This suggests non-primates may possess a relatively sophisticated spatial cognitive ability.

When presented with a spatial configuration of food sources, the amoeboid Physarum polycephalum adapts its morphology to create an efficient path between the food sources which can also be viewed as an approximate solution to TSP. It's considered to present interesting possibilities and it has been studied in the area of natural computing.

For benchmarking of TSP algorithms, TSPLIB is a library of sample instances of the TSP and related problems is maintained, see the TSPLIB external reference. Many of them are lists of actual cities and layouts of actual printed circuits.






</doc>
<doc id="420524" url="https://en.wikipedia.org/wiki?curid=420524" title="Bottleneck traveling salesman problem">
Bottleneck traveling salesman problem

The Bottleneck traveling salesman problem (bottleneck TSP) is a problem in discrete or combinatorial optimization. The problem is to find the Hamiltonian cycle in a weighted graph which minimizes the weight of the most weighty edge of the cycle. It was first formulated by with some additional constraints, and in its full generality by .

The problem is known to be NP-hard. The decision problem version of this, "for a given length is there a Hamiltonian cycle in a graph with no edge longer than ?", is NP-complete. NP-completeness follows immediately by a reduction from the problem of finding a Hamiltonian cycle.

Another reduction, from the bottleneck TSP to the usual TSP (where the goal is to minimize the sum of edge lengths), allows any algorithm for the usual TSP to also be used to solve the bottleneck TSP.
If the edge weights of the bottleneck TSP are replaced by any other numbers that have the same relative order, then the bottleneck solution remains unchanged.
If, in addition, each number in the sequence exceeds the sum of all smaller numbers, then the bottleneck solution will also equal the usual TSP solution.
For instance, such a result may be attained by resetting each weight to where is the number of vertices in the graph and is the rank of the original weight of the edge in the sorted sequence of weights. For instance, following this transformation, the Held–Karp algorithm could be used to solve the bottleneck TSP in time .

Alternatively, the problem can be solved by performing a binary search or sequential search for the smallest such that the subgraph of edges of weight at most has a Hamiltonian cycle. This method leads to solutions whose running time is only a logarithmic factor larger than the time to find a Hamiltonian cycle.

In an asymmetric bottleneck TSP, there are cases where the weight from node "A" to "B" is different from the weight from B to A (e. g. travel time between two cities with a traffic jam in one direction).

The Euclidean bottleneck TSP, or planar bottleneck TSP, is the bottleneck TSP with the distance being the ordinary Euclidean distance. The problem still remains NP-hard. However, many heuristics work better for it than for other distance functions.

The maximum scatter traveling salesman problem is another variation of the traveling salesman problem in which the goal is to find a Hamiltonian cycle that maximizes the minimum edge length rather than minimizing the maximum length. Its applications include the analysis of medical images, and the scheduling of metalworking steps in aircraft manufacture to avoid heat buildup from steps that are nearby in both time and space. It can be translated into an instance of the bottleneck TSP problem by negating all edge lengths (or, to keep the results positive, subtracting them all from a large enough constant). However, although this transformation preserves the optimal solution, it does not preserve the quality of approximations to that solution.

If the graph is a metric space then there is an efficient approximation algorithm that finds a Hamiltonian cycle with maximum edge weight being no more than twice the optimum.
This result follows by Fleischner's theorem, that the square of a 2-vertex-connected graph always contains a Hamiltonian cycle. It is easy to find a threshold value , the smallest value such that the edges of weight form a 2-connected graph. Then provides a valid lower bound on the bottleneck TSP weight, for the bottleneck TSP is itself a 2-connected graph and necessarily contains an edge of weight at least . However, the square of the subgraph of edges of weight at most is Hamiltonian. By the triangle inequality for metric spaces, its Hamiltonian cycle has edges of weight at most .

This approximation ratio is best possible. For, any unweighted graph can be transformed into a metric space by setting its edge weights to and setting the distance between all nonadjacent pairs of vertices to . An approximation with ratio better than in this metric space could be used to determine whether the original graph contains a Hamiltonian cycle, an NP-complete problem.

Without the assumption that the input is a metric space, no finite approximation ratio is possible.



</doc>
<doc id="172564" url="https://en.wikipedia.org/wiki?curid=172564" title="Route inspection problem">
Route inspection problem

In graph theory, a branch of mathematics and computer science, the Chinese postman problem, postman tour or route inspection problem is to find a shortest closed path or circuit that visits every edge of an (connected) undirected graph. When the graph has an Eulerian circuit (a closed walk that covers every edge once), that circuit is an optimal solution. Otherwise, the optimization problem is to find the smallest number of graph edges to duplicate (or the subset of edges with the minimum possible total weight) so that the resulting multigraph does have an Eulerian circuit. It can be solved in polynomial time.

The problem was originally studied by the Chinese mathematician Kwan Mei-Ko in 1960, whose Chinese paper was translated into English in 1962. The original name "Chinese postman problem" was coined in his honor; different sources credit the coinage either to Alan J. Goldman or Jack Edmonds, both of whom were at the U.S. National Bureau of Standards at the time.

A generalization is to choose any set "T" of evenly many vertices that are to be joined by an edge set in the graph whose odd-degree vertices are precisely those of "T". Such a set is called a "T"-join. This problem, the "T"-join problem, is also solvable in polynomial time by the same approach that solves the postman problem.

The undirected route inspection problem can be solved in polynomial time by an algorithm based on the concept of a "T"-join.
Let "T" be a subset of the vertex set of a graph. An edge set "J" is called a "T-join if the collection of vertices that have an odd number of incident edges in "J" is exactly the set "T". A "T"-join exists whenever every connected component of the graph contains an even number of vertices in "T". The "T-join problem is to find a "T"-join with the minimum possible number of edges or the minimum possible total weight.
For any "T", a smallest "T"-join (when it exists) necessarily consists of formula_1 paths that join the vertices of "T" in pairs. The paths will be such that the total length or total weight of all of them is as small as possible. In an optimal solution, no two of these paths will share any edge, but they may have shared vertices. A minimum "T"-join can be obtained by constructing a complete graph on the vertices of "T", with edges that represent shortest paths in the given input graph, and then finding a minimum weight perfect matching in this complete graph. The edges of this matching represent paths in the original graph, whose union forms the desired "T"-join.
Both constructing the complete graph, and then finding a matching in it, can be done in O("n") computational steps.

For the route inspection problem, "T" should be chosen as the set of all odd-degree vertices. By the assumptions of the problem, the whole graph is connected (otherwise no tour exists), and by the handshaking lemma it has an even number of odd vertices, so a "T"-join always exists. Doubling the edges of a "T"-join causes the given graph to become an Eulerian multigraph (a connected graph in which every vertex has even degree), from which it follows that it has an Euler tour, a tour that visits each edge of the multigraph exactly once. This tour will be an optimal solution to the route inspection problem.

On a directed graph, the same general ideas apply, but different techniques must be used. If the directed graph is Eulerian, one need only find an Euler cycle. If it is not, one must find "T"-joins, which in this case entails finding paths from vertices with an in-degree greater than their out-degree to those with an out-degree greater than their in-degree such that they would make in-degree of every vertex equal to its out-degree. This can be solved as an instance of the minimum-cost flow problem in which there is one unit of supply for every unit of excess in-degree, and one unit of demand for every unit of excess out-degree. As such it is solvable in O(|"V"||"E"|) time. A solution exists if and only if the given graph is strongly connected.

The windy postman problem is a variant of the route inspection problem in which the input is an undirected graph, but where each edge may have a different cost for traversing it in one direction than for traversing it in the other direction.
In contrast to the solutions for directed and undirected graphs, it is NP-complete.

Various combinatorial problems have been reduced to the Chinese Postman Problem, including finding a maximum cut in a planar graph 
and a minimum-mean length circuit in an undirected graph.

A few variants of the Chinese Postman Problem have been studied and shown to be NP-complete.




</doc>
<doc id="4553193" url="https://en.wikipedia.org/wiki?curid=4553193" title="Matching">
Matching

Matching may refer to:




</doc>
<doc id="140592" url="https://en.wikipedia.org/wiki?curid=140592" title="Assignment problem">
Assignment problem

The assignment problem is a fundamental combinatorial optimization problem. It consists of finding, in a weighted bipartite graph, a matching of a given size, in which the sum of weights of the edges is a minimum. 

A common variant consists of finding a "maximum-"weight matching. This is a specialization of the maximum weight matching problem, in which the input graph is bipartite. 

In its most general form, the problem is as follows:

If the numbers of agents and tasks are equal, then the problem is called "balanced assignment". Otherwise, it is called "unbalanced assignment".

If the total cost of the assignment for all tasks is equal to the sum of the costs for each agent (or the sum of the costs for each task, which is the same thing in this case), then the problem is called "linear assignment". 

Commonly, when speaking of the "assignment problem" without any additional qualification, then the "linear balanced assignment problem" is meant.

Suppose that a taxi firm has three taxis (the agents) available, and three customers (the tasks) wishing to be picked up as soon as possible. The firm prides itself on speedy pickups, so for each taxi the "cost" of picking up a particular customer will depend on the time taken for the taxi to reach the pickup point. This is a "balanced assignment" problem. Its solution is whichever combination of taxis and customers results in the least total cost.

Now, suppose that there are "four" taxis available, but still only three customers. This is an "unbalanced assignment" problem. One way to solve it is to invent a fourth dummy task, perhaps called "sitting still doing nothing", with a cost of 0 for the taxi assigned to it. This reduces the problem to a balanced assignment problem, which can then be solved in the usual way and still give the best solution to the problem.

Similar adjustments can be done in order to allow more tasks than agents, tasks to which multiple agents must be assigned (for instance, a group of more customers than will fit in one taxi), or maximizing profit rather than minimizing cost.

The formal definition of the assignment problem (or linear assignment problem) is

is minimized.

Usually the weight function is viewed as a square real-valued matrix "C", so that the cost function is written down as:

The problem is "linear" because the cost function to be optimized as well as all the constraints contain only linear terms.

A naive solution for the assignment problem is to check all the assignments and calculate the cost of each one. This may be very inefficient since, with "n" agents and "n" tasks, there are "n"! (factorial of "n") different assignments. Fortunately, there are many algorithms for solving the problem in time polynomial in "n".

The assignment problem is a special case of the transportation problem, which is a special case of the minimum cost flow problem, which in turn is a special case of a linear program. While it is possible to solve any of these problems using the simplex algorithm, each specialization has more efficient algorithms designed to take advantage of its special structure.

In the balanced assignment problem, both parts of the bipartite graph have the same number of vertices, denoted by "n".

One of the first polynomial-time algorithms for balanced assignment was the Hungarian algorithm. It is a "global" algorithm - it is based on improving a matching along augmenting paths (alternating paths between unmatched vertices). Its run-time complexity, when using Fibonacci heaps, is formula_3. This is currently the fastest run-time of a strongly polynomial algorithm for this problem. If all weights are integers, then the run-time can be improved to formula_4, but the resulting algorithm is only weakly-polynomial. If the weights are integers, and all weights are at most "C" (where "C">1 is some integer), then the problem can be solved in formula_5weakly-polynomial time in a method called "weight scaling".

In addition to the global methods, there are "local methods" which are based on finding local updates (rather than full augmenting paths). These methods have worse asymptotic runtime guarantees, but they often work better in practice. These algorithms are called auction algorithms, push-relabel algorithms, or preflow-push algorithms. Some of these algorithms were shown to be equivalent. 

Some of the local methods assume that the graph admits a "perfect matching"; if this is not the case, then some of these methods might run forever. A simple technical way to solve this problem is to extend the input graph to a "complete bipartite graph," by adding artificial edges with very large weights. These weights should exceed the weights of all existing matchings, to prevent appearance of artificial edges in the possible solution. 

As shown by Mulmuley, Vazirani and Vazirani, the problem of minimum weight perfect matching is converted to finding minors in the adjacency matrix of a graph. Using the isolation lemma, a minimum weight perfect matching in a graph can be found with probability at least ½. For a graph with "n" vertices, it requires formula_6 time.

In the unbalanced assignment problem, the larger part of the bipartite graph has "n" vertices and the smaller part has "r"<"n" vertices. There is also a constant "s" which is at most the maximum cardinality of a matching in the graph. The goal is to find a minimum-cost matching of size exactly "s". The most common case is the case in which the graph admits a one-sided-perfect matching (i.e., a matching of size "r"), and "s"="r".

Unbalanced assignment can be reduced to a balanced assignment. The naive reduction is to add formula_7 new vertices to the smaller part and connect them to the larger part using edges of cost 0. However, this requires formula_8 new edges. A more efficient reduction is called the "doubling technique". Here, a new graph "G'" is built from two copies of the original graph "G": a forward copy "Gf" and a backward copy "Gb." The backward copy is "flipped", so that, in each side of "G"', there are now "n"+"r" vertices. Between the copies, we need to add two kinds of linking edges: 


All in all, at most formula_9 new edges are required. The resulting graph always has a perfect matching of size formula_9. A minimum-cost perfect matching in this graph must consist of minimum-cost maximum-cardinality matchings in "Gf" and "Gb." The main problem with this doubling technique is that there is no speed gain when formula_11.

Instead of using reduction, the unbalanced assignment problem can be solved by directly generalizing existing algorithms for balanced assignment. The Hungarian algorithm can be generalized to solve the problem in formula_12 strongly-polynomial time. In particular, if "s"="r" then the runtime is formula_13. If the weights are integers, then Thorup's method can be used to get a runtime of formula_14. 

The assignment problem can be solved by presenting it as a linear program. For convenience we will present the maximization problem. Each edge ("i","j"), where "i" is in A and "j" is in T, has a weight "formula_15". For each edge "(i,j)" we have a variable "formula_16" The variable is 1 if the edge is contained in the matching and 0 otherwise, so we set the domain constraints: formula_17 formula_18

The total weight of the matching is: formula_19. The goal is to find a maximum-weight perfect matching. 

To guarantee that the variables indeed represent a perfect matching, we add constraints saying that each vertex is adjacent to exactly one edge in the matching, i.e, formula_20.

All in all we have the following LP:

formula_21formula_22formula_23formula_24This is an integer linear program. However, we can solve it without the integrality constraints (i.e., drop the last constraint), using standard methods for solving continuous linear programs. While this formulation allows also fractional variable values, in this special case, the LP always has an optimal solution where the variables take integer values. This is because the constraint matrix of the fractional LP is totally unimodular - it satisfies the four conditions of Hoffman and Gale. 

This can also be proved directly. Let "x" be an optimal solution of the fractional LP, "w(x)" be its total weight, and "k(x)" be the number of non-integral variables. If "k(x)"=0 we are done. Otherwise, there is a fractional variable, say "formula_25". Because the sum of variables adjacent to "j2" is 1, which in an integer, there must be another variable adjacent to "j"2 with a fractional value, say "formula_26". By similar considerations on "i"3, there must be another variable adjacent to "i"3 with a fractional value, say "formula_27". By similar considerations we move from one vertex to another, collecting edges with fractional values. Since the graph is finite, at some point we must have a cycle. Without loss of generality we can assume that the cycle ends at vertex "i"1, so the last fractional variable in the cycle is "formula_28". So the number of edges in the cycle is 2"m" - it must be even since the graph is bipartite. 

Suppose we add a certain constant "e" to all even variables in the cycle, and remove the same constant "e" from all odd variables in the cycle. For any such "e", the sum of variables near each vertex remains the same (1), so the vertex constraints are still satisfied. Moreover, if "e" is sufficiently small, all variables remain between 0 and 1, so the domain constraints are still satisfied too. It is easy to find a largest "e" that maintains the domain constraints: it is either the smallest difference between an odd variable and 0, or the smallest difference between an even variable and 1. Now, we have one less fractional variable, so "k"("x") decreases by 1. The objective value remains the same, since otherwise we could increase it by selecting "e" to be positive or negative, in contradiction to the assumption that it is maximal. 

By repeating the cycle-removal process we arrive, after at most "n" steps, at a solution in which all variables are integral.




</doc>
<doc id="681409" url="https://en.wikipedia.org/wiki?curid=681409" title="Stable marriage problem">
Stable marriage problem

In mathematics, economics, and computer science, the stable marriage problem (also stable matching problem or SMP) is the problem of finding a stable matching between two equally sized sets of elements given an ordering of preferences for each element. A matching is a mapping from the elements of one set to the elements of the other set. A matching is "not" stable if:

In other words, a matching is stable when there does not exist any match ("A", "B") which both prefer each other to their current partner under the matching.

The stable marriage problem has been stated as follows:

The existence of two classes that need to be paired with each other (men and women in this example) distinguishes this problem from the stable roommates problem.

Algorithms for finding solutions to the stable marriage problem have applications in a variety of real-world situations, perhaps the best known of these being in the assignment of graduating medical students to their first hospital appointments. In 2012, The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel was awarded to Lloyd S. Shapley and Alvin E. Roth "for the theory of stable allocations and the practice of market design."

An important and large-scale application of stable marriage is in assigning users to servers in a large distributed Internet service. Billions of users access web pages, videos, and other services on the Internet, requiring each user to be matched to one of (potentially) hundreds of thousands of servers around the world that offer that service. A user prefers servers that are proximal enough to provide a faster response time for the requested service, resulting in a (partial) preferential ordering of the servers for each user. Each server prefers to serve users that it can with a lower cost, resulting in a (partial) preferential ordering of users for each server. Content delivery networks that distribute much of the world's content and services solve this large and complex stable marriage problem between users and servers every tens of seconds to enable billions of users to be matched up with their respective servers that can provide the requested web pages, videos, or other services.

In general, there may be many different stable matchings. For example, suppose there are three men (A,B,C) and three women (X,Y,Z) which have preferences of:

There are three stable solutions to this matching arrangement:


All three are stable, because instability requires one of the participants to be happier with an alternative match. Giving one group their first choices ensures that the matches are stable because they would be unhappy with any other proposed match. Giving everyone their second choice ensures that any other match would be disliked by one of the parties. In general, the family of solutions to any instance of the stable marriage problem can be given the structure of a finite distributive lattice,
and this structure leads to efficient algorithms for several problems on stable marriages.

In a uniformly-random instance of the stable marriage problem with men and women, the average number of stable matchings is asymptotically formula_1.
In a stable marriage instance chosen to maximize the number of different stable matchings, this number is an exponential function of .
Counting the number of stable matchings in a given instance is #P-complete.

In 1962, David Gale and Lloyd Shapley proved that, for any equal number of men and women, it is always possible to solve the SMP and make all marriages stable. They presented an algorithm to do so.

The Gale–Shapley algorithm (also known as the deferred acceptance algorithm) involves a number of "rounds" (or "iterations"):

This algorithm is guaranteed to produce a stable marriage for all participants in time formula_2 where formula_3 is the number of men or women.

Among all possible different stable matchings, it always yields the one that is best for all men among all stable matchings, and worst for all women.
It is a truthful mechanism from the point of view of men (the proposing side). I.e, no man can get a better matching for himself by misrepresenting his preferences. Moreover, the GS algorithm is even "group-strategy proof" for men, i.e., no coalition of men can coordinate a misrepresentation of their preferences such that all men in the coalition are strictly better-off. However, it is possible for some coalition to misrepresent their preferences such that some men are better-off and the other men retain the same partner.
The GS algorithm is non-truthful for the women (the reviewing side): each woman may be able to misrepresent her preferences and get a better match.

The Rural hospitals theorem concerns a more general variant of the stable matching problem, like that applying in the problem of matching doctors to positions at hospitals, differing in the following ways from the basic -to- form of the stable marriage problem:
In this case, the condition of stability is that no unmatched pair prefer each other to their situation in the matching (whether that situation is another partner or being unmatched). With this condition, a stable matching will still exist, and can still be found by the Gale–Shapley algorithm.

For this kind of stable matching problem, the rural hospitals theorem states that:

In stable matching with indifference, some men might be indifferent between two or more women and vice versa.

The stable roommates problem is similar to the stable marriage problem, but differs in that all participants belong to a single pool (instead of being divided into equal numbers of "men" and "women").

The hospitals/residents problem – also known as the college admissions problem – differs from the stable marriage problem in that a hospital can take multiple residents, or a college can take an incoming class of more than one student. Algorithms to solve the hospitals/residents problem can be "hospital-oriented" (as the NRMP was before 1995) or "resident-oriented". This problem was solved, with an algorithm, in the same original paper by Gale and Shapley, in which the stable marriage problem was solved.

The hospitals/residents problem with couples allows the set of residents to include couples who must be assigned together, either to the same hospital or to a specific pair of hospitals chosen by the couple (e.g., a married couple want to ensure that they will stay together and not be stuck in programs that are far away from each other). The addition of couples to the hospitals/residents problem renders the problem NP-complete.

The assignment problem seeks to find a matching in a weighted bipartite graph that has maximum weight. Maximum weighted matchings do not have to be stable, but in some applications a maximum weighted matching is better than a stable one.

The matching with contracts problem is a generalization of matching problem, in which participants can be matched with different terms of contracts. An important special case of contracts is matching with flexible wages.





</doc>
<doc id="15698614" url="https://en.wikipedia.org/wiki?curid=15698614" title="Stable roommates problem">
Stable roommates problem

In mathematics, economics and computer science, particularly in the fields of combinatorics, game theory and algorithms, the stable-roommate problem (SRP) is the problem of finding a stable matching for an even-sized set. A matching is a separation of the set into disjoint pairs ("roommates"). The matching is "stable" if there are no two elements which are not roommates and which both prefer each other to their roommate under the matching. This is distinct from the stable-marriage problem in that the stable-roommates problem allows matches between any two elements, not just between classes of "men" and "women".

It is commonly stated as:

Unlike the stable marriage problem, a stable matching may fail to exist for certain sets of participants and their preferences. For a minimal example of a stable pairing not existing, consider 4 people A, B, C, and D, whose rankings are:

In this ranking, each of A, B, and C is the most preferable person for someone. In any solution, one of A, B, or C "must" be paired with D and the other two with each other (for example AD and BC), yet for anyone who is partnered with D, another member will have rated them highest, and D’s partner will in turn prefer this other member over D. In this example, AC is a more favorable pairing than AD, but the necessary remaining pairing of BD then raises the same issue, illustrating the absence of a stable matching for these participants and their preferences.

An efficient algorithm was given in . The algorithm will determine, for any instance of the problem, whether a stable matching exists, and if so, will find such a matching. Irving’s algorithm has O("n") complexity, provided suitable data structures are used to implement the necessary manipulation of the preference lists and identification of rotations.

The algorithm consists of two phases. In Phase 1, participants "propose" to each other, in a manner similar to that of the Gale-Shapley algorithm for the stable marriage problem. Each participant orders the other members by preference, resulting in a preference list—an ordered set of the other participants. Participants then propose to each person on their list, in order, continuing to the next person if and when their current proposal is rejected. A participant will reject a proposal if they already hold a proposal from someone they prefer. A participant will also reject a previously-accepted proposal if they later receive a proposal that they prefer. In this case, the rejected participant will then propose to the next person on their list, continuing until a proposal is again accepted. If any participant is eventually rejected by all other participants, this indicates that no stable matching is possible. Otherwise, Phase 1 will end with each person holding a proposal from one of the others.

Consider two participants, "q" and "p". If "q" holds a proposal from "p", then we remove from "q"s list all participants "x" after "p", and symmetrically, for each removed participant "x", we remove "q" from "x"s list, so that "q" is first in "p"s list; and "p", last in "q"s, since "q" and any "x" cannot be partners in any stable matching. The resulting reduced set of preference lists together is called the Phase 1 table. In this table, if any reduced list is empty, then there is no stable matching. Otherwise, the Phase 1 table is a "stable table". A stable table, by definition, is the set of preference lists from the original table after members have been removed from one or more of the lists, and the following three conditions are satisfied (where reduced list means a list in the stable table):

(i) "p" is first on "q"s reduced list if and only if "q" is last on "p"s 
(ii) "p" is not on "q"s reduced list if and only if "q" is not on "p"s if and only if "q" prefers the last person on their list to "p"; or "p", the last person on their list to "q" 
(iii) no reduced list is empty

Stable tables have several important properties, which are used to justify the remainder of the procedure:

1. Any stable table must be a subtable of the Phase 1 table, where subtable is a table where the preference lists of the subtable are those of the supertable with some individuals removed from each other's lists.

2. In any stable table, if every reduced list contains "exactly" one individual, then pairing each individual with the single person on their list gives a stable matching.

3. If the stable roommates problem instance has a stable matching, then there is a stable matching contained in any one of the stable tables.

4. Any stable subtable of a stable table, and in particular any stable subtable that specifies a stable matching as in 2, can be obtained by a sequence of "rotation eliminations" on the stable table.

These rotation eliminations comprise Phase 2 of Irving’s algorithm.

By 2, if each reduced list of the Phase 1 table contains exactly one individual, then this gives a matching.

Otherwise, the algorithm enters Phase 2. A "rotation" in a stable table "T" is defined as a sequence ("x", "y"), ("x", "y"), ..., ("x", "y") such that the "x" are distinct, "y" is first on "x"'s reduced list (or "x" is last on "y"'s reduced list) and "y" is second on "x"'s reduced list, for i = 0, ..., k-1 where the indices are taken modulo k. It follows that in any stable table with a reduced list containing at least two individuals, such a rotation always exists. To find it, start at such a "p" containing at least two individuals in their reduced list, and define recursively "q" to be the second on "p"'s list and "p" to be the last on "q"'s list, until this sequence repeats some "p", at which point a rotation is found: it is the sequence of pairs starting at the first occurrence of ("p", "q") and ending at the pair before the last occurrence. The sequence of "p" up until the "p" is called the "tail" of the rotation. The fact that it's a stable table in which this search occurs guarantees that each "p" has at least two individuals on their list.

To eliminate the rotation, "y" rejects "x" so that "x" proposes to "y", for each "i". To restore the stable table properties (i) and (ii), for each "i", all successors of "x" are removed from "y"'s list, and "y" is removed from their lists. If a reduced list becomes empty during these removals, then there is no stable matching. Otherwise, the new table is again a stable table, and either already specifies a matching since each list contains exactly one individual or there remains another rotation to find and eliminate, so the step is repeated.

Phase 2 of the algorithm can now be summarized as follows:

<source lang="java">
T = Phase 1 table;
while (true) {


</doc>
<doc id="10532598" url="https://en.wikipedia.org/wiki?curid=10532598" title="Permanent">
Permanent

Permanent may refer to:





</doc>
<doc id="20749642" url="https://en.wikipedia.org/wiki?curid=20749642" title="Computing the permanent">
Computing the permanent

In linear algebra, the computation of the permanent of a matrix is a problem that is thought to be more difficult than the computation of the determinant of a matrix despite the apparent similarity of the definitions.

The permanent is defined similarly to the determinant, as a sum of products of sets of matrix entries that lie in distinct rows and columns. However, where the determinant weights each of these products with a ±1 sign based on the parity of the set, the permanent weights them all with a +1 sign.

While the determinant can be computed in polynomial time by Gaussian elimination, it is generally believed that the permanent cannot be computed in polynomial time. In computational complexity theory, a theorem of Valiant states that computing permanents is #P-hard, and even #P-complete for matrices in which all entries are 0 or 1 . This puts the computation of the permanent in a class of problems believed to be even more difficult to compute than NP. It is known that computing the permanent is impossible for logspace-uniform ACC circuits.

The development of both exact and approximate algorithms for computing the permanent of a matrix is an active area of research.

The permanent of an "n"-by-"n" matrix "A" = ("a") is defined as

The sum here extends over all elements σ of the symmetric group "S", i.e. over all permutations of the numbers 1, 2, ..., "n". This formula differs from the corresponding formula for the determinant only in that, in the determinant, each product is multiplied by the sign of the permutation σ while in this formula each product is unsigned. The formula may be directly translated into an algorithm that naively expands the formula, summing over all permutations and within the sum multiplying out each matrix entry. This requires "n!" "n" arithmetic operations.

The best known general exact algorithm is due to .
Ryser’s method is based on an inclusion–exclusion formula that can be given as follows: Let formula_2 be obtained from "A" by deleting "k" columns, let formula_3 be the product of the row-sums of formula_2, and let formula_5 be the sum of the values of formula_3 over all possible formula_2. Then

It may be rewritten in terms of the matrix entries as follows

Ryser’s formula can be evaluated using formula_10 arithmetic operations, or formula_11 by processing the sets formula_12 in Gray code order.

Another formula that appears to be as fast as Ryser's (or perhaps even twice as fast) is to be found in the two Ph.D. theses; see , ; also
. The methods to find the formula are quite different, being related to the combinatorics of the Muir algebra, and to finite difference theory respectively. Another way, connected with invariant theory is via the polarization identity for a symmetric tensor . The formula generalizes to infinitely many others, as found by all these authors, although it is not clear if they are any faster than the basic one. See .

The simplest known formula of this type (when the characteristic of the field is not two) is
where the outer sum is over all formula_14 vectors formula_15.

The number of perfect matchings in a bipartite graph is counted by the permanent of the graph's biadjacency matrix, and the permanent of any 0-1 matrix can be interpreted in this way as the number of perfect matchings in a graph. For planar graphs (regardless of bipartiteness), the FKT algorithm computes the number of perfect matchings in polynomial time by changing the signs of a carefully chosen subset of the entries in the Tutte matrix of the graph, so that the Pfaffian of the resulting skew-symmetric matrix (the square root of its determinant) is the number of perfect matchings. This technique can be generalized to graphs that contain no subgraph homeomorphic to the complete bipartite graph "K".

George Pólya had asked the question of when it is possible to change the signs of some of the entries of a 01 matrix A so that the determinant of the new matrix is the permanent of A. Not all 01 matrices are "convertible" in this manner; in fact it is known () that
there is no linear map formula_16 such that formula_17 for all formula_18 matrices formula_19. The characterization of "convertible" matrices was given by who showed that such matrices are precisely those that are the biadjacency matrix of bipartite graphs that have a Pfaffian orientation: an orientation of the edges such that for every even cycle formula_20 for which formula_21 has a perfect matching, there are an odd number of edges directed along C (and thus an odd number with the opposite orientation). It was also shown that these graphs are exactly those that do not contain a subgraph homeomorphic to formula_22, as above.

Modulo 2, the permanent is the same as the determinant, as formula_23 It can also be computed modulo formula_24 in time formula_25 for formula_26. However, it is UP-hard to compute the permanent modulo any number that is not a power of 2. 

There are various formulae given by for the computation modulo a prime .
Firstly, there is one using symbolic calculations with partial derivatives.

Secondly, for = 3 there is the following formula for an nxn-matrix formula_19, involving the matrix's principal
minors (): 
where formula_29 is the submatrix of formula_19 induced by the rows and columns of formula_19
indexed by formula_32, and formula_33 is the complement of formula_32 in formula_35, while the determinant of the empty submatrix is defined to be 1.

(Actually the above expansion can be generalized in an arbitrary characteristic p as the following pair of dual identities:

formula_36

formula_37

where in both formulas the sum is taken over all the (p-1)-tuples formula_38 that are partitions of the set formula_35 into p-1 subsets, some of them possibly empty.

The former formula possesses an analog for the hafnian of a symmetric formula_19 and an odd p:

formula_41
with the sum taken over the same set of indexes. Moreover, in characteristic zero a similar convolution sum expression involving both the permanent and the determinant yields the Hamiltonian cycle polynomial (defined as formula_42 where formula_43 is the set of n-permutations having only one cycle):
formula_44 . In characteristic 2 the latter equality turns into formula_45 what therefore provides an opportunity to polynomial-time calculate the Hamiltonian cycle polynomial of any unitary formula_46 (i.e. such that formula_47 where formula_48 is the identity nxn-matrix), because each minor of such a matrix coincides with its algebraic complement: formula_49 where formula_50 is the identity nxn-matrix with the entry of indexes 1,1 replaced by 0. Moreover, it may, in turn, be further generalized for a unitary nxn-matrix formula_46 as formula_52 where formula_53 is a subset of {1...,n}, formula_54 is the identity nxn-matrix with the entries of indexes k,k replaced by 0 for all k belonging to formula_53, and we define formula_56 where formula_57 is the set of n-permutations whose each cycle contains at least one element of formula_53.)

This formula implies the following identities over fields of characteristic 3:

for any invertible formula_19
for any unitary formula_46 , i.e. a square matrix formula_46 such that formula_63 where formula_48 is the identity matrix of the corresponding size,
where formula_66 is the matrix whose entries are the cubes of the corresponding entries of formula_46.

It was also shown () that, if we define a square matrix formula_19 as k-semi-unitary when formula_69 = ,
the permanent of a 1-semi-unitary matrix is computable in polynomial time over fields of characteristic 3, while for > 1 
the problem becomes #3-P-complete. (A parallel theory concerns the Hamiltonian cycle polynomial in characteristic 2: while computing it on the unitary matrices is polynomial-time feasible, the problem is #2-P-complete for the k-semi-unitary ones for any k > 0). The latter result was essentially extended in 2017 () and it was proven that in characteristic 3 there is a simple formula relating the permanents of a square matrix and its partial inverse (for formula_70 and formula_71 being square, formula_70 being invertible):

formula_73

and it allows to polynomial-time reduce the computation of the permanent of an nxn-matrix with a subset of k or k-1 rows expressible as linear combinations of another (disjoint) subset of k rows to the computation of the permanent of an (n-k)x(n-k)- or (n-k+1)x(n-k+1)-matrix correspondingly, hence having introduced a compression operator (analogical to the Gaussian modification applied for calculating the determinant) that "preserves" the permanent in characteristic 3. (Analogically, it would be worth noting that the Hamiltonian cycle polynomial in characteristic 2 does possess its invariant matrix compressions as well, taking into account the fact that ham(A) = 0 for any nxn-matrix A having three equal rows or, if n > 2, a pair of indexes i,j such that its i-th and j-th rows are identical and its i-th and j-th columns are identical too.) The closure of that operator defined as the limit of its sequential application together with the transpose transformation (utilized each time the operator leaves the matrix intact) is also an operator mapping, when applied to classes of matrices, one class to another. While the compression operator maps the class of 1-semi-unitary matrices into itself and the classes of unitary and 2-semi-unitary ones, the compression-closure of the 1-semi-unitary class (as well as the class of matrices received from unitary ones through replacing one row by an arbitrary row vector — the permanent of such a matrix is, via the Laplace expansion, the sum of the permanents of 1-semi-unitary matrices and, accordingly, polynomial-time computable) is yet unknown and tensely related to the general problem of the permanent's computational complexity in characteristic 3 and the chief question of P versus NP: as it was shown in (), if such a compression-closure is the set of all square matrices over a field of characteristic 3 or, at least, contains a matrix class the permanent's computation on is #3-P-complete (like the class of 2-semi-unitary matrices) then the permanent is computable in polynomial time in this characteristic.

Besides, the problem of finding and classifying any possible analogs of the permanent-preserving compressions existing in characteristic 3 for other prime characteristics was formulated (), while giving the following identity for an nxn matrix formula_19 and two n-vectors (having all their entries from the set {0...,p-1}) formula_75 and formula_76 such that formula_77, valid in an arbitrary prime characteristic p:

formula_78

where for an nxm-matrix formula_79, an n-vector formula_80 and an m-vector formula_81, both vectors having all their entries from the set {0...,p-1}, formula_82 denotes the matrix received from formula_79 via repeating formula_84 times its i-th row for i = 1...,n and formula_85 times its j-th column for j = 1...,m (if some row's or column's multiplicity equals zero it would mean that the row or column was removed, and thus this notion is a generalization of the notion of submatrix), and formula_86 denotes the n-vector all whose entries equal unity. This identity is an exact analog of the classical formula expressing a matrix's minor through a minor of its inverse and hence demonstrates (once more) a kind of duality between the determinant and the permanent as relative immanants. 
(Actually its own analogue for the hafnian of a symmetric formula_19 and an odd prime p is 
formula_88 ).

And, as an even wider generalization for the partial inverse case in a prime characteristic p, for formula_70, formula_71 being square, formula_70 being invertible and of size formula_92xformula_92, and formula_77, there holds also the identity

formula_95

where the common row/column multiplicity vectors formula_75 and formula_76 for the matrix formula_19 generate the corresponding row/column multiplicity vectors formula_99 and formula_100, s,t = 1,2, for its blocks (the same concerns formula_19's partial inverse in the equality's right side).

When the entries of "A" are nonnegative, the permanent can be computed approximately in probabilistic polynomial time, up to an error of ε"M", where "M" is the value of the permanent and ε > 0 is arbitrary. In other words, there exists a fully polynomial-time randomized approximation scheme (FPRAS) ().

The most difficult step in the computation is the construction of an algorithm to sample almost uniformly from the set of all perfect matchings in a given bipartite graph: in other words, a fully polynomial almost uniform sampler (FPAUS). This can be done using a Markov chain Monte Carlo algorithm that uses a Metropolis rule to define and run a Markov chain whose distribution is close to uniform, and whose mixing time is polynomial.

It is possible to approximately count the number of perfect matchings in a graph via the self-reducibility of the permanent, by using the FPAUS in combination with a well-known reduction from sampling to counting due to . Let formula_102 denote the number of perfect matchings in formula_103. Roughly, for any particular edge formula_104 in formula_103, by sampling many matchings in formula_103 and counting how many of them are matchings in formula_107, one can obtain an estimate of the ratio formula_108. The number formula_102 is then formula_110, where formula_111 can be approximated by applying the same method recursively.

Another class of matrices for which the permanent can be computed approximately, is the set of positive-semidefinite matrices (the complexity-theoretic problem of approximating the permanent of such matrices to within a multiplicative error is considered open). The corresponding randomized algorithm is based on the model of boson sampling and it uses the tools proper to quantum optics, to represent the permanent of positive-semidefinite matrices as the expected value of a specific random variable. The latter is then approximated by its sample mean. This algorithm, for a certain set of positive-semidefinite matrices, approximates their permanent in polynomial time up to an additive error, which is more reliable than that of the standard classical polynomial-time algorithm by Gurvits.



</doc>
<doc id="403165" url="https://en.wikipedia.org/wiki?curid=403165" title="Maximum flow problem">
Maximum flow problem

In optimization theory, maximum flow problems involve finding a feasible flow through a flow network that is maximum.

The maximum flow problem can be seen as a special case of more complex network flow problems, such as the circulation problem. The maximum value of an s-t flow (i.e., flow from source s to sink t) is equal to the minimum capacity of an s-t cut (i.e., cut severing s from t) in the network, as stated in the max-flow min-cut theorem.

The maximum flow problem was first formulated in 1954 by T. E. Harris and F. S. Ross as a simplified model of Soviet railway traffic flow.
In 1955, Lester R. Ford, Jr. and Delbert R. Fulkerson created the first known algorithm, the Ford–Fulkerson algorithm. In their paper of 1955, Ford and Fulkerson wrote that the problem of Harris and Ross is formulated as follows (see p. 5):Consider a rail network connecting two cities by way of a number of intermediate cities, where each link of the network has a number assigned to it representing its capacity. Assuming a steady state condition, find a maximal flow from one given city to the other.In their book "Flows in Network", in 1962, Ford and Fulkerson wrote:It was posed to the authors in the spring of 1955 by T.E. Harris, who, in conjunction with General F.S. Ross (Ret.), had formulated a simplified model of railway traffic flow, and pinpointed this particular problem as the central one suggested by the model [11].where [11] refers to the 1955 secret report "Fundamentals of a Method for Evaluating Rail net Capacities" by Harris and Ross (see p. 5).

Over the years, various improved solutions to the maximum flow problem were discovered, notably the shortest augmenting path algorithm of Edmonds and Karp and independently Dinitz; the blocking flow algorithm of Dinitz; the push-relabel algorithm of Goldberg and Tarjan; and the binary blocking flow algorithm of Goldberg and Rao. The algorithms of Sherman and Kelner, Lee, Orecchia and Sidford, respectively, find an approximately optimal maximum flow but only work in undirected graphs.

In 2013 James B. Orlin published a paper describing an formula_1algorithm for all values of formula_2and formula_3.

Let formula_4 be a network with formula_5 being the source and the sink of formula_6 respectively.

The maximum flow problem is to maximize formula_20, that is, to route as much flow as possible from formula_18 to formula_22.

The following table lists algorithms for solving the maximum flow problem.

! Method
! Complexity
! Description
The algorithm is only guaranteed to terminate if all weights are rational. Otherwise it is possible that the algorithm will not converge to the maximum value. However, if the algorithm terminates, it is guaranteed to find the maximum value.

Given a directed graph formula_37 and two vertices formula_18 and formula_22, we are to find the maximum number of paths from formula_18 to formula_22. This problem has several variants:

1. The paths must be edge-disjoint. This problem can be transformed to a maximum flow problem by constructing a network formula_4 from formula_43, with formula_18 and formula_22 being the source and the sink of formula_6 respectively, and assigning each edge a capacity of formula_47. In this network, the maximum flow is formula_48 iff there are formula_48 edge-disjoint paths.

2. The paths must be independent, i.e., vertex-disjoint (except for formula_18 and formula_22). We can construct a network formula_4 from formula_43 with vertex capacities, where the capacities of all vertices and all edges are formula_47. Then the value of the maximum flow is equal to the maximum number of independent paths from formula_18 to formula_22.

3. In addition to the paths being edge-disjoint and/or vertex disjoint, the paths also have a length constraint: we count only paths whose length is exactly formula_48, or at most formula_48. Most variants of this problem are NP-complete, except for small values of formula_48.

A closure of a directed graph is a set of vertices with no outgoing edges. That is, the graph should have no edges that start within the closure and end outside the closure. The closure problem is the task of finding the maximum-weight or minimum-weight closure in a vertex-weighted directed graph. It may be solved in polynomial time using a reduction to the maximum flow problem.

In the baseball elimination problem there are "n" teams competing in a league. At a specific stage of the league season, "w" is the number of wins and "r" is the number of games left to play for team "i" and "r" is the number of games left against team "j". A team is eliminated if it has no chance to finish the season in the first place. The task of the baseball elimination problem is to determine which teams are eliminated at each point during the season. Schwartz proposed a method which reduces this problem to maximum network flow. In this method a network is created to determine whether team "k" is eliminated.

Let "G" = ("V", "E") be a network with "s","t" ∈ "V" being the source and the sink respectively. One adds a game node {"i","j"} with "i" < "j" to "V", and connects each of them from "s" by an edge with capacity "r" – which represents the number of plays between these two teams. We also add a team node for each team and connect each game node {"i","j"} with two team nodes "i" and "j" to ensure one of them wins. One does not need to restrict the flow value on these edges. Finally, edges are made from team node "i" to the sink node "t" and the capacity of "w"+"r"–"w" is set to prevent team "i" from winning more than "w"+"r".
Let "S" be the set of all teams participating in the league and let formula_60. In this method it is claimed team "k" is not eliminated if and only if a flow value of size "r"("S" − {"k"}) exists in network "G". In the mentioned article it is proved that this flow value is the maximum flow value from "s" to "t".

In the airline industry a major problem is the scheduling of the flight crews. The airline scheduling problem can be considered as an application of extended maximum network flow. The input of this problem is a set of flights "F" which contains the information about where and when each flight departs and arrives. In one version of airline scheduling the goal is to produce a feasible schedule with at most "k" crews.

In order to solve this problem one uses a variation of the circulation problem called bounded circulation which is the generalization of network flow problems, with the added constraint of a lower bound on edge flows.

Let "G" = ("V", "E") be a network with "s","t" ∈ "V" as the source and the sink nodes. For the source and destination of every flight "i", one adds two nodes to "V", node "s" as the source and node "d" as the destination node of flight "i". One also adds the following edges to "E":

In the mentioned method, it is claimed and proved that finding a flow value of "k" in "G" between "s" and "t" is equal to finding a feasible schedule for flight set "F" with at most "k" crews.

Another version of airline scheduling is finding the minimum needed crews to perform all the flights. In order to find an answer to this problem, a bipartite graph is created where each flight has a copy in set "A" and set "B". If the same plane can perform flight "j" after flight "i", "i"∈"A" is connected to "j"∈"B". A matching in induces a schedule for "F" and obviously maximum bipartite matching in this graph produces an airline schedule with minimum number of crews. As it is mentioned in the Application part of this article, the maximum cardinality bipartite matching is an application of maximum flow problem.

There are some factories that produce goods and some villages where the goods have to be delivered. They are connected by a networks of roads with each road having a capacity for maximum goods that can flow through it. The problem is to find if there is a circulation that satisfies the demand.
This problem can be transformed into a maximum-flow problem.
Let "G" = ("V", "E") be this new network. There exists a circulation that satisfies the demand if and only if :
If there exists a circulation, looking at the max-flow solution would give the answer as to how much goods have to be sent on a particular road for satisfying the demands.

The problem can be extended by adding a lower bound on the flow on some edges.

1. In the minimum-cost flow problem, each edge ("u",v) also has a cost-coefficient "a" in addition to its capacity. If the flow through the edge is "f", then the total cost is "af." It is required to find a flow of a given size "d", with the smallest cost. In most variants, the cost-coefficients may be either positive or negative. There are various polynomial-time algorithms for this problem.

2. The maximum-flow problem can be augmented by disjunctive constraints: a "negative disjunctive constraint" says that a certain pair of edges cannot simultaneously have a nonzero flow; a "positive disjunctive constraints" says that, in a certain pair of edges, at least one must have a nonzero flow. With negative constraints, the problem becomes strongly NP-hard even for simple networks. With positive constraints, the problem is polynomial if fractional flows are allowed, but may be strongly NP-hard when the flows must be integral.



</doc>
<doc id="78130" url="https://en.wikipedia.org/wiki?curid=78130" title="Max-flow min-cut theorem">
Max-flow min-cut theorem

In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the "source" to the "sink" is equal to the total weight of the edges in the minimum cut, i.e. the smallest total weight of the edges which if removed would disconnect the source from the sink.

The max-flow min-cut theorem is a special case of the duality theorem for linear programs and can be used to derive Menger's theorem and the Kőnig–Egerváry theorem. 

The theorem relates two quantities: the maximum flow through a network, and the minimum weight of a cut of the network. To state the theorem, each of these quantities must first be defined. 
Let be a directed graph, where "V" denotes the set of vertices and "E" is the set of edges. Let and be the source and the sink of , respectively. The capacity of an edge is a mapping , denoted by or where . It represents the maximum amount of flow that can pass through an edge.

A flow is a mapping , denoted by or , subject to the following two constraints:
A flow can be visualized as a physical flow of a fluid through the network, following the direction of each edge. The capacity constraint then says that the volume flowing through each edge per unit time is less than or equal to the maximum capacity of the edge, and the conservation constraint says that the amount that flows into each vertex equals the amount flowing out of each vertex, apart from the source and sink vertices. 

The value of a flow is defined by 

where as above is the source node and is the sink node. In the fluid analogy, it represents the amount of fluid entering the network at the source node. Because of the conservation axiom for flows, this is the same as the amount of flow leaving the network at the sink node. 

The maximum flow problem asks for the largest flow on a given network. 

The other half of the max-flow min-cut theorem refers to a different aspect of a network: the collection of cuts. An s-t cut is a partition of such that and . That is, "s"-"t" cut is a division of the vertices of the network into two parts, with the source in one part and the sink in the other. The cut-set formula_4 of a cut is the set of edges that connect the source part of the cut to the sink part: 
Thus, if all the edges in the cut-set of are removed, then no positive flow is possible, because there is no path in the resulting graph from the source to the sink. 

The capacity of an "s-t cut" is the total weight of its edges, 
where formula_7 if formula_8 and formula_9, formula_10 otherwise.

There are typically many cuts in a graph, but cuts with smaller weights are often more difficult to find. 

The main theorem links the maximum flow through a network with the minimum cut of the network.

The max-flow problem and min-cut problem can be formulated as two primal-dual linear programs.

The max-flow LP is straightforward. The dual LP is obtained using the algorithm described in dual linear program. The resulting LP requires some explanation. The interpretation of the variables in the min-cut LP is:

formula_11 

formula_12

The minimization objective sums the capacity over all the edges that are contained in the cut.

The constraints guarantee that the variables indeed represent a legal cut:


Note that, since this is a minimization problem, we do not have to guarantee that an edge is "not" in the cut - we only have to guarantee that each edge that should be in the cut, is summed in the objective function.

The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, "x"*, then the dual program also has an optimal solution, "y"*, such that the optimal values formed by the two solutions are equal.

The figure on the right is a network having a value of flow of 7. The numerical annotation on each arrow, in the form "x"/"y", indicate the actual flow ("x") and the maximum flow capacity ("y"). The flows emanating from the source total seven (4+3=7), as do the flows into the sink (3+4=7). 

The vertex in white and the vertices in grey form the subsets and of an s-t cut, whose cut-set contains the dashed edges. Since the capacity of the s-t cut is 7, which equals to the value of flow, the max-flow min-cut theorem tells us that the value of flow and the capacity of the s-t cut are both optimal in this network.

Note that the flow through each of the dashed edges is at full capacity: this is the 'bottleneck' of the system. By contrast there is spare capacity in the right-hand part of the network. In particular, the flow from node one to node two need not be equal to 1. If there were no flow between nodes one and two, then the inputs to the sink would change to 4/4 and 3/5; the total flow would still be seven (4+3=7). On the other hand, if the flow from node one to node two were doubled to 2, then the inputs to the sink would change to 2/4 and 5/5; the total flow would again remain at seven (2+5=7).

The maximum flow problem can be formulated as the maximization of the electrical current through a network composed of nonlinear resistive elements. In this formulation, the limit of the current between the input terminals of the electrical network as the input voltage approaches formula_20, is equal to the weight of the minimum-weight cut set. 

formula_21
In addition to edge capacity, consider there is capacity at each vertex, that is, a mapping , denoted by , such that the flow has to satisfy not only the capacity constraint and the conservation of flows, but also the vertex capacity constraint

In other words, the amount of "flow" passing through a vertex cannot exceed its capacity. Define an "s-t cut" to be the set of vertices and edges such that for any path from "s" to "t", the path contains a member of the cut. In this case, the "capacity of the cut" is the sum the capacity of each edge and vertex in it.

In this new definition, the generalized max-flow min-cut theorem states that the maximum value of an s-t flow is equal to the minimum capacity of an s-t cut in the new sense.

In the undirected edge-disjoint paths problem, we are given an undirected graph and two vertices and , and we have to find the maximum number of edge-disjoint s-t paths in .

The Menger's theorem states that the maximum number of edge-disjoint s-t paths in an undirected graph is equal to the minimum number of edges in an s-t cut-set.

In the project selection problem, there are projects and machines. Each project yields revenue and each machine costs to purchase. Each project requires a number of machines and each machine can be shared by several projects. The problem is to determine which projects and machines should be selected and purchased respectively, so that the profit is maximized.

Let be the set of projects "not" selected and be the set of machines purchased, then the problem can be formulated as,

Since the first term does not depend on the choice of and , this maximization problem can be formulated as a minimization problem instead, that is,

The above minimization problem can then be formulated as a minimum-cut problem by constructing a network, where the source is connected to the projects with capacity , and the sink is connected by the machines with capacity . An edge with "infinite" capacity is added if project requires machine . The s-t cut-set represents the projects and machines in and respectively. By the max-flow min-cut theorem, one can solve the problem as a maximum flow problem.

The figure on the right gives a network formulation of the following project selection problem:

The minimum capacity of an s-t cut is 250 and the sum of the revenue of each project is 450; therefore the maximum profit "g" is 450 − 250 = 200, by selecting projects and .

The idea here is to 'flow' the project profits through the 'pipes' of the machine. If we cannot fill the pipe, the machine's return is less than its cost, and the min cut algorithm will find it cheaper to cut the project's profit edge instead of the machine's cost edge.

In the image segmentation problem, there are pixels. Each pixel can be assigned a foreground value or a background value . There is a penalty of if pixels are adjacent and have different assignments. The problem is to assign pixels to foreground or background such that the sum of their values minus the penalties is maximum.

Let be the set of pixels assigned to foreground and be the set of points assigned to background, then the problem can be formulated as,

This maximization problem can be formulated as a minimization problem instead, that is,

The above minimization problem can be formulated as a minimum-cut problem by constructing a network where the source (orange node) is connected to all the pixels with capacity , and the sink (purple node) is connected by all the pixels with capacity . Two edges () and () with capacity are added between two adjacent pixels. The s-t cut-set then represents the pixels assigned to the foreground in and pixels assigned to background in .

An account of the discovery of the theorem was given by Ford and Fulkerson in 1962:

"Determining a maximal steady state flow from one point to another in a network subject to capacity limitations on arcs ... was posed to the authors in the spring of 1955 by T.E. Harris, who, in conjunction with General F. S. Ross (Ret.) had formulated a simplified model of railway traffic flow, and pinpointed this particular problem as the central one suggested by the model. It was not long after this until the main result, Theorem 5.1, which we call the max-flow min-cut theorem, was conjectured and established. A number of proofs have since appeared."

Let be a network (directed graph) with and being the source and the sink of respectively.

Consider the flow computed for by Ford–Fulkerson algorithm. In the residual graph obtained for (after the final flow assignment by Ford–Fulkerson algorithm), define two subsets of vertices as follows:

Claim. , where the capacity of an "s-t cut" is defined by 

Now, we know, formula_28 for any subset of vertices, . Therefore, for we need:

To prove the above claim we consider two cases:


Both of the above statements prove that the capacity of cut obtained in the above described manner is equal to the flow obtained in the network. Also, the flow was obtained by Ford-Fulkerson algorithm, so it is the max-flow of the network as well.




</doc>
<doc id="3444072" url="https://en.wikipedia.org/wiki?curid=3444072" title="Push–relabel maximum flow algorithm">
Push–relabel maximum flow algorithm

In mathematical optimization, the push–relabel algorithm (alternatively, preflow–push algorithm) is an algorithm for computing maximum flows in a flow network. The name "push–relabel" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a "preflow" and gradually converts it into a maximum flow by moving flow locally between neighboring nodes using "push" operations under the guidance of an admissible network maintained by "relabel" operations. In comparison, the Ford–Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.

The push–relabel algorithm is considered one of the most efficient maximum flow algorithms. The generic algorithm has a strongly polynomial time complexity, which is asymptotically more efficient than the Edmonds–Karp algorithm. Specific variants of the algorithms achieve even lower time complexities. The variant based on the highest label node selection rule has time complexity and is generally regarded as the benchmark for maximum flow algorithms. Subcubic time complexity can be achieved using dynamic trees, although in practice it is less efficient.

The push–relabel algorithm has been extended to compute minimum cost flows. The idea of distance labels has led to a more efficient augmenting path algorithm, which in turn can be incorporated back into the push–relabel algorithm to create a variant with even higher empirical performance.

The concept of a preflow was originally designed by Alexander V. Karzanov and was published in 1974 in Soviet Mathematical Dokladi 15. This pre-flow algorithm also used a push operation; however, it used distances in the auxiliary network to determine where to push the flow instead of a labeling system.

The push-relabel algorithm was designed by Andrew V. Goldberg and Robert Tarjan. The algorithm was initially presented in November 1986 in STOC '86: Proceedings of the eighteenth annual ACM symposium on Theory of computing, and then officially in October 1988 as an article in the Journal of the ACM. Both papers detail a generic form of the algorithm terminating in along with a sequential implementation, a implementation using dynamic trees, and parallel/distributed implementation. A explained in Goldberg-Tarjan introduced distance labels by incorporating them into the parallel maximum flow algorithm of Yossi Shiloach and Uzi Vishkin.

Let:
and

The push–relabel algorithm uses a nonnegative integer valid labeling function which makes use of "distance labels", or "heights", on nodes to determine which arcs should be selected for the push operation. This labeling function is denoted by . This function must satisfy the following conditions in order to be considered valid:

In the algorithm, the label values of and are fixed. is a lower bound of the unweighted distance from to in   if is reachable from . If has been disconnected from , then is a lower bound of the unweighted distance from to . As a result, if a valid labeling function exists, there are no paths in   because no such paths can be longer than .

An arc   is called admissible if . The admissible network is composed of the set of arcs   that are admissible. The admissible network is acyclic.

The algorithm starts by creating a residual graph, initializing the preflow values to zero and performing a set of saturating push operations on residual arcs exiting the source, . Similarly, the labels are initialized such that the label at the source is the number of nodes in the graph, , and all other nodes are given a label of zero. Once the initialization is complete the algorithm repeatedly performs either the push or relabel operations against active nodes until no applicable operation can be performed.

The push operation applies on an admissible out-arc of an active node in . It moves units of flow from to .

A push operation that causes to reach is called a saturating push since it uses up all the available capacity of the residual arc. Otherwise, all of the excess at the node is pushed across the residual arc. This is called an unsaturating or non-saturating push.

The relabel operation applies on an active node without any admissible out-arcs in . It modifies to be the minimum value such that an admissible out-arc is created. Note that this always increases and never creates a steep arc, which is an arc such that , and .

After a push or relabel operation, remains a valid labeling function with respect to .

For a push operation on an admissible arc , it may add an arc to , where ; it may also remove the arc from , where it effectively removes the constraint .

To see that a relabel operation on node preserves the validity of , notice that this is trivially guaranteed by definition for the out-arcs of "u" in . For the in-arcs of in , the increased can only satisfy the constraints less tightly, not violate them.

The generic push–relabel algorithm is used as a proof of concept only and does not contain implementation details on how to select an active node for the push and relabel operations. This generic version of the algorithm will terminate in .

Since , , and there are no paths longer than in , in order for to satisfy the valid labeling condition must be disconnected from . At initialisation, the algorithm fulfills this requirement by creating a pre-flow that saturates all out-arcs of , after which is trivially valid for all . After initialisation, the algorithm repeatedly executes an applicable push or relabel operation until no such operations apply, at which point the pre-flow has been converted into a maximum flow.

The algorithm maintains the condition that is a valid labeling during its execution. This can be proven true by examining the effects of the push and relabel operations on the label function . The relabel operation increases the label value by the associated minimum plus one which will always satisfy the constraint. The push operation can send flow from to if . This may add to and may delete from . The addition of to will not affect the valid labeling since . The deletion of from removes the corresponding constraint since the valid labeling property only applies to residual arcs in .

If a preflow and a valid labeling for exists then there is no augmenting path from to in the residual graph . This can be proven by contradiction based on inequalities which arise in the labeling function when supposing that an augmenting path does exist. If the algorithm terminates, then all nodes in are not active. This means all have no excess flow, and with no excess the preflow obeys the flow conservation constraint and can be considered a normal flow. This flow is the maximum flow according to the max-flow min-cut theorem since there is no augmenting path from to .

Therefore, the algorithm will return the maximum flow upon termination.

In order to bound the time complexity of the algorithm, we must analyze the number of push and relabel operations which occur within the main loop. The numbers of relabel, saturating push and nonsaturating push operations are analyzed separately.

In the algorithm, the relabel operation can be performed at most times. This is because the labeling value for any node "u" can never decrease, and the maximum label value is at most for all nodes. This means the relabel operation could potentially be performed times for all nodes (i.e. ). This results in a bound of for the relabel operation.

Each saturating push on an admissible arc removes the arc from . For the arc to be reinserted into for another saturating push, must first be relabeled, followed by a push on the arc , then must be relabeled. In the process, increases by at least two. Therefore, there are saturating pushes on , and the total number of saturating pushes is at most . This results in a time bound of for the saturating push operations.

Bounding the number of nonsaturating pushes can be achieved via a potential argument. We use the potential function (i.e. is the sum of the labels of all active nodes). It is obvious that is initially and stays nonnegative throughout the execution of the algorithm. Both relabels and saturating pushes can increase . However, the value of must be equal to 0 at termination since there cannot be any remaining active nodes at the end of the algorithm's execution. This means that over the execution of the algorithm, the nonsaturating pushes must make up the difference of the relabel and saturating push operations in order for to terminate with a value of 0.
The relabel operation can increase by at most . A saturating push on activates if it was inactive before the push, increasing by at most . Hence, the total contribution of all saturating pushes operations to is at most . A nonsaturating push on always deactivates , but it can also activate as in a saturating push. As a result, it decreases by at least . Since relabels and saturating pushes increase , the total number of nonsaturating pushes must make up the difference of . This results in a time bound of for the nonsaturating push operations.

In sum, the algorithm executes relabels, saturating pushes and nonsaturating pushes. Data structures can be designed to pick and execute an applicable operation in time. Therefore, the time complexity of the algorithm is .

The following is a sample execution of the generic push-relabel algorithm, as defined above, on the following simple network flow graph diagram.

In the example, the and values denote the label and excess , respectively, of the node during the execution of the algorithm. Each residual graph in the example only contains the residual arcs with a capacity larger than zero. Each residual graph may contain multiple iterations of the loop.

The example (but with initial flow of 0) can be run here interactively.

While the generic push–relabel algorithm has time complexity, efficient implementations achieve or lower time complexity by enforcing appropriate rules in selecting applicable push and relabel operations. The empirical performance can be further improved by heuristics.

The "current-arc" data structure is a mechanism for visiting the in- and out-neighbors of a node in the flow network in a static circular order. If a singly linked list of neighbors is created for a node, the data structure can be as simple as a pointer into the list that steps through the list and rewinds to the head when it runs off the end.

Based on the "current-arc" data structure, the discharge operation can be defined. A discharge operation applies on an active node and repeatedly pushes flow from the node until it becomes inactive, relabeling it as necessary to create admissible arcs in the process.

Definition of the discharge operation reduces the push–relabel algorithm to repeatedly selecting an active node to discharge. Depending on the selection rule, the algorithm exhibits different time complexities. For the sake of brevity, we ignore and when referring to the nodes in the following discussion.

The FIFO push–relabel algorithm organizes the active nodes into a queue. The initial active nodes can be inserted in arbitrary order. The algorithm always removes the node at the front of the queue for discharging. Whenever an inactive node becomes active, it is appended to the back of the queue.

The algorithm has time complexity.

The relabel-to-front push–relabel algorithm organizes all nodes into a linked list and maintains the invariant that the list is topologically sorted with respect to the admissible network. The algorithm scans the list from front to back and performs a discharge operation on the current node if it is active. If the node is relabeled, it is moved to the front of the list, and the scan is restarted from the front.

The algorithm also has time complexity.

The highest-label push–relabel algorithm organizes all nodes into buckets indexed by their labels. The algorithm always selects an active node with the largest label to discharge.

The algorithm has time complexity. If the lowest-label selection rule is used instead, the time complexity becomes .

Although in the description of the generic push–relabel algorithm above, is set to zero for each node "u" other than and at the beginning, it is preferable to perform a backward breadth-first search from to compute exact labels.

The algorithm is typically separated into two phases. Phase one computes a maximum pre-flow by discharging only active nodes whose labels are below . Phase two converts the maximum preflow into a maximum flow by returning excess flow that cannot reach to . It can be shown that phase two has time complexity regardless of the order of push and relabel operations and is therefore dominated by phase one. Alternatively, it can be implemented using flow decomposition.

Heuristics are crucial to improving the empirical performance of the algorithm. Two commonly used heuristics are the gap heuristic and the global relabeling heuristic. The gap heuristic detects gaps in the labeling function. If there is a label for which there is no node such that , then any node with has been disconnected from and can be relabeled to immediately. The global relabeling heuristic periodically performs backward breadth-first search from in to compute the exact labels of the nodes. Both heuristics skip unhelpful relabel operations, which are a bottleneck of the algorithm and contribute to the ineffectiveness of dynamic trees.

[[Category:Network flow problem]]
[[Category:Graph algorithms]]

</doc>
<doc id="20815865" url="https://en.wikipedia.org/wiki?curid=20815865" title="Closure problem">
Closure problem

In graph theory and combinatorial optimization, a closure of a directed graph is a set of vertices with no outgoing edges.
That is, the graph should have no edges that start within the closure and end outside the closure.
The closure problem is the task of finding the maximum-weight or minimum-weight closure in a vertex-weighted directed graph.
It may be solved in polynomial time using a reduction to the maximum flow problem. It may be used to model various application problems of choosing an optimal subset of tasks to perform, with dependencies between pairs of tasks, one example being in open pit mining.

The maximum-weight closure of a given graph "G" is the same as the complement of the minimum-weight closure on the transpose graph of "G", so the two problems are equivalent in computational complexity.
If two vertices of the graph belong to the same strongly connected component, they must behave the same as each other with respect to all closures: it is not possible for a closure to contain one vertex without containing the other. For this reason, the input graph to a closure problem may be replaced by its condensation, in which every strongly connected component is replaced by a single vertex.
The condensation is always a directed acyclic graph.

As showed,
a maximum-weight closure may be obtained from "G" by solving a maximum flow problem on a graph "H" constructed from "G" by adding to it two additional vertices "s" and "t". For each vertex "v" with positive weight in "G", the augmented graph "H" contains an edge from "s" to "v" with capacity equal to the weight of "v",
and for each vertex "v" with negative weight in "G", the augmented graph "H" contains an edge from "v" to "t" whose capacity is the negation of the weight of "v". All of the edges in "G" are given infinite capacity in "H".

A minimum cut separating "s" from "t" in this graph cannot have any edges of "G" passing in the forward direction across the cut: a cut with such an edge would have infinite capacity and would not be minimum. Therefore, the set of vertices on the same side of the cut as "s" automatically forms a closure "C". The capacity of the cut equals the weight of all positive-weight vertices minus the weight of the vertices in "C", which is minimized when the weight of "C" is maximized. By the max-flow min-cut theorem, a minimum cut, and the optimal closure derived from it, can be found by solving a maximum flow problem.

Alternative algorithms for the maximum closure problem that do not compute flows have also been studied. Their running time is similar to that of the fastest known flow algorithms.

An open pit mine may be modeled as a set of blocks of material which may be removed by mining it once all the blocks directly above it have been removed. A block has a total value, equal to the value of the minerals that can be extracted from it minus the cost of removal and extraction; in some cases, a block has no extraction value but must still be removed to reach other blocks, giving it a negative value.
One may define an acyclic network that has as its vertices the blocks of a mine, with an edge from each block to the blocks above it that must be removed earlier than it. The weight of each vertex in this network is the total value of its block, and the most profitable plan for mining can be determined by finding a maximum weight closure, and then forming a topological ordering of the blocks in this closure.

In military operations, high-value targets such as command centers are frequently protected by layers of defense systems, which may in turn be protected by other systems. In order to reach a target, all of its defenses must be taken down, making it into a secondary target. Each target needs a certain amount of resources to be allocated to it in order to perform a successful attack. The optimal set of targets to attack, to obtain the most value for the resources expended, can be modeled as a closure problem.

The problem of planning a freight delivery system may be modeled by a network in which the vertices represent cities and the (undirected) edges represent potential freight delivery routes between pairs of cities. Each route can achieve a certain profit, but can only be used if freight depots are constructed at both its ends, with a certain cost. The problem of designing a network that maximizes the difference between the profits and the costs can be solved as a closure problem, by subdividing each undirected edge into two directed edges, both directed outwards from the subdivision point. The weight of each subdivision point is a positive number, the profit of the corresponding route, and the weight of each original graph vertex is a negative number, the cost of building a depot in that city. Together with open pit mining, this was one of the original motivating applications for studying the closure problem; it was originally studied in 1970, in two independent papers published in the same issue of the same journal by J. M. W. Rhys and Michel Balinski.

 and describe an application of the closure problem to a version of job shop scheduling in which one is given a collection of tasks to be scheduled to be performed, one at a time. Each task has two numbers associated with it: a weight or priority, and a processing time, the amount of time that it takes to perform that task. In addition the tasks have precedence constraints: certain tasks must be performed before others. These precedence constraints can be described by a directed acyclic graph "G" in which an edge from one task to another indicates that the first task must be performed earlier than the second one. The goal is to choose an ordering that is consistent with these constraints (a topological ordering of "G") that minimizes the total weighted completion time of the tasks.

Although (as Lawler shows) this scheduling problem is NP-complete in general, Sidney describes a decomposition method that can help solve the problem by reducing it to several smaller problems of the same type. In particular, if "S" is a subset of the tasks that (among all subsets) has the largest possible ratio of its total weight to its total processing time, and in addition "S" is minimal among all sets with the same ratio, then there exists an optimal schedule in which all tasks in "S" are performed before all other tasks. As long as "S" is not the whole set of tasks, this partition of the tasks splits the scheduling problem into two smaller problems, one of scheduling "S" and one of scheduling the remaining tasks. Although "S" is a closure (for a graph with reversed edges from "G") the problem of finding "S" is not exactly a maximum weight closure problem, because the value of "S" is a ratio rather than a sum of weights. Nevertheless, Lawler shows that "S" may be found in polynomial time by a binary search algorithm in which each step of the search uses an instance of the closure problem as a subroutine.


</doc>
<doc id="6807932" url="https://en.wikipedia.org/wiki?curid=6807932" title="Minimum-cost flow problem">
Minimum-cost flow problem

The minimum-cost flow problem (MCFP) is an optimization and decision problem to find the cheapest possible way of sending a certain amount of flow through a flow network. A typical application of this problem involves finding the best delivery route from a factory to a warehouse where the road network has some capacity and cost associated. The minimum cost flow problem is one of the most fundamental among all flow and circulation problems because most other such problems can be cast as a minimum cost flow problem and also that it can be solved efficiently using the network simplex algorithm.

A flow network is a directed graph formula_1 with a source vertex formula_2 and a sink vertex formula_3, where each edge formula_4 has capacity formula_5, flow formula_6 and cost formula_7, with most minimum-cost flow algorithms supporting edges with negative costs. The cost of sending this flow along an edge formula_8 is formula_9. The problem requires an amount of flow formula_10 to be sent from source formula_11 to sink formula_12.

The definition of the problem is to minimize the total cost of the flow over all edges:

with the constraints

A variation of this problem is to find a flow which is maximum, but has the lowest cost among the maximum flow solutions. This could be called a minimum-cost maximum-flow problem and is useful for finding minimum cost maximum matchings.

With some solutions, finding the minimum cost maximum flow instead is straightforward. If not, one can find the maximum flow by performing a binary search on formula_10.

A related problem is the minimum cost circulation problem, which can be used for solving minimum cost flow. This is achieved by setting the lower bound on all edges to zero, and then making an extra edge from the sink formula_12 to the source formula_11, with capacity formula_17 and lower bound formula_18, forcing the total flow from formula_11 to formula_12 to also be formula_10.

The problem can be specialized into two other problems：

The minimum cost flow problem can be solved by linear programming, since we optimize a linear function, and all constraints are linear.

Apart from that, many combinatorial algorithms exist, for a comprehensive survey, see . Some of them are generalizations of maximum flow algorithms, others use entirely different approaches.

Well-known fundamental algorithms (they have many variations):


Given a bipartite graph "G" = ("A" ∪ "B", "E"), the goal is to find the maximum cardinality matching in "G" that has minimum cost. Let "w": "E" → "R" be a weight function on the edges of "E". The minimum weight bipartite matching problem or assignment problem is to find a 
perfect matching "M" ⊆ "E" whose total weight is minimized. The idea is to reduce this problem to a network flow problem.

Let "G′" = ("V′" = "A" ∪ "B", "E′" = "E"). Assign the capacity of all the edges in "E′" to 1. Add a source vertex "s" and connect it to all the vertices in "A′" and add a sink 
vertex "t" and connect all vertices inside group "B′" to this vertex. The capacity of all the new edges is 1 and their costs is 0. It is proved that there is minimum weight perfect bipartite matching in "G" if and only if there a minimum cost flow in "G′". 





</doc>
<doc id="24314" url="https://en.wikipedia.org/wiki?curid=24314" title="Planar graph">
Planar graph

In graph theory, a planar graph is a graph that can be embedded in the plane, i.e., it can be drawn on the plane in such a way that its edges intersect only at their endpoints. In other words, it can be drawn in such a way that no edges cross each other. Such a drawing is called a plane graph or planar embedding of the graph. A plane graph can be defined as a planar graph with a mapping from every node to a point on a plane, and from every edge to a plane curve on that plane, such that the extreme points of each curve are the points mapped from its end nodes, and all curves are disjoint except on their extreme points.

Every graph that can be drawn on a plane can be drawn on the sphere as well, and vice versa, by means of stereographic projection.

Plane graphs can be encoded by combinatorial maps.

The equivalence class of topologically equivalent drawings on the sphere is called a planar map. Although a plane graph has an external or unbounded face, none of the faces of a planar map have a particular status.

Planar graphs generalize to graphs drawable on a surface of a given genus. In this terminology, planar graphs have graph genus 0, since the plane (and the sphere) are surfaces of genus 0. See "graph embedding" for other related topics.

The Polish mathematician Kazimierz Kuratowski provided a characterization of planar graphs in terms of forbidden graphs, now known as Kuratowski's theorem:

A subdivision of a graph results from inserting vertices into edges (for example, changing an edge •——• to •—•—•) zero or more times.
Instead of considering subdivisions, Wagner's theorem deals with minors:

A minor of a graph results from taking a subgraph and repeatedly contracting an edge into a vertex, with each neighbor of the original end-vertices becoming a neighbor of the new vertex.

Klaus Wagner asked more generally whether any minor-closed class of graphs is determined by a finite set of "forbidden minors". This is now the Robertson–Seymour theorem, proved in a long series of papers. In the language of this theorem, "K" and "K" are the forbidden minors for the class of finite planar graphs.

In practice, it is difficult to use Kuratowski's criterion to quickly decide whether a given graph is planar. However, there exist fast algorithms for this problem: for a graph with "n" vertices, it is possible to determine in time O("n") (linear time) whether the graph may be planar or not (see planarity testing).

For a simple, connected, planar graph with "v" vertices and "e" edges and "f" faces, the following simple conditions hold for "v" ≥ 3:


In this sense, planar graphs are sparse graphs, in that they have only O("v") edges, asymptotically smaller than the maximum O("v"). The graph "K", for example, has 6 vertices, 9 edges, and no cycles of length 3. Therefore, by Theorem 2, it cannot be planar. These theorems provide necessary conditions for planarity that are not sufficient conditions, and therefore can only be used to prove a graph is not planar, not that it is planar. If both theorem 1 and 2 fail, other methods may be used.


Euler's formula states that if a finite, [[Connectivity (graph theory)|connected]], planar graph is drawn in the plane without any edge intersections, and "v" is the number of vertices, "e" is the number of edges and "f" is the number of faces (regions bounded by edges, including the outer, infinitely large region), then

As an illustration, in the [[butterfly graph]] given above, "v" = 5, "e" = 6 and "f" = 3. 
In general, if the property holds for all planar graphs of "f" faces, any change to the graph that creates an additional face while keeping the graph planar would keep "v" − "e" + "f" an invariant. Since the property holds for all graphs with "f" = 2, by [[mathematical induction]] it holds for all cases. Euler's formula can also be proved as follows: if the graph isn't a [[tree (graph theory)|tree]], then remove an edge which completes a [[cycle (graph theory)|cycle]]. This lowers both "e" and "f" by one, leaving "v" − "e" + "f" constant. Repeat until the remaining graph is a tree; trees have "v" =  "e" + 1 and "f" = 1, yielding "v" − "e" + "f" = 2, i. e., the [[Euler characteristic]] is 2.

In a finite, [[Connectivity (graph theory)|connected]], "[[simple graph|simple]]", planar graph, any face (except possibly the outer one) is bounded by at least three edges and every edge touches at most two faces; using Euler's formula, one can then show that these graphs are "sparse" in the sense that if "v" ≥ 3:

[[File:Dodecahedron schlegel diagram.png|thumb|A [[Schlegel diagram]] of a regular [[dodecahedron]], forming a planar graph from a convex polyhedron.]]
Euler's formula is also valid for [[convex polyhedron|convex polyhedra]]. This is no coincidence: every convex polyhedron can be turned into a connected, simple, planar graph by using the [[Schlegel diagram]] of the polyhedron, a [[perspective projection]] of the polyhedron onto a plane with the center of perspective chosen near the center of one of the polyhedron's faces. Not every planar graph corresponds to a convex polyhedron in this way: the trees do not, for example. [[Steinitz's theorem]] says that the [[polyhedral graph]]s formed from convex polyhedra are precisely the finite [[Connectivity (graph theory)|3-connected]] simple planar graphs. More generally, Euler's formula applies to any polyhedron whose faces are [[simple polygon]]s that form a surface [[homeomorphism|topologically equivalent]] to a sphere, regardless of its convexity.

Connected planar graphs with more than one edge obey the inequality formula_3, because each face has at least three face-edge incidences and each edge contributes exactly two incidences. It follows via algebraic transformations of this inequality with Euler's formula formula_4 that for finite planar graphs the average degree is strictly less than 6. Graphs with higher average degree cannot be planar.

[[File:Circle packing theorem K5 minus edge example.svg|thumb|Example of the circle packing theorem on K, the complete graph on five vertices, minus one edge.]]
We say that two circles drawn in a plane "kiss" (or "[[Osculating circle|osculate]]") whenever they intersect in exactly one point. A "coin graph" is a graph formed by a set of circles, no two of which have overlapping interiors, by making a vertex for each circle and an edge for each pair of circles that kiss. The [[circle packing theorem]], first proved by [[Paul Koebe]] in 1936, states that a graph is planar if and only if it is a coin graph.

This result provides an easy proof of [[Fáry's theorem]], that every planar graph can be embedded in the plane in such a way that its edges are straight [[line segment]]s that do not cross each other. If one places each vertex of the graph at the center of the corresponding circle in a coin graph representation, then the line segments between centers of kissing circles do not cross any of the other edges.

The density formula_5 of a planar graph, or network, is defined as a ratio of the number of edges formula_6 to the number of possible edges in a network with formula_7 nodes, given by a planar graph formula_8, giving formula_9. A completely sparse planar graph has formula_10, alternatively a completely dense planar graph has formula_11

[[File:Goldner-Harary graph.svg|thumb|240px|The [[Goldner–Harary graph]] is maximal planar. All its faces are bounded by three edges.]]
A simple graph is called maximal planar if it is planar but adding any edge (on the given vertex set) would destroy that property. All faces (including the outer one) are then bounded by three edges, explaining the alternative term plane triangulation. The alternative names "triangular graph" or "triangulated graph" have also been used, but are ambiguous, as they more commonly refer to the [[line graph]] of a [[complete graph]] and to the [[chordal graph]]s respectively. Every maximal planar is [[k-vertex-connected graph|3-vertex-connected]].

If a maximal planar graph has "v" vertices with "v" > 2, then it has precisely 3"v" − 6 edges and 2"v" − 4 faces.

[[Apollonian network]]s are the maximal planar graphs formed by repeatedly splitting triangular faces into triples of smaller triangles. Equivalently, they are the planar [[k-tree|3-trees]].

[[Strangulated graph]]s are the graphs in which every [[peripheral cycle]] is a triangle. In a maximal planar graph (or more generally a polyhedral graph) the peripheral cycles are the faces, so maximal planar graphs are strangulated. The strangulated graphs include also the [[chordal graph]]s, and are exactly the graphs that can be formed by [[clique-sum]]s (without deleting edges) of [[complete graph]]s and maximal planar graphs.

[[Outerplanar graph]]s are graphs with an embedding in the plane such that all vertices belong to the unbounded face of the embedding. Every outerplanar graph is planar, but the converse is not true: "K" is planar but not outerplanar. A theorem similar to Kuratowski's states that a finite graph is outerplanar if and only if it does not contain a subdivision of "K" or of "K". The above is a direct corollary of the fact that a graph "G" is outerplanar if the graph formed from "G" by adding a new vertex, with edges connecting it to all the other vertices, is a planar graph.

A 1-outerplanar embedding of a graph is the same as an outerplanar embedding. For "k" > 1 a planar embedding is "k"-outerplanar if removing the vertices on the outer face results in a ("k" − 1)-outerplanar embedding. A graph is "k"-outerplanar if it has a "k"-outerplanar embedding.

A [[Halin graph]] is a graph formed from an undirected plane tree (with no degree-two nodes) by connecting its leaves into a cycle, in the order given by the plane embedding of the tree. Equivalently, it is a [[polyhedral graph]] in which one face is adjacent to all the others. Every Halin graph is planar. Like outerplanar graphs, Halin graphs have low [[treewidth]], making many algorithmic problems on them more easily solved than in unrestricted planar graphs.

An [[apex graph]] is a graph that may be made planar by the removal of one vertex, and a "k"-apex graph is a graph that may be made planar by the removal of at most "k" vertices.

A [[1-planar graph]] is a graph that may be drawn in the plane with at most one simple crossing per edge, and a "k"-planar graph is a graph that may be drawn with at most "k" simple crossings per edge.

A [[map graph]] is a graph formed from a set of finitely many simply-connected interior-disjoint regions in the plane by connecting two regions when they share at least one boundary point. When at most three regions meet at a point, the result is a planar graph, but when four or more regions meet at a point, the result can be nonplanar.

A [[toroidal graph]] is a graph that can be embedded without crossings on the [[torus]]. More generally, the [[genus (mathematics)|genus]] of a graph is the minimum genus of a two-dimensional surface into which the graph may be embedded; planar graphs have genus zero and nonplanar toroidal graphs have genus one.

Any graph may be embedded into three-dimensional space without crossings. However, a three-dimensional analogue of the planar graphs is provided by the [[linkless embedding|linklessly embeddable graphs]], graphs that can be embedded into three-dimensional space in such a way that no two cycles are [[linking number|topologically linked]] with each other. In analogy to Kuratowski's and Wagner's characterizations of the planar graphs as being the graphs that do not contain "K" or "K" as a minor, the linklessly embeddable graphs may be characterized as the graphs that do not contain as a minor any of the seven graphs in the [[Petersen family]]. In analogy to the characterizations of the outerplanar and planar graphs as being the graphs with [[Colin de Verdière graph invariant]] at most two or three, the linklessly embeddable graphs are the graphs that have Colin de Verdière invariant at most four.

An [[Upward planar drawing|upward planar graph]] is a [[directed acyclic graph]] that can be drawn in the plane with its edges as non-crossing curves that are consistently oriented in an upward direction. Not every planar directed acyclic graph is upward planar, and it is NP-complete to test whether a given graph is upward planar.

The [[Asymptotic analysis|asymptotic]] for the number of (labeled) planar graphs on formula_12 vertices is formula_13, where formula_14 and formula_15.

Almost all planar graphs have an exponential number of automorphisms.

The number of unlabeled (non-isomorphic) planar graphs on formula_12 vertices is between formula_17 and formula_18.

The [[Four Color Theorem]] states that every planar graph is 4-[[graph coloring|colorable]] (i.e. 4-partite).

[[Fáry's theorem]] states that every simple planar graph admits an embedding in the plane such that all edges are [[straight line]] segments which don't intersect. A [[universal point set]] is a set of points such that every planar graph with "n" vertices has such an embedding with all vertices in the point set; there exist universal point sets of quadratic size, formed by taking a rectangular subset of the [[integer lattice]]. Every simple outerplanar graph admits an embedding in the plane such that all vertices lie on a fixed circle and all edges are straight line segments that lie inside the disk and don't intersect, so "n"-vertex [[regular polygon]]s are universal for outerplanar graphs.

[[Image:dual graphs.svg|thumb|100px|A planar graph and its [[Dual graph|dual]]]]
Given an embedding "G" of a (not necessarily simple) connected graph in the plane without edge intersections, we construct the [[dual graph]] "G"* as follows: we choose one vertex in each face of "G" (including the outer face) and for each edge "e" in "G" we introduce a new edge in "G"* connecting the two vertices in "G"* corresponding to the two faces in "G" that meet at "e". Furthermore, this edge is drawn so that it crosses "e" exactly once and that no other edge of "G" or "G"* is intersected. Then "G"* is again the embedding of a (not necessarily simple) planar graph; it has as many edges as "G", as many vertices as "G" has faces and as many faces as "G" has vertices. The term "dual" is justified by the fact that "G"** = "G"; here the equality is the equivalence of embeddings on the [[sphere]]. If "G" is the planar graph corresponding to a convex polyhedron, then "G"* is the planar graph corresponding to the dual polyhedron.

Duals are useful because many properties of the dual graph are related in simple ways to properties of the original graph, enabling results to be proven about graphs by examining their dual graphs.

While the dual constructed for a particular embedding is unique (up to [[isomorphism]]), graphs may have different (i.e. non-isomorphic) duals, obtained from different (i.e. non-[[homeomorphic]]) embeddings.
A "Euclidean graph" is a graph in which the vertices represent points in the plane, and the edges are assigned lengths equal to the Euclidean distance between those points; see [[Geometric graph theory]].

A plane graph is said to be "convex" if all of its faces (including the outer face) are [[convex polygon]]s. A planar graph may be drawn convexly if and only if it is a [[Subdivision (graph theory)|subdivision]] of a [[k-vertex-connected graph|3-vertex-connected]] planar graph.

[[Scheinerman's conjecture]] (now a theorem) states that every planar graph can be represented as an [[intersection graph]] of [[line segment]]s in the plane.

The [[planar separator theorem]] states that every "n"-vertex planar graph can be partitioned into two [[Glossary of graph theory#Subgraphs|subgraphs]] of size at most 2"n"/3 by the removal of O() vertices. As a consequence, planar graphs also have [[treewidth]] and [[branch-width]] O().

For two planar graphs with "v" vertices, it is possible to determine in time O("v") whether they are [[graph theory|isomorphic]] or not (see also [[graph isomorphism problem]]).

The [[meshedness coefficient]] of a planar graph normalizes its number of bounded faces (the same as the [[circuit rank]] of the graph, by [[Mac Lane's planarity criterion]]) by dividing it by 2"n" − 5, the maximum possible number of bounded faces in a planar graph with "n" vertices. Thus, it ranges from 0 for trees to 1 for maximal planar graphs.




[[Category:Planar graphs| ]]
[[Category:Graph families]]
[[Category:Intersection classes of graphs]]

</doc>
<doc id="2536864" url="https://en.wikipedia.org/wiki?curid=2536864" title="Dual graph">
Dual graph

In the mathematical discipline of graph theory, the dual graph of a plane graph is a graph that has a vertex for each face of . The dual graph has an edge whenever two faces of are separated from each other by an edge, and a self-loop when the same face appears on both sides of an edge. Thus, each edge of has a corresponding dual edge, whose endpoints are the dual vertices corresponding to the faces on either side of . The definition of the dual depends on the choice of embedding of the graph , so it is a property of plane graphs (graphs that are already embedded in the plane) rather than planar graphs (graphs that may be embedded but for which the embedding is not yet known). For planar graphs generally, there may be multiple dual graphs, depending on the choice of planar embedding of the graph.

Historically, the first form of graph duality to be recognized was the association of the Platonic solids into pairs of dual polyhedra. Graph duality is a topological generalization of the geometric concepts of dual polyhedra and dual tessellations, and is in turn generalized algebraically by the concept of a dual matroid. Variations of planar graph duality include a version of duality for directed graphs, and duality for graphs embedded onto non-planar two-dimensional surfaces.
However, these notions of dual graphs should not be confused with a different notion, the edge-to-vertex dual or line graph of a graph.

The term "dual" is used because the property of being a dual graph is symmetric, meaning that if is a dual of a connected graph , then is a dual of . When discussing the dual of a graph , the graph itself may be referred to as the "primal graph". Many other graph properties and structures may be translated into other natural properties and structures of the dual. For instance, cycles are dual to cuts, spanning trees are dual to the complements of spanning trees, and simple graphs (without parallel edges or self-loops) are dual to 3-edge-connected graphs.

Graph duality can help explain the structure of mazes and of drainage basins. Dual graphs have also been applied in computer vision, computational geometry, mesh generation, and the design of integrated circuits.

The unique planar embedding of a cycle graph divides the plane into only two regions, the inside and outside of the cycle, by the Jordan curve theorem. However, in an -cycle, these two regions are separated from each other by different edges. Therefore, the dual graph of the -cycle is a multigraph with two vertices (dual to the regions), connected to each other by dual edges. Such a graph is called a dipole graph. Conversely, the dual to an -edge dipole graph is an -cycle.

According to Steinitz's theorem, every polyhedral graph (the graph formed by the vertices and edges of a three-dimensional convex polyhedron) must be planar and 3-vertex-connected, and every 3-vertex-connected planar graph comes from a convex polyhedron in this way. Every three-dimensional convex polyhedron has a dual polyhedron; the dual polyhedron has a vertex for every face of the original polyhedron, with two dual vertices adjacent whenever the corresponding two faces share an edge. Whenever two polyhedra are dual, their graphs are also dual. For instance the Platonic solids come in dual pairs, with the octahedron dual to the cube, the dodecahedron dual to the icosahedron, and the tetrahedron dual to itself. Polyhedron duality can also be extended to duality of higher dimensional polytopes, but this extension of geometric duality does not have clear connections to graph-theoretic duality.

A plane graph is said to be self-dual if it is isomorphic to its dual graph. The wheel graphs provide an infinite family of self-dual graphs coming from self-dual polyhedra (the pyramids). However, there also exist self-dual graphs that are not polyhedral, such as the one shown. describe two operations, adhesion and explosion, that can be used to construct a self-dual graph containing a given planar graph; for instance, the self-dual graph shown can be constructed as the adhesion of a tetrahedron with its dual.

It follows from Euler's formula that every self-dual graph with vertices has exactly edges. Every simple self-dual planar graph contains at least four vertices of degree three, and every self-dual embedding has at least four triangular faces.

Many natural and important concepts in graph theory correspond to other equally natural but different concepts in the dual graph. Because the dual of the dual of a connected plane graph is isomorphic to the primal graph, each of these pairings is bidirectional: if concept in a planar graph corresponds to concept in the dual graph, then concept in a planar graph corresponds to concept in the dual.

The dual of a simple graph need not be simple: it may have self-loops (an edge with both endpoints at the same vertex) or multiple edges connecting the same two vertices, as was already evident in the example of dipole multigraphs being dual to cycle graphs. As a special case of the cut-cycle duality discussed below,
the bridges of a planar graph are in one-to-one correspondence with the self-loops of the dual graph. For the same reason, a pair of parallel edges in a dual multigraph (that is, a length-2 cycle) corresponds to a 2-edge cutset in the primal graph (a pair of edges whose deletion disconnects the graph). Therefore, a planar graph is simple if and only if its dual has no 1- or 2-edge cutsets; that is, if it is 3-edge-connected. The simple planar graphs whose duals are simple are exactly the 3-edge-connected simple planar graphs. This class of graphs includes, but is not the same as, the class of 3-vertex-connected simple planar graphs. For instance, the figure showing a self-dual graph is 3-edge-connected (and therefore its dual is simple) but is not 3-vertex-connected.

Because the dual graph depends on a particular embedding, the dual graph of a planar graph is not unique, in the sense that the same planar graph can have non-isomorphic dual graphs. In the picture, the blue graphs are isomorphic but their dual red graphs are not. The upper red dual has a vertex with degree 6 (corresponding to the outer face of the blue graph) while in the lower red graph all degrees are less than 6.

Hassler Whitney showed that if the graph is 3-connected then the embedding, and thus the dual graph, is unique. By Steinitz's theorem, these graphs are exactly the polyhedral graphs, the graphs of convex polyhedra. A planar graph is 3-vertex-connected if and only if its dual graph is 3-vertex-connected. More generally, a planar graph has a unique embedding, and therefore also a unique dual, if and only if it is a subdivision of a 3-vertex-connected planar graph (a graph formed from a 3-vertex-connected planar graph by replacing some of its edges by paths). For some planar graphs that are not 3-vertex-connected, such as the complete bipartite graph , the embedding is not unique, but all embeddings are isomorphic. When this happens, correspondingly, all dual graphs are isomorphic.

Because different embeddings may lead to different dual graphs, testing whether one graph is a dual of another (without already knowing their embeddings) is a nontrivial algorithmic problem. For biconnected graphs, it can be solved in polynomial time by using the SPQR trees of the graphs to construct a canonical form for the equivalence relation of having a shared mutual dual. For instance, the two red graphs in the illustration are equivalent according to this relation. However, for planar graphs that are not biconnected, this relation is not an equivalence relation and the problem of testing mutual duality is NP-complete.

A cutset in an arbitrary connected graph is a subset of edges defined from a partition of the vertices into two subsets, by including an edge in the subset when it has one endpoint on each side of the partition. Removing the edges of a cutset necessarily splits the graph into at least two connected components. A minimal cutset (also called a bond) is a cutset with the property that every proper subset of the cutset is not itself a cut. A minimal cutset of a connected graph necessarily separates its graph into exactly two components, and consists of the set of edges that have one endpoint in each component. A simple cycle is a connected subgraph in which each vertex of the cycle is incident to exactly two edges of the cycle.

In a connected planar graph , every simple cycle of corresponds to a minimal cutset in the dual of , and vice versa. This can be seen as a form of the Jordan curve theorem: each simple cycle separates the faces of into the faces in the interior of the cycle and the faces of the exterior of the cycle, and the duals of the cycle edges are exactly the edges that cross from the interior to the exterior. The girth of any planar graph (the size of its smallest cycle) equals the edge connectivity of its dual graph (the size of its smallest cutset).

This duality extends from individual cutsets and cycles to vector spaces defined from them. The cycle space of a graph is defined as the family of all subgraphs that have even degree at each vertex; it can be viewed as a vector space over the two-element finite field, with the symmetric difference of two sets of edges acting as the vector addition operation in the vector space. Similarly, the cut space of a graph is defined as the family of all cutsets, with vector addition defined in the same way. Then the cycle space of any planar graph and the cut space of its dual graph are isomorphic as vector spaces. Thus, the rank of a planar graph (the dimension of its cut space) equals the cyclotomic number of its dual (the dimension of its cycle space) and vice versa. A cycle basis of a graph is a set of simple cycles that form a basis of the cycle space (every even-degree subgraph can be formed in exactly one way as a symmetric difference of some of these cycles). For edge-weighted planar graphs (with sufficiently general weights that no two cycles have the same weight) the minimum-weight cycle basis of the graph is dual to the Gomory–Hu tree of the dual graph, a collection of nested cuts that together include a minimum cut separating each pair of vertices in the graph. Each cycle in the minimum weight cycle basis has a set of edges that are dual to the edges of one of the cuts in the Gomory–Hu tree. When cycle weights may be tied, the minimum-weight cycle basis may not be unique, but in this case it is still true that the Gomory–Hu tree of the dual graph corresponds to one of the minimum weight cycle bases of the graph.

In directed planar graphs, simple directed cycles are dual to directed cuts (partitions of the vertices into two subsets such that all edges go in one direction, from one subset to the other). Strongly oriented planar graphs (graphs whose underlying undirected graph is connected, and in which every edge belongs to a cycle) are dual to directed acyclic graphs in which no edge belongs to a cycle. To put this another way, the strong orientations of a connected planar graph (assignments of directions to the edges of the graph that result in a strongly connected graph) are dual to acyclic orientations (assignments of directions that produce a directed acyclic graph).

A spanning tree may be defined as a set of edges that, together with all of the vertices of the graph, forms a connected and acyclic subgraph. But, by cut-cycle duality, if a set of edges in a planar graph is acyclic (has no cycles), then the set of edges dual to has no cuts, from which it follows that the complementary set of dual edges (the duals of the edges that are not in ) forms a connected subgraph. Symmetrically, if is connected, then the edges dual to the complement of form an acyclic subgraph. Therefore, when has both properties – it is connected and acyclic – the same is true for the complementary set in the dual graph. That is, each spanning tree of is complementary to a spanning tree of the dual graph, and vice versa. Thus, the edges of any planar graph and its dual can together be partitioned (in multiple different ways) into two spanning trees, one in the primal and one in the dual, that together extend to all the vertices and faces of the graph but never cross each other. In particular, the minimum spanning tree of is complementary to the maximum spanning tree of the dual graph. However, this does not work for shortest path trees, even approximately: there exist planar graphs such that, for every pair of a spanning tree in the graph and a complementary spanning tree in the dual graph, at least one of the two trees has distances that are significantly longer than the distances in its graph.

An example of this type of decomposition into interdigitating trees can be seen in some simple types of mazes, with a single entrance and no disconnected components of its walls. In this case both the maze walls and the space between the walls take the form of a mathematical tree. If the free space of the maze is partitioned into simple cells (such as the squares of a grid) then this system of cells can be viewed as an embedding of a planar graph, in which the tree structure of the walls forms a spanning tree of the graph and the tree structure of the free space forms a spanning tree of the dual graph. Similar pairs of interdigitating trees can also be seen in the tree-shaped pattern of streams and rivers within a drainage basin and the dual tree-shaped pattern of ridgelines separating the streams.

This partition of the edges and their duals into two trees leads to a simple proof of Euler’s formula for planar graphs with vertices, edges, and faces. Any spanning tree and its complementary dual spanning tree partition the edges into two subsets of and edges respectively, and adding the sizes of the two subsets gives the equation
which may be rearranged to form Euler's formula. According to Duncan Sommerville, this proof of Euler's formula is due to K. G. C. Von Staudt’s "Geometrie der Lage" (Nürnberg, 1847).

In nonplanar surface embeddings the set of dual edges complementary to a spanning tree is not a dual spanning tree. Instead this set of edges is the union of a dual spanning tree with a small set of extra edges whose number is determined by the genus of the surface on which the graph is embedded. The extra edges, in combination with paths in the spanning trees, can be used to generate the fundamental group of the surface.

Any counting formula involving vertices and faces that is valid for all planar graphs may be transformed by planar duality into an equivalent formula in which the roles of the vertices and faces have been swapped. Euler's formula, which is self-dual, is one example. Another given by Harary involves the handshaking lemma, according to which the sum of the degrees of the vertices of any graph equals twice the number of edges. In its dual form, this lemma states that in a plane graph, the sum of the numbers of sides of the faces of the graph equals twice the number of edges.

The medial graph of a plane graph is isomorphic to the medial graph of its dual. Two planar graphs can have isomorphic medial graphs only if they are dual to each other.

A planar graph with four or more vertices is maximal (no more edges can be added while preserving planarity) if and only if its dual graph is both 3-vertex-connected and 3-regular.

A connected planar graph is Eulerian (has even degree at every vertex) if and only if its dual graph is bipartite. A Hamiltonian cycle in a planar graph corresponds to a partition of the vertices of the dual graph into two subsets (the interior and exterior of the cycle) whose induced subgraphs are both trees. In particular, Barnette's conjecture on the Hamiltonicity of cubic bipartite polyhedral graphs is equivalent to the conjecture that every Eulerian maximal planar graph can be partitioned into two induced trees.

If a planar graph has Tutte polynomial , then the Tutte polynomial of its dual graph is obtained by swapping and . For this reason, if some particular value of the Tutte polynomial provides information about certain types of structures in , then swapping the arguments to the Tutte polynomial will give the corresponding information for the dual structures. For instance, the number of strong orientations is and the number of acyclic orientations is . For bridgeless planar graphs, graph colorings with colors correspond to nowhere-zero flows modulo  on the dual graph. For instance, the four color theorem (the existence of a 4-coloring for every planar graph) can be expressed equivalently as stating that the dual of every bridgeless planar graph has a nowhere-zero 4-flow. The number of -colorings is counted (up to an easily computed factor) by the Tutte polynomial value and dually the number of nowhere-zero -flows is counted by .

An "st"-planar graph is a connected planar graph together with a bipolar orientation of that graph, an orientation that makes it acyclic with a single source and a single sink, both of which are required to be on the same face as each other. Such a graph may be made into a strongly connected graph by adding one more edge, from the sink back to the source, through the outer face. The dual of this augmented planar graph is itself the augmentation of another "st"-planar graph.

In a directed plane graph, the dual graph may be made directed as well, by orienting each dual edge by a 90° clockwise turn from the corresponding primal edge. Strictly speaking, this construction is not a duality of directed planar graphs, because starting from a graph and taking the dual twice does not return to itself, but instead constructs a graph isomorphic to the transpose graph of , the graph formed from by reversing all of its edges. Taking the dual four times returns to the original graph.

The weak dual of a plane graph is the subgraph of the dual graph whose vertices correspond to the bounded faces of the primal graph. A plane graph is outerplanar if and only if its weak dual is a forest. For any plane graph , let be the plane multigraph formed by adding a single new vertex in the unbounded face of , and connecting to each vertex of the outer face (multiple times, if a vertex appears multiple times on the boundary of the outer face); then, is the weak dual of the (plane) dual of .

The concept of duality applies as well to infinite graphs embedded in the plane as it does to finite graphs. However, care is needed to avoid topological complications such as points of the plane that are neither part of an open region disjoint from the graph nor part of an edge or vertex of the graph. When all faces are bounded regions surrounded by a cycle of the graph, an infinite planar graph embedding can also be viewed as a tessellation of the plane, a covering of the plane by closed disks (the tiles of the tessellation) whose interiors (the faces of the embedding) are disjoint open disks. Planar duality gives rise to the notion of a dual tessellation, a tessellation formed by placing a vertex at the center of each tile and connecting the centers of adjacent tiles.
The concept of a dual tessellation can also be applied to partitions of the plane into finitely many regions. It is closely related to but not quite the same as planar graph duality in this case. For instance, the Voronoi diagram of a finite set of point sites is a partition of the plane into polygons within which one site is closer than any other. The sites on the convex hull of the input give rise to unbounded Voronoi polygons, two of whose sides are infinite rays rather than finite line segments. The dual of this diagram is the Delaunay triangulation of the input, a planar graph that connects two sites by an edge whenever there exists a circle that contains those two sites and no other sites. The edges of the convex hull of the input are also edges of the Delaunay triangulation, but they correspond to rays rather than line segments of the Voronoi diagram. This duality between Voronoi diagrams and Delaunay triangulations can be turned into a duality between finite graphs in either of two ways: by adding an artificial vertex at infinity to the Voronoi diagram, to serve as the other endpoint for all of its rays, or by treating the bounded part of the Voronoi diagram as the weak dual of the Delaunay triangulation. Although the Voronoi diagram and Delaunay triangulation are dual, their embedding in the plane may have additional crossings beyond the crossings of dual pairs of edges. Each vertex of the Delaunay triangle is positioned within its corresponding face of the Voronoi diagram. Each vertex of the Voronoi diagram is positioned at the circumcenter of the corresponding triangle of the Delaunay triangulation, but this point may lie outside its triangle.

The concept of duality can be extended to graph embeddings on two-dimensional manifolds other than the plane. The definition is the same: there is a dual vertex for each connected component of the complement of the graph in the manifold, and a dual edge for each graph edge connecting the two dual vertices on either side of the edge. In most applications of this concept, it is restricted to embeddings with the property that each face is a topological disk; this constraint generalizes the requirement for planar graphs that the graph be connected. With this constraint, the dual of any surface-embedded graph has a natural embedding on the same surface, such that the dual of the dual is isomorphic to and isomorphically embedded to the original graph. For instance, the complete graph is a toroidal graph: it is not planar but can be embedded in a torus, with each face of the embedding being a triangle. This embedding has the Heawood graph as its dual graph.

The same concept works equally well for non-orientable surfaces. For instance, can be embedded in the projective plane with ten triangular faces as the hemi-icosahedron, whose dual is the Petersen graph embedded as the hemi-dodecahedron.

Even planar graphs may have nonplanar embeddings, with duals derived from those embeddings that differ from their planar duals. For instance, the four Petrie polygons of a cube (hexagons formed by removing two opposite vertices of the cube) form the hexagonal faces of an embedding of the cube in a torus. The dual graph of this embedding has four vertices forming a complete graph with doubled edges. In the torus embedding of this dual graph, the six edges incident to each vertex, in cyclic order around that vertex, cycle twice through the three other vertices. In contrast to the situation in the plane, this embedding of the cube and its dual is not unique; the cube graph has several other torus embeddings, with different duals.

Many of the equivalences between primal and dual graph properties of planar graphs fail to generalize to nonplanar duals, or require additional care in their generalization.

Another operation on surface-embedded graphs is the Petrie dual, which uses the Petrie polygons of the embedding as the faces of a new embedding. Unlike the usual dual graph, it has the same vertices as the original graph, but generally lies on a different surface.
Surface duality and Petrie duality are two of the six Wilson operations, and together generate the group of these operations.
An algebraic dual of a connected graph is a graph such that and have the same set of edges, any cycle of is a cut of , and any cut of is a cycle of . Every planar graph has an algebraic dual, which is in general not unique (any dual defined by a plane embedding will do). The converse is actually true, as settled by Hassler Whitney in Whitney's planarity criterion:

The same fact can be expressed in the theory of matroids. If is the graphic matroid of a graph , then a graph is an algebraic dual of if and only if the graphic matroid of is the dual matroid of . Then Whitney's planarity criterion can be rephrased as stating that the dual matroid of a graphic matroid is itself a graphic matroid if and only if the underlying graph of is planar. If is planar, the dual matroid is the graphic matroid of the dual graph of . In particular, all dual graphs, for all the different planar embeddings of , have isomorphic graphic matroids.

For nonplanar surface embeddings, unlike planar duals, the dual graph is not generally an algebraic dual of the primal graph. And for a non-planar graph , the dual matroid of the graphic matroid of is not itself a graphic matroid. However, it is still a matroid whose circuits correspond to the cuts in , and in this sense can be thought of as a generalized algebraic dual of .

The duality between Eulerian and bipartite planar graphs can be extended to binary matroids (which include the graphic matroids derived from planar graphs): a binary matroid is Eulerian if and only if its dual matroid is bipartite. The two dual concepts of girth and edge connectivity are unified in matroid theory by matroid girth: the girth of the graphic matroid of a planar graph is the same as the graph's girth, and the girth of the dual matroid (the graphic matroid of the dual graph) is the edge connectivity of the graph.

Along with its use in graph theory, the duality of planar graphs has applications in several other areas of mathematical and computational study.

In geographic information systems, flow networks (such as the networks showing how water flows in a system of streams and rivers) are dual to cellular networks describing drainage divides. This duality can be explained by modeling the flow network as a spanning tree on a grid graph of an appropriate scale, and modeling the drainage divide as the complementary spanning tree of ridgelines on the dual grid graph.

In computer vision, digital images are partitioned into small square pixels, each of which has its own color. The dual graph of this subdivision into squares has a vertex per pixel and an edge between pairs of pixels that share an edge; it is useful for applications including clustering of pixels into connected regions of similar colors.

In computational geometry, the duality between Voronoi diagrams and Delaunay triangulations implies that any algorithm for constructing a Voronoi diagram can be immediately converted into an algorithm for the Delaunay triangulation, and vice versa. The same duality can also be used in finite element mesh generation. Lloyd's algorithm, a method based on Voronoi diagrams for moving a set of points on a surface to more evenly spaced positions, is commonly used as a way to smooth a finite element mesh described by the dual Delaunay triangulation. This method improves the mesh by making its triangles more uniformly sized and shaped.

In the synthesis of CMOS circuits, the function to be synthesized is represented as a formula in Boolean algebra. Then this formula is translated into two series-parallel multigraphs. These graphs can be interpreted as circuit diagrams in which the edges of the graphs represent transistors, gated by the inputs to the function. One circuit computes the function itself, and the other computes its complement. One of the two circuits is derived by converting the conjunctions and disjunctions of the formula into series and parallel compositions of graphs, respectively. The other circuit reverses this construction, converting the conjunctions and disjunctions of the formula into parallel and series compositions of graphs. These two circuits, augmented by an additional edge connecting the input of each circuit to its output, are planar dual graphs.

The duality of convex polyhedra was recognized by Johannes Kepler in his 1619 book "Harmonices Mundi".
Recognizable planar dual graphs, outside the context of polyhedra, appeared as early as 1725, in Pierre Varignon's posthumously published work, "Nouvelle Méchanique ou Statique". This was even before Leonhard Euler's 1736 work on the Seven Bridges of Königsberg that is often taken to be the first work on graph theory. Varignon analyzed the forces on static systems of struts by drawing a graph dual to the struts, with edge lengths proportional to the forces on the struts; this dual graph is a type of Cremona diagram. In connection with the four color theorem, the dual graphs of maps (subdivisions of the plane into regions) were mentioned by Alfred Kempe in 1879, and extended to maps on non-planar surfaces by in 1891. Duality as an operation on abstract planar graphs was introduced by Hassler Whitney in 1931.


</doc>
<doc id="6805386" url="https://en.wikipedia.org/wiki?curid=6805386" title="Fáry's theorem">
Fáry's theorem

In mathematics, Fáry's theorem states that any simple planar graph can be drawn without crossings so that its edges are straight line segments. That is, the ability to draw graph edges as curves instead of as straight line segments does not allow a larger class of graphs to be drawn. The theorem is named after István Fáry, although it was proved independently by , , and .

One way of proving Fáry's theorem is to use mathematical induction. Let be a simple plane graph with vertices; we may add edges if necessary so that is a maximally plane graph. If < 3, the result is trivial. If ≥ 3, then all faces of must be triangles, as we could add an edge into any face with more sides while preserving planarity, contradicting the assumption of maximal planarity. Choose some three vertices forming a triangular face of . We prove by induction on that there exists a straight-line combinatorially isomorphic re-embedding of in which triangle is the outer face of the embedding. ("Combinatorially isomorphic" means that the vertices, edges, and faces in the new drawing can be made to correspond to those in the old drawing, such that all incidences between edges, vertices, and faces—not just between vertices and edges—are preserved.) As a base case, the result is trivial when and , and are the only vertices in . Otherwise, all vertices in have at least three neighbors. Thus, we may assume that ≥ 4.

By Euler's formula for planar graphs, has edges; equivalently, if one defines the "deficiency" of a vertex in to be , the sum of the deficiencies is . Since has at least four vertices and all faces of are triangles, it follows that every vertex in has degree at least three. Therefore each vertex in has deficiency at most three, so there are at least four vertices with positive deficiency. In particular we can choose a vertex with at most five neighbors that is different from , and . Let be formed by removing from and retriangulating the face formed by removing . By induction, has a combinatorially isomorphic straight line re-embedding in which is the outer face. Because the re-embedding of was combinatorially isomorphic to , removing from it the edges which were added to create leaves the face , which is now a polygon with at most five sides. To complete the drawing to a straight-line combinatorially isomorphic re-embedding of , should be placed in the polygon and joined by straight lines to the vertices of the polygon. By the art gallery theorem, there exists a point interior to at which can be placed so that the edges from to the vertices of do not cross any other edges, completing the proof.

The induction step of this proof is illustrated at right.
De Fraysseix, Pach and Pollack showed how to find in linear time a straight-line drawing in a grid with dimensions linear in the size of the graph, giving a universal point set with quadratic size. A similar method has been followed by Schnyder to prove enhanced bounds and a characterization of planarity based on the incidence partial order. His work stressed the existence of a particular partition of the edges of a maximal planar graph into three trees known as a Schnyder wood.

Tutte's spring theorem states that every 3-connected planar graph can be drawn on a plane without crossings so that its edges are straight line segments and an outside face is a convex polygon (Tutte 1963). It is so called because such an embedding can be found as the equilibrium position for a system of springs representing the edges of the graph.

Steinitz's theorem states that every 3-connected planar graph can be represented as the edges of a convex polyhedron in three-dimensional space. A straight-line embedding of formula_1 of the type described by Tutte's theorem, may be formed by projecting such a polyhedral representation onto the plane.

The Circle packing theorem states that every planar graph may be represented as the intersection graph of a collection of non-crossing circles in the plane. Placing each vertex of the graph at the center of the corresponding circle leads to a straight line representation.
Heiko Harborth raised the question of whether every planar graph has a straight line representation in which all edge lengths are integers. The truth of Harborth's conjecture remains unknown . However, integer-distance straight line embeddings are known to exist for cubic graphs.




</doc>
<doc id="19762817" url="https://en.wikipedia.org/wiki?curid=19762817" title="Steinitz's theorem">
Steinitz's theorem

In polyhedral combinatorics, a branch of mathematics, Steinitz's theorem is a characterization of the undirected graphs formed by the edges and vertices of three-dimensional convex polyhedra: they are exactly the (simple) 3-vertex-connected planar graphs (with at least four 
vertices). That is, every convex polyhedron forms a 3-connected planar graph, and every 3-connected planar graph can be represented as the graph of a convex polyhedron. For this reason, the 3-connected planar graphs are also known as polyhedral graphs. Branko Grünbaum has called this theorem “the most important and deepest known result on 3-polytopes.”

The theorem appears in a 1922 paper of Ernst Steinitz, after whom it is named. It can be proven by mathematical induction (as Steinitz did), by finding the minimum-energy state of a two-dimensional spring system into three dimensions, or by using the circle packing theorem.
Several extensions of the theorem are known, in which the polyhedron that realizes a given graph has additional constraints; for instance, every polyhedral graph is the graph of a convex polyhedron with integer coordinates, or the graph of a convex polyhedron all of whose edges are tangent to a common midsphere.

In higher dimensions, the problem of characterizing the graphs of convex polytopes remains open.

An undirected graph is a system of vertices and edges, each edge connecting two of the vertices. From any polyhedron one can form a graph, by letting the vertices of the graph correspond to the vertices of the polyhedron and by connecting any two graph vertices by an edge whenever the corresponding two polyhedron vertices are the endpoints of an edge of the polyhedron. This graph is known as the skeleton of the polyhedron.

A graph is planar if it can be drawn with its vertices as points in the Euclidean plane, and its edges as curves that connect these points, such that no two edge curves cross each other and such that the point representing a vertex lies on the curve representing an edge only when the vertex is an endpoint of the edge. By Fáry's theorem, it is sufficient to consider only planar drawings in which the curves representing the edges are line segments. A graph is 3-connected if, after the removal of any two of its vertices, any other pair of vertices remain connected by a path.
Steinitz's theorem states that these two conditions are both necessary and sufficient to characterize the skeletons of three-dimensional convex polyhedra: a given graph is the graph of a convex three-dimensional polyhedron, if and only if is planar and 3-vertex-connected.

Steinitz's theorem is named after Ernst Steinitz, who submitted its first proof for publication in 1916.

The name "Steinitz's theorem" has also been applied to other results of Steinitz: 

One direction of Steinitz's theorem (the easier direction to prove) states that the graph of every convex polyhedron is planar and 3-connected. As shown in the illustration, planarity can be shown by using a Schlegel diagram: if one places a light source near one face of the polyhedron, and a plane on the other side, the shadows of the polyhedron edges will form a planar graph, embedded in such a way that the edges are straight line segments. The 3-connectivity of a polyhedral graph is a special case of Balinski's theorem that the graph of any "k"-dimensional convex polytope is "k"-connected.

The other, more difficult, direction of Steinitz's theorem states that every planar 3-connected graph is the graph of a convex polyhedron. There are three standard approaches for this part: proofs by induction, lifting two-dimensional Tutte embeddings into three dimensions using the Maxwell–Cremona correspondence, and methods using the circle packing theorem to generate a canonical polyhedron.

Steinitz' original proof involved finding a sequence of Δ-Y and Y-Δ transforms that reduce any 3-connected planar graph to "K", the graph of the tetrahedron. A Y-Δ transform removes a degree-three vertex from a graph, adding edges between all of its former neighbors if those edges did not already exist; the reverse transformation, a Δ-Y transform, removes the edges of a triangle from a graph and replaces them by a new degree-three vertex adjacent to the same three vertices. Once such a sequence is found, it can be reversed to give a sequence of Δ-Y and Y-Δ transforms that build up the desired polyhedron step by step starting from a polyhedron. Each Y-Δ transform in this sequence can be performed by slicing off a degree-three vertex from a polyhedron. A Δ-Y transform can be performed by removing a triangular face from a polyhedron and extending its neighboring faces until the point where they meet, but only when that triple intersection point of the three neighboring faces is on the correct side of the polyhedron; when the triple intersection point is not on the correct side, a projective transformation of the polyhedron suffices to move it to the correct side. Therefore, by induction on the number of Δ-Y and Y-Δ transforms needed to reduce a given graph to "K", every polyhedral graph can be realized as a polyhedron.

A later work by Epifanov strengthened Steinitz's proof that every polyhedral graph can be reduced to "K" by Δ-Y and Y-Δ transforms. Epifanov proved that if two vertices are specified in a planar graph, then the graph can be reduced to a single edge between those terminals by combining Δ-Y and Y-Δ transforms with series-parallel reductions. Epifanov's proof was complicated and non-constructive, but it was simplified by Truemper using methods based on graph minors. Truemper observed that every grid graph is reducible by Δ-Y and Y-Δ transforms in this way, that this reducibility is preserved by graph minors, and that every planar graph is a minor of a grid graph. This idea can be used to replace Steinitz's lemma that a reduction sequence exists, in a proof of Steinitz's theorem using induction in the same way. However, there exist graphs that require a nonlinear number of steps in any sequence of Δ-Y and Y-Δ transforms. More precisely, "Ω"("n") steps are sometimes necessary, and the best known upper bound on the number of steps is even worse, "O"("n").

An alternative form of induction proof is based on removing edges (and compressing out the degree-two vertices that might be performed by this removal) or contracting edges and forming a minor of the given planar graph. Any polyhedral graph can be reduced to "K" by a linear number of these operations, and again the operations can be reversed and the reversed operations performed geometrically, giving a polyhedral realization of the graph. However, while it is simpler to prove that a reduction sequence exists for this type of argument, and the reduction sequences are shorter, the geometric steps needed to reverse the sequence are more complicated.

If a graph is drawn in the plane with straight line edges, then an equilibrium stress is defined as an assignment of nonzero real numbers (weights) to the edges, with the property that each vertex is in the position given by the weighted sum of its neighbors. According to the Maxwell–Cremona correspondence, an equilibrium stress can be lifted to a piecewise linear continuous three-dimensional surface such that the edges forming the boundaries between the flat parts of the surface project to the given drawing. The weight and length of each edge determines the difference in slopes of the surface on either side of the edge, and the condition that each vertex is in equilibrium with its neighbors is equivalent to the condition that these slope differences cause the surface to meet up with itself correctly in the neighborhood of the vertex. Positive weights translate to convex dihedral angles between two faces of the piecewise linear surface, and negative weights translate to concave dihedral angles. Conversely, every continuous piecewise-linear surface comes from an equilibrium stress in this way. If a finite planar graph is drawn and given an equilibrium stress in such a way that all interior edges of the drawing have positive weights, and all exterior edges have negative weights, then by translating this stress into a three-dimensional surface in this way, and then replacing the flat surface representing the exterior of the graph by its complement in the same plane, one obtains a convex polyhedron, with the additional property that its perpendicular projection onto the plane has no crossings.

The Maxwell–Cremona correspondence has been used to obtain polyhedral realizations of polyhedral graphs by combining it with a planar graph drawing method of W. T. Tutte, the Tutte embedding. Tutte's method begins by fixing one face of a polyhedral graph into convex position in the plane. This face will become the outer face of a drawing of a graph. The method continues by setting up a system of linear equations in the vertex coordinates, according to which each remaining vertex should be placed at the average of its neighbors. Then as Tutte showed, this system of equations will have a unique solution in which each face of the graph is drawn as a convex polygon. The result is almost an equilibrium stress: if one assigns weight one to each interior edge, then each interior vertex of the drawing is in equilibrium. However, it is not always possible to assign negative numbers to the exterior edges so that they, too, are in equilibrium.
Such an assignment is always possible when the outer face is a triangle, and so this method can be used to realize any polyhedral graph that has a triangular face.
If a polyhedral graph does not contain a triangular face, its dual graph does contain a triangle and is also polyhedral, so one can realize the dual in this way and then realize the original graph as the polar polyhedron of the dual realization. It is also possible to realize any polyhedral graph directly by choosing the outer face to be any face with at most five vertices (something that exists in all polyhedral graphs) and choosing more carefully the fixed shape of this face in such a way that the Tutte embedding can be lifted, or by using an incremental method instead of Tutte's method to find a liftable planar drawing that does not have equal weights for all the interior edges.

According to one variant of the circle packing theorem, for every polyhedral graph and its dual graph, there exists a system of circles in the plane or on any sphere,
representing the vertices of both graphs, so that two adjacent vertices in the same graph are represented by tangent circles, a primal and dual vertex that represent a vertex and face that touch each other are represented by orthogonal circles, and all remaining pairs of circles are disjoint. From such a representation on a sphere, one can find a polyhedral realization of the given graph as the intersection of a collection of halfspaces, one for each circle that represents a dual vertex, with the boundary of the halfspace containing the circle. Alternatively and equivalently, one can find the same polyhedron as the convex hull of a collection of points (its vertices), such that the horizon seen when viewing the sphere from any vertex equals the circle that corresponds to that vertex. The sphere becomes the midsphere of the realization: each edge of the polyhedron is tangent to it, at the point where two tangent primal circles and two dual circles orthogonal to the primal circles and tangent to each other all meet.

It is possible to prove a stronger form of Steinitz's theorem, that any polyhedral graph can be realized by a convex polyhedron for which all of the vertex coordinates are integers. For instance,
Steinitz's original induction-based proof can be strengthened in this way. However, the integers that would result from this construction are doubly exponential in the number of vertices of the given polyhedral graph. Writing down numbers of this magnitude in binary notation would require an exponential number of bits.

Subsequent researchers have found lifting-based realization algorithms that use only O("n") bits per vertex. It is also possible to relax the requirement that the coordinates be integers, and assign coordinates in such a way that the "x"-coordinates of the vertices are distinct integers in the range [0,2"n" − 4] and the other two coordinates are real numbers in the range [0,1], so that each edge has length at least one while the overall polyhedron has volume O("n"). Some polyhedral graphs are known to be realizable on grids of only polynomial size; in particular this is true for the pyramids (realizations of wheel graphs), prisms (realizations of prism graphs), and stacked polyhedra (realizations of Apollonian networks).

A Halin graph is a planar graph formed from a planar-embedded tree (with no degree-two vertices) by connecting the leaves of the tree into a cycle. Every Halin graph can be realized by a polyhedron in which this cycle forms a horizontal base face, every other face lies directly above the base face (as in the polyhedra realized through lifting), and every face has the same slope. Equivalently, the straight skeleton of the base face is combinatorially equivalent to the tree from which the Halin graph was formed. The proof of this result uses induction: any rooted tree may reduced to a smaller tree by removing the leaves from an internal node whose children are all leaves, the Halin graph formed from the smaller tree has a realization by the induction hypothesis, and it is possible to modify this realization in order to add any number of leaf children to the tree node whose children were removed.

In any polyhedron that represents a given polyhedral graph "G", the faces of "G" are exactly the cycles in "G" that do not separate "G" into two components: that is, removing a facial cycle from "G" leaves the rest of "G" as a connected subgraph. Thus, the faces are uniquely determined from the graph structure.
Another strengthening of Steinitz's theorem, by Barnette and Grünbaum, states that for any polyhedral graph, any face of the graph, and any convex polygon representing that face, it is possible to find a polyhedral realization of the whole graph that has the specified shape for the designated face. This is related to a theorem of Tutte, that any polyhedral graph can be drawn in the plane with all faces convex and any specified shape for its outer face. However, the planar graph drawings produced by Tutte's method do not necessarily lift to convex polyhedra. Instead, Barnette and Grünbaum prove this result using an inductive method It is also always possible, given a polyhedral graph "G" and an arbitrary cycle "C", to find a realization such that "C" forms the silhouette of the realization under parallel projection.

The Koebe–Andreev–Thurston circle packing theorem can be interpreted as providing another strengthening of Steinitz's theorem, that every 3-connected planar graph may be represented as a convex polyhedron in such a way that all of its edges are tangent to the same unit sphere. By performing a carefully chosen Möbius transformation of a circle packing before transforming it into a polyhedron, it is possible to find a polyhedral realization that realizes all the symmetries of the underlying graph, in the sense that every graph automorphism is a symmetry of the polyhedral realization. More generally, if "G" is a polyhedral graph and "K" is any smooth three-dimensional convex body, it is possible to find a polyhedral representation of "G" in which all edges are tangent to "K".

Circle packing methods can also be used to characterize the graphs of polyhedra that have a circumsphere or insphere. The characterization involves edge weights, constrained by systems of linear inequalities. These weights correspond to the angles made by adjacent circles in a system of circles, made by the intersections of the faces of the polyhedron with their circumsphere or the horizons of the vertices of the polyhedron on its insphere.

In any dimension higher than three, the algorithmic Steinitz problem (given a lattice, determine whether it is the face lattice of a convex polytope) is complete for the existential theory of the reals by Richter-Gebert's universality theorem. However, because a given graph may correspond to more than one face lattice, it is difficult to extend this completeness result to the problem of recognizing the graphs of 4-polytopes, and this problem's complexity remains open.

Researchers have also found graph-theoretic characterizations of the graphs of certain special classes of three-dimensional non-convex polyhedra and four-dimensional convex polytopes. However, in both cases, the general problem remains unsolved. Indeed, even the problem of determining which complete graphs are the graphs of non-convex polyhedra (other than "K" for the tetrahedron and "K" for the Császár polyhedron) remains unsolved.

László Lovász has shown a correspondence between polyhedral representations of graphs and matrices realizing the Colin de Verdière graph invariants of the same graphs.


</doc>
<doc id="15972636" url="https://en.wikipedia.org/wiki?curid=15972636" title="Planarity testing">
Planarity testing

In graph theory, the planarity testing problem is the algorithmic problem of testing whether a given graph is a planar graph (that is, whether it can be drawn in the plane without edge intersections). This is a well-studied problem in computer science for which many practical algorithms have emerged, many taking advantage of novel data structures. Most of these methods operate in O("n") time (linear time), where "n" is the number of edges (or vertices) in the graph, which is asymptotically optimal. Rather than just being a single Boolean value, the output of a planarity testing algorithm may be a planar graph embedding, if the graph is planar, or an obstacle to planarity such as a Kuratowski subgraph if it is not.

Planarity testing algorithms typically take advantage of theorems in graph theory that characterize the set of planar graphs in terms that are independent of graph drawings.
These include

The Fraysseix–Rosenstiehl planarity criterion can be used directly as part of algorithms for planarity testing, while Kuratowski's and Wagner's theorems have indirect applications: if an algorithm can find a copy of "K" or "K" within a given graph, it can be sure that the input graph is not planar and return without additional computation.

Other planarity criteria, that characterize planar graphs mathematically but are less central to planarity testing algorithms, include Whitney's planarity criterion that a graph is planar if and only if its graphic matroid is also cographic, Mac Lane's planarity criterion characterizing planar graphs by the bases of their cycle spaces, Schnyder's theorem characterizing planar graphs by the order dimension of an associated partial order, and Colin de Verdière's planarity criterion using spectral graph theory.

The classic "path addition" method of Hopcroft and Tarjan was the first published linear-time planarity testing algorithm in 1974. An implementation of Hopcroft and Tarjan's algorithm is provided in the Library of Efficient Data types and Algorithms by Mehlhorn, Mutzel and Näher 

. In 2012, Taylor extended this algorithm to generate all permutations of cyclic edge-order for planar embeddings of biconnected components.

Vertex addition methods work by maintaining a data structure representing the possible embeddings of an induced subgraph of the given graph, and adding vertices one at a time to this data structure. These methods began with an inefficient O("n") method conceived by Lempel, Even and Cederbaum in 1967. It was improved by Even and Tarjan, who found a linear-time solution for the "s","t"-numbering step, and by Booth and Lueker, who developed the PQ tree data structure. With these improvements it is linear-time and outperforms the path addition method in practice. This method was also extended to allow a planar embedding (drawing) to be efficiently computed for a planar graph. In 1999, Shih and Hsu simplified these methods using the PC tree (an unrooted variant of the PQ tree) and a postorder traversal of the depth-first search tree of the vertices.

In 2004, John Boyer and Wendy Myrvold developed a simplified O("n") algorithm, originally inspired by the PQ tree method, which gets rid of the PQ tree and uses edge additions to compute a planar embedding, if possible. Otherwise, a Kuratowski subdivision (of either "K" or "K") is computed. This is one of the two current state-of-the-art algorithms today (the other one is the planarity testing algorithm of de Fraysseix, de Mendez and Rosenstiehl). See for an experimental comparison with a preliminary version of the Boyer and Myrvold planarity test. Furthermore, the Boyer–Myrvold test was extended to extract multiple Kuratowski subdivisions of a non-planar input graph in a running time linearly dependent on the output size. The source code for the planarity test and the extraction of multiple Kuratowski subdivisions is publicly available. Algorithms that locate a Kuratowski subgraph in linear time in vertices were developed by Williamson in the 1980s.

A different method uses an inductive construction of 3-connected graphs to incrementally build planar embeddings of every 3-connected component of "G" (and hence a planar embedding of "G" itself). The construction starts with "K" and is defined in such a way that every intermediate graph on the way to the full component is again 3-connected. Since such graphs have a unique embedding (up to flipping and the choice of the external face), the next bigger graph, if still planar, must be a refinement of the former graph. This allows to reduce the planarity test to just testing for each step whether the next added edge has both ends in the external face of the current embedding. While this is conceptually very simple (and gives linear running time), the method itself suffers from the complexity of finding the construction sequence.


</doc>
<doc id="3125155" url="https://en.wikipedia.org/wiki?curid=3125155" title="Left-right planarity test">
Left-right planarity test

In graph theory, a branch of mathematics, the left-right planarity test
or de Fraysseix–Rosenstiehl planarity criterion is a characterization of planar graphs based on the properties of the depth-first search trees, published by and used by them with Patrice Ossona de Mendez to develop a linear time planarity testing algorithm. In a 2003 experimental comparison of six planarity testing algorithms, this was one of the fastest algorithms tested.

For any depth-first search of a graph "G", the edges
encountered when discovering a vertex for the first time define a depth-first search tree "T" of "G". This is a Trémaux tree, meaning that the remaining edges (the cotree) each connect a pair of vertices that are related to each other as an ancestor and descendant in "T". Three types of patterns can be used to define two relations between pairs of cotree edges, named the T"-alike and T"-opposite relations.

In the following figures, simple circle nodes represent vertices, double circle nodes represent subtrees, twisted segments represent tree paths, and curved arcs represent cotree edges. The root of each tree is shown at the bottom of the figure. In the first figure, the edges labeled formula_1 and formula_2 are "T"-alike, meaning that, at the endpoints nearest the root of the tree, they will both be on the same side of the tree in every planar drawing. In the next two figures, the edges with the same labels are "T"-opposite, meaning that they will be on different sides of the tree in every planar drawing.

Let "G" be a graph and let "T" be a Trémaux tree of "G". The graph "G" is planar if and only if there exists a partition of the cotree edges of "G" into two classes so that any two edges belong to a same class if they are "T"-alike and any two edges belong to different classes if they are "T"-opposite.

This characterization immediately leads to an (inefficient) planarity test: determine for all pairs of edges whether they are "T"-alike or "T"-opposite, form an auxiliary graph that has a vertex for each
connected component of "T"-alike edges and an edge for each pair of "T"-opposite edges, and check whether this auxiliary graph is bipartite. Making this algorithm efficient involves finding a subset of the "T"-alike and "T"-opposite pairs that is sufficient to carry out this method without determining the relation between all edge pairs in the input graph.


</doc>
<doc id="325813" url="https://en.wikipedia.org/wiki?curid=325813" title="Graph drawing">
Graph drawing

Graph drawing is an area of mathematics and computer science combining methods from geometric graph theory and information visualization to derive two-dimensional depictions of graphs arising from applications such as social network analysis, cartography, linguistics, and bioinformatics.

A drawing of a graph or network diagram is a pictorial representation of the vertices and edges of a graph. This drawing should not be confused with the graph itself: very different layouts can correspond to the same graph. In the abstract, all that matters is which pairs of vertices are connected by edges. In the concrete, however, the arrangement of these vertices and edges within a drawing affects its understandability, usability, fabrication cost, and aesthetics. The problem gets worse if the graph changes over time by adding and deleting edges (dynamic graph drawing) and the goal is to preserve the user's mental map.

Graphs are frequently drawn as node–link diagrams in which the vertices are represented as disks, boxes, or textual labels and the edges are represented as line segments, polylines, or curves in the Euclidean plane. Node–link diagrams can be traced back to the 13th century work of Ramon Llull, who drew diagrams of this type for complete graphs in order to analyze all pairwise combinations among sets of metaphysical concepts.

In the case of directed graphs, arrowheads form a commonly used graphical convention to show their orientation; however, user studies have shown that other conventions such as tapering provide this information more effectively. Upward planar drawing uses the convention that every edge is oriented from a lower vertex to a higher vertex, making arrowheads unnecessary.

Alternative conventions to node–link diagrams include adjacency representations such as circle packings, in which vertices are represented by disjoint regions in the plane and edges are represented by adjacencies between regions; intersection representations in which vertices are represented by non-disjoint geometric objects and edges are represented by their intersections; visibility representations in which vertices are represented by regions in the plane and edges are represented by regions that have an unobstructed line of sight to each other; confluent drawings, in which edges are represented as smooth curves within mathematical train tracks; fabrics, in which nodes are represented as horizontal lines and edges as vertical lines; and visualizations of the adjacency matrix of the graph.

Many different quality measures have been defined for graph drawings, in an attempt to find objective means of evaluating their aesthetics and usability. In addition to guiding the choice between different layout methods for the same graph, some layout methods attempt to directly optimize these measures.


There are many different graph layout strategies:


Graphs and graph drawings arising in other areas of application include

In addition, the placement and routing steps of electronic design automation (EDA) are similar in many ways to graph drawing, as is the problem of greedy embedding in distributed computing, and the graph drawing literature includes several results borrowed from the EDA literature. However, these problems also differ in several important ways: for instance, in EDA, area minimization and signal length are more important than aesthetics, and the routing problem in EDA may have more than two terminals per net while the analogous problem in graph drawing generally only involves pairs of vertices for each edge.

Software, systems, and providers of systems for drawing graphs include:




</doc>
<doc id="710331" url="https://en.wikipedia.org/wiki?curid=710331" title="Force-directed graph drawing">
Force-directed graph drawing

Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically-pleasing way. Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space so that all the edges are of more or less equal length and there are as few crossing edges as possible, by assigning forces among the set of edges and the set of nodes, based on their relative positions, and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.

While graph drawing can be a difficult problem, force-directed algorithms, being physical simulations, usually require no special knowledge about graph theory such as planarity.

Force-directed graph drawing algorithms assign forces among the set of edges and the set of nodes of a graph drawing. Typically, spring-like attractive forces based on Hooke's law are used to attract pairs of endpoints of the graph's edges towards each other, while simultaneously repulsive forces like those of electrically charged particles based on Coulomb's law are used to separate all pairs of nodes. In equilibrium states for this system of forces, the edges tend to have uniform length (because of the spring forces), and nodes that are not connected by an edge tend to be drawn further apart (because of the electrical repulsion). Edge attraction and vertex repulsion forces may be defined using functions that are not based on the physical behavior of springs and particles; for instance, some force-directed systems use springs whose attractive force is logarithmic rather than linear.

An alternative model considers a spring-like force for every pair of nodes formula_1 where the ideal length formula_2 of each spring is proportional to the graph-theoretic distance between nodes "i" and "j", without using a separate repulsive force. Minimizing the difference (usually the squared difference) between Euclidean and ideal distances between nodes is then equivalent to a metric multidimensional scaling problem.

A force-directed graph can involve forces other than mechanical springs and electrical repulsion. A force analogous to gravity may be used to pull vertices towards a fixed point of the drawing space; this may be used to pull together different connected components of a disconnected graph, which would otherwise tend to fly apart from each other because of the repulsive forces, and to draw nodes with greater centrality to more central positions in the drawing; it may also affect the vertex spacing within a single component. Analogues of magnetic fields may be used for directed graphs. Repulsive forces may be placed on edges as well as on nodes in order to avoid overlap or near-overlap in the final drawing. In drawings with curved edges such as circular arcs or spline curves, forces may also be placed on the control points of these curves, for instance to improve their angular resolution.

Once the forces on the nodes and edges of a graph have been defined, the behavior of the entire graph under these sources may then be simulated as if it were a physical system. In such a simulation, the forces are applied to the nodes, pulling them closer together or pushing them further apart. This is repeated iteratively until the system comes to a mechanical equilibrium state; i.e., their relative positions do not change anymore from one iteration to the next. The positions of the nodes in this equilibrium are used to generate a drawing of the graph.

For forces defined from springs whose ideal length is proportional to the graph-theoretic distance, stress majorization gives a very well-behaved (i.e., monotonically convergent) and mathematically elegant way to minimise these differences and, hence, find a good layout for the graph.

It is also possible to employ mechanisms that search more directly for energy minima, either instead of or in conjunction with physical simulation. Such mechanisms, which are examples of general global optimization methods, include simulated annealing and genetic algorithms.

The following are among the most important advantages of force-directed algorithms:

The main disadvantages of force-directed algorithms include the following:


Force-directed methods in graph drawing date back to the work of , who showed that polyhedral graphs may be drawn in the plane with all faces convex by fixing the vertices of the outer face of a planar embedding of the graph into convex position, placing a spring-like attractive force on each edge, and letting the system settle into an equilibrium. Because of the simple nature of the forces in this case, the system cannot get stuck in local minima, but rather converges to a unique global optimum configuration. Because of this work, embeddings of planar graphs with convex faces are sometimes called Tutte embeddings.

The combination of attractive forces on adjacent vertices, and repulsive forces on all vertices, was first used by ; additional pioneering work on this type of force-directed layout was done by . The idea of using only spring forces between all pairs of vertices, with ideal spring lengths equal to the vertices' graph-theoretic distance, is from .





</doc>
<doc id="33211287" url="https://en.wikipedia.org/wiki?curid=33211287" title="Layered graph drawing">
Layered graph drawing

Layered graph drawing or hierarchical graph drawing is a type of graph drawing in which the vertices of a directed graph are drawn in horizontal rows or layers with the edges generally directed downwards. It is also known as Sugiyama-style graph drawing after Kozo Sugiyama, who first developed this drawing style.

The ideal form for a layered drawing would be an upward planar drawing, in which all edges are oriented in a consistent direction and no pairs of edges cross. However, graphs often contain cycles, minimizing the number of inconsistently-oriented edges is NP-hard, and minimizing the number of crossings is also NP-hard, so layered graph drawing systems typically apply a sequence of heuristics that reduce these types of flaws in the drawing without guaranteeing to find a drawing with the minimum number of flaws.

The construction of a layered graph drawing proceeds in a sequences of steps:

In its simplest form, layered graph drawing algorithms may require O("mn") time in graphs with "n" vertices and "m" edges, because of the large number of dummy vertices that may be created. However, for some variants of the algorithm, it is possible to simulate the effect of the dummy vertices without actually constructing them explicitly, leading to a near-linear time implementation.

The "dot" tool in Graphviz produces layered drawings. A layered graph drawing algorithm is also included in Microsoft Automatic Graph Layout and in Tulip.

Although typically drawn with vertices in rows and edges proceeding from top to bottom, layered graph drawing algorithms may instead be drawn with vertices in columns and edges proceeding from left to right. The same algorithmic framework has also been applied to radial layouts in which the graphs are arranged in concentric circles around some starting node and to three-dimensional layered drawings of graphs.

In layered graph drawings with many long edges, edge clutter may be reduced by grouping sets of edges into bundles and routing them together through the same set of dummy vertices. Similarly, for drawings with many edges crossing between pairs of consecutive layers, the edges in maximal bipartite subgraphs may be grouped into confluent bundles.

Drawings in which the vertices are arranged in layers may be constructed by algorithms that do not follow Sugiyama's framework. For instance, it is possible to tell whether an undirected graph has a drawing with at most "k" crossings, using "h" layers, in an amount of time that is polynomial for any fixed choice of "k" and "h", using the fact that the graphs that have drawings of this type have bounded pathwidth.

For layered drawings of concept lattices, a hybrid approach combining Sugiyama's framework with additive methods (in which each vertex represents a set and the position of the vertex is a sum of vectors representing elements in the set) may be used. In this hybrid approach, the vertex permutation and coordinate assignment phases of the algorithm are replaced by a single phase in which the horizontal position of each vertex is chosen as a sum of scalars representing the elements for that vertex.
Layered graph drawing methods have also been used to provide an initial placement for force-directed graph drawing algorithms.


</doc>
<doc id="40148474" url="https://en.wikipedia.org/wiki?curid=40148474" title="Upward planar drawing">
Upward planar drawing

In graph drawing, an upward planar drawing of a directed acyclic graph is an embedding of the graph into the Euclidean plane, in which the edges are represented as non-crossing monotonic upwards curves. That is, the curve representing each edge should have the property that every horizontal line intersects it in at most one point, and no two edges may intersect except at a shared endpoint. In this sense, it is the ideal case for layered graph drawing, a style of graph drawing in which edges are monotonic curves that may cross, but in which crossings are to be minimized.

A directed acyclic graph must be planar in order to have an upward planar drawing, but not every planar acyclic graph has such a drawing. Among the planar directed acyclic graphs with a single source (vertex with no incoming edges) and sink (vertex with no outgoing edges), the graphs with upward planar drawings are the "st"-planar graphs, planar graphs in which the source and sink both belong to the same face of at least one of the planar embeddings of the graph. More generally, a graph "G" has an upward planar drawing if and only if it is directed and acyclic, and is a subgraph of an "st"-planar graph on the same vertex set.

In an upward embedding, the sets of incoming and outgoing edges incident to each vertex are contiguous in the cyclic ordering of the edges at the vertex. A planar embedding of a given directed acyclic graph is said to be "bimodal" when it has this property. Additionally, the angle between two consecutive edges with the same orientation at a given vertex may be labeled as "small" if it is less than π, or "large" if it is greater than π. Each source or sink must have exactly one large angle, and each vertex that is neither a source nor a sink must have none. Additionally, each internal face of the drawing must have two more small angles than large ones, and the external face must have two more large angles than small ones. A "consistent assignment" is a labeling of the angles that satisfies these properties; every upward embedding has a consistent assignment. Conversely, every directed acyclic graph that has a bimodal planar embedding with a consistent assignment has an upward planar drawing, that can be constructed from it in linear time.

Another characterization is possible for graphs with a single source. In this case an upward planar embedding must have the source on the outer face, and every undirected cycle of the graph must have at least one vertex at which both cycle edges are incoming (for instance, the vertex with the highest placement in the drawing). Conversely, if an embedding has both of these properties, then it is equivalent to an upward embedding.

Several special cases of upward planarity testing are known to be possible in polynomial time:
However, it is NP-complete to determine whether a planar directed acyclic graph with multiple sources and sinks has an upward planar drawing.

Fáry's theorem states that every planar graph has a drawing in which its edges are represented by straight line segments, and the same is true of upward planar drawing: every upward planar graph has a straight upward planar drawing.
A straight-line upward drawing of a transitively reduced "st"-planar graph may be obtained by the technique of dominance drawing, with all vertices having integer coordinates within an "n" × "n" grid. However, certain other upward planar graphs may require exponential area in all of their straight-line upward planar drawings. If a choice of embedding is fixed, even oriented series parallel graphs and oriented trees may require exponential area.

Upward planar drawings are particularly important for Hasse diagrams of partially ordered sets, as these diagrams are typically required to be drawn upwardly. In graph-theoretic terms, these correspond to the transitively reduced directed acyclic graphs; such a graph can be formed from the covering relation of a partial order, and the partial order itself forms the reachability relation in the graph. If a partially ordered set has one minimal element, has one maximal element, and has an upward planar drawing, then it must necessarily form a lattice, a set in which every pair of elements has a unique greatest lower bound and a unique least upper bound. The Hasse diagram of a lattice is planar if and only if its order dimension is at most two. However, some partial orders of dimension two and with one minimal and maximal element do not have an upward planar drawing (take the order defined by the transitive closure of formula_1).




</doc>
<doc id="8149170" url="https://en.wikipedia.org/wiki?curid=8149170" title="Graph embedding">
Graph embedding

In topological graph theory, an embedding (also spelled imbedding) of a graph formula_1 on a surface formula_2 is a representation of formula_1 on formula_2 in which points of formula_2 are associated with vertices and simple arcs (homeomorphic images of formula_6) are associated with edges in such a way that:

Here a surface is a compact, connected formula_9-manifold.

Informally, an embedding of a graph into a surface is a drawing of the graph on the surface in such a way that its edges may intersect only at their endpoints. It is well known that any finite graph can be embedded in 3-dimensional Euclidean space formula_10. A planar graph is one that can be embedded in 2-dimensional Euclidean space formula_11

Often, an embedding is regarded as an equivalence class (under homeomorphisms of formula_2) of representations of the kind just described.

Some authors define a weaker version of the definition of "graph embedding" by omitting the non-intersection condition for edges. In such contexts the stricter definition is described as "non-crossing graph embedding".

This article deals only with the strict definition of graph embedding. The weaker definition is discussed in the articles "graph drawing" and "crossing number".

If a graph formula_1 is embedded on a closed surface formula_2, the complement of the union of the points and arcs associated with
the vertices and edges of formula_1 is a family of regions (or faces). A 2-cell embedding, cellular embedding or map is an embedding in which every face is homeomorphic to an open disk. A closed 2-cell embedding is an embedding in which the closure of every face is homeomorphic to a closed disk.

The genus of a graph is the minimal integer formula_16 such that the graph can be embedded in a surface of genus formula_16. In particular, a planar graph has genus formula_18, because it can be drawn on a sphere without self-crossing. The non-orientable genus of a graph is the minimal integer formula_16 such that the graph can be embedded in a non-orientable surface of (non-orientable) genus formula_16.

The Euler genus of a graph is the minimal integer formula_16 such that the graph can be embedded in an orientable surface of (orientable) genus formula_22 or in a non-orientable surface of (non-orientable) genus formula_16. A graph is orientably simple if its Euler genus is smaller than its non-orientable genus.

The maximum genus of a graph is the maximal integer formula_16 such that the graph can be formula_9-cell embedded in an orientable surface of genus formula_16.

An embedded graph uniquely defines cyclic orders of edges incident to the same vertex. The set of all these cyclic orders is called a rotation system. Embeddings with the same rotation system are considered to be equivalent and the corresponding equivalence class of embeddings is called combinatorial embedding (as opposed to the term topological embedding, which refers to the previous definition in terms of points and curves). Sometimes, the rotation system itself is called a "combinatorial embedding".

An embedded graph also defines natural cyclic orders of edges which constitutes the boundaries of the faces of the embedding. However handling these face-based orders is less straightforward, since in some cases some edges may be traversed twice along a face boundary. For example this is always the case for embeddings of trees, which have a single face. To overcome this combinatorial nuisance, one may consider that every edge is "split" lengthwise in two "half-edges", or "sides". Under this convention in all face boundary traversals each half-edge is traversed only once and the two half-edges of the same edge are always traversed in opposite directions.

Other equivalent representations for cellular embeddings include the ribbon graph, a topological space formed by gluing together topological disks for the vertices and edges of an embedded graph, and the graph-encoded map, an edge-colored cubic graph with four vertices for each edge of the embedded graph.

The problem of finding the graph genus is NP-hard (the problem of determining whether an formula_16-vertex graph has genus formula_28 is NP-complete).

At the same time, the graph genus problem is fixed-parameter tractable, i.e., polynomial time algorithms are known to check whether a graph can be embedded into a surface of a given fixed genus as well as to find the embedding.

The first breakthrough in this respect happened in 1979, when algorithms of time complexity
"O"("n") were independently submitted to the Annual ACM Symposium on Theory of Computing: one by I. Filotti and G.L. Miller and another one by John Reif. Their approaches were quite different, but upon the suggestion of the program committee they presented a joint paper. However, Wendy Myrvold and William Kocay proved in 2011 that the algorithm given by Filotti, Miller and Reif was incorrect.

In 1999 it was reported that the fixed-genus case can be solved in time linear in the graph size and doubly exponential in the genus.

It is known that any finite graph can be embedded into a three-dimensional space.

One method for doing this is to place the points on any line in space and to draw the edges as curves each of which lies in a distinct halfplane, with all halfplanes having that line as their common boundary. An embedding like this in which the edges are drawn on halfplanes is called a book embedding of the graph. This metaphor comes from imagining that each of the planes where an edge is drawn is like a page of a book. It was observed that in fact several edges may be drawn in the same "page"; the "book thickness" of the graph is the minimum number of halfplanes needed for such a drawing.

Alternatively, any finite graph can be drawn with straight-line edges in three dimensions without crossings by placing its vertices in general position so that no four are coplanar. For instance, this may be achieved by placing the "i"th vertex at the point ("i","i","i") of the moment curve.

An embedding of a graph into three-dimensional space in which no two of the cycles are topologically linked is called a linkless embedding. A graph has a linkless embedding if and only if it does not have one of the seven graphs of the Petersen family as a minor.



</doc>
<doc id="363423" url="https://en.wikipedia.org/wiki?curid=363423" title="Interval graph">
Interval graph

In graph theory, an interval graph is an undirected graph formed from a set of intervals on the real line,
with a vertex for each interval and an edge between vertices whose intervals intersect. It is the intersection graph of the intervals.

Interval graphs are chordal graphs and perfect graphs. They can be recognized in linear time, and an optimal graph coloring or maximum clique in these graphs can be found in linear time. The interval graphs include all proper interval graphs, graphs defined in the same way from a set of unit intervals.

These graphs have been used to model food webs, and to study scheduling problems in which one must select a subset of tasks to be performed at non-overlapping times.
Other applications include assembling contiguous subsequences in DNA mapping, and temporal reasoning.

An interval graph is an undirected graph "G" formed from a family of intervals
by creating one vertex "v" for each interval "S", and connecting two vertices "v" and "v" by an edge whenever the corresponding two sets have a nonempty intersection, that is, the edge set of "G" is


</doc>
<doc id="744165" url="https://en.wikipedia.org/wiki?curid=744165" title="Chordal graph">
Chordal graph

In the mathematical area of graph theory, a chordal graph is one in which all cycles of four or more vertices have a "chord", which is an edge that is not part of the cycle but connects two vertices of the cycle. Equivalently, every induced cycle in the graph should have exactly three vertices. The chordal graphs may also be characterized as the graphs that have perfect elimination orderings, as the graphs in which each minimal separator is a clique, and as the intersection graphs of subtrees of a tree. They are sometimes also called rigid circuit graphs or triangulated graphs.

Chordal graphs are a subset of the perfect graphs. They may be recognized in polynomial time, and several problems that are hard on other classes of graphs such as graph coloring may be solved in polynomial time when the input is chordal. The treewidth of an arbitrary graph may be characterized by the size of the cliques in the chordal graphs that contain it.

A "perfect elimination ordering" in a graph is an ordering of the vertices of the graph such that, for each vertex "v", "v" and the neighbors of "v" that occur after "v" in the order form a clique. A graph is chordal if and only if it has a perfect elimination ordering.

Since both this lexicographic breadth first search process and the process of testing whether an ordering is a perfect elimination ordering can be performed in linear time, it is possible to recognize chordal graphs in linear time. The graph sandwich problem on chordal graphs is NP-complete
whereas the probe graph problem on chordal graphs has polynomial-time 
complexity.

The set of all perfect elimination orderings of a chordal graph can be modeled as the "basic words" of an antimatroid; use this connection to antimatroids as part of an algorithm for efficiently listing all perfect elimination orderings of a given chordal graph.

Another application of perfect elimination orderings is finding a maximum clique of a chordal graph in polynomial-time, while the same problem for general graphs is NP-complete. More generally, a chordal graph can have only linearly many maximal cliques, while non-chordal graphs may have exponentially many. To list all maximal cliques of a chordal graph, simply find a perfect elimination ordering, form a clique for each vertex "v" together with the neighbors of "v" that are later than "v" in the perfect elimination ordering, and test whether each of the resulting cliques is maximal.

The clique graphs of chordal graphs are the dually chordal graphs.

The largest maximal clique is a maximum clique, and, as chordal graphs are perfect, the size of this clique equals the chromatic number of the chordal graph. Chordal graphs are perfectly orderable: an optimal coloring may be obtained by applying a greedy coloring algorithm to the vertices in the reverse of a perfect elimination ordering.

The chromatic polynomial of a chordal graph is easy to compute. Find a perfect elimination ordering formula_1 Let "N" equal the number of neighbors of "v" that come after "v" in that ordering. For instance, "N" = 0. The chromatic polynomial equals formula_2 (The last factor is simply "x", so "x" divides the polynomial, as it should.) Clearly, this computation depends on chordality.

In any graph, a vertex separator is a set of vertices the removal of which leaves the remaining graph disconnected; a separator is minimal if it has no proper subset that is also a separator. According to a theorem of , chordal graphs are graphs in which each minimal separator is a clique; Dirac used this characterization to prove that chordal graphs are perfect.

The family of chordal graphs may be defined inductively as the graphs whose vertices can be divided into three nonempty subsets "A", "S", and "B", such that "A" ∪ "S" and "S" ∪ "B" both form chordal induced subgraphs, "S" is a clique, and there are no edges from "A" to "B". That is, they are the graphs that have a recursive decomposition by clique separators into smaller subgraphs. For this reason, chordal graphs have also sometimes been called decomposable graphs.

An alternative characterization of chordal graphs, due to , involves trees and their subtrees.

From a collection of subtrees of a tree, one can define a subtree graph, which is an intersection graph that has one vertex per subtree and an edge connecting any two subtrees that overlap in one or more nodes of the tree. Gavril showed that the subtree graphs are exactly the chordal graphs.

A representation of a chordal graph as an intersection of subtrees forms a tree decomposition of the graph, with treewidth equal to one less than the size of the largest clique in the graph; the tree decomposition of any graph "G" can be viewed in this way as a representation of "G" as a subgraph of a chordal graph. The tree decomposition of a graph is also the junction tree of the junction tree algorithm.

Interval graphs are the intersection graphs of subtrees of path graphs, a special case of trees. Therefore, they are a subfamily of chordal graphs.

Split graphs are graphs that are both chordal and the complements of chordal graphs. showed that, in the limit as n goes to infinity, the fraction of n-vertex chordal graphs that are split approaches one.

Ptolemaic graphs are graphs that are both chordal and distance hereditary.
Quasi-threshold graphs are a subclass of Ptolemaic graphs that are both chordal and cographs. Block graphs are another subclass of Ptolemaic graphs in which every two maximal cliques have at most one vertex in common. A special type is windmill graphs, where the common vertex is the same for every pair of cliques.

Strongly chordal graphs are graphs that are chordal and contain no "n"-sun (for "n" ≥ 3) as an induced subgraph. Here an "n"-sun is an "n"-vertex chordal graph "G" together with a collection of "n" degree-two vertices, adjacent to the edges of a Hamiltonian cycle in "G".

"K"-trees are chordal graphs in which all maximal cliques and all maximal clique separators have the same size. Apollonian networks are chordal maximal planar graphs, or equivalently planar 3-trees. Maximal outerplanar graphs are a subclass of 2-trees, and therefore are also chordal.

Chordal graphs are a subclass of the well known perfect graphs. 
Other superclasses of chordal graphs include weakly chordal graphs, cop-win graphs, odd-hole-free graphs, even-hole-free graphs, and Meyniel graphs. Chordal graphs are precisely the graphs that are both odd-hole-free and even-hole-free (see holes in graph theory).

Every chordal graph is a strangulated graph, a graph in which every peripheral cycle is a triangle, because peripheral cycles are a special case of induced cycles. Strangulated graphs are graphs that can be formed by clique-sums of chordal graphs and maximal planar graphs. Therefore, strangulated graphs include maximal planar graphs.

If "G" is an arbitrary graph, a chordal completion of "G" (or minimum fill-in) is a chordal graph that contains "G" as a subgraph. The parameterized version of minimum fill-in is fixed parameter tractable, and moreover, is solvable in parameterized subexponential time. 
The treewidth of "G" is one less than the number of vertices in a maximum clique of a chordal completion chosen to minimize this clique size.
The "k"-trees are the graphs to which no additional edges can be added without increasing their treewidth to a number larger than "k".
Therefore, the "k"-trees are their own chordal completions, and form a subclass of the chordal graphs. Chordal completions can also be used to characterize several other related classes of graphs.




</doc>
<doc id="670531" url="https://en.wikipedia.org/wiki?curid=670531" title="Perfect graph">
Perfect graph

In graph theory, a perfect graph is a graph in which the chromatic number of every induced subgraph equals the size of the largest clique of that subgraph (clique number). Equivalently stated in symbolic terms an arbitrary graph formula_1 is perfect if and only if we have: formula_2

The perfect graphs include many important families of graphs and serve to unify results relating colorings and cliques in those families. For instance, in all perfect graphs, the graph coloring problem, maximum clique problem, and maximum independent set problem can all be solved in polynomial time. In addition, several important min-max theorems in combinatorics, such as Dilworth's theorem, can be expressed in terms of the perfection of certain associated graphs.


See below section for more details.

The theory of perfect graphs developed from a 1958 result of Tibor Gallai that in modern language can be interpreted as stating that the complement of a bipartite graph is perfect; this result can also be viewed as a simple equivalent of Kőnig's theorem, a much earlier result relating matchings and vertex covers in bipartite graphs. The first use of the phrase "perfect graph" appears to be in a 1963 paper of Claude Berge, after whom Berge graphs are named. In this paper he unified Gallai's result with several similar results by defining perfect graphs, and he conjectured the equivalence of the perfect graph and Berge graph definitions; his conjecture was proved in 2002 as the strong perfect graph theorem.

Some of the more well-known perfect graphs are:


In all graphs, the clique number provides a lower bound for the chromatic number, as all vertices in a clique must be assigned distinct colors in any proper coloring. The perfect graphs are those for which this lower bound is tight, not just in the graph itself but in all of its induced subgraphs. For graphs that are not perfect, the chromatic number and clique number can differ; for instance, a cycle of length five requires three colors in any proper coloring but its largest clique has size two.

A proof that a class of graphs is perfect can be seen as a min-max theorem: the minimum number of colors needed for these graphs equals the maximum size of a clique. Many important min-max theorems in combinatorics can be expressed in these terms. For instance, Dilworth's theorem states that the minimum number of chains in a partition of a partially ordered set into chains equals the maximum size of an antichain, and can be rephrased as stating that the complements of comparability graphs are perfect. Mirsky's theorem states that the minimum number of antichains into a partition into antichains equals the maximum size of a chain, and corresponds in the same way to the perfection of comparability graphs.

The perfection of permutation graphs is equivalent to the statement that, in every sequence of ordered elements, the length of the longest decreasing subsequence equals the minimum number of sequences in a partition into increasing subsequences. The Erdős–Szekeres theorem is an easy consequence of this statement.

Kőnig's theorem in graph theory states that a minimum vertex cover in a bipartite graph corresponds to a maximum matching, and vice versa; it can be interpreted as the perfection of the complements of bipartite graphs. Another theorem about bipartite graphs, that their chromatic index equals their maximum degree, is equivalent to the perfection of the line graphs of bipartite graphs.

In his initial work on perfect graphs, Berge made two important conjectures on their structure that were only proved later.

The first of these two theorems was the perfect graph theorem of Lovász (1972), stating that a graph is perfect if and only if its complement is perfect. Thus, perfection (defined as the equality of maximum clique size and chromatic number in every induced subgraph) is equivalent to the equality of maximum independent set size and clique cover number.
The second theorem, conjectured by Berge, provided a forbidden graph characterization of perfect graphs. An induced cycle of odd length at least is called an odd hole. An induced subgraph that is the complement of an odd hole is called an odd antihole. An odd cycle of length greater than cannot be perfect, because its chromatic number is three and its clique number is two. Similarly, the complement of an odd cycle of length cannot be perfect, because its chromatic number is and its clique number is . (Alternatively, the imperfection of this graph follows from the perfect graph theorem and the imperfection of the complementary odd cycle). Because these graphs are not perfect, every perfect graph must be a Berge graph, a graph with no odd holes and no odd antiholes. Berge conjectured the converse, that every Berge graph is perfect. This was finally proven as the strong perfect graph theorem of Chudnovsky, Robertson, Seymour, and Thomas (2006). It trivially implies the perfect graph theorem, hence the name.

The perfect graph theorem has a short proof, but the proof of the strong perfect graph theorem is long and technical, based on a deep structural decomposition of Berge graphs. Related decomposition techniques have also borne fruit in the study of other graph classes, and in particular for the claw-free graphs.

There is a third theorem, again due to Lovász, which was originally suggested by Hajnal. It states that a graph is perfect if the sizes of the largest clique, and the largest independent set, when multiplied together, equal or exceed the number of vertices of the graph, and the same is true for any induced subgraph. It is an easy consequence of the strong perfect graph theorem, while the perfect graph theorem is an easy consequence of it.

The Hajnal characterization is not met by odd -cycles or their complements for : the odd cycle on vertices has clique number and independence number . The reverse is true for the complement, so in both cases the product is .

In all perfect graphs, the graph coloring problem, maximum clique problem, and maximum independent set problem can all be solved in polynomial time . The algorithm for the general case involves the Lovász number of these graphs, which (for the complement of a given graph) is sandwiched between the chromatic number and clique number. Calculating the Lovász number can be formulated as a semidefinite program and approximated numerically in polynomial time using the ellipsoid method for linear programming. For perfect graphs, rounding this approximation to an integer gives the chromatic number and clique number in polynomial time; the maximum independent set can be found by applying the same approach to the complement of the graph.
However, this method is complicated and has a high polynomial exponent. More efficient combinatorial algorithms are known for many special cases.

For many years the complexity of recognizing Berge graphs and perfect graphs remained open. From the definition of Berge graphs, it follows immediately that their recognition is in co-NP (Lovász 1983). Finally, subsequent to the proof of the strong perfect graph theorem, a polynomial time algorithm was discovered by Chudnovsky, Cornuéjols, Liu, Seymour, and Vušković.




</doc>
<doc id="7726759" url="https://en.wikipedia.org/wiki?curid=7726759" title="Intersection graph">
Intersection graph

In the mathematical area of graph theory, an intersection graph is a graph that represents the pattern of intersections of a family of sets. Any graph can be represented as an intersection graph, but some important special classes of graphs can be defined by the types of sets that are used to form an intersection representation of them.

For an overview of both the theory of intersection graphs and important special classes of intersection graphs, see .

Formally, an intersection graph is an undirected graph formed from a family of sets
by creating one vertex "v" for each set "S", and connecting two vertices "v" and "v" by an edge whenever the corresponding two sets have a nonempty intersection, that is,


</doc>
<doc id="3686677" url="https://en.wikipedia.org/wiki?curid=3686677" title="Unit disk graph">
Unit disk graph

In geometric graph theory, a unit disk graph is the intersection graph of a family of unit disks in the Euclidean plane. That is, it is a graph with one vertex for each disk in the family, and with an edge between two vertices whenever the corresponding vertices lie within a unit distance of each other.

They are commonly formed from a Poisson point process, making them a simple example of a random structure.

There are several possible definitions of the unit disk graph, equivalent to each other up to a choice of scale factor:

Every induced subgraph of a unit disk graph is also a unit disk graph. An example of a graph that is not a unit disk graph is the star K with one central node connected to seven leaves: if each of seven unit disks touches a common unit disk, some two of the seven disks must touch each other (as the kissing number in the plane is 6). Therefore, unit disk graphs cannot contain an induced K subgraph.

Beginning with the work of , unit disk graphs have been used in computer science to model the topology of ad hoc wireless communication networks. In this application, nodes are connected through a direct wireless connection without a base station. It is assumed that all nodes are homogeneous and equipped with omnidirectional antennas. Node locations are modelled as Euclidean points, and the area within which a signal from one node can be received by another node is modelled as a circle. If all nodes have transmitters of equal power, these circles are all equal. Random geometric graphs, formed as unit disk graphs with randomly generated disk centres, have also been used as a model of percolation and various other phenomena.

If one is given a collection of unit disks (or their centres) in a space of any fixed dimension, it is possible to construct the corresponding unit disk graph in linear time, by rounding the centres to nearby integer grid points, using a hash table to find all pairs of centres within constant distance of each other, and filtering the resulting list of pairs for the ones whose circles intersect. The ratio of the number of pairs considered by this algorithm to the number of edges in the eventual graph is a constant, giving the linear time bound. However, this constant grows exponentially as a function of the dimension .

It is NP-hard (more specifically, complete for the existential theory of the reals) to determine whether a graph, given without geometry, can be represented as a unit disk graph. Additionally, it is provably impossible in polynomial time to output explicit coordinates of a unit disk graph representation: there exist unit disk graphs that require exponentially many bits of precision in any such representation.

However, many important and difficult graph optimization problems such as maximum independent set, graph coloring, and minimum dominating set can be approximated efficiently by using the geometric structure of these graphs, and the maximum clique problem can be solved exactly for these graphs in polynomial time, given a disk representation. Even if a disk representation is not known, and an abstract graph is given as input, it is possible in polynomial time to produce either a maximum clique or a proof that the graph is not a unit disk graph, and to 3-approximate the optimum coloring by using a greedy coloring algorithm.

When a given vertex set forms a subset of a triangular lattice, a necessary and sufficient condition for the perfectness of a unit graph is known. For the perfect graphs, a number of NP-complete optimization problems (graph coloring problem, maximum clique problem, and maximum independent set problem) are polynomially solvable.




</doc>
<doc id="675231" url="https://en.wikipedia.org/wiki?curid=675231" title="Line graph">
Line graph

In the mathematical discipline of graph theory, the line graph of an undirected graph "G" is another graph "L"("G") that represents the adjacencies between edges of "G". The name line graph comes from a paper by although both and used the construction before this. Other terms used for the line graph include the covering graph, the derivative, the edge-to-vertex dual, the conjugate, the representative graph, and the ϑ-obrazom, as well as the edge graph, the interchange graph, the adjoint graph, and the derived graph.

Various extensions of the concept of a line graph have been studied, including line graphs of line graphs, line graphs of multigraphs, line graphs of hypergraphs, and line graphs of weighted graphs.

Given a graph "G", its line graph "L"("G") is a graph such that
That is, it is the intersection graph of the edges of "G", representing each edge by the set of its two endpoints.

The following figures show a graph (left, with blue vertices) and its line graph (right, with green vertices). Each vertex of the line graph is shown labeled with the pair of endpoints of the corresponding edge in the original graph. For instance, the green vertex on the right labeled 1,3 corresponds to the edge on the left between the blue vertices 1 and 3. Green vertex 1,3 is adjacent to three other green vertices: 1,4 and 1,2 (corresponding to edges sharing the endpoint 1 in the blue graph) and 4,3 (corresponding to an edge sharing the endpoint 3 in the blue graph).

Properties of a graph "G" that depend only on adjacency between edges may be translated into equivalent properties in "L"("G") that depend on adjacency between vertices. For instance, a matching in "G" is a set of edges no two of which are adjacent, and corresponds to a set of vertices in "L"("G") no two of which are adjacent, that is, an independent set.

Thus,

If the line graphs of two connected graphs are isomorphic, then the underlying graphs are isomorphic, except in the case of the triangle graph "K" and the claw "K", which have isomorphic line graphs but are not themselves isomorphic.

As well as "K" and "K", there are some other exceptional small graphs with the property that their line graph has a higher degree of symmetry than the graph itself. For instance, the diamond graph "K" (two triangles sharing an edge) has four graph automorphisms but its line graph "K" has eight. In the illustration of the diamond graph shown, rotating the graph by 90 degrees is not a symmetry of the graph, but is a symmetry of its line graph. However, all such exceptional cases have at most four vertices. A strengthened version of the Whitney isomorphism theorem states that, for connected graphs with more than four vertices, there is a one-to-one correspondence between isomorphisms of the graphs and isomorphisms of their line graphs.

Analogues of the Whitney isomorphism theorem have been proven for the line graphs of multigraphs, but are more complicated in this case.

The line graph of the complete graph "K" is also known as the triangular graph, the Johnson graph "J"("n",2), or the complement of the Kneser graph "KG". Triangular graphs are characterized by their spectra, except for "n" = 8. They may also be characterized (again with the exception of "K") as the strongly regular graphs with parameters srg("n"("n" − 1)/2, 2("n" − 2), "n" − 2, 4). The three strongly regular graphs with the same parameters and spectrum as "L"("K") are the Chang graphs, which may be obtained by graph switching from "L"("K").

The line graph of a bipartite graph is perfect (see Kőnig's theorem), but need not be bipartite as the example of the claw graph shows. The line graphs of bipartite graphs form one of the key building blocks of perfect graphs, used in the proof of the strong perfect graph theorem. A special case of these graphs are the rook's graphs, line graphs of complete bipartite graphs. Like the line graphs of complete graphs, they can be characterized with one exception by their numbers of vertices, numbers of edges, and number of shared neighbors for adjacent and non-adjacent points. The one exceptional case is "L"("K"), which shares its parameters with the Shrikhande graph. When both sides of the bipartition have the same number of vertices, these graphs are again strongly regular.

More generally, a graph "G" is said to be a line perfect graph if "L"("G") is a perfect graph. The line perfect graphs are exactly the graphs that do not contain a simple cycle of odd length greater than three. Equivalently, a graph is line perfect if and only if each of its biconnected components is either bipartite or of the form "K" (the tetrahedron) or "K" (a book of one or more triangles all sharing a common edge). Every line perfect graph is itself perfect.

All line graphs are claw-free graphs, graphs without an induced subgraph in the form of a three-leaf tree. As with claw-free graphs more generally, every connected line graph "L"("G") with an even number of edges has a perfect matching; equivalently, this means that if the underlying graph "G" has an even number of edges, its edges can be partitioned into two-edge paths.

The line graphs of trees are exactly the claw-free block graphs. These graphs have been used to solve a problem in extremal graph theory, of constructing a graph with a given number of edges and vertices whose largest tree induced as a subgraph is as small as possible.

All eigenvalues of the adjacency matrix formula_1 of a line graph are at least −2. The reason for this is that formula_1 can be written as formula_3, where formula_4 is the signless incidence matrix of the pre-line graph and formula_5 is the identity. In particular, formula_6 is the Gramian matrix of a system of vectors: all graphs with this property have been called generalized line graphs.

For an arbitrary graph "G", and an arbitrary vertex "v" in "G", the set of edges incident to "v" corresponds to a clique in the line graph "L"("G"). The cliques formed in this way partition the edges of "L"("G"). Each vertex of "L"("G") belongs to exactly two of them (the two cliques corresponding to the two endpoints of the corresponding edge in "G").

The existence of such a partition into cliques can be used to characterize the line graphs:
A graph "L" is the line graph of some other graph or multigraph if and only if it is possible to find a collection of cliques in "L" (allowing some of the cliques to be single vertices) that partition the edges of "L", such that each vertex of "L" belongs to exactly two of the cliques. It is the line graph of a graph (rather than a multigraph) if this set of cliques satisfies the additional condition that no two vertices of "L" are both in the same two cliques. Given such a family of cliques, the underlying graph "G" for which "L" is the line graph can be recovered by making one vertex in "G" for each clique, and an edge in "G" for each vertex in "L" with its endpoints being the two cliques containing the vertex in "L". By the strong version of Whitney's isomorphism theorem, if the underlying graph "G" has more than four vertices, there can be only one partition of this type.

For example, this characterization can be used to show that the following graph is not a line graph:
In this example, the edges going upward, to the left, and to the right from the central degree-four vertex do not have any cliques in common. Therefore, any partition of the graph's edges into cliques would have to have at least one clique for each of these three edges, and these three cliques would all intersect in that central vertex, violating the requirement that each vertex appear in exactly two cliques. Thus, the graph shown is not a line graph.

Another characterization of line graphs was proven in (and reported earlier without proof by ). He showed that there are nine minimal graphs that are not line graphs, such that any graph that is not a line graph has one of these nine graphs as an induced subgraph. That is, a graph is a line graph if and only if no subset of its vertices induces one of these nine graphs. In the example above, the four topmost vertices induce a claw (that is, a complete bipartite graph "K"), shown on the top left of the illustration of forbidden subgraphs. Therefore, by Beineke's characterization, this example cannot be a line graph. For graphs with minimum degree at least 5, only the six subgraphs in the left and right columns of the figure are needed in the characterization.

 and described linear time algorithms for recognizing line graphs and reconstructing their original graphs. generalized these methods to directed graphs. described an efficient data structure for maintaining a dynamic graph, subject to vertex insertions and deletions, and maintaining a representation of the input as a line graph (when it exists) in time proportional to the number of changed edges at each step.

The algorithms of and are based on characterizations of line graphs involving odd triangles (triangles in the line graph with the property that there exists another vertex adjacent to an odd number of triangle vertices). However, the algorithm of uses only Whitney's isomorphism theorem. It is complicated by the need to recognize deletions that cause the remaining graph to become a line graph, but when specialized to the static recognition problem only insertions need to be performed, and the algorithm performs the following steps:
Each step either takes constant time, or involves finding a vertex cover of constant size within a graph "S" whose size is proportional to the number of neighbors of "v". Thus, the total time for the whole algorithm is proportional to the sum of the numbers of neighbors of all vertices, which (by the handshaking lemma) is proportional to the number of input edges.

 consider the sequence of graphs
They show that, when "G" is a finite connected graph, only four behaviors are possible for this sequence:
If "G" is not connected, this classification applies separately to each component of "G".

For connected graphs that are not paths, all sufficiently high numbers of iteration of the line graph operation produce graphs that are Hamiltonian.

When a planar graph "G" has maximum vertex degree three, its line graph is planar, and every planar embedding of "G" can be extended to an embedding of "L"("G"). However, there exist planar graphs with higher degree whose line graphs are nonplanar. These include, for example, the 5-star "K", the gem graph formed by adding two non-crossing diagonals within a regular pentagon, and all convex polyhedra with a vertex of degree four or more.

An alternative construction, the medial graph, coincides with the line graph for planar graphs with maximum degree three, but is always planar. It has the same vertices as the line graph, but potentially fewer edges: two vertices of the medial graph are adjacent if and only if the corresponding two edges are consecutive on some face of the planar embedding. The medial graph of the dual graph of a plane graph is the same as the medial graph of the original plane graph.

For regular polyhedra or simple polyhedra, the medial graph operation can be represented geometrically by the operation of cutting off each vertex of the polyhedron by a plane through the midpoints of all its incident edges. This operation is known variously as the second truncation, degenerate truncation, or rectification.

The total graph "T"("G") of a graph "G" has as its vertices the elements (vertices or edges) of "G", and has an edge between two elements whenever they are either incident or adjacent. The total graph may also be obtained by subdividing each edge of "G" and then taking the square of the subdivided graph.

The concept of the line graph of "G" may naturally be extended to the case where "G" is a multigraph. In this case, the characterizations of these graphs can be simplified: the characterization in terms of clique partitions no longer needs to prevent two vertices from belonging to the same to cliques, and the characterization by forbidden graphs has seven forbidden graphs instead of nine.

However, for multigraphs, there are larger numbers of pairs of non-isomorphic graphs that have the same line graphs. For instance a complete bipartite graph "K" has the same line graph as the dipole graph and Shannon multigraph with the same number of edges. Nevertheless, analogues to Whitney's isomorphism theorem can still be derived in this case.

It is also possible to generalize line graphs to directed graphs. If "G" is a directed graph, its directed line graph or line digraph has one vertex for each edge of "G". Two vertices representing directed edges from "u" to "v" and from "w" to "x" in "G" are connected by an edge from "uv" to "wx" in the line digraph when "v" = "w". That is, each edge in the line digraph of "G" represents a length-two directed path in "G". The de Bruijn graphs may be formed by repeating this process of forming directed line graphs, starting from a complete directed graph.

In a line graph "L"("G"), each vertex of degree "k" in the original graph "G" creates "k(k-1)/2" edges in the line graph. For many types of analysis this means high-degree nodes in "G" are over-represented in the line graph "L"("G"). For instance, consider a random walk on the vertices of the original graph "G". This will pass along some edge "e" with some frequency "f". On the other hand, this edge "e" is mapped to a unique vertex, say "v", in the line graph "L"("G"). If we now perform the same type of random walk on the vertices of the line graph, the frequency with which "v" is visited can be completely different from "f". If our edge "e" in "G" was connected to nodes of degree "O(k)", it will be traversed "O(k)" more frequently in the line graph "L"("G"). Put another way, the Whitney graph isomorphism theorem guarantees that the line graph almost always encodes the topology of the original graph "G" faithfully but it does not guarantee that dynamics on these two graphs have a simple relationship. One solution is to construct a weighted line graph, that is, a line graph with weighted edges. There are several natural ways to do this. For instance if edges "d" and "e" in the graph "G" are incident at a vertex "v" with degree "k", then in the line graph "L"("G") the edge connecting the two vertices "d" and "e" can be given weight "1/(k-1)". In this way every edge in "G" (provided neither end is connected to a vertex of degree '1') will have strength "2" in the line graph "L"("G") corresponding to the two ends that the edge has in "G". It is straightforward to extend this definition of a weighted line graph to cases where the original graph "G" was directed or even weighted. The principle in all cases is to ensure the line graph "L"("G") reflects the dynamics as well as the topology of the original graph "G".

The edges of a hypergraph may form an arbitrary family of sets, so the line graph of a hypergraph is the same as the intersection graph of the sets from the family.



</doc>
<doc id="12415907" url="https://en.wikipedia.org/wiki?curid=12415907" title="Claw-free graph">
Claw-free graph

In graph theory, an area of mathematics, a claw-free graph is a graph that does not have a claw as an induced subgraph.

A claw is another name for the complete bipartite graph "K" (that is, a star graph with three edges, three leaves, and one central vertex). A claw-free graph is a graph in which no induced subgraph is a claw; i.e., any subset of four vertices has other than only three edges connecting them in this pattern. Equivalently, a claw-free graph is a graph in which the neighborhood of any vertex is the complement of a triangle-free graph.

Claw-free graphs were initially studied as a generalization of line graphs, and gained additional motivation through three key discoveries about them: the fact that all claw-free connected graphs of even order have perfect matchings, the discovery of polynomial time algorithms for finding maximum independent sets in claw-free graphs, and the characterization of claw-free perfect graphs. They are the subject of hundreds of mathematical research papers and several surveys.


It is straightforward to verify that a given graph with "n" vertices and "m" edges is claw-free in time O("n"), by testing each 4-tuple of vertices to determine whether they induce a claw. With more efficiency, and greater complication, one can test whether a graph is claw-free by checking, for each vertex of the graph, that the complement graph of its neighbors does not contain a triangle. A graph contains a triangle if and only if the cube of its adjacency matrix contains a nonzero diagonal element, so finding a triangle may be performed in the same asymptotic time bound as "n" × "n" matrix multiplication. Therefore, using the Coppersmith–Winograd algorithm, the total time for this claw-free recognition algorithm would be O("n").

Because claw-free graphs include complements of triangle-free graphs, the number of claw-free graphs on "n" vertices grows at least as quickly as the number of triangle-free graphs, exponentially in the square of "n".
The numbers of connected claw-free graphs on "n" nodes, for "n" = 1, 2, ... are
If the graphs are allowed to be disconnected, the numbers of graphs are even larger: they are
A technique of allows the number of claw-free cubic graphs to be counted very efficiently, unusually for graph enumeration problems.

 and, independently, proved that every claw-free connected graph with an even number of vertices has a perfect matching. That is, there exists a set of edges in the graph such that each vertex is an endpoint of exactly one of the matched edges. The special case of this result for line graphs implies that, in any graph with an even number of edges, one can partition the edges into paths of length two. Perfect matchings may be used to provide another characterization of the claw-free graphs: they are exactly the graphs in which every connected induced subgraph of even order has a perfect matching.

Sumner's proof shows, more strongly, that in any connected claw-free graph one can find a pair of adjacent vertices the removal of which leaves the remaining graph connected. To show this, Sumner finds a pair of vertices "u" and "v" that are as far apart as possible in the graph, and chooses "w" to be a neighbor of "v" that is as far from "u" as possible; as he shows, neither "v" nor "w" can lie on any shortest path from any other node to "u", so the removal of "v" and "w" leaves the remaining graph connected. Repeatedly removing matched pairs of vertices in this way forms a perfect matching in the given claw-free graph.

The same proof idea holds more generally if "u" is any vertex, "v" is any vertex that is maximally far from "u", and "w" is any neighbor of "v" that is maximally far from "u". Further, the removal of "v" and "w" from the graph does not change any of the other distances from "u". Therefore, the process of forming a matching by finding and removing pairs "vw" that are maximally far from "u" may be performed by a single postorder traversal of a breadth first search tree of the graph, rooted at "u", in linear time. provide an alternative linear-time algorithm based on depth-first search, as well as efficient parallel algorithms for the same problem.

An independent set in a line graph corresponds to a matching in its underlying graph, a set of edges no two of which share an endpoint. The blossom algorithm of finds a maximum matching in any graph in polynomial time, which is equivalent to computing a maximum independent set in line graphs. This has been independently extended to an algorithm for all claw-free graphs by and .

Both approaches use the observation that in claw-free graphs, no vertex can have more than two neighbors in an independent set, and so the symmetric difference of two independent sets must induce a subgraph of degree at most two; that is, it is a union of paths and cycles. In particular, if "I" is a non-maximum independent set, it differs from any maximum independent set by even cycles and so called "augmenting paths": induced paths which alternate between vertices not in "I" and vertices in "I", and for which both endpoints have only one neighbor in "I". As the symmetric difference of "I" with any augmenting path gives a larger independent set, the task thus reduces to searching for augmenting paths until no more can be found, analogously as in algorithms for finding maximum matchings.

Sbihi's algorithm recreates the blossom contraction step of Edmonds' algorithm and adds a similar, but more complicated, "clique contraction" step. Minty's approach is to transform the problem instance into an auxiliary line graph and use Edmonds' algorithm directly to find the augmenting paths. After a correction by , Minty's result may also be used to solve in polynomial time the more general problem of finding in claw-free graphs an independent set of maximum weight. 
Generalizations of these results to wider classes of graphs are also known.
By showing a novel structure theorem, gave a cubic time algorithm, which also works in the weighted setting.

A perfect graph is a graph in which the chromatic number and the size of the maximum clique are equal, and in which this equality persists in every induced subgraph. It is now known (the strong perfect graph theorem) that perfect graphs may be characterized as the graphs that do not have as induced subgraphs either an odd cycle or the complement of an odd cycle (a so-called "odd hole"). However, for many years this remained an unsolved conjecture, only proven for special subclasses of graphs. One of these subclasses was the family of claw-free graphs: it was discovered by several authors that claw-free graphs without odd cycles and odd holes are perfect. Perfect claw-free graphs may be recognized in polynomial time. In a perfect claw-free graph, the neighborhood of any vertex forms the complement of a bipartite graph. It is possible to color perfect claw-free graphs, or to find maximum cliques in them, in polynomial time.
In general, it is NP-hard to find the largest clique in a claw-free graph. It is also NP-hard to find an optimal coloring of the graph, because (via line graphs) this problem generalizes the NP-hard problem of computing the chromatic index of a graph. For the same reason, it is NP-hard to find a coloring that achieves an approximation ratio better than 4/3. However, an approximation ratio of two can be achieved by a greedy coloring algorithm, because the chromatic number of a claw-free graph is greater than half its maximum degree. A generalization of the edge list coloring conjecture states that, for claw-free graphs, the list chromatic number equals the chromatic number; these two numbers can be far apart in other kinds of graphs.

The claw-free graphs are "χ"-bounded, meaning that every claw-free graph of large chromatic number contains a large clique. More strongly, it follows from Ramsey's theorem that every claw-free graph of large maximum degree contains a large clique, of size roughly proportional to the square root of the degree. For connected claw-free graphs that include at least one three-vertex independent set, a stronger relation between chromatic number and clique size is possible: in these graphs, there exists a clique of size at least half the chromatic number.

Although not every claw-free graph is perfect, claw-free graphs satisfy another property, related to perfection. A graph is called domination perfect if it has a minimum dominating set that is independent, and if the same property holds in all of its induced subgraphs. Claw-free graphs have this property. To see this, let "D" be a dominating set in a claw-free graph, and suppose that "v" and "w" are two adjacent vertices in "D"; then the set of vertices dominated by "v" but not by "w" must be a clique (else "v" would be the center of a claw). If every vertex in this clique is already dominated by at least one other member of "D", then "v" can be removed producing a smaller independent dominating set, and otherwise "v" can be replaced by one of the undominated vertices in its clique producing a dominating set with fewer adjacencies. By repeating this replacement process one eventually reaches a dominating set no larger than "D", so in particular when the starting set "D" is a minimum dominating set this process forms an equally small independent dominating set.

Despite this domination perfectness property, it is NP-hard to determine the size of the minimum dominating set in a claw-free graph. However, in contrast to the situation for more general classes of graphs, finding the minimum dominating set or the minimum connected dominating set in a claw-free graph is fixed-parameter tractable: it can be solved in time bounded by a polynomial in the size of the graph multiplied by an exponential function of the dominating set size.

 overview a series of papers in which they prove a structure theory for claw-free graphs, analogous to the graph structure theorem for minor-closed graph families proven by Robertson and Seymour, and to the structure theory for perfect graphs that Chudnovsky, Seymour and their co-authors used to prove the strong perfect graph theorem. The theory is too complex to describe in detail here, but to give a flavor of it, it suffices to outline two of their results. First, for a special subclass of claw-free graphs which they call "quasi-line graphs" (equivalently, locally co-bipartite graphs), they state that every such graph has one of two forms:

Chudnovsky and Seymour classify arbitrary connected claw-free graphs into one of the following:
Much of the work in their structure theory involves a further analysis of antiprismatic graphs. The Schläfli graph, a claw-free strongly regular graph with parameters srg(27,16,10,8), plays an important role in this part of the analysis. This structure theory has led to new advances in polyhedral combinatorics and new bounds on the chromatic number of claw-free graphs, as well as to new fixed-parameter-tractable algorithms for dominating sets in claw-free graphs.




</doc>
<doc id="16970848" url="https://en.wikipedia.org/wiki?curid=16970848" title="Median graph">
Median graph

In graph theory, a division of mathematics, a median graph is an undirected graph in which every three vertices "a", "b", and "c" have a unique "median": a vertex "m"("a","b","c") that belongs to shortest paths between each pair of "a", "b", and "c".

The concept of median graphs has long been studied, for instance by or (more explicitly) by , but the first paper to call them "median graphs" appears to be . As Chung, Graham, and Saks write, "median graphs arise naturally in the study of ordered sets and discrete distributive lattices, and have an extensive literature". In phylogenetics, the Buneman graph representing all maximum parsimony evolutionary trees is a median graph. Median graphs also arise in social choice theory: if a set of alternatives has the structure of a median graph, it is possible to derive in an unambiguous way a majority preference among them.

Additional surveys of median graphs are given by , , and .

Every tree is a median graph. To see this, observe that in a tree, the union of the three shortest paths between pairs of the three vertices "a", "b", and "c" is either itself a path, or a subtree formed by three paths meeting at a single central node with degree three. If the union of the three paths is itself a path, the median "m"("a","b","c") is equal to one of "a", "b", or "c", whichever of these three vertices is between the other two in the path. If the subtree formed by the union of the three paths is not a path, the median of the three vertices is the central degree-three node of the subtree.

Additional examples of median graphs are provided by the grid graphs. In a grid graph, the coordinates of the median "m"("a","b","c") can be found as the median of the coordinates of "a", "b", and "c". Conversely, it turns out that, in every median graph, one may label the vertices by points in an integer lattice in such a way that medians can be calculated coordinatewise in this way.
Squaregraphs, planar graphs in which all interior faces are quadrilaterals and all interior vertices have four or more incident edges, are another subclass of the median graphs. A polyomino is a special case of a squaregraph and therefore also forms a median graph.

The simplex graph κ("G") of an arbitrary undirected graph "G" has a vertex for every clique (complete subgraph) of "G"; two vertex of κ("G") are linked by an edge if the corresponding cliques differ by one vertex of "G" . The simplex graph is always a median graph, in which the median of a given triple of cliques may be formed by using the majority rule to determine which vertices of the cliques to include.

No cycle graph of length other than four can be a median graph. Every such cycle has three vertices "a", "b", and "c" such that the three shortest paths wrap all the way around the cycle without having a common intersection. For such a triple of vertices, there can be no median.

In an arbitrary graph, for each two vertices "a" and "b", the minimal number of edges between them is called their "distance", denoted by "d"("x","y"). The "interval" of vertices that lie on shortest paths between "a" and "b" is defined as
A median graph is defined by the property that, for every three vertices "a", "b", and "c", these intervals intersect in a single point:

Equivalently, for every three vertices "a", "b", and "c" one can find a vertex "m"("a","b","c") such that the unweighted distances in the graph satisfy the equalities
and "m"("a","b","c") is the only vertex for which this is true.

It is also possible to define median graphs as the solution sets of 2-satisfiability problems, as the retracts of hypercubes, as the graphs of finite median algebras, as the Buneman graphs of Helly split systems, and as the graphs of windex 2; see the sections below.

In lattice theory, the graph of a finite lattice has a vertex for each lattice element and an edge for each pair of elements in the covering relation of the lattice. Lattices are commonly presented visually via Hasse diagrams, which are drawings of graphs of lattices. These graphs, especially in the case of distributive lattices, turn out to be closely related to median graphs.

In a distributive lattice, Birkhoff's self-dual ternary median operation
satisfies certain key axioms, which it shares with the usual median of numbers in the range from 0 to 1 and with median algebras more generally:
The distributive law may be replaced by an associative law:
The median operation may also be used to define a notion of intervals for distributive lattices:
The graph of a finite distributive lattice has an edge between vertices "a" and "b" whenever "I"("a","b") = {"a","b"}. For every two vertices "a" and "b" of this graph, the interval "I"("a","b") defined in lattice-theoretic terms above consists of the vertices on shortest paths from "a" to "b", and thus coincides with the graph-theoretic intervals defined earlier. For every three lattice elements "a", "b", and "c", "m"("a","b","c") is the unique intersection of the three intervals "I"("a","b"), "I"("a","c"), and "I"("b","c"). Therefore, the graph of an arbitrary finite distributive lattice is a median graph. Conversely, if a median graph "G" contains two vertices 0 and 1 such that every other vertex lies on a shortest path between the two (equivalently, "m"(0,"a",1) = "a" for all "a"), then we may define a distributive lattice in which "a" ∧ "b" = "m"("a",0,"b") and "a" ∨ "b" = "m"("a",1,"b"), and "G" will be the graph of this lattice.

In a median graph, a set "S" of vertices is said to be convex if, for every two vertices "a" and "b" belonging to "S", the whole interval "I"("a","b") is a subset of "S". Equivalently, given the two definitions of intervals above, "S" is convex if it contains every shortest path between two of its vertices, or if it contains the median of every set of three points at least two of which are from "S". Observe that the intersection of every pair of convex sets is itself convex.

The convex sets in a median graph have the Helly property: if "F" is an arbitrary family of pairwise-intersecting convex sets, then all sets in "F" have a common intersection. For, if "F" has only three convex sets "S", "T", and "U" in it, with "a" in the intersection of the pair "S" and "T", "b" in the intersection of the pair "T" and "U", and "c" in the intersection of the pair "S" and "U", then every shortest path from "a" to "b" must lie within "T" by convexity, and similarly every shortest path between the other two pairs of vertices must lie within the other two sets; but "m"("a","b","c") belongs to paths between all three pairs of vertices, so it lies within all three sets, and forms part of their common intersection. If "F" has more than three convex sets in it, the result follows by induction on the number of sets, for one may replace an arbitrary pair of sets in "F" by their intersection, using the result for triples of sets to show that the replaced family is still pairwise intersecting.

A particularly important family of convex sets in a median graph, playing a role similar to that of halfspaces in Euclidean space, are the sets
defined for each edge "uv" of the graph. In words, "W" consists of the vertices closer to "u" than to "v", or equivalently the vertices "w" such that some shortest path from "v" to "w" goes through "u". 
To show that "W" is convex, let "w""w"..."w" be an arbitrary shortest path that starts and ends within "W"; then "w" must also lie within "W", for otherwise the two points "m" = "m"("u","w","w") and "m" = "m"("m","w"..."w") could be shown (by considering the possible distances between the vertices) to be distinct medians of "u", "w", and "w", contradicting the definition of a median graph which requires medians to be unique. Thus, each successive vertex on a shortest path between two vertices of "W" also lies within "W", so "W" contains all shortest paths between its nodes, one of the definitions of convexity.

The Helly property for the sets "W" plays a key role in the characterization of median graphs as the solution of 2-satisfiability instances, below.

Median graphs have a close connection to the solution sets of 2-satisfiability problems that can be used both to characterize these graphs and to relate them to adjacency-preserving maps of hypercubes.

A 2-satisfiability instance consists of a collection of Boolean variables and a collection of "clauses", constraints on certain pairs of variables requiring those two variables to avoid certain combinations of values. Usually such problems are expressed in conjunctive normal form, in which each clause is expressed as a disjunction and the whole set of constraints is expressed as a conjunction of clauses, such as
A solution to such an instance is an assignment of truth values to the variables that satisfies all the clauses, or equivalently that causes the conjunctive normal form expression for the instance to become true when the variable values are substituted into it. The family of all solutions has a natural structure as a median algebra, where the median of three solutions is formed by choosing each truth value to be the majority function of the values in the three solutions; it is straightforward to verify that this median solution cannot violate any of the clauses. Thus, these solutions form a median graph, in which the neighbor of each solution is formed by negating a set of variables that are all constrained to be equal or unequal to each other.

Conversely, every median graph "G" may be represented in this way as the solution set to a 2-satisfiability instance. To find such a representation, create a 2-satisfiability instance in which each variable describes the orientation of one of the edges in the graph (an assignment of a direction to the edge causing the graph to become directed rather than undirected) and each constraint allows two edges to share a pair of orientations only when there exists a vertex "v" such that both orientations lie along shortest paths from other vertices to "v". Each vertex "v" of "G" corresponds to a solution to this 2-satisfiability instance in which all edges are directed towards "v". Each
solution to the instance must come from some vertex "v" in this way, where "v" is the common intersection of the sets "W" for edges directed from "w" to "u"; this common intersection exists due to the Helly property of the sets "W". Therefore, the solutions to this 2-satisfiability instance correspond one-for-one with the vertices of "G".

A "retraction" of a graph "G" is an adjacency-preserving map from "G" to one of its subgraphs. More precisely, it is graph homomorphism φ from "G" to itself such that φ("v") = "v" for each vertex "v" in the subgraph φ(G). The image of the retraction is called a "retract" of "G". 
Retractions are examples of metric maps: the distance between φ("v") and φ("w"), for every "v" and "w", is at most equal to the distance between "v" and "w", and is equal whenever "v" and "w" both belong to φ("G"). Therefore, a retract must be an "isometric subgraph" of "G": distances in the retract equal those in "G".

If "G" is a median graph, and "a", "b", and "c" are an arbitrary three vertices of a retract φ("G"), then φ("m"("a","b","c")) must be a median of "a", "b", and "c", and so must equal "m"("a","b","c"). Therefore, φ("G") contains medians of all triples of its vertices, and must also be a median graph. In other words, the family of median graphs is closed under the retraction operation.

A hypercube graph, in which the vertices correspond to all possible "k"-bit bitvectors and in which two vertices are adjacent when the corresponding bitvectors differ in only a single bit, is a special case of a "k"-dimensional grid graph and is therefore a median graph. The median of three bitvectors "a", "b", and "c" may be calculated by computing, in each bit position, the majority function of the bits of "a", "b", and "c". Since median graphs are closed under retraction, and include the hypercubes, every retract of a hypercube is a median graph.

Conversely, every median graph must be the retract of a hypercube. This may be seen from the connection, described above, between median graphs and 2-satisfiability: let "G" be the graph of solutions to a 2-satisfiability instance; without loss of generality this instance can be formulated in such a way that no two variables are always equal or always unequal in every solution. Then the space of all truth assignments to the variables of this instance forms a hypercube. For each clause, formed as the disjunction of two variables or their complements, in the 2-satisfiability instance, one can form a retraction of the hypercube in which truth assignments violating this clause are mapped to truth assignments in which both variables satisfy the clause, without changing the other variables in the truth assignment. The composition of the retractions formed in this way for each of the clauses gives a retraction of the hypercube onto the solution space of the instance, and therefore gives a representation of "G" as the retract of a hypercube. In particular, median graphs are isometric subgraphs of hypercubes, and are therefore partial cubes. However, not all partial cubes are median graphs; for instance, a six-vertex cycle graph is a partial cube but is not a median graph.

As describe, an isometric embedding of a median graph into a hypercube may be constructed in time O("m" log "n"), where "n" and "m" are the numbers of vertices and edges of the graph respectively.

The problems of testing whether a graph is a median graph, and whether a graph is triangle-free, both had been well studied when observed that, in some sense, they are computationally equivalent. Therefore, the best known time bound for testing whether a graph is triangle-free, O("m"), applies as well to testing whether a graph is a median graph, and any improvement in median graph testing algorithms would also lead to an improvement in algorithms for detecting triangles in graphs.

In one direction, suppose one is given as input a graph "G", and must test whether "G" is triangle-free. From "G", construct a new graph "H" having as vertices each set of zero, one, or two adjacent vertices of "G". Two such sets are adjacent in "H" when they differ by exactly one vertex. An equivalent description of "H" is that it is formed by splitting each edge of "G" into a path of two edges, and adding a new vertex connected to all the original vertices of "G". This graph "H" is by construction a partial cube, but it is a median graph only when "G" is triangle-free: if "a", "b", and "c" form a triangle in "G", then {"a","b"}, {"a","c"}, and {"b","c"} have no median in "H", for such a median would have to correspond to the set {"a","b","c"}, but sets of three or more vertices of "G" do not form vertices in "H". Therefore, "G" is triangle-free if and only if "H" is a median graph. In the case that "G" is triangle-free, "H" is its simplex graph. An algorithm to test efficiently whether "H" is a median graph could by this construction also be used to test whether "G" is triangle-free. This transformation preserves the computational complexity of the problem, for the size of "H" is proportional to that of "G".

The reduction in the other direction, from triangle detection to median graph testing, is more involved and depends on the previous median graph recognition algorithm of , which tests several necessary conditions for median graphs in near-linear time. The key new step involves using a breadth first search to partition the graph's vertices into levels according to their distances from some arbitrarily chosen root vertex, forming a graph from each level in which two vertices are adjacent if they share a common neighbor in the previous level, and searching for triangles in these graphs. The median of any such triangle must be a common neighbor of the three triangle vertices; if this common neighbor does not exist, the graph is not a median graph. If all triangles found in this way have medians, and the previous algorithm finds that the graph satisfies all the other conditions for being a median graph, then it must actually be a median graph. This algorithm requires, not just the ability to test whether a triangle exists, but a list of all triangles in the level graph. In arbitrary graphs, listing all triangles sometimes requires Ω("m") time, as some graphs have that many triangles, however Hagauer et al. show that the number of triangles arising in the level graphs of their reduction is near-linear, allowing the Alon et al. fast matrix multiplication based technique for finding triangles to be used.

Phylogeny is the inference of evolutionary trees from observed characteristics of species; such a tree must place the species at distinct vertices, and may have additional "latent vertices", but the latent vertices are required to have three or more incident edges and must also be labeled with characteristics. A characteristic is "binary" when it has only two possible values, and a set of species and their characteristics exhibit perfect phylogeny when there exists an evolutionary tree in which the vertices (species and latent vertices) labeled with any particular characteristic value form a contiguous subtree. If a tree with perfect phylogeny is not possible, it is often desired to find one exhibiting maximum parsimony, or equivalently, minimizing the number of times the endpoints of a tree edge have different values for one of the characteristics, summed over all edges and all characteristics.

To form the Buneman graph for a set of species and characteristics, first, eliminate redundant species that are indistinguishable from some other species and redundant characteristics that are always the same as some other characteristic. Then, form a latent vertex for every combination of characteristic values such that every two of the values exist in some known species. In the example shown, there are small brown tailless mice, small silver tailless mice, small brown tailed mice, large brown tailed mice, and large silver tailed mice; the Buneman graph method would form a latent vertex corresponding to an unknown species of small silver tailed mice, because every pairwise combination (small and silver, small and tailed, and silver and tailed) is observed in some other known species. However, the method would not infer the existence of large brown tailless mice, because no mice are known to have both the large and tailless traits. Once the latent vertices are determined, form an edge between every pair of species or latent vertices that differ in a single characteristic.

One can equivalently describe a collection of binary characteristics as a "split system", a family of sets having the property that the complement set of each set in the family is also in the family. This split system has a set for each characteristic value, consisting of the species that have that value. When the latent vertices are included, the resulting split system has the Helly property: every pairwise intersecting subfamily has a common intersection. In some sense median graphs are characterized as coming from Helly split systems: the pairs ("W", "W") defined for each edge "uv" of a median graph form a Helly split system, so if one applies the Buneman graph construction to this system no latent vertices will be needed and the result will be the same as the starting graph.




</doc>
<doc id="247577" url="https://en.wikipedia.org/wiki?curid=247577" title="Graph isomorphism">
Graph isomorphism

In graph theory, an isomorphism of graphs "G" and "H" is a bijection between the vertex sets of "G" and "H"
such that any two vertices "u" and "v" of "G" are adjacent in "G" if and only if "f"("u") and "f"("v") are adjacent in "H". This kind of bijection is commonly described as "edge-preserving bijection", in accordance with the general notion of isomorphism being a structure-preserving bijection

If an isomorphism exists between two graphs, then the graphs are called isomorphic and denoted as formula_2. In the case when the bijection is a mapping of a graph onto itself, i.e., when "G" and "H" are one and the same graph, the bijection is called an automorphism of "G".

Graph isomorphism is an equivalence relation on graphs and as such it partitions the class of all graphs into equivalence classes. A set of graphs isomorphic to each other is called an isomorphism class of graphs.

The two graphs shown below are isomorphic, despite their different looking drawings.

In the above definition, graphs are understood to be uni-directed non-labeled non-weighted graphs. However, the notion of isomorphic may be applied to all other variants of the notion of graph, by adding the requirements to preserve the corresponding additional elements of structure: arc directions, edge weights, etc., with the following exception.

For labeled graphs, two definitions of isomorphism are in use.

Under one definition, an isomorphism is a vertex bijection which is both edge-preserving and label-preserving.

Under another definition, an isomorphism is an edge-preserving vertex bijection which preserves equivalence classes of labels, i.e., vertices with equivalent (e.g., the same) labels are mapped onto the vertices with equivalent labels and vice versa; same with edge labels.

For example, the formula_3 graph with the two vertices labelled with 1 and 2 has a single automorphism under the first definition, but under the second definition there are two auto-morphisms.

The second definition is assumed in certain situations when graphs are endowed with "unique labels" commonly taken from the integer range 1...,"n", where "n" is the number of the vertices of the graph, used only to uniquely identify the vertices. In such cases two labeled graphs are sometimes said to be isomorphic if the corresponding underlying unlabeled graphs are isomorphic (otherwise the definition of isomorphism would be trivial).

The formal notion of "isomorphism", e.g., of "graph isomorphism", captures the informal notion that some objects have "the same structure" if one ignores individual distinctions of "atomic" components of objects in question. Whenever individuality of "atomic" components (vertices and edges, for graphs) is important for correct representation of whatever is modeled by graphs, the model is refined by imposing additional restrictions on the structure, and other mathematical objects are used: digraphs, labeled graphs, colored graphs, rooted trees and so on. The isomorphism relation may also be defined for all these generalizations of graphs: the isomorphism bijection must preserve the elements of structure which define the object type in question: arcs, labels, vertex/edge colors, the root of the rooted tree, etc.

The notion of "graph isomorphism" allows us to distinguish graph properties inherent to the structures of graphs themselves from properties associated with graph representations: graph drawings, data structures for graphs, graph labelings, etc. For example, if a graph has exactly one cycle, then all graphs in its isomorphism class also have exactly one cycle. On the other hand, in the common case when the vertices of a graph are ("represented" by) the integers 1, 2... "N", then the expression
may be different for two isomorphic graphs.

The Whitney graph isomorphism theorem, shown by Hassler Whitney, states that two connected graphs are isomorphic if and only if their line graphs are isomorphic, with a single exception: "K", the complete graph on three vertices, and the complete bipartite graph "K", which are not isomorphic but both have "K" as their line graph. The Whitney graph theorem can be extended to hypergraphs.

While graph isomorphism may be studied in a classical mathematical way, as exemplified by the Whitney theorem, it is recognized that it is a problem to be tackled with an algorithmic approach. The computational problem of determining whether two finite graphs are isomorphic is called the graph isomorphism problem.

Its practical applications include primarily cheminformatics, mathematical chemistry (identification of chemical compounds), and electronic design automation (verification of equivalence of various representations of the design of an electronic circuit).

The graph isomorphism problem is one of few standard problems in computational complexity theory belonging to NP, but not known to belong to either of its well-known (and, if P ≠ NP, disjoint) subsets: P and NP-complete. It is one of only two, out of 12 total, problems listed in whose complexity remains unresolved, the other being integer factorization. It is however known that if the problem is NP-complete then the polynomial hierarchy collapses to a finite level.

In November 2015, László Babai, a mathematician and computer scientist at the University of Chicago, claimed to have proven that the graph isomorphism problem is solvable in quasi-polynomial time. This work has not yet been vetted. In January 2017, Babai briefly retracted the quasi-polynomiality claim and stated a sub-exponential time time complexity bound instead. He restored the original claim five days later.

Its generalization, the subgraph isomorphism problem, is known to be NP-complete.

The main areas of research for the problem are design of fast algorithms and theoretical investigations of its computational complexity, both for the general problem and for special classes of graphs.



</doc>
<doc id="1950766" url="https://en.wikipedia.org/wiki?curid=1950766" title="Graph isomorphism problem">
Graph isomorphism problem

The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.

The problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. It is known that the graph isomorphism problem is in the low hierarchy of class NP, which implies that it is not NP-complete unless the polynomial time hierarchy collapses to its second level. At the same time, isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently.

This problem is a special case of the subgraph isomorphism problem, which asks whether a given graph "G" contains a subgraph that is isomorphic to another given graph "H"; this problem is known to be NP-complete. It is also known to be a special case of the non-abelian hidden subgroup problem over the symmetric group.

In the area of image recognition it is known as the exact graph matching.

The best currently accepted theoretical algorithm is due to , and is based on the earlier work by combined with a "subfactorial" algorithm of V. N. Zemlyachenko . The algorithm has run time 2 for graphs with "n" vertices and relies on the classification of finite simple groups. Without the CFSG theorem, a slightly weaker bound 

In November 2015, Babai announced a quasipolynomial time algorithm for all graphs, that is, one with running time formula_1 for some fixed formula_2. On January 4, 2017, Babai retracted the quasi-polynomial claim and stated a sub-exponential time bound instead after Harald Helfgott discovered a flaw in the proof. On January 9, 2017, Babai announced a correction (published in full on January 19) and restored the quasi-polynomial claim, with Helfgott confirming the fix. Helfgott further claims that one can take , so the running time is . The new proof has not been fully peer-reviewed yet.

There are several competing practical algorithms for graph isomorphism, such as those due to , , and . While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.

The graph isomorphism problem is computationally equivalent to the problem of computing the automorphism group of a graph, and is weaker than the permutation group isomorphism problem and the permutation group intersection problem. For the latter two problems, obtained complexity bounds similar to that for graph isomorphism.

A number of important special cases of the graph isomorphism problem have efficient, polynomial-time solutions:

Since the graph isomorphism problem is neither known to be NP-complete nor known to be tractable, researchers have sought to gain insight into the problem by defining a new class GI, the set of problems with a polynomial-time Turing reduction to the graph isomorphism problem. If in fact the graph isomorphism problem is solvable in polynomial time, GI would equal P.

As is common for complexity classes within the polynomial time hierarchy, a problem is called GI-hard if there is a polynomial-time Turing reduction from any problem in GI to that problem, i.e., a polynomial-time solution to a GI-hard problem would yield a polynomial-time solution to the graph isomorphism problem (and so all problems in GI). A problem formula_3 is called complete for GI, or GI-complete, if it is both GI-hard and a polynomial-time solution to the GI problem would yield a polynomial-time solution to formula_3.

The graph isomorphism problem is contained in both NP and co-AM. GI is contained in and low for Parity P, as well as contained in the potentially much smaller class SPP. That it lies in Parity P means that the graph isomorphism problem is no harder than determining whether a polynomial-time nondeterministic Turing machine has an even or odd number of accepting paths. GI is also contained in and low for ZPP. This essentially means that an efficient Las Vegas algorithm with access to an NP oracle can solve graph isomorphism so easily that it gains no power from being given the ability to do so in constant time.

There are a number of classes of mathematical objects for which the problem of isomorphism is a GI-complete problem. A number of them are graphs endowed with additional properties or restrictions:
A class of graphs is called GI-complete if recognition of isomorphism for graphs from this subclass is a GI-complete problem. The following classes are GI-complete:

Many classes of digraphs are also GI-complete.

There are other nontrivial GI-complete problems in addition to isomorphism problems.


 have shown a probabilistic checker for programs for graph isomorphism. Suppose "P" is a claimed polynomial-time procedure that checks if two graphs are isomorphic, but it is not trusted. To check if "G" and "H" are isomorphic:


This procedure is polynomial-time and gives the correct answer if "P" is a correct program for graph isomorphism. If "P" is not a correct program, but answers correctly on "G" and "H", the checker will either give the correct answer, or detect invalid behaviour of "P".
If "P" is not a correct program, and answers incorrectly on "G" and "H", the checker will detect invalid behaviour of "P" with high probability, or answer wrong with probability 2.

Notably, "P" is used only as a blackbox.

Graphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching, i.e., identification of similarities between graphs, is an important tools in these areas. In these areas graph isomorphism problem is known as the exact graph matching.

In cheminformatics and in mathematical chemistry, graph isomorphism testing is used to identify a chemical compound within a chemical database. Also, in organic mathematical chemistry graph isomorphism testing is useful for generation of molecular graphs and for computer synthesis.

Chemical database search is an example of graphical data mining, where the graph canonization approach is often used. In particular, a number of identifiers for chemical substances, such as SMILES and InChI, designed to provide a standard and human-readable way to encode molecular information and to facilitate the search for such information in databases and on the web, use canonization step in their computation, which is essentially the canonization of the graph which represents the molecule.

In electronic design automation graph isomorphism is the basis of the Layout Versus Schematic (LVS) circuit design step, which is a verification whether the electric circuits represented by a circuit schematic and an integrated circuit layout are the same.






</doc>
<doc id="20199287" url="https://en.wikipedia.org/wiki?curid=20199287" title="Graph canonization">
Graph canonization

In graph theory, a branch of mathematics, graph canonization is the problem finding a canonical form of a given graph "G". A canonical form is a labeled graph Canon("G") that is isomorphic to "G", such that every graph that is isomorphic to "G" has the same canonical form as "G". Thus, from a solution to the graph canonization problem, one could also solve the problem of graph isomorphism: to test whether two graphs "G" and "H" are isomorphic, compute their canonical forms Canon("G") and Canon("H"), and test whether these two canonical forms are identical.

The canonical form of a graph is an example of a complete graph invariant: every two isomorphic graphs have the same canonical form, and every two non-isomorphic graphs have different canonical forms. Conversely, every complete invariant of graphs may be used to construct a canonical form. The vertex set of an "n"-vertex graph may be identified with the integers from 1 to "n", and using such an identification a canonical form of a graph may also be described as a permutation of its vertices. Canonical forms of a graph are also called canonical labelings, and graph canonization is also sometimes known as graph canonicalization.

Clearly, the graph canonization problem is at least as computationally hard as the graph isomorphism problem. In fact, graph isomorphism is even AC-reducible to graph canonization. However it is still an open question whether the two problems are polynomial time equivalent.

While the existence of (deterministic) polynomial algorithms for graph isomorphism is still an open problem in computational complexity theory, in 1977 László Babai reported that with probability at least 1 − exp(−O("n")), a simple vertex classification algorithm produces a canonical labeling of a graph chosen uniformly at random from the set of all "n"-vertex graphs after only two refinement steps. Small modifications and an added depth-first search step produce canonical labeling of such uniformly-chosen random graphs in linear expected time. This result sheds some light on the fact why many reported graph isomorphism algorithms behave well in practice. This was an important breakthrough in probabilistic complexity theory which became widely known in its manuscript form and which was still cited as an "unpublished manuscript" long after it was reported at a symposium.

A commonly known canonical form is the lexicographically smallest graph within the isomorphism class, which is the graph of the class with lexicographically smallest adjacency matrix considered as a linear string.
However, the computation of the lexicographically smallest graph is NP-hard.

For trees, a concise polynomial canonization algorithm requiring O(n) space is presented by . Begin by labeling each vertex with the string 01. Iteratively for each non-leaf x remove the leading 0 and trailing 1 from x's label; then sort x's label along with the labels of all adjacent leaves in lexicographic order. Concatenate these sorted labels, add back a leading 0 and trailing 1, make this the new label of x, and delete the adjacent leaves. If there are two vertices remaining, concatenate their labels in lexicographic order.

Graph canonization is the essence of many graph isomorphism algorithms. One of the leading tools is Nauty.

A common application of graph canonization is in graphical data mining, in particular in chemical database applications.

A number of identifiers for chemical substances, such as SMILES and InChI use canonicalization steps in their computation, which is essentially the canonicalization of the graph which represents the molecule.

These identifiers are designed to provide a standard (and sometimes human-readable) way to encode molecular information and to facilitate the search for such information in databases and on the web.


</doc>
<doc id="450062" url="https://en.wikipedia.org/wiki?curid=450062" title="Subgraph isomorphism problem">
Subgraph isomorphism problem

In theoretical computer science, the subgraph isomorphism problem is a computational task in which two graphs "G" and "H" are given as input, and one must determine whether "G" contains a subgraph that is isomorphic to "H".
Subgraph isomorphism is a generalization of both the maximum clique problem and the problem of testing whether a graph contains a Hamiltonian cycle, and is therefore NP-complete. However certain other cases of subgraph isomorphism may be solved in polynomial time.

Sometimes the name subgraph matching is also used for the same problem. This name puts emphasis on finding such a subgraph as opposed to the bare decision problem.

To prove subgraph isomorphism is NP-complete, it must be formulated as a decision problem. The input to the decision problem is a pair of graphs "G" and "H". The answer to the problem is positive if "H" is isomorphic to a subgraph of "G", and negative otherwise.

Formal question:

Let formula_1, formula_2 be graphs. Is there a subgraph formula_3 such that formula_4? I. e., does there exist a bijection formula_5 such that formula_6?

The proof of subgraph isomorphism being NP-complete is simple and based on reduction of the clique problem, an NP-complete decision problem in which the input is a single graph "G" and a number "k", and the question is whether "G" contains a complete subgraph with "k" vertices. To translate this to a subgraph isomorphism problem, simply let "H" be the complete graph "K"; then the answer to the subgraph isomorphism problem for "G" and "H" is equal to the answer to the clique problem for "G" and "k". Since the clique problem is NP-complete, this polynomial-time many-one reduction shows that subgraph isomorphism is also NP-complete.

An alternative reduction from the Hamiltonian cycle problem translates a graph "G" which is to be tested for Hamiltonicity into the pair of graphs "G" and "H", where "H" is a cycle having the same number of vertices as "G". Because the Hamiltonian cycle problem is NP-complete even for planar graphs, this shows that subgraph isomorphism remains NP-complete even in the planar case.

Subgraph isomorphism is a generalization of the graph isomorphism problem, which asks whether "G" is isomorphic to "H": the answer to the graph isomorphism problem is true if and only if "G" and "H" both have the same numbers of vertices and edges and the subgraph isomorphism problem for "G" and "H" is true. However the complexity-theoretic status of graph isomorphism remains an open question.

In the context of the Aanderaa–Karp–Rosenberg conjecture on the query complexity of monotone graph properties, showed that any subgraph isomorphism problem has query complexity Ω("n"); that is, solving the subgraph isomorphism requires an algorithm to check the presence or absence in the input of Ω("n") different edges in the graph.

 describes a recursive backtracking procedure for solving the subgraph isomorphism problem. Although its running time is, in general, exponential, it takes polynomial time for any fixed choice of "H" (with a polynomial that depends on the choice of "H"). When "G" is a planar graph (or more generally a graph of bounded expansion) and "H" is fixed, the running time of subgraph isomorphism can be reduced to linear time.

As subgraph isomorphism has been applied in the area of cheminformatics to find similarities between chemical compounds from their structural formula; often in this area the term substructure search is used. A query structure is often defined graphically using a structure editor program; SMILES based database systems typically define queries using SMARTS, a SMILES extension.

The closely related problem of counting the number of isomorphic copies of a graph "H" in a larger graph "G" has been applied to pattern discovery in databases, the bioinformatics of protein-protein interaction networks, and in exponential random graph methods for mathematically modeling social networks.

The problem is also of interest in artificial intelligence, where it is considered part of an array of pattern matching in graphs problems; an extension of subgraph isomorphism known as graph mining is also of interest in that area.




</doc>
<doc id="22469695" url="https://en.wikipedia.org/wiki?curid=22469695" title="Color-coding">
Color-coding

In computer science and graph theory, the term color-coding refers to an algorithmic technique which is useful in the discovery of network motifs. For example, it can be used to detect a simple path of length in a given graph. The traditional color-coding algorithm is probabilistic, but it can be derandomized without much overhead in the running time. 

Color-coding also applies to the detection of cycles of a given length, and more generally it applies to the subgraph isomorphism problem (an NP-complete problem), where it yields polynomial time algorithms when the subgraph pattern that it is trying to detect has bounded treewidth.

The color-coding method was proposed and analyzed in 1994 by Noga Alon, Raphael Yuster, and Uri Zwick.

The following results can be obtained through the method of color-coding:


To solve the problem of finding a subgraph formula_3 in a given graph , where can be a path, a cycle, or any bounded treewidth graph where formula_4, the method of color-coding begins by randomly coloring each vertex of with formula_5 colors, and then tries to find a colorful copy of in colored . Here, a graph is colorful if every vertex in it is colored with a distinct color. This method works by repeating (1) random coloring a graph and (2) finding colorful copy of the target subgraph, and eventually the target subgraph can be found if the process is repeated a sufficient number of times.

Suppose a copy of in becomes colorful with some non-zero probability . It immediately follows that if the random coloring is repeated times, then this copy is expected to become colorful once. Note that though is small, it is shown that if formula_4, is only polynomially small. Suppose again there exists an algorithm such that, given a graph and a coloring which maps each vertex of to one of the colors, it finds a copy of colorful , if one exists, within some runtime . Then the expected time to find a copy of in , if one exists, is formula_7.

Sometimes it is also desirable to use a more restricted version of colorfulness. For example, in the context of finding cycles in planar graphs, it is possible to develop an algorithm that finds well-colored cycles. Here, a cycle is well-colored if its vertices are colored by consecutive colors.

An example would be finding a simple cycle of length in graph .

By applying random coloring method, each simple cycle has a probability of formula_8 to become colorful, since there are formula_9 ways of coloring the vertices on the cycle, among which there are formula_10 colorful occurrences. Then an algorithm (described next) can be used to find colorful cycles in the randomly colored graph in time formula_1, where formula_12 is the matrix multiplication constant. Therefore, it takes formula_13 overall time to find a simple cycle of length in .

The colorful cycle-finding algorithm works by first finding all pairs of vertices in that are connected by a simple path of length , and then checking whether the two vertices in each pair are connected. Given a coloring function to color graph , enumerate all partitions of the color set into two subsets of size formula_14 each. Note that can be divided into and accordingly, and let and denote the subgraphs induced by and respectively. Then, recursively find colorful paths of length formula_15 in each of and . Suppose the boolean matrix and represent the connectivity of each pair of vertices in and by a colorful path, respectively, and let be the matrix describing the adjacency relations between vertices of and those of , the boolean product formula_16 gives all pairs of vertices in that are connected by a colorful path of length . Thus, the recursive relation of matrix multiplications is formula_17, which yields a runtime of formula_18. Although this algorithm finds only the end points of the colorful path, another algorithm by Alon and Naor that finds colorful paths themselves can be incorporated into it.

The derandomization of color-coding involves enumerating possible colorings of a graph , such that the randomness of coloring is no longer required. For the target subgraph in to be discoverable, the enumeration has to include at least one instance where the is colorful. To achieve this, enumerating a -perfect family of hash functions from to is sufficient. By definition, is -perfect if for every subset of where formula_19, there exists a hash function in such that is perfect. In other words, there must exist a hash function in that colors any given vertices with distinct colors.

There are several approaches to construct such a -perfect hash family:


In the case of derandomizing well-coloring, where each vertex on the subgraph is colored consecutively, a -perfect family of hash functions from to is needed. A sufficient -perfect family which maps from to can be constructed in a way similar to the approach 3 above (the first step). In particular, it is done by using random bits that are almost independent, and the size of the resulting -perfect family will be formula_25.

The derandomization of color-coding method can be easily parallelized, yielding efficient NC algorithms.

Recently, color-coding has attracted much attention in the field of bioinformatics. One example is the detection of signaling pathways in protein-protein interaction (PPI) networks. Another example is to discover and to count the number of motifs in PPI networks. Studying both signaling pathways and motifs allows a deeper understanding of the similarities and differences of many biological functions, processes, and structures among organisms.

Due to the huge amount of gene data that can be collected, searching for pathways or motifs can be highly time consuming. However, by exploiting the color-coding method, the motifs or signaling pathways with formula_26 vertices in a network with vertices can be found very efficiently in polynomial time. Thus, this enables us to explore more complex or larger structures in PPI networks.



</doc>
<doc id="6892618" url="https://en.wikipedia.org/wiki?curid=6892618" title="Induced subgraph isomorphism problem">
Induced subgraph isomorphism problem

In complexity theory and graph theory, induced subgraph isomorphism is an NP-complete decision problem that involves finding a given graph as an induced subgraph of a larger graph.

Formally, the problem takes as input two graphs "G"=("V", "E") and "G"=("V", "E"), where the number of vertices in "V" can be assumed to be less than or equal to the number of vertices in "V". "G" is isomorphic to an induced subgraph of "G" if there is an injective function "f" which maps the vertices of "G" to vertices of "G" such that for all pairs of vertices "x", "y" in "V", edge ("x", "y") is in "E" if and only if the edge ("f"("x"), "f"("y")) is in "E". The answer to the decision problem is yes if this function "f" exists, and no otherwise.

This is different from the subgraph isomorphism problem in that the absence of an edge in "G" implies that the corresponding edge in "G" must also be absent. In subgraph isomorphism, these "extra" edges in "G" may be present.

The complexity of induced subgraph isomorphism separates outerplanar graphs from their generalization series-parallel graphs: it may be solved in polynomial time for 2-connected outerplanar graphs, but is NP-complete for 2-connected series-parallel graphs.

The special case of finding a long path as an induced subgraph of a hypercube has been particularly well-studied, and is called the snake-in-the-box problem. The maximum independent set problem is also an induced subgraph isomorphism problem in which one seeks to find a large independent set as an induced subgraph of a larger graph, and the maximum clique problem is an induced subgraph isomorphism problem in which one seeks to find a large clique graph as an induced subgraph of a larger graph.

Although the induced subgraph isomorphism problem seems only slightly different from the subgraph isomorphism problem, the "induced" restriction introduces changes large enough that we can witness differences from a computational complexity point of view. 

For example, the subgraph isomorphism problem is NP-complete on connected proper interval graphs and on connected bipartite permutation graphs, but the induced subgraph isomorphism problem can be solved in polynomial time on these two classes.

Moreover, the induced subtree isomorphism problem (i.e. the induced subgraph isomorphism problem where "G" is restricted to be a tree) can be solved in polynomial time on interval graphs, while the subtree isomorphism problem is NP-complete on proper interval graphs.


</doc>
<doc id="4288963" url="https://en.wikipedia.org/wiki?curid=4288963" title="Maximum common induced subgraph">
Maximum common induced subgraph

In graph theory and theoretical computer science, a maximum common induced subgraph of two graphs "G" and "H" is a graph that is an induced subgraph of both "G" and "H",
and that has as many vertices as possible.

Finding this graph is NP-hard.
In the associated decision problem, the input is two graphs "G" and "H" and a number "k". The problem is to decide whether "G" and "H" have a common induced subgraph with at least "k" vertices. This problem is NP-complete. It is a generalization of the induced subgraph isomorphism problem, which arises when "k" equals the number of vertices in the smaller of "G" and "H", so that this entire graph must appear as an induced subgraph of the other graph.

Based on hardness of approximation results for the maximum independent set problem, the maximum common induced subgraph problem is also hard to approximate. This implies that, unless P = NP, there is no approximation algorithm that, in polynomial time on formula_1-vertex graphs, always finds a solution within a factor of formula_2 of optimal, for any formula_3.

One possible solution for this problem is to build a modular product graph of "G" and "H".
In this graph, the largest clique corresponds to a maximum common induced subgraph of "G" and "H". Therefore, algorithms for finding maximum cliques can be used to find the maximum common induced subgraph.

Maximum common induced subgraph algorithms have a long tradition in cheminformatics and pharmacophore mapping.



</doc>
<doc id="36037560" url="https://en.wikipedia.org/wiki?curid=36037560" title="Maximum common edge subgraph">
Maximum common edge subgraph

Given two graphs formula_1 and formula_2, the maximum common edge subgraph problem is the problem of finding a graph formula_3 with as many edges as possible which is isomorphic to both a subgraph of formula_1 and a subgraph of formula_2.

The maximum common edge subgraph problem on general graphs is NP-complete as it is a generalization of subgraph isomorphism: a graph formula_3 is isomorphic to a subgraph of another graph formula_1 if and only if the maximum common edge subgraph of formula_1 and formula_3 has the same number of edges as formula_3. Unless the two inputs formula_1 and formula_2 to the maximum common edge subgraph problem are required to have the same number of vertices, the problem is APX-hard.



</doc>
<doc id="11973947" url="https://en.wikipedia.org/wiki?curid=11973947" title="Graph partition">
Graph partition

In mathematics, a graph partition is the reduction of a graph to a smaller graph by partitioning its nodes into mutually exclusive groups. Edges of the original graph that cross between the groups will produce edges in the partitioned graph. If the number of resulting edges is small compared to the original graph, then the partitioned graph may be better suited for analysis and problem-solving than the original. Finding a partition that simplifies graph analysis is a hard problem, but one that has applications to scientific computing, VLSI circuit design, and task scheduling in multiprocessor computers, among others. Recently, the graph partition problem has gained importance due to its application for clustering and detection of cliques in social, pathological and biological networks. For a survey on recent trends in computational methods and applications see .

Typically, graph partition problems fall under the category of NP-hard problems. Solutions to these problems are generally derived using heuristics and approximation algorithms. However, uniform graph partitioning or a balanced graph partition problem can be shown to be NP-complete to approximate within any finite factor. Even for special graph classes such as trees and grids, no reasonable approximation algorithms exist, unless P=NP. Grids are a particularly interesting case since they model the graphs resulting from Finite Element Model (FEM) simulations. When not only the number of edges between the components is approximated, but also the sizes of the components, it can be shown that no reasonable fully polynomial algorithms exist for these graphs.

Consider a graph "G" = ("V", "E"), where "V" denotes the set of "n" vertices and "E" the set of edges. For a ("k","v") balanced partition problem, the objective is to partition "G" into "k" components of at most size "v" · ("n"/"k"), while minimizing the capacity of the edges between separate components. Also, given "G" and an integer "k" > 1, partition "V" into "k" parts (subsets) "V", "V", ..., "V" such that the parts are disjoint and have equal size, and the number of edges with endpoints in different parts is minimized. Such partition problems have been discussed in literature as bicriteria-approximation or resource augmentation approaches. A common extension is to hypergraphs, where an edge can connect more than two vertices. A hyperedge is not cut if all vertices are in one partition, and cut exactly once otherwise, no matter how many vertices are on each side. This usage is common in electronic design automation.

For a specific ("k", 1 + "ε") balanced partition problem, we seek to find a minimum cost partition of "G" into "k" components with each component containing a maximum of (1 + "ε")·("n"/"k") nodes. We compare the cost of this approximation algorithm to the cost of a ("k",1) cut, wherein each of the "k" components must have the same size of ("n"/"k") nodes each, thus being a more restricted problem. Thus,

We already know that (2,1) cut is the minimum bisection problem and it is NP-complete. Next, we assess a 3-partition problem wherein "n" = 3"k", which is also bounded in polynomial time. Now, if we assume that we have a finite approximation algorithm for ("k", 1)-balanced partition, then, either the 3-partition instance can be solved using the balanced ("k",1) partition in "G" or it cannot be solved. If the 3-partition instance can be solved, then ("k", 1)-balanced partitioning problem in "G" can be solved without cutting any edge. Otherwise, if the 3-partition instance cannot be solved, the optimum ("k", 1)-balanced partitioning in "G" will cut at least one edge. An approximation algorithm with a finite approximation factor has to differentiate between these two cases. Hence, it can solve the 3-partition problem which is a contradiction under the assumption that "P" = "NP". Thus, it is evident that ("k",1)-balanced partitioning problem has no polynomial-time approximation algorithm with a finite approximation factor unless "P" = "NP".

The planar separator theorem states that any "n"-vertex planar graph can be partitioned into roughly equal parts by the removal of O() vertices. This is not a partition in the sense described above, because the partition set consists of vertices rather than edges. However, the same result also implies that every planar graph of bounded degree has a balanced cut with O() edges.

Since graph partitioning is a hard problem, practical solutions are based on heuristics. There are two broad categories of methods, local and global. Well known local methods are the Kernighan–Lin algorithm, and Fiduccia-Mattheyses algorithms, which were the first effective 2-way cuts by local search strategies. Their major drawback is the arbitrary initial partitioning of the vertex set, which can affect the final solution quality. Global approaches rely on properties of the entire graph and do not rely on an arbitrary initial partition. The most common example is spectral partitioning, where a partition is derived from approximate eigenvectors of the adjacency matrix, or spectral clustering that groups graph vertices using the eigendecomposition of the graph Laplacian matrix.

A multi-level graph partitioning algorithm works by applying one or more stages. Each stage reduces the size of
the graph by collapsing vertices and edges, partitions the smaller graph, then maps back and refines this partition of the original graph. A wide variety of partitioning and refinement methods can be applied within the overall multi-level scheme. In many cases, this approach can give both fast execution times and very high quality results. 
One widely used example of such an approach is METIS, a graph partitioner, and hMETIS, the corresponding partitioner for hypergraphs.
An alternative approach originated from 
and implemented, e.g., in scikit-learn is spectral clustering with the partitioning determined from eigenvectors of the graph Laplacian matrix for the original graph computed by LOBPCG solver with multigrid preconditioning.

Given a graph formula_2 with adjacency matrix formula_3, where an entry formula_4 implies an edge between node formula_5 and formula_6, and degree matrix formula_7, which is a diagonal matrix, where each diagonal entry of a row formula_5, formula_9, represents the node degree of node formula_5. The Laplacian matrix formula_11 is defined as formula_12. Now, a ratio-cut partition for graph formula_13 is defined as a partition of formula_14 into disjoint formula_15, and formula_16, minimizing the ratio
of the number of edges that actually cross this cut to the number of pairs of vertices that could support such edges. Spectral graph partitioning can be motivated by analogy with partitioning of a vibrating string or a mass-spring system.

In such a scenario, the second smallest eigenvalue (formula_18) of formula_11, yields a "lower bound" on the optimal cost (formula_20) of ratio-cut partition with formula_21. The eigenvector (formula_22) corresponding to formula_18, called the "Fiedler vector", bisects the graph into only two communities based on the "sign of the corresponding vector entry". Division into a larger number of communities can be achieved by repeated "bisection" or by using "multiple eigenvectors" corresponding to the smallest eigenvalues. The examples in Figures 1,2 illustrate the spectral bisection approach.

Minimum cut partitioning however fails when the number of communities to be partitioned, or the partition sizes are unknown. For instance, optimizing the cut size for free group sizes puts all vertices in the same community. Additionally, cut size may be the wrong thing to minimize since a good division is not just one with small number of edges between communities. This motivated the use of Modularity (Q) as a metric to optimize a balanced graph partition. The example in Figure 3 illustrates 2 instances of the same graph such that in "(a)" modularity (Q) is the partitioning metric and in "(b)", ratio-cut is the partitioning metric.

Another objective function used for graph partitioning is Conductance which is the ratio between the number of cut edges and the volume of the smallest part. Conductance is related to electrical flows and random walks. The Cheeger bound guarantees that spectral bisection provides partitions with nearly optimal conductance. The quality of this approximation depends on the second smallest eigenvalue of the Laplacian λ.

Spin models have been used for clustering of multivariate data wherein similarities are translated into coupling strengths. The properties of ground state spin configuration can be directly interpreted as communities. Thus, a graph is partitioned to minimize the Hamiltonian of the partitioned graph. The Hamiltonian (H) is derived by assigning the following partition rewards and penalties.

Additionally, Kernel-PCA-based Spectral clustering takes a form of least squares Support Vector Machine framework, and hence it becomes possible to project the data entries to a kernel induced feature space that has maximal variance, thus implying a high separation between the projected communities.

Some methods express graph partitioning as a multi-criteria optimization problem which can be solved using local methods expressed in a game theoretic framework where each node makes a decision on the partition it chooses.

For very large-scale distributed graphs classical partition methods might not apply (e.g., spectral partitioning, Metis) since they require full access to graph data in order to perform global operations. For such large-scale scenarios distributed graph partitioning is used to perform partitioning through asynchronous local operations only (e.g., Ja-be-ja, Stad algorithms based on Gossip protocol).

Scikit-learn implements spectral clustering with the partitioning determined from eigenvectors of the graph Laplacian matrix for the original graph computed by ARPACK, or by LOBPCG solver with multigrid preconditioning.

Chaco, due to Hendrickson and Leland, implements the multilevel approach outlined above and basic local search algorithms. 
Moreover, they implement spectral partitioning techniques.

METIS is a graph partitioning family by Karypis and Kumar. Among this family, kMetis aims at greater partitioning speed, hMetis, applies to hypergraphs and aims at partition quality, and ParMetis is a parallel implementation of the Metis graph partitioning algorithm.

PaToH is another hypergraph partitioner.

KaHyPar is a multilevel hypergraph partitioning framework providing direct k-way and recursive bisection based partitioning algorithms. It instantiates the multilevel approach in its most extreme version, removing only a single vertex in every level of the hierarchy. By using this very fine grained "n"-level approach combined with strong local search heuristics, it computes solutions of very high quality.

Scotch is graph partitioning framework by Pellegrini. It uses recursive multilevel bisection and includes sequential as well as parallel partitioning techniques.

Jostle is a sequential and parallel graph partitioning solver developed by Chris Walshaw. 
The commercialized version of this partitioner is known as NetWorks.

Party implements the Bubble/shape-optimized framework and the Helpful Sets algorithm.

The software packages DibaP and its MPI-parallel variant PDibaP by Meyerhenke implement the Bubble framework using diffusion; DibaP also uses AMG-based techniques for coarsening and solving linear systems arising in the diffusive approach.

Sanders and Schulz released a graph partitioning package KaHIP (Karlsruhe High Quality Partitioning) that implements for example flow-based methods, more-localized local searches and several parallel and sequential meta-heuristics.

The tools Parkway by Trifunovic and
Knottenbelt as well as Zoltan by Devine et al. focus on hypergraph
partitioning.

List of free open-source frameworks:



</doc>
<doc id="23174224" url="https://en.wikipedia.org/wiki?curid=23174224" title="Kernighan–Lin algorithm">
Kernighan–Lin algorithm

The Kernighan–Lin algorithm is a heuristic algorithm for finding partitions of graphs.
The algorithm has important applications in the layout of digital circuits and components in VLSI.

The input to the algorithm is an undirected graph with vertex set , edge set , and (optionally) numerical weights on the edges in . The goal of the algorithm is to partition into two disjoint subsets and of equal (or nearly equal) size, in a way that minimizes the sum of the weights of the subset of edges that cross from to . If the graph is unweighted, then instead the goal is to minimize the number of crossing edges; this is equivalent to assigning weight one to each edge. The algorithm maintains and improves a partition, in each pass using a greedy algorithm to pair up vertices of with vertices of , so that moving the paired vertices from one side of the partition to the other will improve the partition. After matching the vertices, it then performs a subset of the pairs chosen to have the best overall effect on the solution quality .
Given a graph with vertices, each pass of the algorithm runs in time . 

In more detail, for each formula_1, let formula_2 be the "internal cost" of "a", that is, the sum of the costs of edges between "a" and other nodes in "A", and let formula_3 be the "external cost" of "a", that is, the sum of the costs of edges between "a" and nodes in "B". Similarly, define formula_4, formula_5 for each formula_6. Furthermore, let 
be the difference between the external and internal costs of "s". If "a" and "b" are interchanged, then the reduction in cost is
where formula_9 is the cost of the possible edge between "a" and "b".

The algorithm attempts to find an optimal series of interchange operations between elements of formula_10 and formula_11 which maximizes formula_12 and then executes the operations, producing a partition of the graph to "A" and "B".

See 



</doc>
<doc id="159023" url="https://en.wikipedia.org/wiki?curid=159023" title="Tree decomposition">
Tree decomposition

In graph theory, a tree decomposition is a mapping of a graph into a tree that can be used to define the treewidth of the graph and speed up solving certain computational problems on the graph.

In machine learning, tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition.

The concept of tree decompositions was originally introduced by . Later it was rediscovered by and has since been studied by many other authors.

Intuitively, a tree decomposition represents the vertices of a given graph "G" as subtrees of a tree, in such a way that vertices in the given graph are adjacent only when the corresponding subtrees intersect. Thus, "G" forms a subgraph of the intersection graph of the subtrees. The full intersection graph is a chordal graph.

Each subtree associates a graph vertex with a set of tree nodes. To define this formally, we represent each tree node as the set of vertices associated with it.
Thus, given a graph "G" = ("V", "E"), a tree decomposition is a pair ("X", "T"), where "X" = {"X", ..., "X"} is a family of subsets (sometimes called "bags") of "V", and "T" is a tree whose nodes are the subsets "X", satisfying the following properties:


The tree decomposition of a graph is far from unique; for example, a trivial tree decomposition contains all vertices of the graph in its single root node.

A tree decomposition in which the underlying tree is a path graph is called a path decomposition, and the width parameter derived from these special types of tree decompositions is known as pathwidth.

A tree decomposition ("X", "T" = ("I", "F")) of treewidth "k" is "smooth", if for all formula_8, and for all formula_9.

The minimum number of trees in a tree decomposition is the tree number of "G."

The "width" of a tree decomposition is the size of its largest set "X" minus one. The treewidth tw("G") of a graph "G" is the minimum width among all possible tree decompositions of "G". In this definition, the size of the largest set is diminished by one in order to make the treewidth of a tree equal to one. Treewidth may also be defined from other structures than tree decompositions, including chordal graphs, brambles, and havens.

It is NP-complete to determine whether a given graph "G" has treewidth at most a given variable "k".
However, when "k" is any fixed constant, the graphs with treewidth "k" can be recognized, and a width "k" tree decomposition constructed for them, in linear time. The time dependence of this algorithm on "k" is exponential.

At the beginning of the 1970s, it was observed that a large class of combinatorial optimization problems defined on graphs could be efficiently solved by non-serial dynamic programming as long as the graph had a bounded "dimension", a parameter related to treewidth. Later, several authors independently observed, at the end of the 1980s, that many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently by dynamic programming for graphs of bounded treewidth, using the tree-decompositions of these graphs.

As an example, consider the problem of finding the maximum independent set in a graph of treewidth "k". To solve this problem, first choose one of the nodes of the tree decomposition to be the root, arbitrarily. For a node "X" of the tree decomposition, let "D" be the union of the sets "X" descending from "X". For an independent set "S" ⊂ "X", let "A"("S","i") denote the size of the largest independent subset "I" of "D" such that "I" ∩ "X" = "S". Similarly, for an adjacent pair of nodes "X" and "X", with "X" farther from the root of the tree than "X", and an independent set "S" ⊂ "X" ∩ "X", let "B"("S","i","j") denote the size of the largest independent subset "I" of "D" such that "I" ∩ "X" ∩ "X" = "S". We may calculate these "A" and "B" values by a bottom-up traversal of the tree:
where the sum in the calculation of formula_12 is over the children of node formula_1.

At each node or edge, there are at most 2 sets "S" for which we need to calculate these values, so if "k" is a constant then the whole calculation takes constant time per edge or node. The size of the maximum independent set is the largest value stored at the root node, and the maximum independent set itself can be found (as is standard in dynamic programming algorithms) by backtracking through these stored values starting from this largest value. Thus, in graphs of bounded treewidth, the maximum independent set problem may be solved in linear time. Similar algorithms apply to many other graph problems.

This dynamic programming approach is used in machine learning via the junction tree algorithm for belief propagation in graphs of bounded treewidth. It also plays a key role in algorithms for computing the treewidth and constructing tree decompositions: typically, such algorithms have a first step that approximates the treewidth, constructing a tree decomposition with this approximate width, and then a second step that performs dynamic programming in the approximate tree decomposition to compute the exact value of the treewidth.




</doc>
<doc id="16823137" url="https://en.wikipedia.org/wiki?curid=16823137" title="Branch-decomposition">
Branch-decomposition

In graph theory, a branch-decomposition of an undirected graph "G" is a hierarchical clustering of the edges of "G", represented by an unrooted binary tree "T" with the edges of "G" as its leaves. Removing any edge from "T" partitions the edges of "G" into two subgraphs, and the width of the decomposition is the maximum number of shared vertices of any pair of subgraphs formed in this way. 
The branchwidth of "G" is the minimum width of any branch-decomposition of "G".

Branchwidth is closely related to tree-width: for all graphs, both of these numbers are within a constant factor of each other, and both quantities may be characterized by forbidden minors. And as with treewidth, many graph optimization problems may be solved efficiently for graphs of small branchwidth. However, unlike treewidth, the branchwidth of planar graphs may be computed exactly, in polynomial time. Branch-decompositions and branchwidth may also be generalized from graphs to matroids.

An unrooted binary tree is a connected undirected graph with no cycles in which each non-leaf node has exactly three neighbors. A branch-decomposition may be represented by an unrooted binary tree "T", together with a bijection between the leaves of "T" and the edges of the given graph "G" = ("V","E").
If "e" is any edge of the tree "T", then removing "e" from "T" partitions it into two subtrees "T" and "T". This partition of "T" into subtrees induces a partition of the edges associated with the leaves of "T" into two subgraphs "G" and "G" of "G". This partition of "G" into two subgraphs is called an e-separation.

The width of an e-separation is the number of vertices of "G" that are incident both to an edge of "E" and to an edge of "E"; that is, it is the number of vertices that are shared by the two subgraphs "G" and "G". The width of the branch-decomposition is the maximum width of any of its e-separations. The branchwidth of "G" is the minimum width of a branch-decomposition of "G".

Branch-decompositions of graphs are closely related to tree decompositions, and branch-width is closely related to tree-width: the two quantities are always within a constant factor of each other. In particular, in the paper in which they introduced branch-width, Neil Robertson and Paul Seymour showed that for a graph "G"
with tree-width "k" and branchwidth 

Carving width is a concept defined similarly to branch width, except with edges replaced by vertices and vice versa. A carving decomposition is an unrooted binary tree with each leaf representing a vertex in the original graph, and the width of a cut is the number (or total weight in a weighted graph) of edges that are incident to a vertex in both subtrees.

Branch width algorithms typically work by reducing to an equivalent carving width problem. In particular, the carving width of the medial graph of a planar graph is exactly twice the branch width of the original graph.

It is NP-complete to determine whether a graph "G" has a branch-decomposition of width at most "k", when "G" and "k" are both considered as inputs to the problem. However, the graphs with branchwidth at most "k" form a minor-closed family of graphs, from which it follows that computing the branchwidth is fixed-parameter tractable: there is an algorithm for computing optimal branch-decompositions whose running time, on graphs of branchwidth "k" for any fixed constant "k", is linear in the size of the input graph.

For planar graphs, the branchwidth can be computed exactly in polynomial time. This in contrast to treewidth for which the complexity on planar graphs is a well known open problem. The original algorithm for planar branchwidth, by Paul Seymour and Robin Thomas, took time O("n") on graphs with "n" vertices, and their algorithm for constructing a branch decomposition of this width took time O("n"). This was later sped up to O("n").

As with treewidth, branchwidth can be used as the basis of dynamic programming algorithms for many NP-hard optimization problems, using an amount of time that is exponential in the width of the input graph or matroid. For instance, apply branchwidth-based dynamic programming to a problem of merging multiple partial solutions to the travelling salesman problem into a single global solution, by forming a sparse graph from the union of the partial solutions, using a spectral clustering heuristic to find a good branch-decomposition of this graph, and applying dynamic programming to the decomposition. argue that branchwidth works better than treewidth in the development of fixed-parameter-tractable algorithms on planar graphs, for multiple reasons: branchwidth may be more tightly bounded by a function of the parameter of interest than the bounds on treewidth, it can be computed exactly in polynomial time rather than merely approximated, and the algorithm for computing it has no large hidden constants.

It is also possible to define a notion of branch-decomposition for matroids that generalizes branch-decompositions of graphs. A branch-decomposition of a matroid is a hierarchical clustering of the matroid elements, represented as an unrooted binary tree with the elements of the matroid at its leaves. An e-separation may be defined in the same way as for graphs, and results in a partition of the set "M" of matroid elements into two subsets "A" and "B". If ρ denotes the rank function of the matroid, then the width of an e-separation is defined as , and the width of the decomposition and the branchwidth of the matroid are defined analogously. The branchwidth of a graph and the branchwidth of the corresponding graphic matroid may differ: for instance, the three-edge path graph and the three-edge star have different branchwidths, 2 and 1 respectively, but they both induce the same graphic matroid with branchwidth 1. However, for graphs that are not trees, the branchwidth of the graph is equal to the branchwidth of its associated graphic matroid. The branchwidth of a matroid is equal to the branchwidth of its dual matroid, and in particular this implies that the branchwidth of any planar graph that is not a tree is equal to that of its dual.

Branchwidth is an important component of attempts to extend the theory of graph minors to matroid minors: although treewidth can also be generalized to matroids, and plays a bigger role than branchwidth in the theory of graph minors, branchwidth has more convenient properties in the matroid setting. Robertson and Seymour conjectured that the matroids representable over any particular finite field are well-quasi-ordered, analogously to the Robertson–Seymour theorem for graphs, but so far this has been proven only for the matroids of bounded branchwidth. Additionally, if a minor-closed family of matroids representable over a finite field does not include the graphic matroids of all planar graphs, then there is a constant bound on the branchwidth of the matroids in the family, generalizing similar results for minor-closed graph families.

For any fixed constant "k", the matroids with branchwidth at most "k" can be recognized in polynomial time by an algorithm that has access to the matroid via an independence oracle.

By the Robertson–Seymour theorem, the graphs of branchwidth "k" can be characterized by a finite set of forbidden minors. The graphs of branchwidth 0 are the matchings; the minimal forbidden minors are a two-edge path graph and a triangle graph (or the two-edge cycle, if multigraphs rather than simple graphs are considered). The graphs of branchwidth 1 are the graphs in which each connected component is a star; the minimal forbidden minors for branchwidth 1 are the triangle graph (or the two-edge cycle, if multigraphs rather than simple graphs are considered) and the three-edge path graph. The graphs of branchwidth 2 are the graphs in which each biconnected component is a series-parallel graph; the only minimal forbidden minor is the complete graph "K" on four vertices. A graph has branchwidth three if and only if it has treewidth three and does not have the cube graph as a minor; therefore, the four minimal forbidden minors are three of the four forbidden minors for treewidth three (the graph of the octahedron, the complete graph "K", and the Wagner graph) together with the cube graph.

Forbidden minors have also been studied for matroid branchwidth, despite the lack of a full analogue to the Robertson–Seymour theorem in this case. A matroid has branchwidth one if and only if every element is either a loop or a coloop, so the unique minimal forbidden minor is the uniform matroid U(2,3), the graphic matroid of the triangle graph. A matroid has branchwidth two if and only if it is the graphic matroid of a graph of branchwidth two, so its minimal forbidden minors are the graphic matroid of "K" and the non-graphic matroid U(2,4). The matroids of branchwidth three are not well-quasi-ordered without the additional assumption of representability over a finite field, but nevertheless the matroids with any finite bound on their branchwidth have finitely many minimal forbidden minors, all of which have a number of elements that is at most exponential in the branchwidth.



</doc>
<doc id="18978005" url="https://en.wikipedia.org/wiki?curid=18978005" title="Planar separator theorem">
Planar separator theorem

In graph theory, the planar separator theorem is a form of isoperimetric inequality for planar graphs, that states that any planar graph can be split into smaller pieces by removing a small number of vertices. Specifically, the removal of O(√"n") vertices from an "n"-vertex graph (where the "O" invokes big O notation) can partition the graph into disjoint subgraphs each of which has at most 2"n"/3 vertices.

A weaker form of the separator theorem with O(√"n" log "n") vertices in the separator instead of O(√"n") was originally proven by , and the form with the tight asymptotic bound on the separator size was first proven by . Since their work, the separator theorem has been reproven in several different ways, the constant in the O(√"n") term of the theorem has been improved, and it has been extended to certain classes of nonplanar graphs.

Repeated application of the separator theorem produces a separator hierarchy which may take the form of either a tree decomposition or a branch-decomposition of the graph. Separator hierarchies may be used to devise efficient divide and conquer algorithms for planar graphs, and dynamic programming on these hierarchies can be used to devise exponential time and fixed-parameter tractable algorithms for solving NP-hard optimization problems on these graphs. Separator hierarchies may also be used in nested dissection, an efficient variant of Gaussian elimination for solving sparse systems of linear equations arising from finite element methods.

Bidimensionality theory of Demaine, Fomin, Hajiaghayi, and Thilikos generalizes and greatly expands the applicability of the separator theorem 
for a vast set of minimization problems on planar graphs and more generally graphs excluding a fixed minor.

As it is usually stated, the separator theorem states that, in any "n"-vertex planar graph "G" = ("V","E"), there exists a partition of the vertices of "G" into three sets "A", "S", and "B", such that each of "A" and "B" has at most 2"n"/3 vertices, "S" has O(√"n") vertices, and there are no edges with one endpoint in "A" and one endpoint in "B". It is not required that "A" or "B" form connected subgraphs of "G". "S" is called the separator for this partition.

An equivalent formulation is that the edges of any "n"-vertex planar graph "G" may be subdivided into two edge-disjoint subgraphs "G" and "G" in such a way that both subgraphs have at least "n"/3 vertices and such that the intersection of the vertex sets of the two subgraphs has O(√"n") vertices in it. Such a partition is known as a separation. If a separation is given, then the intersection of the vertex sets forms a separator, and the vertices that belong to one subgraph but not the other form the separated subsets of at most 2"n"/3 vertices. In the other direction, if one is given a partition into three sets "A", "S", and "B" that meet the conditions of the planar separator theorem, then one may form a separation in which the edges with an endpoint in "A" belong to "G", the edges with an endpoint in "B" belong to "G", and the remaining edges (with both endpoints in "S") are partitioned arbitrarily.

The constant 2/3 in the statement of the separator theorem is arbitrary and may be replaced by any other number in the open interval (1/2,1) without changing the form of the theorem: a partition into more equal subsets may be obtained from a less-even partition by repeatedly splitting the larger sets in the uneven partition and regrouping the resulting connected components.

Consider a grid graph with "r" rows and "c" columns; the number "n" of vertices equals "rc". For instance, in the illustration, "r" = 5, "c" = 8, and "n" = 40. If "r" is odd, there is a single central row, and otherwise there are two rows equally close to the center; similarly, if "c" is odd, there is a single central column, and otherwise there are two columns equally close to the center. Choosing "S" to be any of these central rows or columns, and removing "S" from the graph, partitions the graph into two smaller connected subgraphs "A" and "B", each of which has at most "n"/2 vertices. If "r" ≤ "c" (as in the illustration), then choosing a central column will give a separator "S" with "r" ≤ √"n" vertices, and similarly if "c" ≤ "r" then choosing a central row will give a separator with at most √"n" vertices. Thus, every grid graph has a separator "S" of size at most √"n", the removal of which partitions it into two connected components, each of size at most "n"/2.

The planar separator theorem states that a similar partition can be constructed in any planar graph. The case of arbitrary planar graphs differs from the case of grid graphs in that the separator has size O(√"n") but may be larger than √"n", the bound on the size of the two subsets "A" and "B" (in the most common versions of the theorem) is 2"n"/3 rather than "n"/2, and the two subsets "A" and "B" need not themselves form connected subgraphs.

 augment the given planar graph by additional edges, if necessary, so that it becomes maximal planar (every face in a planar embedding is a triangle). They then perform a breadth-first search, rooted at an arbitrary vertex "v", and partition the vertices into levels by their distance from "v". If "l" is the median level (the level such that the numbers of vertices at higher and lower levels are both at most "n"/2) then there must be levels "l" and "l" that are O(√"n") steps above and below "l" respectively and that contain O(√"n") vertices, respectively, for otherwise there would be more than "n" vertices in the levels near "l". They show that there must be a separator "S" formed by the union of "l" and "l", the endpoints "e" of an edge of "G" that does not belong to the breadth-first search tree and that lies between the two levels, and the vertices on the two breadth-first search tree paths from "e" back up to level "l". The size of the separator "S" constructed in this way is at most √8√"n", or approximately 2.83√"n". The vertices of the separator and the two disjoint subgraphs can be found in linear time.

This proof of the separator theorem applies as well to weighted planar graphs, in which each vertex has a non-negative cost. The graph may be partitioned into three sets "A", "S", and "B" such that "A" and "B" each have at most 2/3 of the total cost and "S" has O(√"n") vertices, with no edges from "A" to "B". By analysing a similar separator construction more carefully, shows that the bound on the size of "S" can be reduced to √6√"n", or approximately 2.45√"n".

For a graph that is already maximal planar it is possible to show a stronger construction of a simple cycle separator, a cycle of small length such that the inside and the outside of the cycle (in the unique planar embedding of the graph) each have at most 2"n"/3 vertices. proves this (with a separator size of √8√"n") by using the Lipton–Tarjan technique for a modified version of breadth first search in which the levels of the search form simple cycles.

According to the Koebe–Andreev–Thurston circle-packing theorem, any planar graph may be represented by a packing of circular disks in the plane with disjoint interiors, such that two vertices in the graph are adjacent if and only if the corresponding pair of disks are mutually tangent. As show, for such a packing, there exists a circle that has at most 3"n"/4 disks touching or inside it, at most 3"n"/4 disks touching or outside it, and that crosses O(√"n") disks.

To prove this, Miller et al. use stereographic projection to map the packing onto the surface of a unit sphere in three dimensions. By choosing the projection carefully, the center of the sphere can be made into a centerpoint of the disk centers on its surface, so that any plane through the center of the sphere partitions it into two halfspaces that each contain or cross at most 3"n"/4 of the disks. If a plane through the center is chosen uniformly at random, a disk will be crossed with probability proportional to its radius. Therefore, the expected number of disks that are crossed is proportional to the sum of the radii of the disks. However, the sum of the squares of the radii is proportional to the total area of the disks, which is at most the total surface area of the unit sphere, a constant. An argument involving Jensen's inequality shows that, when the sum of squares of "n" non-negative real numbers is bounded by a constant, the sum of the numbers themselves is O(√"n"). Therefore, the expected number of disks crossed by a random plane is O(√"n") and there exists a plane that crosses at most that many disks. This plane intersects the sphere in a great circle, which projects back down to a circle in the plane with the desired properties. The O(√"n") disks crossed by this circle correspond to the vertices of a planar graph separator that separates the vertices whose disks are inside the circle from the vertices whose disks are outside the circle, with at most 3"n"/4 vertices in each of these two subsets.

This method leads to a randomized algorithm that finds such a separator in linear time, and a less-practical deterministic algorithm with the same linear time bound. By analyzing this algorithm carefully using known bounds on the packing density of circle packings, it can be shown to find separators of size at most
Although this improved separator size bound comes at the expense of a more-uneven partition of the graph, argue that it provides an improved constant factor in the time bounds for nested dissection compared to the separators of . The size of the separators it produces can be further improved, in practice, by using a nonuniform distribution for the random cutting planes.

The stereographic projection in the Miller et al. argument can be avoided by considering the smallest circle containing a constant fraction of the centers of the disks and then expanding it by a constant picked uniformly in the range [1,2]. It is easy to argue, as in Miller et al., that the disks intersecting the expanded circle form a valid separator, and that, in expectation, the separator is of the right size. The resulting constants are somewhat worse.

Spectral clustering methods, in which the vertices of a graph are grouped by the coordinates of the eigenvectors of matrices derived from the graph, have long been used as a heuristic for graph partitioning problems for nonplanar graphs. As show, spectral clustering can also be used to derive an alternative proof for a weakened form of the planar separator theorem that applies to planar graphs with bounded degree. In their method, the vertices of a given planar graph are sorted by the second coordinates of the eigenvectors of the Laplacian matrix of the graph, and this sorted order is partitioned at the point that minimizes the ratio of the number of edges cut by the partition to the number of vertices on the smaller side of the partition. As they show, every planar graph of bounded degree has a partition of this type in which the ratio is O(1/√"n"). Although this partition may not be balanced, repeating the partition within the larger of the two sides and taking the union of the cuts formed at each repetition will eventually lead to a balanced partition with O(√"n") edges. The endpoints of these edges form a separator of size O(√"n").

A variation of the planar separator theorem involves edge separators, small sets of edges forming a cut between two subsets "A" and "B" of the vertices of the graph. The two sets "A" and "B" must each have size at most a constant fraction of the number "n" of vertices of the graph (conventionally, both sets have size at most 2"n"/3), and each vertex of the graph belongs to exactly one of "A" and "B". The separator consists of the edges that have one endpoint in "A" and one endpoint in "B". Bounds on the size of an edge separator involve the degree of the vertices as well as the number of vertices in the graph: the planar graphs in which one vertex has degree "n" − 1, including the wheel graphs and star graphs, have no edge separator with a sublinear number of edges, because any edge separator would have to include all the edges connecting the high degree vertex to the vertices on the other side of the cut. However, every planar graph with maximum degree Δ has an edge separator of size O(√(Δ"n")).

A simple cycle separator in the dual graph of a planar graph forms an edge separator in the original graph.
Applying the simple cycle separator theorem of to the dual graph of a given planar graph strengthens the O(√(Δ"n")) bound for the size of an edge separator by showing that every planar graph has an edge separator whose size is proportional to the Euclidean norm of its vector of vertex degrees.

In a √"n" × √"n" grid graph, a set "S" of "s" < √"n" points can enclose a subset of at most "s"("s" − 1)/2 grid points, where the maximum is achieved by arranging "S" in a diagonal line near a corner of the grid. Therefore, in order to form a separator that separates at least "n"/3 of the points from the remaining grid, "s" needs to be at least √(2"n"/3), approximately 0.82√"n".

There exist "n"-vertex planar graphs (for arbitrarily large values of "n") such that, for every separator "S" that partitions the remaining graph into subgraphs of at most 2"n"/3 vertices, "S" has at least √(4π√3)√"n" vertices, approximately 1.56√"n". The construction involves approximating a sphere by a convex polyhedron, replacing each of the faces of the polyhedron by a triangular mesh, and applying isoperimetric theorems for the surface of the sphere.

Separators may be combined into a separator hierarchy of a planar graph, a recursive decomposition into smaller graphs. A separator hierarchy may be represented by a binary tree in which the root node represents the given graph itself, and the two children of the root are the roots of recursively constructed separator hierarchies for the induced subgraphs formed from the two subsets "A" and "B" of a separator.

A separator hierarchy of this type forms the basis for a tree decomposition of the given graph, in which the set of vertices associated with each tree node is the union of the separators on the path from that node to the root of the tree. Since the sizes of the graphs go down by a constant factor at each level of the tree, the upper bounds on the sizes of the separators also go down by a constant factor at each level, so the sizes of the separators on these paths add in a geometric series to O(√"n"). That is, a separator formed in this way has width O(√"n"), and can be used to show that every planar graph has treewidth O(√"n").

Constructing a separator hierarchy directly, by traversing the binary tree top down and applying a linear-time planar separator algorithm to each of the induced subgraphs associated with each node of the binary tree, would take a total of O("n" log "n") time. However, it is possible to construct an entire separator hierarchy in linear time, by using the Lipton–Tarjan breadth-first layering approach and by using appropriate data structures to perform each partition step in sublinear time.

If one forms a related type of hierarchy based on separations instead of separators, in which the two children of the root node are roots of recursively constructed hierarchies for the two subgraphs "G" and "G" of a separation of the given graph, then the overall structure forms a branch-decomposition instead of a tree decomposition. The width of any separation in this decomposition is, again, bounded by the sum of the sizes of the separators on a path from any node to the root of the hierarchy, so any branch-decomposition formed in this way has width O(√"n") and any planar graph has branchwidth O(√"n"). Although many other related graph partitioning problems are NP-complete, even for planar graphs, it is possible to find a minimum-width branch-decomposition of a planar graph in polynomial time.

By applying methods of more directly in the construction of branch-decompositions, show that every planar graph has branchwidth at most 2.12√"n", with the same constant as the one in the simple cycle separator theorem of Alon et al. Since the treewidth of any graph is at most 3/2 its branchwidth, this also shows that planar graphs have treewidth at most 3.18√"n".

Some sparse graphs do not have separators of sublinear size: in an expander graph, deleting up to a constant fraction of the vertices still leaves only one connected component.

Possibly the earliest known separator theorem is a result of that any tree can be partitioned into subtrees of at most "n"/2 vertices each by the removal of a single vertex. In particular, the vertex that minimizes the maximum component size has this property, for if it did not then its neighbor in the unique large subtree would form an even better partition. By applying the same technique to a tree decomposition of an arbitrary graph, it is possible to show that any graph has a separator of size at most equal to its treewidth.

If a graph "G" is not planar, but can be embedded on a surface of genus "g", then it has a separator with O(("gn")) vertices. prove this by using a similar approach to that of . They group the vertices of the graph into breadth-first levels and find two levels the removal of which leaves at most one large component consisting of a small number of levels. This remaining component can be made planar by removing a number of breadth-first paths proportional to the genus, after which the Lipton–Tarjan method can be applied to the remaining planar graph. The result follows from a careful balancing of the size of the removed two levels against the number of levels between them. If the graph embedding is given as part of the input, its separator can be found in linear time. Graphs of genus "g" also have edge separators of size O(("g"Δ"n")).

Graphs of bounded genus form an example of a family of graphs closed under the operation of taking minors, and separator theorems also apply to arbitrary minor-closed graph families. In particular, if a graph family has a forbidden minor with "h" vertices, then it has a separator with O("h"√"n") vertices, and such a separator can be found in time O("n") for any ε > 0.
The circle separator method of generalizes to the intersection graphs of any system of "d"-dimensional balls with the property that any point in space is covered by at most some constant number "k" of balls, to "k"-nearest-neighbor graphs in "d" dimensions, and to the graphs arising from finite element meshes. The sphere separators constructed in this way partition the input graph into subgraphs of at most vertices. The size of the separators for "k"-ply ball intersection graphs and for "k"-nearest-neighbor graphs is O("k""n").

Separator decompositions can be of use in designing efficient divide and conquer algorithms for solving problems on planar graphs. As an example, one problem that can be solved in this way is to find the shortest cycle in a weighted planar digraph. This may be solved by the following steps:
The time for the two recursive calls to "A" and "B" in this algorithm is dominated by the time to perform the O(√"n") calls to Dijkstra's algorithm, so this algorithm finds the shortest cycle in O("n" log "n") time.

A faster algorithm for the same shortest cycle problem, running in time O("n" log"n"), was given by . His algorithm uses the same separator-based divide and conquer structure, but uses simple cycle separators rather than arbitrary separators, so that the vertices of "S" belong to a single face of the graphs inside and outside the cycle separator. He then replaces the O(√"n") separate calls to Dijkstra's algorithm with more sophisticated algorithms to find shortest paths from all vertices on a single face of a planar graph and to combine the distances from the two subgraphs. For weighted but undirected planar graphs, the shortest cycle is equivalent to the minimum cut in the dual graph and can be found in O("n" log log "n") time, and the shortest cycle in an unweighted undirected planar graph (its girth) may be found in time O("n"). (However, the faster algorithm for unweighted graphs is not based on the separator theorem.)

Frederickson proposed another faster algorithm for single source shortest paths by implementing separator theorem in planar graphs in 1986. This is an improvement of Dijkstra's algorithm with iterative search on a carefully selected subset of the vertices. This version takes O("n" √(log "n")) time in an "n"-vertex graph. Separators are used to find a division of a graph, that is, a partition of the edge-set into two or more subsets, called regions. A node is said to be contained in a region if some edge of the region is incident to the node. A node contained in more than one region is called a boundary node of the regions containing it. The method uses the notion of a "r"-division of an "n"-node graph that is a graph division into O("n"/"r") regions, each containing O("r") nodes including O(√"r") boundary nodes. Frederickson showed that an "r"-division can be found in O("n" log "n") time by recursive application of separator theorem.

The sketch of his algorithm to solve the problem is as follows.

1. Preprocessing Phase: Partition the graph into carefully selected subsets of vertices and determine the shortest paths between all pairs of vertices in these subsets, where intermediate vertices on this path are not in the subset. This phase requires a planar graph "G"  to be transformed into "G" with no vertex having degree greater than 3. From a corollary of Euler's formula, the number of vertices in the resulting graph will be "n" ≤ 6"n" -12, where "n"  is the number of vertices in "G" . This phase also ensures the following properties of a suitable "r"-division. A suitable "r"-division of a planar graph is an "r"-division such that,

2. Search Phase:

Henzinger et. al. extended Frederickson's "r"-division technique for the single source shortest path algorithm in planar graphs for nonnegative edge-lengths and proposed a linear time algorithm. Their method generalizes Frederickson's notion of graph-divisions such that now an ("r","s")-division of an "n"-node graph be a division into O("n"/"r") regions, each containing "r"  nodes, each having at most "s" boundary nodes. If an ("r", "s")-division is repeatedly divided into smaller regions, that is called get a recursive division. This algorithm uses approximately log*"n" levels of divisions. The recursive division is represented by a rooted tree whose leaves are labeled by distinct edge of "G". The root of the tree represents the region consisting of full-"G", the children of the root represent the subregions into which that region is divided and so on. Each leaf (atomic region) represents a region containing exactly one edge.

Nested dissection is a separator based divide and conquer variation of Gaussian elimination for solving sparse symmetric systems of linear equations with a planar graph structure, such as the ones arising from the finite element method. It involves finding a separator for the graph describing the system of equations, recursively eliminating the variables in the two subproblems separated from each other by the separator, and then eliminating the variables in the separator. The fill-in of this method (the number of nonzero coefficients of the resulting Cholesky decomposition of the matrix) is O("n" log "n"), allowing this method to be competitive with iterative methods for the same problems.

Klein, Mozes and Weimann gave an O("n" log "n")-time, linear-space algorithm to find the shortest path distances from "s" to all nodes for a directed planar graph with positive and negative arc-lengths containing no negative cycles. Their algorithm uses planar graph separators to find a Jordan curve "C" that passes through O(√"n") nodes (and no arcs) such that between "n"/3 and 2"n"/3 nodes are enclosed by "C". Nodes through which "C" passes are boundary nodes. The original graph "G" is separated into two subgraphs "G"  and "G"  by cutting the planar embedding along "C" and duplicating the boundary nodes. For "i" = 0 and 1, in "G"  the boundary nodes lie on the boundary of a single face "F" .

The overview of their approach is given below. 

An important part of this algorithm is the use of Price Functions and Reduced Lengths. For a directed graph "G" with arc-lengths ι(·), a price function is a function φ from the nodes of "G" to the real numbers. For an arc "uv", the reduced length with respect to φ is ιφ("uv") = ι("uv") + φ("u") − φ("v"). A feasible price function is a price function that induces nonnegative reduced lengths on all arcs of "G". It is useful in transforming a shortest-path problem involving positive and negative lengths into one involving only nonnegative lengths, which can then be solved using Dijkstra’s algorithm.

The separator based divide and conquer paradigm has also been used to design data structures for dynamic graph algorithms and point location, algorithms for polygon triangulation, shortest paths, and the construction of nearest neighbor graphs, and approximation algorithms for the maximum independent set of a planar graph.

By using dynamic programming on a tree decomposition or branch-decomposition of a planar graph, many NP-hard optimization problems may be solved in time exponential in √"n" or √"n" log "n". For instance, bounds of this form are known for finding maximum independent sets, Steiner trees, and Hamiltonian cycles, and for solving the travelling salesman problem on planar graphs. Similar methods involving separator theorems for geometric graphs may be used to solve Euclidean travelling salesman problem and Steiner tree construction problems in time bounds of the same form.

For parameterized problems that admit a kernelization that preserves planarity and reduces the input graph to a kernel of size linear in the input parameter, this approach can be used to design fixed-parameter tractable algorithms the running time of which depends polynomially on the size of the input graph and exponentially on √"k", where "k" is the parameter of the algorithm. For instance, time bounds of this form are known for finding vertex covers and dominating sets of size "k".

 observed that the separator theorem may be used to obtain polynomial time approximation schemes for NP-hard optimization problems on planar graphs such as finding the maximum independent set. Specifically, by truncating a separator hierarchy at an appropriate level, one may find a separator of size O("n"/√log "n") the removal of which partitions the graph into subgraphs of size "c" log "n", for any constant "c". By the four-color theorem, there exists an independent set of size at least "n"/4, so the removed nodes form a negligible fraction of the maximum independent set, and the maximum independent sets in the remaining subgraphs can be found independently in time exponential in their size. By combining this approach with later linear-time methods for separator hierarchy construction and with table lookup to share the computation of independent sets between isomorphic subgraphs, it can be made to construct independent sets of size within a factor of 1 − O(1/√log "n") of optimal, in linear time. However, for approximation ratios even closer to 1 than this factor, a later approach of (based on tree-decomposition but not on planar separators) provides better tradeoffs of time versus approximation quality.

Similar separator-based approximation schemes have also been used to approximate other hard problems such as vertex cover. use separators in a different way to approximate the travelling salesman problem for the shortest path metric on weighted planar graphs; their algorithm uses dynamic programming to find the shortest tour that, at each level of a separator hierarchy, crosses the separator a bounded number of times, and they show that as the crossing bound increases the tours constructed in this way have lengths that approximate the optimal tour.

Separators have been used as part of data compression algorithms for representing planar graphs and other separable graphs using a small number of bits. The basic principle of these algorithms is to choose a number "k" and repeatedly subdivide the given planar graph using separators into O("n"/"k") subgraphs of size at most "k", with O("n"/√"k") vertices in the separators. With an appropriate choice of "k" (at most proportional to the logarithm of "n") the number of non-isomorphic "k"-vertex planar subgraphs is significantly less than the number of subgraphs in the decomposition, so the graph can be compressed by constructing a table of all the possible non-isomorphic subgraphs and representing each subgraph in the separator decomposition by its index into the table. The remainder of the graph, formed by the separator vertices, may be represented explicitly or by using a recursive version of the same data structure. Using this method, planar graphs and many more restricted families of graphs may be encoded using a number of bits that is information-theoretically optimal: if there are "P" "n"-vertex graphs in the family of graphs to be represented, then an individual graph in the family can be represented using only (1 + o("n"))log"P" bits. It is also possible to construct representations of this type in which one may test adjacency between vertices, determine the degree of a vertex, and list neighbors of vertices in constant time per query, by augmenting the table of subgraphs with additional tabular information representing the answers to the queries.

A universal graph for a family "F" of graphs is a graph that contains every member of "F" as a subgraphs. Separators can be used to show that the "n"-vertex planar graphs have universal graphs with "n" vertices and O("n") edges.

The construction involves a strengthened form of the separator theorem in which the size of the three subsets of vertices in the separator does not depend on the graph structure: there exists a number "c", the magnitude of which at most a constant times √"n", such that the vertices of every "n"-vertex planar graph can be separated into subsets "A", "S", and "B", with no edges from "A" to "B", with |"S"| = "c", and with |"A"| = |"B"| = ("n" − "c")/2. This may be shown by using the usual form of the separator theorem repeatedly to partition the graph until all the components of the partition can be arranged into two subsets of fewer than "n"/2 vertices, and then moving vertices from these subsets into the separator as necessary until it has the given size.

Once a separator theorem of this type is shown, it can be used to produce a separator hierarchy for "n"-vertex planar graphs that again does not depend on the graph structure: the tree-decomposition formed from this hierarchy has width O(√"n") and can be used for any planar graph. The set of all pairs of vertices in this tree-decomposition that both belong to a common node of the tree-decomposition forms a trivially perfect graph with O("n") vertices that contains every "n"-vertex planar graph as a subgraph. A similar construction shows that bounded-degree planar graphs have universal graphs with O("n" log "n") edges, where the constant hidden in the O notation depends on the degree bound. Any universal graph for planar graphs (or even for trees of unbounded degree) must have Ω("n" log "n") edges, but it remains unknown whether this lower bound or the O("n") upper bound is tight for universal graphs for arbitrary planar graphs.




</doc>
<doc id="35810608" url="https://en.wikipedia.org/wiki?curid=35810608" title="Courcelle's theorem">
Courcelle's theorem

In the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth. The result was first proved by Bruno Courcelle in 1990 and independently rediscovered by .
It is considered the archetype of algorithmic meta-theorems.

In one variation of monadic second-order graph logic known as MSO, the graph is described by a set of vertices "V" and a binary adjacency relation adj(..), and the restriction to monadic logic means that the graph property in question may be defined in terms of sets of vertices of the given graph, but not in terms of sets of edges, or sets of tuples of vertices.

As an example, the property of a graph being colorable with three colors (represented by three sets of vertices "R", "G", and "B") may be defined by the monadic second-order formula

The first part of this formula ensures that the three color classes cover all the vertices of the graph, and the second ensures that they each form an independent set. (It would also be possible to add clauses to the formula to ensure that the three color classes are disjoint, but this makes no difference to the result.) Thus, by Courcelle's theorem, 3-colorability of graphs of bounded treewidth may be tested in linear time.

For this variation of graph logic, Courcelle's theorem can be extended from treewidth to clique-width: for every fixed MSO property "P", and every fixed bound "b" on the clique-width of a graph, there is a linear-time algorithm for testing whether a graph of clique-width at most "b" has property "P". The original formulation of this result required the input graph to be given together with a construction proving that it has bounded clique-width, but later approximation algorithms for clique-width removed this requirement.

Courcelle's theorem may also be used with a stronger variation of monadic second-order logic known as MSO. In this formulation, a graph is represented by a set "V" of vertices, a set 
"E" of edges, and an incidence relation between vertices and edges. This variation allows quantification over sets of vertices or edges, but not over more complex relations on tuples of vertices or edges.

For instance, the property of having a Hamiltonian cycle may be expressed in MSO by describing the cycle as a set of edges that includes exactly two edges incident to each vertex, such that every nonempty proper subset of vertices has an edge in the putative cycle with exactly one endpoint in the subset. However, Hamiltonicity cannot be expressed in MSO.

It is possible to apply the same results to graphs in which the vertices or edges have labels from a fixed finite set, either by augmenting the graph logic to incorporate predicates describing the labels, or by representing the labels by unquantified vertex set or edge set variables.

Another direction for extending Courcelle's theorem concerns logical formulas that include predicates for counting the size of the test.
In this context, it is not possible to perform arbitrary arithmetic operations on set sizes, nor even to test whether two sets have the same size.
However, MSO and MSO can be extended to logics called CMSO and CMSO, that include for every two constants "q" and "r" a predicate formula_1 which tests whether the cardinality of set "S" is congruent to "r" modulo "q". Courcelle's theorem can be extended to these logics.

As stated above, Courcelle's theorem applies primarily to decision problems: does a graph have a property or not. However, the same methods also allow the solution to optimization problems in which the vertices or edges of a graph have integer weights, and one seeks the minimum or maximum weight vertex set that satisfies a given property, expressed in second-order logic. These optimization problems can be solved in linear time on graphs of bounded clique-width.

Rather than bounding the time complexity of an algorithm that recognizes an MSO property on bounded-treewidth graphs, it is also possible to analyze the space complexity of such an algorithm; that is, the amount of memory needed above and beyond the size of the input itself (which is assumed to be represented in a read-only way so that its space requirements cannot be put to other purposes).
In particular, it is possible to recognize the graphs of bounded treewidth, and any MSO property on these graphs, by a deterministic Turing machine that uses only logarithmic space.

The typical approach to proving Courcelle's theorem involves the construction of a finite bottom-up tree automaton that acts on the tree decompositions of the given graph.

In more detail, two graphs "G" and "G", each with a specified subset "T" of vertices called terminals, may be defined to be equivalent with respect to an MSO formula "F" if, for all other graphs "H" whose intersection with "G" and "G" consists only of vertices in "T", the two graphs
"G" ∪ "H" and "G" ∪ "H" behave the same with respect to "F": either they both model "F" or they both do not model "F". This is an equivalence relation, and it can be shown by induction on the length of "F" that (when the sizes of "T" and "F" are both bounded) it has finitely many equivalence classes.

A tree decomposition of a given graph "G" consists of a tree and, for each tree node, a subset of the vertices of "G" called a bag. It must satisfy two properties: for each vertex "v" of "G", the bags containing "v" must be associated with a contiguous subtree of the tree, and for each edge "uv" of "G", there must be a bag containing both "u" and "v".
The vertices in a bag can be thought of as the terminals of a subgraph of "G", represented by the subtree of the tree decomposition descending from that bag. When "G" has bounded treewidth, it has a tree decomposition in which all bags have bounded size, and such a decomposition can be found in fixed-parameter tractable time. Moreover, it is possible to choose this tree decomposition so that it forms a binary tree, with only two child subtrees per bag. Therefore, it is possible to perform a bottom-up computation on this tree decomposition, computing an identifier for the equivalence class of the subtree rooted at each bag by combining the edges represented within the bag with the two identifiers for the equivalence classes of its two children.

The size of the automaton constructed in this way is not an elementary function of the size of the input MSO formula. This non-elementary complexity is necessary, in the sense that (unless P = NP) it is not possible to test MSO properties on trees in a time that is fixed-parameter tractable with an elementary dependence on the parameter.

The proofs of Courcelle's theorem show a stronger result: not only can every (counting) monadic second-order property be recognized in linear time for graphs of bounded treewidth, but also it can be recognized by a finite-state tree automaton. Courcelle conjectured a converse to this: if a property of graphs of bounded treewidth is recognized by a tree automaton, then it can be defined in counting monadic second-order logic. In 1998 , claimed a resolution of the conjecture. However, the proof is widely regarded as unsatisfactory. Until 2016, only a few special cases were resolved: in particular, the conjecture has been proved for graphs of treewidth at most three, for k-connected graphs of treewidth k, for graphs of constant treewidth and chordality, and for k-outerplanar graphs.
The general version of the conjecture was finally proved by Mikołaj Bojańczyk and Michał Pilipczuk.

Moreover, for Halin graphs (a special case of treewidth three graphs) counting is not needed: for these graphs, every property that can be recognized by a tree automaton can also be defined in monadic second-order logic. The same is true more generally for certain classes of graphs in which a tree decomposition can itself be described in MSOL. However, it cannot be true for all graphs of bounded treewidth, because in general counting adds extra power over monadic second-order logic without counting. For instance, the graphs with an even number of vertices can be recognized using counting, but not without.

The satisfiability problem for a formula of monadic second-order logic is the problem of determining whether there exists at least one graph (possibly within a restricted family of graphs) for which the formula is true. For arbitrary graph families, and arbitrary formulas, this problem is undecidable. However, satisfiability of MSO formulas is decidable for the graphs of bounded treewidth, and satisfiability of MSO formulas is decidable for graphs of bounded clique-width. The proof involves building a tree automaton for the formula and then testing whether the automaton has an accepting path.

As a partial converse, proved that, whenever a family of graphs has a decidable MSO satisfiability problem, the family must have bounded treewidth. The proof is based on a theorem of Robertson and Seymour that the families of graphs with unbounded treewidth have arbitrarily large grid minors. Seese also conjectured that every family of graphs with a decidable MSO satisfiability problem must have bounded clique-width; this has not been proven, but a weakening of the conjecture that replaces MSO by CMSO is true.

 used Courcelle's theorem to show that computing the crossing number of a graph "G" is fixed-parameter tractable with a quadratic dependence on the size of "G", improving a cubic-time algorithm based on the Robertson–Seymour theorem. An additional later improvement to linear time by follows the same approach. If the given graph "G" has small treewidth, Courcelle's theorem can be applied directly to this problem. On the other hand, if "G" has large treewidth, then it contains a large grid minor, within which the graph can be simplified while leaving the crossing number unchanged. Grohe's algorithm performs these simplifications until the remaining graph has a small treewidth, and then applies Courcelle's theorem to solve the reduced subproblem.

In computational topology, extend Courcelle's theorem from MSO to a form of monadic second-order logic on simplicial complexes of bounded dimension that allows quantification over simplices of any fixed dimension. As a consequence, they show how to compute certain quantum invariants of 3-manifolds as well as how to solve certain problems in discrete Morse theory efficiently, when the manifold has a triangulation (avoiding degenerate simplices) whose dual graph has small treewidth.

Methods based on Courcelle's theorem have also been applied to database theory, knowledge representation and reasoning, automata theory, and model checking.


</doc>
<doc id="351769" url="https://en.wikipedia.org/wiki?curid=351769" title="Robertson–Seymour theorem">
Robertson–Seymour theorem

In graph theory, the Robertson–Seymour theorem (also called the graph minor theorem) states that the undirected graphs, partially ordered by the graph minor relationship, form a well-quasi-ordering. Equivalently, every family of graphs that is closed under minors can be defined by a finite set of forbidden minors, in the same way that Wagner's theorem characterizes the planar graphs as being the graphs that do not have the complete graph "K" or the complete bipartite graph "K" as minors.

The Robertson–Seymour theorem is named after mathematicians Neil Robertson and Paul D. Seymour, who proved it in a series of twenty papers spanning over 500 pages from 1983 to 2004. Before its proof, the statement of the theorem was known as Wagner's conjecture after the German mathematician Klaus Wagner, although Wagner said he never conjectured it.

A weaker result for trees is implied by Kruskal's tree theorem, which was conjectured in 1937 by Andrew Vázsonyi and proved in 1960 independently by Joseph Kruskal and S. Tarkowski.

A minor of an undirected graph "G" is any graph that may be obtained from "G" by a sequence of zero or more contractions of edges of "G" and deletions of edges and vertices of "G". The minor relationship forms a partial order on the set of all distinct finite undirected graphs, as it obeys the three axioms of partial orders: it is reflexive (every graph is a minor of itself), transitive (a minor of a minor of "G" is itself a minor of "G"), and antisymmetric (if two graphs "G" and "H" are minors of each other, then they must be isomorphic). However, if graphs that are isomorphic may nonetheless be considered as distinct objects, then the minor ordering on graphs forms a preorder, a relation that is reflexive and transitive but not necessarily antisymmetric.

A preorder is said to form a well-quasi-ordering if it contains neither an infinite descending chain nor an infinite antichain. For instance, the usual ordering on the non-negative integers is a well-quasi-ordering, but the same ordering on the set of all integers is not, because it contains the infinite descending chain 0, −1, −2, −3...

The Robertson–Seymour theorem states that finite undirected graphs and graph minors form a well-quasi-ordering. The graph minor relationship does not contain any infinite descending chain, because each contraction or deletion reduces the number of edges and vertices of the graph (a non-negative integer). The nontrivial part of the theorem is that there are no infinite antichains, infinite sets of graphs that are all unrelated to each other by the minor ordering. If "S" is a set of graphs, and "M" is a subset of "S" containing one representative graph for each equivalence class of minimal elements (graphs that belong to "S" but for which no proper minor belongs to "S"), then "M" forms an antichain; therefore, an equivalent way of stating the theorem is that, in any infinite set "S" of graphs, there must be only a finite number of non-isomorphic minimal elements.

Another equivalent form of the theorem is that, in any infinite set "S" of graphs, there must be a pair of graphs one of which is a minor of the other. The statement that every infinite set has finitely many minimal elements implies this form of the theorem, for if there are only finitely many minimal elements, then each of the remaining graphs must belong to a pair of this type with one of the minimal elements. And in the other direction, this form of the theorem implies the statement that there can be no infinite antichains, because an infinite antichain is a set that does not contain any pair related by the minor relation.

A family "F" of graphs is said to be closed under the operation of taking minors if every minor of a graph in "F" also belongs to "F". If "F" is a minor-closed family, then let "S" be the set of graphs that are not in "F" (the complement of "F"). According to the Robertson–Seymour theorem, there exists a finite set "H" of minimal elements in "S". These minimal elements form a forbidden graph characterization of "F": the graphs in "F" are exactly the graphs that do not have any graph in "H" as a minor. The members of "H" are called the excluded minors (or forbidden minors, or minor-minimal obstructions) for the family "F".

For example, the planar graphs are closed under taking minors: contracting an edge in a planar graph, or removing edges or vertices from the graph, cannot destroy its planarity. Therefore, the planar graphs have a forbidden minor characterization, which in this case is given by Wagner's theorem: the set "H" of minor-minimal nonplanar graphs contains exactly two graphs, the complete graph "K" and the complete bipartite graph "K", and the planar graphs are exactly the graphs that do not have a minor in the set {"K", "K"}.

The existence of forbidden minor characterizations for all minor-closed graph families is an equivalent way of stating the Robertson–Seymour theorem. For, suppose that every minor-closed family "F" has a finite set "H" of minimal forbidden minors, and let "S" be any infinite set of graphs. Define "F" from "S" as the family of graphs that do not have a minor in "S". Then "F" is minor-closed and has a finite set "H" of minimal forbidden minors. Let "C" be the complement of "F". "S" is a subset of "C" since "S" and "F" are disjoint, and "H" are the minimal graphs in "C". Consider a graph "G" in "H". "G" cannot have a proper minor in "S" since "G" is minimal in "C". At the same time, "G" must have a minor in "S", since otherwise "G" would be an element in "F". Therefore, "G" is an element in "S", i.e., "H" is a subset of "S", and all other graphs in "S" have a minor among the graphs in "H", so "H" is the finite set of minimal elements of "S".

For the other implication, assume that every set of graphs has a finite subset of minimal graphs and let a minor-closed set "F" be given. We want to find a set "H" of graphs such that a graph is in "F" if and only if it does not have a minor in "H". Let "E" be the graphs which are not minors of any graph in "F", and let "H" be the finite set of minimal graphs in "E". Now, let an arbitrary graph "G" be given. Assume first that "G" is in "F". "G" cannot have a minor in "H" since "G" is in "F" and "H" is a subset of "E". Now assume that "G" is not in "F". Then "G" is not a minor of any graph in "F", since "F" is minor-closed. Therefore, "G" is in "E", so "G" has a minor in "H".

The following sets of finite graphs are minor-closed, and therefore (by the Robertson–Seymour theorem) have forbidden minor characterizations:

Some examples of finite obstruction sets were already known for specific classes of graphs before the Robertson–Seymour theorem was proved. For example, the obstruction for the set of all forests is the loop graph (or, if one restricts to simple graphs, the cycle with three vertices). This means that a graph is a forest if and only if none of its minors is the loop (or, the cycle with three vertices, respectively). The sole obstruction for the set of paths is the tree with four vertices, one of which has degree 3. In these cases, the obstruction set contains a single element, but in general this is not the case. Wagner's theorem states that a graph is planar if and only if it has neither "K" nor "K" as a minor. In other words, the set {"K", "K"} is an obstruction set for the set of all planar graphs, and in fact the unique minimal obstruction set. A similar theorem states that "K" and "K" are the forbidden minors for the set of outerplanar graphs.

Although the Robertson–Seymour theorem extends these results to arbitrary minor-closed graph families, it is not a complete substitute for these results, because it does not provide an explicit description of the obstruction set for any family. For example, it tells us that the set of toroidal graphs has a finite obstruction set, but it does not provide any such set. The complete set of forbidden minors for toroidal graphs remains unknown, but contains at least 16000 graphs.

The Robertson–Seymour theorem has an important consequence in computational complexity, due to the proof by Robertson and Seymour that, for each fixed graph "G", there is a polynomial time algorithm for testing whether larger graphs have "G" as a minor. The running time of this algorithm can be expressed as a cubic polynomial in the size of the larger graph (although there is a constant factor in this polynomial that depends superpolynomially on the size of "G"), which has been improved to quadratic time by Kawarabayashi, Kobayashi, and Reed. As a result, for every minor-closed family "F", there is polynomial time algorithm for testing whether a graph belongs to "F": simply check, for each of the forbidden minors for "F", whether the given graph contains that forbidden minor.

However, this method requires a specific finite obstruction set to work, and the theorem does not provide one. The theorem proves that such a finite obstruction set exists, and therefore the problem is polynomial because of the above algorithm. However, the algorithm can be used in practice only if such a finite obstruction set is provided. As a result, the theorem proves that the problem can be solved in polynomial time, but does not provide a concrete polynomial-time algorithm for solving it. Such proofs of polynomiality are non-constructive: they prove polynomiality of problems without providing an explicit polynomial-time algorithm. In many specific cases, checking whether a graph is in a given minor-closed family can be done more efficiently: for example, checking whether a graph is planar can be done in linear time.

For graph invariants with the property that, for each "k", the graphs with invariant at most "k" are minor-closed, the same method applies. For instance, by this result, treewidth, branchwidth, and pathwidth, vertex cover, and the minimum genus of an embedding are all amenable to this approach, and for any fixed "k" there is a polynomial time algorithm for testing whether these invariants are at most "k", in which the exponent in the running time of the algorithm does not depend on "k". A problem with this property, that it can be solved in polynomial time for any fixed "k" with an exponent that does not depend on "k", is known as fixed-parameter tractable.

However, this method does not directly provide a single fixed-parameter-tractable algorithm for computing the parameter value for a given graph with unknown "k", because of the difficulty of determining the set of forbidden minors. Additionally, the large constant factors involved in these results make them highly impractical. Therefore, the development of explicit fixed-parameter algorithms for these problems, with improved dependence on "k", has continued to be an important line of research.

 showed that the following theorem exhibits the independence phenomenon by being "unprovable" in various formal systems that are much stronger than Peano arithmetic, yet being "provable" in systems much weaker than ZFC:




</doc>
<doc id="28140890" url="https://en.wikipedia.org/wiki?curid=28140890" title="Bidimensionality">
Bidimensionality

Bidimensionality theory characterizes a broad range of graph problems (bidimensional) that admit efficient approximate, fixed-parameter or kernel solutions in a broad range of graphs. These graph classes include planar graphs, map graphs, bounded-genus graphs and graphs excluding any fixed minor. In particular, bidimensionality theory builds on the graph minor theory of Robertson and Seymour by extending the mathematical results and building new algorithmic tools. The theory was introduced in the work of Demaine, Fomin, Hajiaghayi, and Thilikos, for which the authors received the Nerode Prize in 2015.

A parameterized problem formula_1 is a subset of formula_2 for some finite alphabet formula_3. An instance of a parameterized problem consists of "(x,k)", where "k" is called the parameter.

A parameterized problem formula_4 is "minor-bidimensional" if

Examples of minor-bidimensional problems are the parameterized versions of vertex cover, feedback vertex set, minimum maximal matching, and longest path.

Let formula_19 be the graph obtained from the formula_20-grid by
triangulating internal faces such that all internal vertices become of degree 6,
and then one corner of degree two joined by edges with all vertices
of the external face.
A parameterized problem formula_4 is "contraction-bidimensional" if

Examples of contraction-bidimensional problems are dominating set, connected dominating set, max-leaf spanning tree, and edge dominating set.

All algorithmic applications of bidimensionality are based on the following combinatorial property: either the treewidth of a graph is small, or the graph contains a large grid as a minor or contraction. More precisely, 


Halin's grid theorem is an analogous excluded grid theorem for infinite graphs.

Let formula_1 be a minor-bidimensional problem such that for any graph "G" excluding some fixed graph as a minor and of treewidth at most "t", deciding whether formula_34 can be done in time formula_35. Then for every graph "G" excluding some fixed graph as a minor, deciding whether formula_34 can be done in time formula_37. Similarly, for contraction-bidimensional problems, for graph "G" excluding some fixed apex graph as a minor, inclusion formula_34 can be decided in time formula_37.

Thus many bidimensional problems like Vertex Cover, Dominating Set, k-Path, are solvable in time formula_37 on graphs excluding some fixed graph as a minor.

Bidimensionality theory has been used to obtain polynomial-time approximation schemes for many bidimensional problems.
If a minor (contraction) bidimensional problem has several additional properties then the problem poses efficient polynomial-time approximation schemes on (apex) minor-free graphs.

In particular, by making use of bidimensionality, it was shown that feedback vertex set, vertex cover, connected vertex cover, cycle packing, diamond hitting set, maximum induced forest, maximum induced bipartite subgraph and maximum induced planar subgraph admit an EPTAS on H-minor-free graphs. Edge dominating set, dominating set, r-dominating set, connected dominating set, r-scattered set, minimum maximal matching, independent set, maximum full-degree spanning tree, maximum induced at most d-degree subgraph, maximum internal spanning tree, induced matching, triangle packing, partial r-dominating set and partial vertex cover admit an EPTAS on apex-minor-free graphs.

A parameterized problem with a parameter "k" is said to admit a linear vertex kernel if there is a polynomial time reduction, called a kernelization algorithm, that maps the input instance to an equivalent instance with at most "O(k)" vertices.

Every minor-bidimensional problem formula_4 with additional properties, namely, with the separation property and with finite integer index, has a linear vertex kernel on graphs excluding some fixed graph as a minor. Similarly, every contraction-bidimensional problem formula_4 with the separation property and with finite integer index has a linear vertex kernel on graphs excluding some fixed apex graph as a minor.




</doc>
